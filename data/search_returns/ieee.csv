"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Intelligent Hearing System using Assistive Technology for Hearing-Impaired Patients","W. Shehieb; M. O. Nasri; N. Mohammed; O. Debsi; K. Arshad","Department of Electrical Engineering, Ajman University, Ajman, United Arab Emirates; Department of Electrical Engineering, Ajman University, Ajman, United Arab Emirates; Department of Electrical Engineering, Ajman University, Ajman, United Arab Emirates; Department of Electrical Engineering, Ajman University, Ajman, United Arab Emirates; Department of Electrical Engineering, Ajman University, Ajman, United Arab Emirates","2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)","17 Jan 2019","2018","","","725","729","Hearing is an important sense of coexistence, but majority of people taken it for granted unless it is weakened or lost. In this paper, an Assistive Intelligent Hearing Aid System (AIHAS) is proposed that supports hearing impaired patients and allow them to live a normal life. The patients will be required to wear smart glasses equipped with bone conduction technology and wirelessly connected with an application running on patient's smartphone. The AIHAS is designed to: (1) detect multiple ear damages, (2) evaluates the degree of patient's hearing loss, (3) having smart filters with two different modes based on surrounding environment, and (4) assist children with articulation development. The patients will be able to choose between two types of filters i.e. Quiet Room (QR) and Noisy Room (NR) filter depending on the surrounding environment. Additionally, an Auditory Assistive mode is also added to AIHAS to train and assist children with speech disorders caused by hearing impairment at a younger age. The AIHAS also allows interfacing with a smartwatch for easier system access. The system prototype has been developed and tested with multiple patients. The proposed AIHAS is an intelligent, low cost, reliable and a portable solution.","","978-1-5386-7266-2","10.1109/IEMCON.2018.8615021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8615021","Artificial intelligence;Assistive technology;Computer vision;Ear damage detection;Otitis Media;Perforated Eardrum;Hearing impairment;Online Database","Auditory system;Ear;Pediatrics;Databases;Hearing aids;Glass;Noise measurement","","3","","15","IEEE","17 Jan 2019","","","IEEE","IEEE Conferences"
"Artificial Neural Network-Assisted Classification of Hearing Prognosis of Sudden Sensorineural Hearing Loss With Vertigo","S. -C. Lin; M. -Y. Lin; B. -H. Kang; Y. -S. Lin; Y. -H. Liu; C. -Y. Yin; P. -S. Lin; C. -W. Lin","Department of Biomedical Engineering, College of Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Otorhinolaryngology—Head and Neck Surgery, Kaohsiung Veterans General Hospital, Kaohsiung, Taiwan; Department of Otorhinolaryngology—Head and Neck Surgery, Kaohsiung Veterans General Hospital, Kaohsiung, Taiwan; Department of Otorhinolaryngology—Head and Neck Surgery, Kaohsiung Veterans General Hospital, Kaohsiung, Taiwan; Department of Otorhinolaryngology—Head and Neck Surgery, Kaohsiung Veterans General Hospital, Kaohsiung, Taiwan; Department of Otorhinolaryngology—Head and Neck Surgery, Kaohsiung Veterans General Hospital, Kaohsiung, Taiwan; Department of Otorhinolaryngology—Head and Neck Surgery, Kaohsiung Veterans General Hospital, Kaohsiung, Taiwan; Department of Biomedical Engineering, College of Engineering, National Cheng Kung University, Tainan, Taiwan","IEEE Journal of Translational Engineering in Health and Medicine","15 Feb 2023","2023","11","","170","181","This study aimed to determine the impact on hearing prognosis of the coherent frequency with high magnitude-squared wavelet coherence (MSWC) in video head impulse test (vHIT) among patients with sudden sensorineural hearing loss with vertigo (SSNHLV) undergoing high-dose steroid treatment. This study was a retrospective cohort study. SSNHLV patients treated at our referral center from December 2016 to December 2020 were examined. The cohort comprised 64 patients with SSNHLV undergoing high-dose steroid treatment. MSWC was measured by calculating the wavelet coherence analysis (WCA) at various frequencies from a vHIT. The hearing prognosis were analyzed using a multivariable Cox regression model and convolution neural network (CNN) of WCA. There were 64 patients with a male-to-female ratio of 1:1.67. The greater highest coherent frequency of the posterior semicircular canal (SCC) was associated with the complete recovery (CR) of hearing. After adjustment for other factors, the result remained robust (hazard ratio [HR] 2.11, 95% confidence interval [CI] 1.86-2.35). In the feature extraction with Resnet-50 and proceeding SVM in the horizontal image cropping style, the classification accuracy [STD] for (CR vs. partial + no recovery [PR + NR]), (over-sampling of CR vs. PR + NR), (extensive data extraction of CR vs. PR + NR), and (interpolation of time series of CR vs. PR + NR) were 83.6% [7.4], 92.1% [6.8], 88.9% [7.5], and 91.6% [6.4], respectively. The high coherent frequency of the posterior SCC was a significantly independent factor that was associated with good hearing prognosis in the patients who have SSNHLV. WCA may be provided with comprehensive ability in vestibulo-ocular reflex (VOR) evaluation. CNN could be utilized to classify WCA, predict treatment outcomes, and facilitate vHIT interpretation. Feature extraction in CNN with proceeding SVM and horizontal cropping style of wavelet coherence plot performed better accuracy and offered more stable model for hearing outcomes in patients with SSNHLV than pure CNN classification. Clinical and Translational Impact Statement—High coherent frequency in vHIT results in good hearing outcomes in SSNHLV and facilitates AI classification.","2168-2372","","10.1109/JTEHM.2023.3242339","Kaohsiung Veterans General Hospital(grant numbers:VGHKS108-082); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10038487","Artificial intelligence;hearing prognosis;sudden sensorineural hearing loss;video head impulse test;wavelet coherence","Auditory system;Head;Coherence;Wavelet transforms;Time-frequency analysis;Time series analysis;Prognostics and health management","Humans;Male;Female;Retrospective Studies;Vertigo;Hearing;Hearing Loss, Sudden;Hearing Loss, Sensorineural;Prognosis;Steroids","1","","35","CCBY","6 Feb 2023","","","IEEE","IEEE Journals"
"On the Use of Machine Learning for Classifying Auditory Brainstem Responses: A Scoping Review","R. A. Osman; H. A. Osman","Faculty of Health Sciences, School of Rehabilitation Sciences, University of Ottawa, Ottawa, ON, Canada; Faculty of Engineering, School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada","IEEE Access","12 Aug 2021","2021","9","","110592","110600","Recent advances in machine learning have led to a surge of interest in classification of the auditory brainstem response. In this work, we conducted a search in the PubMed, Google Scholar, SpringerLink, ScienceDirect, and Scopus databases, and identified twelve studies that explored the use of machine learning to classify the auditory brainstem response as a complementary and objective method to (a) help clinicians better diagnose hearing impairment by discerning between healthy and pathological auditory brainstem response waveforms, (b) present a neural marker for potential applications in hearing aid tuning, and (c) provide a biometric marker for discriminating between subjects. A comparison between the studies presented in this review is not possible as they used different test subjects, group sizes, and stimuli, and evaluated auditory brainstem response differently. Instead, the result of these studies will be presented and their limitations as well as their potential applications will be discussed. Overall, the findings of these studies suggest that ABR classification using machine learning is a promising tool for assessing patients with hearing loss, optimizing technologies for tuning hearing aids, and discriminating between subjects.","2169-3536","","10.1109/ACCESS.2021.3102096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9504576","Auditory brainstem response;classification;decoding;feature extraction;machine learning","Brainstem;Auditory system;Machine learning;Support vector machines;Hearing aids;Transient analysis;Hidden Markov models","","3","","44","CCBY","3 Aug 2021","","","IEEE","IEEE Journals"
"Light-Emitting Device for Supporting Auditory Awareness of Hearing-Impaired People during Group Conversations","Y. Kaneko; I. Chung; K. Suzuki","Artificial Intelligence Laboratory, University of Tsukuba, Tsukuba, Japan; Artificial Intelligence Laboratory, University of Tsukuba, Tsukuba, Japan; Artificial Intelligence Laboratory, University of Tsukuba, Tsukuba, Japan","2013 IEEE International Conference on Systems, Man, and Cybernetics","27 Jan 2014","2013","","","3567","3572","This study proposes a novel wearable device that augments the auditory awareness of hearing impaired people to help them identify the speaker during group conversations. A number of hearing impaired people are able to understand speech by using auditory-oral methods such as lip-reading, however they always need to watch the speaker closely. The proposed device estimates the direction of the sound source and indicates the estimated direction in real time with light-emitting diodes (LEDs), thus aiding hearing impaired people during group conversations. The device has 4 unidirectional microphones and estimates the sound source direction by comparing the signal amplitudes in each microphone. Hearing assistance devices are required to be small and wearable in order to assist anytime in daily life. The proposed device is small, wearable, and can easily be used in various situations.","1062-922X","978-1-4799-0652-9","10.1109/SMC.2013.608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6722361","Hearing impairments;Assistive devices;Wearable devices;Auditory awareness support","Auditory system;Microphones;Light emitting diodes;Estimation;Speech;Visualization;Performance evaluation","","7","","10","IEEE","27 Jan 2014","","","IEEE","IEEE Conferences"
"Generation Method of Hierarchical Pronunciation Font for Hearing-impaired Children Based on Artificial Intelligence","Z. Xu; W. Zou; D. Lin","JKFZ Cambridge International School, Nanchang, Jiangxi, PRC; School of Information Engineering, Nanchang University, Nanchang, PRC; School of Software, Nanchang University, Nanchang, PRC","2020 IEEE International Conference on Advances in Electrical Engineering and Computer Applications( AEECA)","6 Oct 2020","2020","","","591","595","Setting up personalized hierarchical pronunciation font for hearing-impaired children is one of the most important segments when designing individualized rehabilitation teaching. Creating an individual Chinese pronunciation training word stock is a big challenge in speech rehabilitation training for children with hearing impairment. This research applies the speech recognition technology and Chinese syllable classification theory, and proposes a convenient method of automatic generation of personalized hierarchical word base. The method includes first analyzing the initials and vowel of children's test pronunciation by calling the API of artificial intelligence speech recognition. Subsequently, on the basis of the established multi-dimensional information table of pronunciation classification of Mandarin initials and vowel, the multi-level pronunciation ability of the initials and vowel tested by children is rated. As a result, the datasets of initials and vowel of different levels are obtained. and according to the combination of initials and vowel of different levels, it is convenient to generate a personalized graded Chinese character library. Finaly, The effectiveness of this method is proved by a typical example.","","978-1-7281-6521-9","10.1109/AEECA49918.2020.9213534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9213534","artificial intelligence;hearing-impaired Children;multi-level pronunciation","Training;Tongue;Speech recognition;Periodic structures;Mouth;Auditory system;Shape","","","","11","IEEE","6 Oct 2020","","","IEEE","IEEE Conferences"
"The text analysis software for hearing-impaired persons","V. Andrunyk; T. Shestakevych; D. Koshtura","Information System and Networks Department, Lviv polytechnic National University, Lviv, Ukraine; Information System and Networks Department, Lviv polytechnic National University, Lviv, Ukraine; Information System and Networks Department, Lviv polytechnic National University, Lviv, Ukraine","2021 IEEE 16th International Conference on Computer Sciences and Information Technologies (CSIT)","27 Dec 2021","2021","1","","119","123","Using information technologies for hearing-impaired persons allows society to include citizens that were to some extent isolated because of communication difficulties. Although different applications exist to help such persons, it is always actual to have a nationally-oriented product. Being properly modeled with consideration of needs a person with hearing impairments, such application with allow improving living standards not only for such persons, but also their families and close ones.","2766-3639","978-1-6654-4257-2","10.1109/CSIT52700.2021.9648605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9648605","hearing impairment;special needs;IT-support;text analysis","Text analysis;Social networking (online);Conferences;Auditory system;Software;Internet;Electronic mail","","2","","28","IEEE","27 Dec 2021","","","IEEE","IEEE Conferences"
"Methods for evaluating and improving audiometric examinations - ORL system Fowler","V. Dolinay; L. Pivnickova; V. Vasek","Faculty of Applied Informatics, Tomas Bata University, Zlin, Zlin, Czech Republic; Faculty of Applied Informatics, Tomas Bata University, Zlin, Zlin, Czech Republic; Faculty of Applied Informatics, Tomas Bata University, Zlin, Zlin, Czech Republic","Proceedings of the 13th International Carpathian Control Conference (ICCC)","2 Jul 2012","2012","","","115","118","The purpose of this work is to design the system, which eliminates redundant and repetitive tasks in the certain types of hearing function examinations. Proposed system reduces time needed to examine patient and also provides the full digitization of the results. The obtained database can recently become the base for subsequent processing in artificial intelligence expert systems. Such systems can increase diagnostic potentials. In cooperation with physicians, the above mentioned system began to be developed. Beside the steps automatizing examination there will be also described expert system based on the obtained data. The system is focused on pointing the examination which can reveal otosclerosis diagnosis.","","978-1-4577-1868-7","10.1109/CarpathianCC.2012.6228626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228626","Audiometry;examination;hearing lost;Fowler;ORL","Auditory system;Medical services;Bones;Databases;Medical diagnostic imaging;Computers;Hardware","","","","7","IEEE","2 Jul 2012","","","IEEE","IEEE Conferences"
"Hand Gesture Recognition using AI Algorithm for Hearing Impaired","B. J. Flavia; P. K. Priya; N. Chandravanshi; M. V. Amirtham","Dept. of CSE, SRMIST, Chennai, Tamil Nadu, India; Dept. of CSE, SRMIST, Chennai, Tamil Nadu, India; Dept. of CSE, SRMIST, Chennai, Tamil Nadu, India; Dept. of CSE, SRMIST, Chennai, Tamil Nadu, India","2022 4th International Conference on Inventive Research in Computing Applications (ICIRCA)","29 Dec 2022","2022","","","1069","1072","HRI can be challenging to facilitate communication between humans and robots. This is because robots cannot understand human language without a mediator, and this causes problems for HRI when it comes to hearing impaired patients, and elderly persons. In today's world HGR plays an important role and the HGR system is widely used in many applications throughout the world. HGR can help establish transmission between humans and machines by helping these groups of people. ML is part of AI that focuses on the evolution of a system that rely on data. The main issue of HGR is that the machine doesn't recognize the human language directly and human machine interaction is in need of media for conveying which can be recognized by machine and as well as humans, to help the hearing impaired persons and elderly persons, so HGR as transmitting media is required to give instructions to computer. This project is based on HGR as an input method using two clustering methods, Fuzzy C Means clustering and Decision K Means clustering. The Euclidean distance will be used to compare the different data elements in order to get an output at the end of the project. The output had higher accuracies compared to other algorithms.","","978-1-6654-9707-7","10.1109/ICIRCA54612.2022.9985748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985748","Artificial intelligence;sign detection;feature extraction;classification;Euclidean distance;Fuzzy C;K means algorithm","Heuristic algorithms;Human-robot interaction;Clustering algorithms;Gesture recognition;Auditory system;Media;Assistive technologies","","","","11","IEEE","29 Dec 2022","","","IEEE","IEEE Conferences"
"""Now I Understand"": an application to bridge the gap in communication with hearing impaired people","K. Baz; J. P. Grille; C. Mayr; G. Gretter","Facultad de Ingenieria, Universidad de Montevideo, Montevideo, Uruguay; Facultad de Ingenieria, Universidad de Montevideo, Montevideo, Uruguay; Facultad de Ingenieria, Universidad de Montevideo, Montevideo, Uruguay; Facultad de Ingenieria, Universidad de Montevideo, Montevideo, Uruguay","2023 IEEE Global Humanitarian Technology Conference (GHTC)","20 Dec 2023","2023","","","278","281","""Now I Understand"" is a mobile application that enables bilateral communication between hearing and non-hearing people using artificial intelligence techniques. The application uses deep learning models to recognize sign language and translate it into text in real-time, while also translating text into sign language. The details of the application implementation are discussed, including data acquisition and preprocessing, selection and training of deep learning models, and the user interface. This application has the potential to revolutionize communication and accessibility for people with hearing impairments by providing a reliable and accurate tool for bilateral communication, as well as improving the general public's awareness and understanding of sign language.","2473-5728","979-8-3503-2132-6","10.1109/GHTC56179.2023.10354884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10354884","mobile application;non-hearing;bilateral communication;artificial intelligence;sign language","Deep learning;Training;Text recognition;Gesture recognition;Auditory system;Assistive technologies;User interfaces","","","","8","IEEE","20 Dec 2023","","","IEEE","IEEE Conferences"
"Toward Better Ear Disease Diagnosis: A Multi-Modal Multi-Fusion Model Using Endoscopic Images of the Tympanic Membrane and Pure-Tone Audiometry","T. Kim; S. Kim; J. Kim; Y. Lee; J. Choi","Department of Applied Artificial Intelligence, Major in Bio Artificial Intelligence, Hanyang University, Ansan, Republic of Korea; Department of Otorhinolaryngology-Head and Neck Surgery, Ansan Hospital, Korea University College of Medicine, Ansan, Republic of Korea; Core Research and Development Center, Korea University Ansan Hospital, Ansan, Republic of Korea; Department of Applied Artificial Intelligence, Major in Bio Artificial Intelligence, Hanyang University, Ansan, Republic of Korea; Department of Otorhinolaryngology-Head and Neck Surgery, Ansan Hospital, Korea University College of Medicine, Ansan, Republic of Korea","IEEE Access","26 Oct 2023","2023","11","","116721","116731","Chronic otitis media is characterized by recurrent infections, leading to serious complications, such as meningitis, facial palsy, and skull base osteomyelitis. Therefore, active treatment based on early diagnosis is essential. This study developed a multi-modal multi-fusion (MMMF) model that automatically diagnoses ear diseases by applying endoscopic images of the tympanic membrane (TM) and pure-tone audiometry (PTA) data to a deep learning model. The primary aim of the proposed MMMF model is adding “normal with hearing loss” as a category, and improving the diagnostic accuracy of the conventional four ear diseases: normal, TM perforation, retraction, and cholesteatoma. To this end, the MMMF model was trained on 1,480 endoscopic images of the TM and PTA data to distinguish five ear disease states: normal, TM perforation, retraction, cholesteatoma, and normal (hearing loss). It employs a feature fusion strategy of cross-attention, concatenation, and gated multi-modal units in a multi-modal architecture encompassing a convolutional neural network (CNN) and multi-layer perceptron. We expanded the classification capability to include an additional category, normal (hearing loss), thereby enhancing the diagnostic performance of extant ear disease classification. The MMMF model demonstrated superior performance when implemented with EfficientNet-B7, achieving 92.9% accuracy and 90.9% recall, thereby outpacing the existing feature fusion methods. In addition, five-fold cross-validation experiments were conducted, in which the model consistently demonstrated robust performance when endoscopic images of the TM and PTA data were applied to the deep learning model across all datasets. The proposed MMMF model is the first to include a category of normal ear disease state with hearing loss. The developed model demonstrated superior performance compared to existing CNN models and feature fusion methods. Consequently, this study substantiates the utility of simultaneously applying PTA data and endoscopic images of the TM for the automated diagnosis of ear diseases in clinical settings and validates the usefulness of the multi-fusion method.","2169-3536","","10.1109/ACCESS.2023.3325346","Institute of Information and Communications Technology Planning and Evaluation (IITP); Korean Government for the Ministry of Science and ICT (MSIT), South Korea, through the Artificial Intelligence Convergence Innovation Human Resources Development; Hanyang University (ERICA)(grant numbers:RS-2022-00155885); Artificial Intelligence Convergence Research Center, Hanyang University (ERICA)(grant numbers:2020-0-01343); National Research Foundation of Korea (NRF); Korean Government (MSIT)(grant numbers:NRF-2022R1F1A1074999); Korea University Grant and the Medical Data-Driven Hospital Support Project through the Korea Health Information Service (KHIS); Ministry of Health and Welfare, Republic of Korea; MSIT under the ICT Challenge and Advanced Network (ICAN) of HRD Program Supervised; IITP(grant numbers:IITP-2022-RS-2022-00156439); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286540","Artificial intelligence;biomedical imaging;classification algorithms;computer aided diagnosis;convolutional neural networks;deep learning;electronic medical records","Diseases;Auditory system;Data models;Artificial intelligence;Bones;Media;Biomedical imaging;Classification algorithms;Computer aided diagnosis;Convolutional neural networks;Deep learning;Electronic medical records;Ear","","","","42","CCBY","17 Oct 2023","","","IEEE","IEEE Journals"
"A Novel Method for Audiogram Digitization in Audiological Reports","T. -W. Yang; C. -Y. Yang; Y. -H. Lee; P. -Y. Chen; P. -H. Lin; W. -C. Wu; C. -C. Wu; C. -F. Chou","Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Department of Otolaryngology, National Taiwan University Hospital Hsin-Chu Branch, Hsinchu, Taiwan; Department of Otolaryngology, National Taiwan University Hospital, Taipei, Taiwan; Department of Otolaryngology, National Taiwan University Hospital, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Department of Otolaryngology, National Taiwan University Hospital, Taipei, Taiwan; Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan","IEEE Access","18 Mar 2024","2024","12","","37862","37872","An audiogram records the hearing status, including each hearing threshold at multiple frequencies. While deep learning is gradually maturing in the clinical research approach, audiologists could speed up their diagnostic process with audiogram digitization for handwritten graphs or electronically generated images from instruments. However, given the diversity of audiogram symbols and formats, the existing audiogram digitization model has room for improvement in recognition accuracy. We propose a multi-stage workflow to enhance accuracy by integrating YOLOv5 and the optical character recognition (OCR) model. Our proposed audiogram digitization model could identify all audiogram symbols with an accuracy rate of 98%. We hope that this model could help future research in audiology.","2169-3536","","10.1109/ACCESS.2024.3375362","Ministry of Science and Technology of Taiwan(grant numbers:111-2314-B303-016); National Science and Technology Council(grant numbers:112-2221-E-002-118,112-2314-B-303-001,111-2622-8-002-028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10464288","Audiogram;chart recognition;digitization","Symbols;Auditory system;Ear;Optical character recognition;Medical diagnostic imaging;Hospitals;Data mining;Electronic healthcare","","","","16","CCBYNCND","11 Mar 2024","","","IEEE","IEEE Journals"
"Mining Audiograms to Improve the Interpretability of Automated Audiometry Measurements","F. Charih; M. Bromwich; R. Lefrançois; A. E. Mark; J. R. Green","Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Children’s Hospital of Eastern Ontario Faculty of Medicine, University of Ottawa Clearwater Clinical Limited, Ottawa, ON, Canada; Clearwater Clinical Limited, Ottawa, ON, Canada; Clearwater Clinical Limited., Children’s Hospital of Eastern Ontario Research Institute, Ottawa, ON, Canada; Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada","2018 IEEE International Symposium on Medical Measurements and Applications (MeMeA)","19 Aug 2018","2018","","","1","6","Many people with hearing loss are unaware of it and do not seek benefit from available interventions such as hearing aids. This is in part due to the limited accessibility to qualified hearing healthcare providers in developing and developed countries alike. Automated audiometry, which has gained in popularity amidst the torrent of advances in telemedicine and mobile health, makes it possible to deliver basic hearing tests to remote or otherwise underserved communities at low cost. While this technology makes it possible to perform hearing assessments outside of a sound booth, many individuals administering the test are non-specialists, and thus, have a limited ability to interpret audiometric measurements and to make tailored recommendations. In this paper, we present the first steps towards the development of a flexible, supervised learning approach for the classification of audiograms in terms of their shape, severity and symmetry. More specifically, we outline our approach to building a set of non-redundant, annotation-ready audiograms from a much larger dataset. In addition, we present a Rapid Audiogram Annotation Environment (RAAE) designed specifically for the collection of audiogram annotations from a large community of expert audiologists. Preliminary results indicate that annotations provided through our environment are consistent leading to low intra-coder variability. Data gathered through the RAAE will form the basis of learning algorithms to help non-experts make better decisions from audiometric data.","","978-1-5386-3392-2","10.1109/MeMeA.2018.8438746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8438746","automated audiometry;telemedicine;mobile health;audiogram mining;audiogram annotation","Auditory system;Ear;Shape;Medical services;Pediatrics;Supervised learning;Standards","","4","","26","IEEE","19 Aug 2018","","","IEEE","IEEE Conferences"
"Inferring Hearing Loss from Learned Speech Kernels","B. Banerjee; M. H. Kapourchali; S. Najnin; L. L. Mendel; S. Lee; C. Patro; M. Pousson","University of Memphis School of Law, Memphis, TN, US; Dept. of Electrical & Computer Eng.; Dept. of Electrical & Computer Eng.; School of Communication Sci. & Disorders, University of Memphis, Memphis, TN, USA; School of Communication Sci. & Disorders, University of Memphis, Memphis, TN, USA; School of Communication Sci. & Disorders, University of Memphis, Memphis, TN, USA; School of Communication Sci. & Disorders, University of Memphis, Memphis, TN, USA","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","2 Feb 2017","2016","","","26","31","Does a hearing-impaired individual's speech reflect his hearing loss, and if it does, can the nature of hearing loss be inferred from his speech? To investigate these questions, at least four hours of speech data were recorded from each of 37 adult individuals, both male and female, belonging to four classes: 7 normal, and 30 severely-to-profoundly hearing impaired with high, medium or low speech intelligibility. Acoustic kernels were learned for each individual by capturing the distribution of his speech data points represented as 20 ms duration windows. These kernels were evaluated using a set of neurophysiological metrics, namely, distribution of characteristic frequencies, equal loudness contour, bandwidth and Q10 value of tuning curve. Our experimental results reveal that a hearing-impaired individual's speech does reflect his hearing loss provided his loss of hearing has considerably affected the intelligibility of his speech. For such individuals, the lack of tuning in any frequency range can be inferred from his learned speech kernels.","","978-1-5090-6167-9","10.1109/ICMLA.2016.0014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838118","Acoustic feature learning;clustering;tuning curve;bandwidth;equal loudness contour;audiogram","Speech;Auditory system;Kernel;Bandwidth;Measurement;Acoustics;Tuning","","","","39","IEEE","2 Feb 2017","","","IEEE","IEEE Conferences"
"DEEP-HEAR: A Multimodal Subtitle Positioning System Dedicated to Deaf and Hearing-Impaired People","R. Tapu; B. Mocanu; T. Zaharia","ARTEMIS Department, Institut Mines-Télécom, Télécom SudParis, Évry, France; ARTEMIS Department, Institut Mines-Télécom, Télécom SudParis, Évry, France; ARTEMIS Department, Institut Mines-Télécom, Télécom SudParis, Évry, France","IEEE Access","15 Jul 2019","2019","7","","88150","88162","In this paper, we introduce the DEEP-HEAR framework, a multimodal dynamic subtitle positioning system designed to increase the accessibility of deaf and hearing impaired people (HIP) to multimedia documents. The proposed system exploits both computer vision algorithms and deep convolutional neural networks specifically designed and tuned in order to detect and recognize the identity of the active speaker. The main contributions of the paper concern: a novel method dedicated to recognizing various characters existent in the video stream. A video temporal segmentation algorithm that divides the video sequence into semantic units, based on face tracks and visual consistency. Finally, the core of our approach concerns a novel active speaker recognition method relying on the multimodal information fusion from the text, audio, and video streams. The experimental results carried out on a large scale dataset of more than 30 videos, validate the proposed methodology with average accuracy and recognition rates superior to 90%. Moreover, the method shows robustness to important object/camera motion and face pose variation, yielding gains of more than 8% in precision and recall rates when compared with state-of-the-art techniques. The subjective evaluation of the proposed dynamic subtitle positioning system demonstrates the effectiveness of our approach.","2169-3536","","10.1109/ACCESS.2019.2925806","French Plan d’Investissement d’Avenir (PIA) SUBTIL Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8751956","Active speaker recognition;face recognition;dynamic subtitle positioning;convolutional neural networks;assistive framework for deaf and hearing impaired people","Auditory system;Streaming media;Face;Visualization;Speaker recognition;Face recognition;Cameras","","5","","30","CCBY","1 Jul 2019","","","IEEE","IEEE Journals"
"Automatic Subtitle Synchronization and Positioning System Dedicated to Deaf and Hearing Impaired People","B. Mocanu; R. Tapu","Telecommunication Department, Faculty of ETTI, University Politehnica of Bucharest, Bucharest, Romania; Telecommunication Department, Faculty of ETTI, University Politehnica of Bucharest, Bucharest, Romania","IEEE Access","19 Oct 2021","2021","9","","139544","139555","In this paper, we introduce a subtitle synchronization and positioning system designed to increase the accessibility of deaf and hearing impaired people to multimedia documents. The main contributions of the paper concern: a novel synchronization algorithm able to robustly align, without any human intervention, the closed caption with the audio transcript and a timestamp refinement technique that adjusts the subtitle segments duration with respect to the audiovisual recommendations. Finally, we introduce a novel method that performs a high level understanding of the multimedia content, in order to determine the subtitle optimal positions, within the video frame, such that they do not overlap with other relevant textual information. The experimental evaluation performed on a large dataset of 30 videos taken from the French national television validates the approach with average accuracy scores superior to 90% regardless on the video genre. The subjective evaluation of the proposed subtitle synchronization and positioning system, performed with actual hearing impaired people, demonstrates the effectiveness of our approach.","2169-3536","","10.1109/ACCESS.2021.3119201","Romanian Ministry of Education and Research, CNCS - UEFISCDI(grant numbers:PN-III-P1-1.1-TE-2019-0420,PNCDI III); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565923","Subtitle/closed caption synchronization;audiovisual recommendations;anchor words;tokens;subtitle positioning","Synchronization;Speech recognition;TV;Closed captioning;Hidden Markov models;Auditory system;Training","","","","31","CCBY","8 Oct 2021","","","IEEE","IEEE Journals"
"Sign Language Translation Techniques Using Artificial Intelligence for the Hearing Impaired Community in Sri Lanka: A Review","M. Priyankara; A. Gunasekara; K. Ilmini","Faculty of Computing, General Sir John Kotelawala Defence University, Ratmalana, Sri Lanka; Faculty of Computing, General Sir John Kotelawala Defence University, Ratmalana, Sri Lanka; Faculty of Computing, General Sir John Kotelawala Defence University, Ratmalana, Sri Lanka","2023 7th SLAAI International Conference on Artificial Intelligence (SLAAI-ICAI)","25 Dec 2023","2023","","","1","6","Hearing Impaired individuals routinely encounter limitations in their involvement in social interactions, access to intriguing information, and participation in everyday activities, among various other aspects. However, the hardest part of their interactions with regular people is communication, because sign language is the primary language of those who are hearing impaired. However, the general public is unaware of sign language. Each country has its own sign language. However, there are some striking similarities between them. In Sri Lanka, hearing impaired people use Sri Lankan Sign Language (SLSL) as their communication language. There is several research done on Sign Language recognition and translation. But no fully functioning system is utilized for Sri Lankan Sign Language translation. To find the gap in this area, we conducted a systematic literature review using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method that analyses 12 studies on Sign Language Translation (SLT). As per the literature review, Image Processing (IP) and Convolutional Neural Networks (CNN) are the most used techniques for Sign Language translation. But these methods have limitations: not enough data, differences in how people use sign language, difficulty in translating in real-time, not capturing cultural aspects, needing specific equipment, and understanding the context of conversations. Recognizing and solving these problems is important, especially for languages like SLSL. Future research should focus on getting more data, making translation work for different cultures, and improving real-time translation. This will help hearing impaired people communicate better with others.","","979-8-3503-1926-2","10.1109/SLAAI-ICAI59257.2023.10365012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10365012","Sign Language Translation;Sri Lankan Sign Language;Hidden Markov Model;Image Processing;Machine Learning;Convolutional Neural Network;Support Vector Machine","Surveys;Image segmentation;Systematics;Motion segmentation;Gesture recognition;Auditory system;Assistive technologies","","","","42","IEEE","25 Dec 2023","","","IEEE","IEEE Conferences"
"AudioGene: Computer-based prediction of genetic factors involved in non-syndromic hearing impairment","K. R. Taylor; A. P. DeLuca; C. W. Goodman; B. W. Tompkins; T. E. Scheetz; M. S. Hildebrand; P. L. M. Huygen; R. J. H. Smith; T. A. Braun; T. L. Casavant","Center for Bioinformatics and Computational Biology, University of Iowa, Iowa City, IA, USA; Center for Bioinformatics and Computational Biology, University of Iowa, Iowa City, IA, USA; Center for Bioinformatics and Computational Biology, University of Iowa, Iowa City, IA, USA; Center for Bioinformatics and Computational Biology, University of Iowa, Iowa City, IA, USA; Center for Bioinformatics and Computational Biology, University of Iowa, Iowa City, IA, USA; Department of Otolaryngology, Head and Neck Surgery, University of Iowa, Iowa City, IA, USA; Department of Otorhinolaryngology, University Hospital, Nijmegen, The Netherlands; Department of Otolaryngology, Head and Neck Surgery, University of Iowa, Iowa City, IA, USA; Center for Bioinformatics and Computational Biology, University of Iowa, Iowa City, IA, USA; Center for Bioinformatics and Computational Biology, University of Iowa, Iowa City, IA, USA","2011 9th IEEE/ACS International Conference on Computer Systems and Applications (AICCSA)","12 Jan 2012","2011","","","75","79","AudioGene is a software system developed at the University of Iowa to classify and predict gene mutations that indicate causal or increased risk factors of disease. We focus on a concise example — the most likely genetic causes of a particular form of inherited hearing loss — ADNSHL. Whereas the cost and throughput involved in the collection of genomic data have advanced dramatically during the past decade, gathering and interpreting clinical information regarding disease diagnosis remains slow, costly and error-prone. AudioGene employs machine-learning techniques in an iterative procedure to prioritize probable genetic risk factors of disease, which are then verified with a molecular (wet lab) assay. In our current implementation AudioGene achieves 67% first-choice accuracy (versus 23% using a majority classifier). When the top three choices are considered, accuracy increases to 83%. This has numerous implications for reducing the cost of genetic screening as well as increasing the power of novel gene discovery efforts. While AudioGene is focused on hearing loss, the design and underlying mechanisms are generalizable to many other diseases including heart disease, cancer and mental illness.","2161-5330","978-1-4577-0476-5","10.1109/AICCSA.2011.6126605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126605","","Auditory system;Genetics;Accuracy;Training;Support vector machines;Diseases;Educational institutions","","1","","5","IEEE","12 Jan 2012","","","IEEE","IEEE Conferences"
"Deaf Helper Mobile Application for Interaction of Hearing Disorders Communities","A. Setyawan; G. X. Naphan; K. Dynata; J. E. Friry; H. L. H. S. Warnars","Information Systems Department, School of Information System, Bina Nusantara University, Jakarta, Indonesia; Information Systems Department, School of Information System, Bina Nusantara University, Jakarta, Indonesia; Information Systems Department, School of Information System, Bina Nusantara University, Jakarta, Indonesia; Information Systems Department, School of Information System, Bina Nusantara University, Jakarta, Indonesia; Computer Science Department, BINUS Graduate Program— Computer Science, Bina Nusantara University, Jakarta, Indonesia","2022 Second International Conference on Artificial Intelligence and Smart Energy (ICAIS)","30 Mar 2022","2022","","","958","963","People with hearing loss in this world have not received much serious attention from the authorities. This makes these sufferers confused in choosing learning media to interact with and isolated from their social environment. This application was created to help people with hearing loss to be noticed and understood from the way they communicate using sign language through the mobile application called Assistant for the Deaf, which has many features such as registration, interactive videos, sign language translator, forums, customer service, library, information, history, events, donations, and shops. The application is designed using use case diagrams and class diagrams modeling the database, and the implementation used Android Studio and MySQL database.","","978-1-6654-0052-7","10.1109/ICAIS53314.2022.9742988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9742988","Deaf Helper Mobile Application;Hearing Loss Mobile Application;Sign Language Mobile Application","Databases;Customer services;Auditory system;Gesture recognition;Assistive technologies;Media;Libraries","","1","","31","IEEE","30 Mar 2022","","","IEEE","IEEE Conferences"
"Enhancing Digital Content Accessibility for the Hearing Impaired through AI-Driven Visual Representations","M. Hatami; M. Chegini","Faculty of Electrical, Computer, IT, & Biomedical Engineering, Islamic Azad Universuty of Qazvin, Qazvin, Iran; Faculty of Electrical, Computer, IT, & Biomedical Engineering, Islamic Azad Universuty of Qazvin, Qazvin, Iran","2024 10th International Conference on Artificial Intelligence and Robotics (QICAR)","16 Apr 2024","2024","","","322","328","Ensuring inclusivity for individuals with hearing impairments is paramount in an increasingly digital world. This article explores an innovative solution that leverages artificial intelligence (AI) and machine learning to enhance content comprehension for this demographic. We present a system that utilizes deep learning models and natural language processing algorithms to convert spoken language into visual representations, including emojis and images. Our approach empowers the hearing impaired to access and interpret information more effectively by bridging the gap between audio-based content and visual cues. Extensive experiments and user studies confirm the system's effectiveness, significantly improving content understanding and engagement. This novel technology opens new avenues for independent and comprehensive comprehension of audio-based content by individuals with hearing impairments.","","979-8-3503-4887-3","10.1109/QICAR61538.2024.10496621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496621","Natural Language Processing;Deep Learning;Artificial Intelligence;Image Conversion;Voice-to-Emoji","Deep learning;Visualization;Machine learning algorithms;Auditory system;Natural language processing;Iterative methods;Robots","","","","26","IEEE","16 Apr 2024","","","IEEE","IEEE Conferences"
"Hand Gesture Recognition with Augmented Reality and Leap Motion Controller","J. Huo; K. L. Keung; C. K. M. Lee; H. Y. Ng","Laboratory for Artificial Intelligence in Design, New Territories, Hong Kong, China; Department of Industrial and Systems Engineering, The Hong Kong Polytechnic University, Hong Kong, China; Laboratory for Artificial Intelligence in Design, New Territories, Hong Kong, China; Department of Industrial and Systems Engineering, The Hong Kong Polytechnic University, Hong Kong, China","2021 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)","19 Jan 2022","2021","","","1015","1019","The use of hand gestures is one of the commonly used communication approaches in human daily life, especially for the deaf and dumb. Hand gesture recognition can be adopted in human-computer interaction for converting hand gestures into words or sentences. Unfortunately, the same gesture may have diverse meanings in different countries. With the aim of eliminating the communication barriers between hearing-impaired communities and the general people, an efficient interaction user interface created with the augmented reality technique and leap motion controller for hand gesture recognition and translation is proposed in this paper. Five hand gestures captured by a leap motion controller were used for learning and recognizing through machine learning methodologies, including Support Vector Machine, K-Nearest Neighbor, Convolutional Neural Network, Deep Neural Network and Decision Tree. The experimental results from different classifiers reveal the practicability of employing hand gesture recognition in text translation. The hand gesture recognition system should be capable of reducing the communication gap between hearing disabilities and the public so as to avoid deaf and mute people being isolated from society.","","978-1-6654-3771-4","10.1109/IEEM50564.2021.9672611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672611","Hand gesture recognition;leap motion controller;machine learning","Support vector machines;Machine learning algorithms;Target recognition;Thumb;Gesture recognition;Auditory system;Decision trees","","8","","11","IEEE","19 Jan 2022","","","IEEE","IEEE Conferences"
"An Efficient Approach for Interpretation of Indian Sign Language using Machine Learning","D. S; K. H. K B; A. M; S. M; D. S; K. V","Department of ECE, PSG College of Technology, Coimbatore, India; Department of ECE, PSG College of Technology, Coimbatore, India; Department of ECE, PSG College of Technology, Coimbatore, India; Department of ECE, PSG College of Technology, Coimbatore, India; Department of ECE, PSG College of Technology, Coimbatore, India; Department of ECE, PSG College of Technology, Coimbatore, India","2021 3rd International Conference on Signal Processing and Communication (ICPSC)","15 Jun 2021","2021","","","130","133","Non-verbal communication involves the usage of Sign Language. The sign language is used by people with hearing / speech disabilities to express their thoughts and feelings. But normally, people find it difficult to understand the hand gestures of the specially challenged people as they do not know the meaning of the sign language gestures. Usually, a translator is needed when a speech / hearing impaired person wants to communicate with an ordinary person and vice versa. In order to enable the specially challenged people to effectively communicate with the people around them, a system that translates the Indian Sign Language (ISL) hand gestures of numbers (1-9), English alphabets (A-Z) and a few English words to understandable text and vice versa has been proposed in this paper. This is done using image processing techniques and Machine Learning algorithms. Different neural network classifiers are developed, tested and validated for their performance in gesture recognition and the most efficient classifier is identified.","","978-1-6654-2864-4","10.1109/ICSPC51351.2021.9451692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451692","Indian Sign Language;hand gestures;interpreter;SURF;Convolutional Neural Network;Recurrent Neural Network;K-means clustering;Support Vector Machine","Support vector machines;Machine learning algorithms;Assistive technology;Neural networks;Gesture recognition;Speech recognition;Auditory system","","6","","8","IEEE","15 Jun 2021","","","IEEE","IEEE Conferences"
"End-to-End Chinese Lip-Reading Recognition Based on Multi-modal Fusion","Y. Liu; C. Lin; M. Wang; S. Liang; Z. Chen; L. Chen","School of Applied Science and Civil Engineering, Beijing Institute of Technology, Zhuhai, China; School of Applied Science and Civil Engineering, Beijing Institute of Technology, Zhuhai, China; School of Applied Science and Civil Engineering, Beijing Institute of Technology, Zhuhai, China; School of Applied Science and Civil Engineering, Beijing Institute of Technology, Zhuhai, China; Faculty of Innovation Engineering, Macau University of Science and Technology, China; School of Applied Science and Civil Engineering, Beijing Institute of Technology, Zhuhai, China","2022 4th International Conference on Frontiers Technology of Information and Computer (ICFTIC)","27 Mar 2023","2022","","","794","801","With around 1.5 billion people worldwide suffering from hearing impairment, it is particularly important to communicate between non-disabled people and people with hearing or speech impairment and to build a barrier-free society. Multi-modal learning provides an excellent artificial intelligence channel for this purpose. In this article, we create an End-to-end Chinese Lip-Reading Recognition System based on multi-modal fusion to implement Chinese lip translation in order to facilitate communication between individuals with hearing impairment. Our system adopts the End-to-end Audio-visual feature fusion Lip-reading Recognition Architecture (EALRA), with feature extraction based on a MobileNet0.25 tuned CNN skeleton and the encoder back-end using the Conformer self-attentive convolution encoder for modelling. The largest Chinese Mandarin Lip-Reading (CMLR) was selected as the dataset for the empirical study, and the performance metric for Chinese lip recognition was the character error rate (CER). The results of our experiments show that the CER metric of EALRA in the lip-recognition model is 8.0, which is on average 23.74% lower than the CER metrics of other lip-recognition models, indicating that EALRA performs better in fusing image features and audio features.","","979-8-3503-2195-1","10.1109/ICFTIC57696.2022.10075247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10075247","Audio-visual speech recognition;EALRA;Hearing impairment;Multimodal fusion","Measurement;Lips;Computational modeling;Auditory system;Speech recognition;Learning (artificial intelligence);Feature extraction","","1","","21","IEEE","27 Mar 2023","","","IEEE","IEEE Conferences"
"CLERC: An Intelligent Sign Language Translator for Improved Accessibility using Machine Learning","V. V; S. K. S; D. L; P. D. S. V","Department of Computer Science and Engineering, KPR Institute of Engineering and Technology, Coimbatore, India; Department of Artificial Intelligence and Data Science, KPR Institute of Engineering and Technology, Coimbatore, India; Department of Artificial Intelligence and Data Science, KPR Institute of Engineering and Technology, Coimbatore, India; Department of Artificial Intelligence and Data Science, KPR Institute of Engineering and Technology, Coimbatore, India","2023 International Conference on Communication, Security and Artificial Intelligence (ICCSAI)","16 Feb 2024","2023","","","955","959","Clerc's system is a modern smart language interpreter using advanced methods like artificial intelligence, imagery recognition, and natural languages translation. The Clergs System includes these are just some of tools used. The system is able to do this by turning gestures in sign language into verbal and written outputs smoothly thus helping people with hearing impairment to communicate easily in real time. In this research, an object recognition SSD MobileNets and FDN feature extraction models are used for accurate gesture detection. A machine-learning model can also be used to predict accurate translations. The efficacy of the Clerc system has been tested and validated to the utmost extent in different settings such as public places, schools, hospitals amongst others. This research seeks to explore possibilities of improving the system's precision, efficacy, scalability, and use with regards to the practical application.","","979-8-3503-6996-0","10.1109/ICCSAI59793.2023.10421422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10421422","Computer vision;Clerc system;SSD Mobile Net Object recognition model;FPN-lite feature extractor;scalability","Gesture recognition;Auditory system;Machine learning;Assistive technologies;Feature extraction;Turning;Object recognition","","","","20","IEEE","16 Feb 2024","","","IEEE","IEEE Conferences"
"Deep learning reinvents the hearing aid","D. Wang","Institute for Space Imaging Science, University of Lethbridge, Lethbridge, Alberta, Canada","IEEE Spectrum","28 Feb 2017","2017","54","3","32","37","My mother began to lose her hearing while I was away at college. I would return home to share what I'd learned, and she would lean in to hear. Soon it became difficult for her to hold a conversation if more than one person spoke at a time. Now, even with a hearing aid, she struggles to distinguish the sounds of each voice. When my family visits for dinner, she still pleads with us to speak in turn. My mother's hardship reflects a classic problem for hearing aid manufacturers. The human auditory system can naturally pick out a voice in a crowded room, but creating a hearing aid that mimics that ability has stumped signal processing specialists, artificial intelligence experts, and audiologists for decades. British cognitive scientist Colin Cherry first dubbed this the ""cocktail party problem"" in 1953.","1939-9340","","10.1109/MSPEC.2017.7864754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7864754","","Speech;Noise measurement;Auditory system;Hearing aids;Time-frequency analysis;Training;Feature extraction","","87","1","","IEEE","28 Feb 2017","","","IEEE","IEEE Magazines"
"Action Detection for Sign Language Using Machine Learning","A. S Sushmitha Urs; V. B Raj; P. S; P. Kumar K; M. B R; V. Kumar S","Department of AIML, Jyothy Institute of Technology, Bengaluru, India; Department of AIML, Jyothy Institute of Technology, Bengaluru, India; Department of AIML, Jyothy Institute of Technology, Bengaluru, India; Department of AIML, Jyothy Institute of Technology, Bengaluru, India; Department of AIML, Jyothy Institute of Technology, Bengaluru, India; Department of AIML, Jyothy Institute of Technology, Bengaluru, India","2023 International Conference on Network, Multimedia and Information Technology (NMITCON)","17 Oct 2023","2023","","","1","6","This research intends to build an effective and quick algorithm for identifying the alphabets in American Sign Language (ASL) using natural hand movements, increasing communication accessibility for people with hearing impaired limitations. The system's ultimate goal is to act as a translator between spoken language and sign language, enabling more effective and efficient communication between those with hearing loss and others who don't have any hearing loss. The research uses image processing, machine learning, and CNN-based artificial intelligence to recognize ASL movements and generate outputs that are simple to interpret. The potential impact of this work on communication accessibility for people who have hearing loss is significant.","","979-8-3503-0082-6","10.1109/NMITCON58196.2023.10275950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275950","Convolution neural network (CNN);American Sign Language (ASL);Teachable Machine;TensorFlow;OpenCV’ Matplotlib;Media pipe and CVZone","Neural networks;Gesture recognition;Auditory system;Machine learning;Speech recognition;Assistive technologies;Libraries","","","","12","IEEE","17 Oct 2023","","","IEEE","IEEE Conferences"
"Real-Time Recognition of Indian Sign Language","H. Muthu Mariappan; V. Gomathi","Department of Computer Science and Engineering, National Engineering College, Kovilpatti, Tamil Nadu, India; Department of Computer Science and Engineering, National Engineering College, Kovilpatti, Tamil Nadu, India","2019 International Conference on Computational Intelligence in Data Science (ICCIDS)","11 Oct 2019","2019","","","1","6","The real-time sign language recognition system is developed for recognising the gestures of Indian Sign Language (ISL). Generally, sign languages consist of hand gestures and facial expressions. For recognising the signs, the Regions of Interest (ROI) are identified and tracked using the skin segmentation feature of OpenCV. The training and prediction of hand gestures are performed by applying fuzzy c-means clustering machine learning algorithm. The gesture recognition has many applications such as gesture controlled robots and automated homes, game control, Human-Computer Interaction (HCI) and sign language interpretation. The proposed system is used to recognize the real-time signs. Hence it is very much useful for hearing and speech impaired people to communicate with normal people.","","978-1-5386-9471-8","10.1109/ICCIDS.2019.8862125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8862125","ISL;Sign language recognition;HCI;Fuzzy c-means clustering","Gesture recognition;Assistive technology;Feature extraction;Videos;Real-time systems;Auditory system;Classification algorithms","","49","","11","IEEE","11 Oct 2019","","","IEEE","IEEE Conferences"
"An Artificial Intelligence Hearing Aid Based on Two-level Neural Network","C. Zhao; C. Liu","School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China","2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)","5 Jan 2022","2021","2","","1045","1050","Hearing aids have become an indispensable part of the lives of some hearing-impaired people. Traditional hearing aids will be adjusted according to the personal hearing curve and allowing patients to avoid noise-induced harm. However, there is no sound classification or intelligent noise reduction, which cannot meet the higher demand for hearing aids. This paper designed a hearing aid based on a two-level neural network, and the Urbansound8K data set was used to train the neural network. It can simulate the human auditory attention mechanism and intelligently control the output volume. At the same time, the noise reduction model is used to perform corresponding noise reduction processing on different speech streams. Experimental results show that the hearing aid can differentially amplify various sounds in different scenes. The noise part of the sound heard by the user will be suppressed to a certain extent, which can improve the comfort of long-term wearing.","2770-4254","978-1-6654-2605-3","10.1109/IDAACS53288.2021.9660975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660975","embedded hearing aids;neural networks;speech enhancement;noise cancellation","Conferences;Neural networks;Noise reduction;Data acquisition;Auditory system;Hearing aids;Noise measurement","","1","","22","IEEE","5 Jan 2022","","","IEEE","IEEE Conferences"
"Breaking Barriers: The Evolution of Sign Language Detection with Artificial Intelligence","S. A. Baharudin; M. H. N. Ruslan; A. Lajis; S. B. Munisamy; D. A. Kadir; N. M. Ralim","Universiti Kuala Lumpur, Malaysian Institute of Information Technology, Kuala Lumpur, Malaysia; Universiti Kuala Lumpur, Malaysian Institute of Information Technology, Kuala Lumpur, Malaysia; Universiti Kuala Lumpur, Malaysian Institute of Information Technology, Kuala Lumpur, Malaysia; Universiti Kuala Lumpur, Malaysian Institute of Information Technology, Kuala Lumpur, Malaysia; Universiti Kuala Lumpur, Malaysian Institute of Information Technology, Kuala Lumpur, Malaysia; Universiti Kuala Lumpur, Malaysian Institute of Information Technology, Kuala Lumpur, Malaysia","2024 18th International Conference on Ubiquitous Information Management and Communication (IMCOM)","12 Feb 2024","2024","","","1","4","Since the beginning of time, God has gifted us with the ability to hear and speak. People with hearing loss, on the other hand, use sign language as an alternative way of communication that consists of a collection of human actions that reflect a certain expression. Communication between verbal and nonverbal groups has grown difficult because members of the communities are unable to comprehend the meaning of sign language. This study describes the construction of a real-time sign language detection system utilizing a deep learning approach and combining artificial intelligence technology with object detection. This translator will be able to recognize hand motions and translate them into text output in the alphabet using object identification and comprehensive machine vision. This translator serves as a medium for disabled people to communicate and share their ideas with the rest of the community.","","979-8-3503-3101-1","10.1109/IMCOM60618.2024.10418440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418440","Artificial Intelligences;Image Processing;Machine Vision;ASL","Deep learning;Text recognition;Gesture recognition;Auditory system;Assistive technologies;Real-time systems;Artificial intelligence","","","","20","IEEE","12 Feb 2024","","","IEEE","IEEE Conferences"
"Audio to Indian and American Sign Language Converter using Machine Translation and NLP Technique","A. Dixit; S. Sharma; P. Dhamini Rao; V. Reddy; M. Janaki; R. Thirumalaivasan; M. Monica Subashini","School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, India","2022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT)","18 Oct 2022","2022","","","874","879","People with hearing loss use Sign Language as their mother tongue communication. Unlike acoustic hearing, sign language is a visual language that uses body language and physical communication to communicate effectively thoughts. It usually consists of hand gestures and facial expressions. Generally, communicating with different disabled people seems very difficult. This is because it takes a long time to learn the language, not just the language, people who are not disabled but also people who are. Establishment to communicate in such cases, both parties need to know sign language, or they use a human translator to make communication possible. Information Technologies with their own modern methods such as artificial intelligence, cloud computing has an amazing role to play in improving communication people with speech impairments and ordinary people. Sign language recognition can be done in two ways, glove-based or vision-based recognition. The solution proposed in this paper will produce software that takes over input in the form of speech and indicates appropriate Sign Language. The software is developed in Python platform to convert speech to Indian and American sign languages (ISL and ASL) which provides hearing impairment assistant. This software can be useful in many areas, such as in educational institutes, hospitals, police stations, and for general everyday life conversation.","","978-1-6654-1005-2","10.1109/ICICICT54557.2022.9917614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9917614","NLP;SLR;GUI;style;styling;insert","Visualization;Tongue;Law enforcement;Hospitals;Gesture recognition;Auditory system;Oral communication","","5","","15","IEEE","18 Oct 2022","","","IEEE","IEEE Conferences"
"IndieSign: A Learning Module for Indian Sign Language using Supervised Machine Learning Techniques","A. Kulkarni; A. Raina; R. Ramteke; S. Kamat; J. Deshmukh","Computer Engineering and Information Technology Department, VJTI, Mumbai; Computer Engineering and Information Technology Department, VJTI, Mumbai; Computer Engineering and Information Technology Department, VJTI, Mumbai; Computer Engineering and Information Technology Department, VJTI, Mumbai; Computer Engineering and Information Technology Department, VJTI, Mumbai","2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)","16 Jun 2022","2022","","","346","353","Globalization is moving at a rapid pace with virtually no borders amongst various cultures and societies, the hearing-impaired face a monumental challenge to be included in the multi-faceted society. There has been a recent push towards introducing inclusiveness policies so that the hearing impaired shall have an easier way to be included in the society. However, as hearing-impaired people are stigmatized as being “diseased”, people do not interact with them. This affects their mental well-being and discourages future interactions. With a recent push towards digitization, it is believed that providing an online learning module can promote inclusiveness between the two parties. The issue of not being taught the concept of a word makes the learning incomplete. This is not only restricted to day-to-day concepts but also niche categories like technology, agriculture etc. To bridge this gap, this paper presents a unique model through which any person would be able to learn the words along with its concepts and their signage. As people with no hearing impairment tend to not learn sign language, this paper discusses methods through which learning sign language can be made unexacting.","","978-1-6654-9710-7","10.1109/ICAAIC53929.2022.9793170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793170","Indian Sign Language;Sign Language Recognition Systems;Machine Learning;Support Vector Machine;Natural Language Processing","Support vector machines;Bridges;Face recognition;Globalization;Gesture recognition;Auditory system;Machine learning","","","","15","IEEE","16 Jun 2022","","","IEEE","IEEE Conferences"
"Role of Natural Language Processing in Improving Lives of Disabled","W. Nadeem; V. K. Shukla; P. V K; G. Kaur; A. B. Bhardwaj","Department of Information Technology, Amity University Dubai, United Arab Emirates; Department of Engineering and Architecture, Amity University Dubai, UAE; Department of Computing, University of Stirling, RAK, United Arab Emirates; School of Law, University of Petroleum and Energy Studies, Dehradun, India; Department of Psychology, Amity University, Dubai, Dubai, United Arab Emirates","2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","8 Dec 2022","2022","","","1","6","Natural Language Processing is a branch of AI that aims to assist the machine in deciphering natural language. We can implement this Artificial Intelligence to assist Real Intelligence where people of determination need it. A vast amount of potential can be reached in NLP to help people with impairments. NLP has both a negative and positive role in the lives of the disabled, but correct implementation can advance recent work to improve the future. This paper talks about the role of natural language processing in the lives of people of determination. It suggests a model that could most likely be the tool that hearing, and visually impaired people need, to keep up with abled people. It also mentions the negative side of NLP in the lives of people of determination and social biases. Possible faults can arise during implementation that has been addressed along with other work from authors. A system must understand natural language to proceed and implement it, which is a difficult task due to the many complexities of natural language. However, once implemented, NLP can be used as a tool in many areas of life, from aiding the disabled to allowing diverse people to communicate easily.","","978-1-6654-7433-7","10.1109/ICRITO56286.2022.9964626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964626","Natural Language processing (NLP);Artificial Intelligence (AI);Disabled;ADHD;Audio-Visual","Auditory system;Assistive technologies;Programming;Market research;Natural language processing;Hardware;Reliability","","","","30","IEEE","8 Dec 2022","","","IEEE","IEEE Conferences"
"Communicaton On Mobile Phone For The Deaf Using Image Recognition","B. Soewito; A. K. Satyadhana; Raymond; S. Maria","Computer Science Department, BINUS Graduate Program – Master of Computer Science, Bina Nusantara University, Jakarta, Indonesia; Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia; Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, Indonesia","2020 International Conference on Information Management and Technology (ICIMTech)","2 Oct 2020","2020","","","594","598","Sign language is an absolute thing that is mastered by some people, especially people who have hearing or deaf problems. Problems will occur if they want to communicate via mobile devices. They have to use an application that translate sign language to text and vice versa. The research in this field still open because each language will have different sign language. In the other hand the image that was captured by the mobile devices depend on the environment such as brightness and the background. The sign language captured with static background has high accuracy compared with dynamic background that has colorful background. Most of research do not include the dynamic background. Therefore in this study introduced an application used image recognition that can translate sign language with dynamic background into a text so that it can be used to communicate among people who have hearing problems and also between deaf and normal people. The experimental showed that accuracy of our system is 87%.","","978-1-7281-7071-8","10.1109/ICIMTech50083.2020.9211126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211126","artificial intelligence;machine vision;image recognition;gesture recognition;sign language","Assistive technology;Gesture recognition;Auditory system;Artificial neural networks;Neurons;Computer science;Mobile handsets","","2","","21","IEEE","2 Oct 2020","","","IEEE","IEEE Conferences"
"VoiceMitra - A Speech to Visual Translation Approach for Disabled","S. Pate; K. Vayadande; D. Pawal; K. Paralkar; A. Pathak; S. Patil","Artificial Intelligence and Data Science, Vishwarkarma Institute of Technology, Pune, India; Artificial Intelligence and Data Science, Vishwarkarma Institute of Technology, Pune, India; Artificial Intelligence and Data Science, Vishwarkarma Institute of Technology, Pune, India; Artificial Intelligence and Data Science, Vishwarkarma Institute of Technology, Pune, India; Artificial Intelligence and Data Science, Vishwarkarma Institute of Technology, Pune, India; Artificial Intelligence and Data Science, Vishwarkarma Institute of Technology, Pune, India","2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)","23 Jan 2024","2023","","","1","6","Nowadays, Internet’s expansion has multiplied greatly with a variety of information in all contexts available. Although it has been beneficial for the majority group of people, those with special needs, such as the deaf, have few resources at their disposal. The visual graphics and hand gestures can be used to communicate in such cases. The objective of this research is to develop a system that transforms speech and text input into a sequence of visuals representing sign language. This system basically consists of three modules- 1. Speech to Text, which converts the spoken audio into text format, 2. Language translator, wherein different standard languages can also be given as input to get the output and 3. Sign Language Converter, through which the given audio/text is converted to either hand gestures or related gif. These graphics are understood by them, and so a well understood communication can be established. The language translation modules give an average confidence level of 0.916823557. In addition to these three modules, the feature of saving the history in multiple languages is also added, making this system a user-friendly system useful in future as well.","","979-8-3503-4834-7","10.1109/ICACTA58201.2023.10393103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10393103","audio visual translator;communication medium for deaf;language translation;graphical output","Computers;Visualization;Gesture recognition;Transforms;Auditory system;Assistive technologies;Libraries","","","","15","IEEE","23 Jan 2024","","","IEEE","IEEE Conferences"
"A Deep Learning Based Wearable Healthcare Iot Device for AI-Enabled Hearing Assistance Automation","F. YOUNG; L. ZHANG; R. JIANG; H. LIU; C. WALL","Department of Computer and Information Sciences, Northumbria University, Newcastle, UK; Department of Computer and Information Sciences, Northumbria University, Newcastle, UK; School of Computing and Communications, Faculty of Science and Technology, Lancaster University, Lancaster, UK; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Computer and Information Sciences, Northumbria University, Newcastle, UK","2020 International Conference on Machine Learning and Cybernetics (ICMLC)","5 Jul 2021","2020","","","235","240","With the recent booming of artificial intelligence (AI), particularly deep learning techniques, digital healthcare is one of the prevalent areas that could gain benefits from AI-enabled functionality. This research presents a novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266 platform capable of assisting those who suffer from impairment of hearing or deafness to communicate with others in conversations. In the proposed solution, a server application is created that leverages Google’s online speech recognition service to convert the received conversations into texts, then deployed to a micro-display attached to the glasses to display the conversation contents to deaf people, to enable and assist conversation as normal with the general population. Furthermore, in order to raise alert of traffic or dangerous scenarios, an ‘urban-emergency’ classifier is developed using a deep learning model, Inception-v4, with transfer learning to detect/recognize alerting/alarming sounds, such as a horn sound or a fire alarm, with texts generated to alert the prospective user. The training of Inception-v4 was carried out on a consumer desktop PC and then implemented into the AI-based IoT application. The empirical results indicate that the developed prototype system achieves an accuracy rate of 92% for sound recognition and classification with real-time performance.","2160-1348","978-1-6654-1943-7","10.1109/ICMLC51923.2020.9469537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9469537","Deep Learning;Convolutional Neural Network;Healthcare;Deafness Assistance;Wearables;Internet of Things","Deep learning;Training;Wearable computers;Transfer learning;Speech recognition;Auditory system;Real-time systems","","5","","37","IEEE","5 Jul 2021","","","IEEE","IEEE Conferences"
"Communication systems for people with severe hearing loss","D. T. Pop; C. Dehelean; L. Miclea","Department of Automation Technical, University Cluj-Napoca, Romania; Department of Automation, Technical University Cluj-Napoca, Romania; Department of Automation Technical, University Cluj-Napoca, Romania","2018 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","5 Jul 2018","2018","","","1","5","Automatic speech recognition has long been an object of studies, as it is based on the idea of the dialog between man and computer. The first work on speech recognition was published in 1952, and it described a system created by the Bell Laboratory. Many people with severe hearing loss rely on sign language as the main means of communication, yet, in time, advanced research in the field has lead to the human communication with the computer, albeit with some limitations. Nowadays, communication with the computer may be done by using a speech recognition system and a voice synthesis system.","","978-1-5386-2205-6","10.1109/AQTR.2018.8402706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402706","accessibility;tactile vibrations;hearing loss;software system for people with hearing loss;sound frequencies;eLearning;artificial intelligence (AI);speech understanding;voice system analysis;vibration interfaces;intuitive communication system","Vibrations;Auditory system;Speech recognition;Assistive technology;Gesture recognition;Education;Computational modeling","","","","11","IEEE","5 Jul 2018","","","IEEE","IEEE Conferences"
"A Systematic Review on Systems-Based Sensory Gloves for Sign Language Pattern Recognition: An Update From 2017 to 2022","Z. R. Saeed; Z. B. Zainol; B. B. Zaidan; A. H. Alamoodi","School of Computer Sciences, Universiti Sains Malaysia, George Town, Penang, Malaysia; School of Computer Sciences, Universiti Sains Malaysia, George Town, Penang, Malaysia; Future Technology Research Center, National Yunlin University of Science and Technology, Douliu, Taiwan; Department of Computing, FSKIK, Universiti Pendidikan Sultan Idris, Tanjong Malim, Malaysia","IEEE Access","1 Dec 2022","2022","10","","123358","123377","Sign language is the predominant mode of communication for the Hearing impaired community. For the millions of people who suffer from hearing loss around the world, interaction with people who have the ability to hear and do not suffer from hearing impairment or loss is considered as complicated. In line with this issue, technology is perceived as a crucial factor in being an enabler of providing solutions to enhance the quality of life of the hearing impairment by increasing accessibility. This research aims to review and analyze articles related to sign language recognition based on the sensor- based glove system, in order to identify academic motivations, challenges, and recommendations related to this field. The search for the relevant review materials and articles was performed on four major databases ranging from 2017 to 2022: Science Direct, Web of Science, IEEE Xplore, and Scopus. The articles were chosen based on our inclusion and exclusion criteria. The literature findings of this paper indicate the dataset size to be open issues and challenges for hand gesture recognition. Furthermore, the majority of research on sign language recognition based on data glove was performed on static, single hand, and isolated gestures. Moreover, recognition accuracy typically achieved results higher than 90%. However, most experiments were carried out with a limited number of gestures. Overall, it is hoped that this study will serve as a roadmap for future research and raise awareness among researchers in the field of sign language recognition.","2169-3536","","10.1109/ACCESS.2022.3219430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9938436","Gesture recognition;glove;sign language;sensor;man-machine interface;pattern recognition","Assistive technologies;Gesture recognition;Auditory system;Databases;Systematics;Sign language;Human computer interaction;Pattern recognition;Man-machine systems;Sensors;Deafness","","6","","118","CCBY","4 Nov 2022","","","IEEE","IEEE Journals"
"Designing Soft-Hardware Complex for Gesture Language Recognition using Neural Network Methods","M. D. Artemov; L. I. Voronova; A. G. Vovik","Moscow Technical University of Communications and Informatics, Moscow, Russia; Moscow Technical University of Communications and Informatics, Moscow, Russia; Moscow Technical University of Communications and Informatics, Moscow, Russia","2020 International Conference on Engineering Management of Communication and Technology (EMCTECH)","30 Nov 2020","2020","","","1","6","In recent years, the problem of social adaptation of people with disabilities has been actively solved using developments including artificial intelligence methods. The article describes elaboration of a software and hardware complex (SHWC) for recognizing the sign language of disabled people with hearing and speech impairments. The analysis of analogue products for automatic sign language translation is carried out, technical and design requirements for the soft-hardware complex are formulated, the architecture of the SHWC, the functionality of the server and user application subsystems are described. The design and implementation of a subsystem for the collection and processing of photo and video materials with elements of sign language, including static and dynamic fingerprints, gestures, simple phrases, was carried out. The structure of the file data storage and metadata base has been developed. Scenarios and algorithms for sequencing and transforming video data are described. The sequence of data preprocessing when forming a training set using the augmentation method is described. A model for detecting hands in an image is described.","","978-1-6654-0448-8","10.1109/EMCTECH49634.2020.9261564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9261564","image recognition methods;fingerprints;sign language;soft-hardware complex for automation of sign language translation","Assistive technology;Gesture recognition;Servers;Neural networks;Auditory system;Cameras;Cloud computing","","3","","26","IEEE","30 Nov 2020","","","IEEE","IEEE Conferences"
"A Cloud-Based 3D Digital Twin for Arabic Sign Language Alphabet Using Machine Learning Object Detection Model","M. Abduljabbar; M. Gochoo; M. T. Sultan; G. Batnasan; M. -E. Otgonbold; J. Berengueres; F. Alnajjar; A. A. Rasheed; A. Alshamsi; N. Alsaedi","Department of Computer Science and Software Engineering, United Arab Emirates University, UAE; Department of Computer Science and Software Engineering, United Arab Emirates University, UAE; Department of Computer Science and Software Engineering, United Arab Emirates University, UAE; Department of Computer Science and Software Engineering, United Arab Emirates University, UAE; Department of Computer Science and Software Engineering, United Arab Emirates University, UAE; Department of Computer Science and Software Engineering, United Arab Emirates University, UAE; Department of Computer Science and Software Engineering, United Arab Emirates University, UAE; Department of Computer Science and Software Engineering, United Arab Emirates University, UAE; Department of Computer Science and Software Engineering, United Arab Emirates University, UAE; Department of Computer Science and Software Engineering, United Arab Emirates University, UAE","2023 15th International Conference on Innovations in Information Technology (IIT)","25 Dec 2023","2023","","","208","212","People with hearing loss or hard hearing struggle with daily life activities as sign language is not widely known by the public. There are many attempts to use technology to help assist hearing loss individuals. However, most proposed solutions are standalone applications or require special hardware like a wearable glove. Our goal is to leverage cloud computing and artificial intelligence (AI) to provide a solution that is portable and does not require any special hardware. We created a lightweight 3D model and rendered it on the browser along with another lightweight object detection model for Arabic Sign Language (ArSL) for real-time detection. Our contribution is primarily based on integrating our novel functional lightweight 3D avatar model and a lightweight ArSL alphabet detection model, which is trained on public ArSL21L dataset, that are suitable to be given as a cloud service. Prototypes of the 3D digital twin avatar model and AI model are publicly offered for the research community on GitHub. We will be working on a full-scale real-time cloud-based communication system in ArSL.","2473-2052","979-8-3503-8239-6","10.1109/IIT59782.2023.10366491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10366491","Sign Language;3D Avatar;Digital Twin;Meta AI;Neural Network","Solid modeling;Cloud computing;Three-dimensional displays;Computational modeling;Gesture recognition;Auditory system;Object detection","","","","28","IEEE","25 Dec 2023","","","IEEE","IEEE Conferences"
"Marathi Speech Intelligibility Enhancement Using I-AMS Based Neuro-Fuzzy Classifier Approach for Hearing Aid Users","P. G. Patil; T. H. Jaware; S. P. Patil; R. D. Badgujar; F. Albu; I. Mahariq; B. Al-Sheikh; C. Nayak","R. C. Patel Institute of Technology, Shirpur, Maharashtra, India; R. C. Patel Institute of Technology, Shirpur, Maharashtra, India; R. C. Patel Institute of Technology, Shirpur, Maharashtra, India; R. C. Patel Institute of Technology, Shirpur, Maharashtra, India; Department of Electronics, Valahia University of Targoviste, Targoviste, Romania; College of Engineering and Technology, American University of the Middle East, Egaila, Kuwait; Department of Biomedical Systems and Informatics Engineering, Yarmouk University, Irbid, Jordan; Department of Communication Engineering, School of Electronics Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu, India","IEEE Access","29 Nov 2022","2022","10","","123028","123042","Globally, 1.6 billion individuals suffered from hearing disability in 2019. According to the World Health Organization, by 2050, the number of people with hearing impairments will rise to 2.5 billion. Speech perception in noisy surroundings is a challenge for hearing aid users. This study aimed to design a novel methodology to improve the speech recognition ability of hearing aid users from various backgrounds. To improve speech enhancement, we propose a discrete cosine transform (DCT)-based improved amplitude-magnitude spectrogram (I-AMS) algorithm with a fuzzy classifier. First, the I-AMS approach disintegrates speech signals containing noise into time-frequency units and eliminates the noise present in the signal. Next, the time frequency units (t-f units), modulation frequency (fm), and centre frequency (fc) are extracted from the denoised signal. A neuro-fuzzy classifier was used to classify the background speech environment into three different classes. The proposed I-AMS algorithm was tested, achieved improvements in terms of sensitivity (+1.02%) and accuracy (+11.80%). Speech denoising revealed a 1.27% improvement in speech recognition performance.","2169-3536","","10.1109/ACCESS.2022.3223365","Romanian Ministry of Research, Innovation and Digitization, National Scientific Research Council (CNCS); Executive Unit for Financing Higher Education, Research, Development and Innovation (UEFISCDI), through the National Research-Development and Innovation Plan (PNCDI)(grant numbers:PN-III-P4-PCE-2021-0780); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955536","Improved amplitude magnitude spectrogram;insertion gain;intelligibility;marathi speech;neuro-fuzzy classifier;time-frequency units","Speech recognition;Frequency modulation;Hearing aids;Feature extraction;Auditory system;Speech enhancement;Gain control","","7","","79","CCBY","18 Nov 2022","","","IEEE","IEEE Journals"
"Early Prediction of Neonatal Jaundice using Artificial Intelligence Techniques","Y. Kumar; N. P. Patel; A. Koul; A. Gupta","Department of Computer Science and Engineering, Indus University, Ahmedabad, Gujarat, India; Department of Computer Science and Engineering, Indus University, Ahmedabad, Gujarat, India; Department of Computer Science and Engineering, Punjabi University, Patiala, Punjab, India; Apex Institute of Technology, Chandigarh University, Punjab, India","2022 2nd International Conference on Innovative Practices in Technology and Management (ICIPTM)","18 Apr 2022","2022","2","","222","226","Jaundice in newborns is a prevalent problem all over the world. This syndrome can induce brain damage and kernicterus, which is characterized by repeated and uncontrollable movements, an upward gaze, and hearing loss. As a result, early detection and treatment can prevent long-term harm. As a result, in this study, we have investigated several researchers' strategies for detecting jaundice among newborn babies using various artificial intelligence-based techniques. We have also drawn some findings based on our analysis of the multiple AI techniques. In addition, the report highlighted their accomplishments and the challenges they have faced in this field.","","978-1-6654-6643-1","10.1109/ICIPTM54933.2022.9753884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9753884","Neonatal jaundice;Artificial Intelligence;Machine Learning;Computer vision Deep Learning","Deep learning;Pediatrics;Auditory system;Timing;Artificial intelligence;Biological neural networks","","14","","35","IEEE","18 Apr 2022","","","IEEE","IEEE Conferences"
"Design and Integration of Alert Signal Detector and Separator for Hearing Aid Applications","G. S. Bhat; N. Shankar; I. M. S. Panahi","Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA","IEEE Access","15 Jun 2020","2020","8","","106296","106309","Alert signals like sirens and home alarms are important as they warn people of precarious situations. This work presents the detection and separation of these acoustically important alert signals, not to be attenuated as noise, to assist the hearing impaired listeners. The proposed method is based on convolutional neural network (CNN) and convolutional-recurrent neural network (CRNN). The developed method consists of two blocks, the detector block, and the separator block. The entire setup is integrated with speech enhancement (SE) algorithms, and before the compression stage, used in a hearing aid device (HAD) signal processing pipeline. The detector recognizes the presence of alert signal in various noisy environments. The separator block separates the alert signal from the mixture of noisy signals before passing it through SE to ensure minimal or no attenuation of the alert signal. It is implemented on a smartphone as an application that seamlessly works with HADs in real-time. This smartphone assistive setup allows the hearing aid users to know the presence of the alert sounds even when these are out of sight. The algorithm is computationally efficient with a low processing delay. The key contribution of this paper includes the development and integration of alert signal separator block with SE and the realization of the entire setup on a smartphone in real-time. The proposed method is compared with several state-of-the-art techniques through objective measures in various noisy conditions. The experimental analysis demonstrates the effectiveness and practical usefulness of the developed setup in real-world noisy scenarios.","2169-3536","","10.1109/ACCESS.2020.2999546","National Institute of the Deafness and Other Communication Disorders (NIDCD) of the National Institutes of Health (NIH)(grant numbers:1R01DC015430-04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106356","Alert signals;convolutional-recurrent neural networks (CRNN);detection;separation;speech enhancement (SE);hearing aid (HA);smartphone;real-time","Noise measurement;Detectors;Particle separators;Auditory system;Real-time systems;Speech enhancement;Signal processing algorithms","","4","","53","CCBY","2 Jun 2020","","","IEEE","IEEE Journals"
"Asma’ak: An Emarati Sign Language Translator","M. Ahmed; S. Jasem; K. Saleh; A. Khattak; O. Alfandi","College of Technological Innovation, Zayed University, UAE; College of Technological Innovation, Zayed University, UAE; College of Technological Innovation, Zayed University, UAE; College of Technological Innovation, Zayed University, UAE; College of Technological Innovation, Zayed University, UAE","2023 14th International Conference on Information and Communication Technology Convergence (ICTC)","23 Jan 2024","2023","","","48","52","This research highlights the challenges faced by individuals who are deaf in communicating with those who do not understand sign language. Artificial Intelligence (AI) has emerged as a promising solution to this problem, with deep learning enabling machines to process sequences of data and accurately recognize sign language gestures. The Asma'ak sign language recognition system was developed to detect Emirati Sign Language hand gestures and instantly translate them into text, thus promoting greater inclusivity and engagement within society. The system's reliability and validity are demonstrated through testing on various operating systems, genders, and age groups, achieving a high level of accuracy and precision. Overall, Asma’ak holds significant potential for improving communication and breaking down linguistic barriers for individuals with hearing impairments.","2162-1241","979-8-3503-1327-7","10.1109/ICTC58733.2023.10392859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10392859","AI;Sign Language;Deep Learning;Hand Gestures","Training;Text recognition;Operating systems;Neural networks;Gesture recognition;Auditory system;Assistive technologies","","","","14","IEEE","23 Jan 2024","","","IEEE","IEEE Conferences"
"Artificial Intelligence With Deep Learning Based Automated Ear Infection Detection","I. M. Mehedi; M. S. Hanif; M. Bilal; M. T. Vellingiri; T. Palaniswamy","Department of Electrical and Computer Engineering (ECE), King Abdulaziz University, Jeddah, Saudi Arabia; Department of Electrical and Computer Engineering (ECE), King Abdulaziz University, Jeddah, Saudi Arabia; Department of Electrical and Computer Engineering (ECE), King Abdulaziz University, Jeddah, Saudi Arabia; Department of Electrical and Computer Engineering (ECE), King Abdulaziz University, Jeddah, Saudi Arabia; Department of Electrical and Computer Engineering (ECE), King Abdulaziz University, Jeddah, Saudi Arabia","IEEE Access","5 Apr 2024","2024","12","","48335","48348","Artificial intelligence (AI) related to intelligent control in healthcare denotes using AI techniques to enhance the management and control of healthcare processes and systems. Damage to the inner and middle ear caused by accidents and diseases even causes hearing impairment in the ear that has been harmed or injured. Traditional otoscopy devices were utilized to check the tympanic membrane (TM) to identify OM in medical practice, and a conclusion is drawn depending on the outcomes of the examination. While developing a computer-aided method to support the OM diagnosis, it is possible to focus on methods like feature extraction, image pre-processing, classification, and image segmentation. The existing methodology of detecting the ear infection experiences a reduction of accuracy due to the influence of the noise in the input ear image. This presence of noise affects the feature extraction process, directly influences the accuracy in detection process. To overcome this issue, in this manuscript, a Deep learning (DL) is utilized to find biomedical ear infections by examining images of the eardrum and ear canal. The process includes training a DL method with a large dataset of ear images, where the images were labeled as either not infected or infected. With this motivation, this article emphasizes the design of Bayesian optimization with a deep learning-based automated ear infection detection and classification (BODL-AEIDC) model. The BODL-AEIDC technique exploits the DL model with a metaheuristic optimization algorithm for the ear infection classification process. The BODL-AEIDC technique employs a Wiener filtering (WF) based noise removal process to eliminate the noise data. In addition, the BODL-AEIDC technique exploits W-Net-based segmentation and the EfficientNet model for feature extraction purposes. Moreover, the BODL-AEIDC technique employs a fuzzy Restricted Boltzmann machine (FRBM) model for ear infection detection. Furthermore, the BO algorithm is utilized to adjust the FRBM technique’s hyperparameter values effectively. The BODL-AEIDC technique’s experimental outcomes occur using the medical dataset. The comprehensive comparative study stated the enhanced performance of the BODL-AEIDC approach over other existing methods.","2169-3536","","10.1109/ACCESS.2024.3383835","Institutional Fund Projects(grant numbers:IFPIP: 1739-135-1443); Ministry of Education and King Abdulaziz University, DSR, Jeddah, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10487914","Intelligent control;healthcare sector;deep learning;Bayesian optimization;segmentation;artificial intelligence","Ear;Noise measurement;Image segmentation;Feature extraction;Convolution;Medical diagnostic imaging;Auditory system;Intelligent control;Deep learning;Bayes methods;Artificial intelligence;Infectious diseases","","","","24","CCBYNCND","2 Apr 2024","","","IEEE","IEEE Journals"
"Improving Sign Language Recognition with Machine Learning and Artificial Intelligence","A. Ashrafi; V. S. Mokhnachev; A. E. Harlamenkov","Department of Infocognitive Technologies, Moscow Polytechnic University, Moscow, Russian Federation; Department of Infocognitive Technologies, Moscow Polytechnic University, Moscow, Russian Federation; Department of Infocognitive Technologies, Moscow Polytechnic University, Moscow, Russian Federation","2024 6th International Youth Conference on Radio Electronics, Electrical and Power Engineering (REEPE)","9 Apr 2024","2024","","","1","6","The use of machine learning (ML) and artificial intelligence (AI) has shown great potential in improving sign language recognition for the hearing impaired community. By leveraging large datasets of sign language videos, these technologies can help to develop more accurate and efficient recognition systems that can greatly enhance communication and accessibility. However, there are still significant challenges that need to be addressed, such as the lack of standardized sign language and the need for real-time recognition. Despite these challenges, by conducting more study and development in this area, it will be possible to create a society where everyone has equal access to knowledge and communication, regardless of language or aptitude. This paper reviews the advances in Artificial Intelligence and Machine Learning in sign language recognition, focusing on Russian, and Bengali sign languages, highlighting the potential benefits and challenges of these technologies. Both static and dynamic signs are used to improve the sign language recognition methods. While the result demonstrates approximately 94% accuracy for the static signs with convolutional neural network models, the dynamic sign recognition not only shows lower accuracy but also highlights the significance of using hybrid methods to overcome issues related to frame rate, alignment, and other aspects of video datasets.","2831-7262","979-8-3503-8289-1","10.1109/REEPE60449.2024.10479844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10479844","Sign Language Recognition;machine learning;artificial intelligence;Russian Sign Language;Video Classification","Sign language;Power engineering;Reviews;Machine learning;Auditory system;Assistive technologies;Real-time systems","","","","28","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"Detection of Different Types of Ear Diseases in Infants Using Deep Learning for Early Treatment","S. Divakaran; T. Sudhakar; L. C. V; L. Parthiban; F. P. M S; B. M. Preeta; S. Krishnakumar","Department of Biomedical Engineering, School of Bio and Chemical Engineering, Sathyabama Institute of Science and Technology, Chennai, Tamilnadu; Department of Biomedical Engineering, School of Bio and Chemical Engineering, Sathyabama Institute of Science and Technology, Chennai, Tamilnadu; Department of Biomedical Engineering, School of Bio and Chemical Engineering, Sathyabama Institute of Science and Technology, Chennai, Tamilnadu; Department of Biomedical Engineering, School of Bio and Chemical Engineering, Sathyabama Institute of Science and Technology, Chennai, Tamilnadu; Department of Biomedical Engineering, School of Bio and Chemical Engineering, Sathyabama Institute of Science and Technology, Chennai, Tamilnadu; Department of Biomedical Engineering, School of Bio and Chemical Engineering, Sathyabama Institute of Science and Technology, Chennai, Tamilnadu; Department of Biomedical Engineering, School of Bio and Chemical Engineering, Sathyabama Institute of Science and Technology, Chennai, Tamilnadu","2023 International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)","6 Dec 2023","2023","","","603","608","The loss or inability of auditory sense is the most common sensory organ deficiency. Out of every 1000 new-borns, 5–6 are deaf or hard of hearing. They will not be detected until they are two or three years old, after which permanent trauma would have happened. One of the most prominent reasons for deafness in infants is due to ear diseases. Early detection of ear diseases is the top priority to prevent deafness in infants. This research work has developed and applied modified deep learning architecture such as AlexNet, mini GoogLeNet, and LeNet and compared the training by the three architectures. Of the three, AlexNet architecture detected and identified the presence of different ear ailments such as acute otitis media, glue ear, safe CSOM, otomycosis, other infections and also checked whether the ear is normal, with high accuracy.","","979-8-3503-0085-7","10.1109/ICSSAS57918.2023.10331703","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10331703","Heard of hearing;ear diseases;AlexNet;mini GoogleNet;LeNet","Deep learning;Training;Deafness;Costs;Ear;Computer architecture;Manuals","","","","12","IEEE","6 Dec 2023","","","IEEE","IEEE Conferences"
"Hand Gesture Recognition Using CNN & Publication of World's Largest ASL Database","A. Kannoth; C. Yang; M. A. Guanipa Larice","Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada; Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada; Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada","2021 IEEE Symposium on Computers and Communications (ISCC)","15 Dec 2021","2021","","","1","6","Sign language is used throughout the world by the hearing impaired to communicate. Recent advancements in Computer Vision and Deep Learning has given rise to many machine learning based translators. In this research paper, a solution to recognize the English alphabet presented as static signs in the American Sign Language (ASL) is proposed. The classifications are achieved by a four layer CNN. The model is trained and tested on a dataset created for this paper. This dataset will be published as a contribution to the community and is currently the world's largest ASL database consisting of 624,000 images. Split into two sections, the database contains images in both the IR and RGB spectrum. Classifications on both sets of data achieve state-of-the-art results when compared to similar research. An accuracy of 99.89% and 99.91 % are achieved when classifying the IR and RGB datasets respectively.","2642-7389","978-1-6654-2744-9","10.1109/ISCC53001.2021.9631255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9631255","American Sign Language;Convolutional Neural Networks;Deep Learning;Artificial Intelligence","Training;Deep learning;Computer vision;Databases;Computational modeling;Gesture recognition;Auditory system","","2","","20","IEEE","15 Dec 2021","","","IEEE","IEEE Conferences"
"Real Time Sign Language Recognition Framework For Two Way Communication","Y. Tewari; P. Soni; S. Singh; M. S. Turlapati; A. Bhuva","Dept. of Computer Engineering, NMIMS, MPSTME, Mumbai, India; Dept. of Computer Engineering, NMIMS, MPSTME, Mumbai, India; Dept. of Computer Engineering, NMIMS, MPSTME, Mumbai, India; Dept. of Computer Engineering, NMIMS, MPSTME, Mumbai, India; Dept. of Computer Engineering, NMIMS, MPSTME, Mumbai, India","2021 International Conference on Communication information and Computing Technology (ICCICT)","12 Aug 2021","2021","","","1","6","The need to express oneself clearly and freely is a fundamental desire and must be available to all regardless of any handicap they face. Often, people that endure hearing and speech impairments use sign language as their only way of communication. This leaves a major communication barrier between people who speak sign language and those that communicate otherwise. Not a lot of people who communicate via vocal languages are inclined to learn sign language. The contribution of this paper is to develop a model which converts signs from the American Sign Language (ASL) into English characters using a CNN which segments the signs to their English language alphabets. The model was able to detect 26 alphabets and 3 additional characters with upto 99.78% accuracy. This paper also proposes inclusion of a speech to text module in order to facilitate seamless two way communication.","","978-1-6654-0430-3","10.1109/ICCICT50803.2021.9510094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9510094","Artificial Intelligence;Deep Learning;Neural Networks;Image Processing;Sign Language;Gesture Recognition","Computers;Assistive technology;Computational modeling;Face recognition;Gesture recognition;Auditory system;Real-time systems","","4","","25","IEEE","12 Aug 2021","","","IEEE","IEEE Conferences"
"Audio to Sign Language Translation using NLP","K. R. Prabha; B. Nataraj; B. Thiruthanikachalam; S. Surya NarayananS; M. Vishnu Prasath","Sri Ramakrishna Engineering College, Tamilnadu, India; Sri Ramakrishna Engineering College, Tamilnadu, India; Sri Ramakrishna Engineering College, Tamilnadu, India; Sri Ramakrishna Engineering College, Tamilnadu, India; Sri Ramakrishna Engineering College, Tamilnadu, India","2023 Third International Conference on Smart Technologies, Communication and Robotics (STCR)","22 Jan 2024","2023","1","","1","4","Communication is an essential aspect of human life, but deaf people often face challenges in communicating with hearing people due to the language barrier. According to the World Health Organization, 466 million individuals, or 5% of the global population, have severe auditory loss. In India, there are roughly 18 million people who are deaf. Sign language is the primary way of communicating information for the deaf, but it requires significant time and effort to learn. To bridge this gap, we propose an Audio to Sign Language Translation solution that converts live voice or audio recordings through text forms and shows pertinent Animated GIFs or Sign Language images. The application uses frontend Easy Gui, PyAudio for microphone input, Google Speech API, and Sphinx for speech recognition, NLP for text preprocessing, and a dictionary-based machine translation approach for sign language generation. The algorithm follows a logical sequence of steps and provides an accurate and reliable solution to facilitate communication between deaf and hearing people. The purpose of this application is to give deaf individuals access to information and services in Indian sign language and to create a scalable project that can record the whole ISL vocabulary using both manual and non-manual signs. Overall, by enabling deaf people to communicate, the Audio to Sign Language Translation has the ability to significantly improve their quality of life to communicate easily and effectively with others.","","979-8-3503-7086-7","10.1109/STCR59085.2023.10397050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10397050","Sign Language;NLP;Audio;Communication","Bridges;Vocabulary;Sociology;Gesture recognition;Auditory system;Speech recognition;Assistive technologies","","","","5","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"Characterization of Synthetic Health Data Using Rule-Based Artificial Intelligence Models","M. Lenatti; A. Paglialonga; V. Orani; M. Ferretti; M. Mongelli","CNR-IEIIT, Torino, Italy; CNR-IEIIT, Torino, Italy; CNR-IEIIT, Torino, Italy; CNR-IEIIT, Torino, Italy; CNR-IEIIT, Torino, Italy","IEEE Journal of Biomedical and Health Informatics","7 Aug 2023","2023","27","8","3760","3769","The aim of this study is to apply and characterize eXplainable AI (XAI) to assess the quality of synthetic health data generated using a data augmentation algorithm. In this exploratory study, several synthetic datasets are generated using various configurations of a conditional Generative Adversarial Network (GAN) from a set of 156 observations related to adult hearing screening. A rule-based native XAI algorithm, the Logic Learning Machine, is used in combination with conventional utility metrics. The classification performance in different conditions is assessed: models trained and tested on synthetic data, models trained on synthetic data and tested on real data, and models trained on real data and tested on synthetic data. The rules extracted from real and synthetic data are then compared using a rule similarity metric. The results indicate that XAI may be used to assess the quality of synthetic data by (i) the analysis of classification performance and (ii) the analysis of the rules extracted on real and synthetic data (number, covering, structure, cut-off values, and similarity). These results suggest that XAI can be used in an original way to assess synthetic health data and extract knowledge about the mechanisms underlying the generated data.","2168-2208","","10.1109/JBHI.2023.3236722","Capita Foundation; Project WHISPER, Widespread Hearing Impairment Screening and PrEvention of Risk; 2020 Auditory Research; WHISPER dataset; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016704","Data augmentation;eXplainable AI (XAI);hearing screening;rule similarity;Generative Adversarial Networks (GAN)","Synthetic data;Measurement;Auditory system;Generative adversarial networks;Data models;Biomedical measurement;Data mining","Humans;Adult;Artificial Intelligence;Algorithms;Benchmarking;Knowledge","3","","44","CCBY","13 Jan 2023","","","IEEE","IEEE Journals"
"An unsupervised noise classification smartphone app for hearing improvement devices","N. Alamdari; F. Saki; A. Sehgal; N. Kehtarnavaz","University of Texas at Dallas, Richardson, TX, USA; University of Texas at Dallas, Richardson, TX, USA; University of Texas at Dallas, Richardson, TX, USA; University of Texas at Dallas, Richardson, TX, USA","2017 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)","15 Jan 2018","2017","","","1","5","This paper presents an app for running a previously developed unsupervised noise classifier in realtime on smartphone/tablet platforms. The steps taken to enable the development of this app are discussed. The app is utilized to carry out field testing of the unsupervised classification of actual encountered noise environments without any prior training and without specifying the number of noise classes or clusters. Two objective measures of cluster purity and normalized mutual information are considered to examine the performance of the app in the field with the user acting as the identifier of the ground truth classes. The results obtained indicate the effectiveness of this real-time smartphone app for carrying out the environmental noise classification in an unsupervised manner.","2473-716X","978-1-5386-4873-5","10.1109/SPMB.2017.8257031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8257031","","Classification algorithms;Feature extraction;Clustering algorithms;Smart phones;Auditory system;Training;Mutual information","","3","","13","IEEE","15 Jan 2018","","","IEEE","IEEE Conferences"
"Audiogram Digitization Tool for Audiological Reports","F. Charih; J. R. Green","Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada","IEEE Access","25 Oct 2022","2022","10","","110761","110769","Multiple private and public insurers compensate workers whose hearing loss can be directly attributed to excessive exposure to noise in the workplace. The claim assessment process is typically lengthy and requires significant effort from human adjudicators who must interpret hand-recorded audiograms, often sent via fax or equivalent. In this work, we present a solution developed in partnership with the Workplace Safety Insurance Board of Ontario to streamline the adjudication process. We present a flexible and open-source audiogram digitization algorithm capable of automatically extracting the hearing thresholds from a scanned or faxed audiology report as a proof-of-concept. The algorithm extracts most thresholds within 5 dB accuracy, allowing to substantially lessen the time required to convert an audiogram into digital format in a semi-supervised fashion, and is a first step towards the automation of the adjudication process. The source code for the digitization algorithm and a desktop-based implementation of our NIHL annotation portal is publicly available on GitHub https://github.com/GreenCUBIC/AudiogramDigitization.","2169-3536","","10.1109/ACCESS.2022.3215972","Workplace Safety Insurance Board of Ontario; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925220","Machine vision;pattern recognition;deep learning;audiology","Auditory system;Symbols;Employment;Insurance;Ears;Transforms;Transfer learning;Machine vision;Pattern recognition;Deep learning","","1","","17","CCBY","19 Oct 2022","","","IEEE","IEEE Journals"
"Enabling Two-Way Communication of Deaf Using Saudi Sign Language","M. Faisal; M. Alsulaiman; M. Mekhtiche; B. M. Abdelkader; M. Algabri; T. B. S. Alrayes; G. Muhammad; H. Mathkour; W. Abdul; Y. Alohali; M. Al-Hammadi; H. Altaheri; T. Alfakih","Department of Computer Science and Engineering, Faculty of Engineering, Kuwait College of Science and Technology (KCST), Kuwait City, Kuwait; Computer Engineering Department, College of Computer and Information Science, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, College of Computer and Information Science, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, College of Computer and Information Science, King Saud University, Riyadh, Saudi Arabia; Computer Science and Information Systems Department, College of Applied Sciences, AlMaarefa University, Riyadh, Saudi Arabia; Department of Special Education, College of Education, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, College of Computer and Information Science, King Saud University, Riyadh, Saudi Arabia; Computer Science Department, College of Computer and Information Science, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, College of Computer and Information Science, King Saud University, Riyadh, Saudi Arabia; Computer Science Department, College of Computer and Information Science, King Saud University, Riyadh, Saudi Arabia; Department of Civil and Environmental Engineering, Faculty of Engineering, Norwegian University of Science and Technology, Høgskoleringen, Trondheim, Norway; Computer Engineering Department, College of Computer and Information Science, King Saud University, Riyadh, Saudi Arabia; Computer Science Department, College of Computer and Information Science, King Saud University, Riyadh, Saudi Arabia","IEEE Access","7 Dec 2023","2023","11","","135423","135434","Disabled people are facing many difficulties communicating with others and involving in society. Modern societies have dedicated significant efforts to promote the integration of disabled individuals into their societies and services. Currently, smart healthcare systems are used to facilitate disabled people. The objective of this paper is to enable two-way communication of deaf individuals with the rest of society, thus enabling their migration from marginal elements of society to mainstream contributing elements. In the proposed system, we developed three modules; the sign recognition module (SRM) that recognizes the signs of a deaf individual, the speech recognition and synthesis module (SRSM) that processes the speech of a non-deaf individual and converts it to text, and an Avatar module (AM) to generate and perform the corresponding sign of the non-deaf speech, which were integrated into the sign translation companion system called Saudi deaf companion system (SDCS) to facilitate the communication from the deaf to the hearing and vice versa. This paper also contributes to the literature by utilizing our self-developed database, the largest Saudi Sign Language (SSL) database—the King Saud University Saudi-SSL (KSU-SSL). The proposed SDCS system performs 293 Saudi signs that are recommended by the Saudi Association for Hearing Impairment (SAHI) from 10 domains (healthcare, common, alphabets, verbs, pronouns and adverbs, numbers, days, kings, family, and regions).","2169-3536","","10.1109/ACCESS.2023.3337514","Targeted Research Grant Program, National Transformation Program in King Abdulaziz City for Science and Technology, Saudi Arabia(grant numbers:5-18-03-001-0003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330906","Avatar;Saudi Sig language;speech recognition system;sign language recognition","Assistive technologies;Gesture recognition;Speech recognition;Avatars;Auditory system;Medical services;STEM","","","","61","CCBYNCND","28 Nov 2023","","","IEEE","IEEE Journals"
"eNext: An IoT and AI Driven Solution to the Plugged-Ear Pandemic","M. N. Anjum","Ted Rogers School of Information Technology Management, Toronto Metropolitan University, Toronto, Canada","IEEE Internet of Things Journal","22 Jun 2023","2023","10","13","11940","11941","According to World Health Organization (WHO), more than a billion people are at risk due to the unsafe use of earphones. This article proposes the concept of a next-generation earphone called “eNext.” Unlike existing traditional earphones eNext comprises artificial intelligence (AI) and IoT-driven technologies in order to overcome the hearing health hazards, ensure pedestrian safety, and provide health monitoring facilities. Additionally, this article formulates an IoT-based technical layout of the proposed system model and provides possible AI-driven solutions to implement the proposed services.","2327-4662","","10.1109/JIOT.2023.3244800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10044194","Artificial intelligence (AI);body sensor network;earphone;earplug;eHealth;eNext;headphone;IoT;smart device;wearable","Headphones;Electronic healthcare;Temperature measurement;Artificial intelligence;Auditory system;Transducers;Internet of Things","","","","13","IEEE","14 Feb 2023","","","IEEE","IEEE Journals"
"Predicting and Explaining Hearing Aid Usage Using Encoder-Decoder with Attention Mechanism and SHAP","Q. Su; E. Iliadou","Department of Computer Science City, University of London, London, UK; Department of Otorhinolaryngology Head and Neck Surgery, National and Kapodistrian University of Athens Medical School, Athens, Greece","2022 16th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)","10 Apr 2023","2022","","","308","315","Understanding the factors that contribute to optimal hearing aid fitting and hearing aid user experiences is crucial in order to increase the satisfaction and quality of life of hearing loss patients, as well as reduce societal and financial burdens. This work proposes a novel framework that uses Encoder-decoder with attention mechanism (attn-ED) for predicting future hearing aid usage and SHAP to explain the factors contributing to this prediction. It has been demonstrated in experiments that attn-ED performs well at predicting future hearing aid usage, and that SHAP can be utilized to calculate the contribution of different factors affecting hearing aid usage. This framework aims to establish confidence that AI models can be utilized in the medical domain with the use of XAI methods. Moreover, the proposed framework can also assist clinicians in determining the nature of interventions.","","978-1-6654-6495-6","10.1109/SITIS57111.2022.00053","European Commission; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10090071","XAI;Hearing Loss;Encoder-Decoder;Attention Mechanism;Hearing Aid Usage","Employee welfare;Adaptation models;Fitting;Auditory system;Predictive models;Hearing aids;Stakeholders","","","","38","IEEE","10 Apr 2023","","","IEEE","IEEE Conferences"
"Sign Language Finger Alphabet Recognition from Gabor-PCA Representation of Hand Gestures","M. A. Amin; H. Yan","Department of Electrical Engineering, City University of Hong Kong, Hong Kong, China; Department of Electrical Engineering, City University of Hong Kong, Hong Kong, China","2007 International Conference on Machine Learning and Cybernetics","29 Oct 2007","2007","4","","2218","2223","During recent years a large number of computer aided applications have been developed to help the disabled people. This has improved the communication between the able and the hearing impaired community. An intelligent signed alphabet recognizer can work as an aiding agent to translate the signs to words (and also sentences) and vice versa. To achieve this goal few steps to be followed, among which the first complicated task is to recognize the sign-language alphabets from hand gesture images. In this paper, we propose a system that is able to recognize American Sign Language (ASL) alphabets from hand gesture with average 93.23% accuracy. The classification is performed with fuzzy-c-mean clustering on a lower dimensional data which is acquired from the Principle Component Analysis (PCA) of Gabor representation of hand gesture images. Out of the top 20 Principle Components (PCs) the best combination of PCs is determined by finding the best fuzzy cluster for the corresponding PCs of the training data. The best result is obtained from the combination of the fourth to seventh principle components.","2160-1348","978-1-4244-0972-3","10.1109/ICMLC.2007.4370514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4370514","Sign language;Finger alphabet recognition;Gabor wavelet;PCA;Clustering algorithm","Handicapped aids;Fingers;Personal communication networks;Computer applications;Application software;Auditory system;Intelligent agent;Image recognition;Image analysis;Performance analysis","","33","","21","IEEE","29 Oct 2007","","","IEEE","IEEE Conferences"
"Determining User Requirements for an Audiology Information System","J. A. Gahan; B. Kane","Health Service Executive, Naas, Leinster, IE; Information Systems, Karlstad University, Karlstad, Sweden","2018 IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS)","23 Jul 2018","2018","","","268","273","This paper examines user requirements for a new audiology information system for use in audiology departments by audiology staff. There is a lack of literature on requirements for an audiology information system. This ethnographic study describes the current audiology service and the equipment used in the diagnosis and treatment of hearing loss. Eliciting user requirements from all the users of a clinical information system provides the best possible outcomes for all user groups: clinical, administration and hearing aid technicians. The study includes a review of audiology internal policies and a survey of all potential end users of a new audiology information system in the Health Service Executive (HSE) community service. The findings identify the six most important user requirements in a new audiology information system. These priority requirements include the storage of all Noah files, multiple patient search options, have all test results stored in the patient's electronic record, be capable of generating IT-based reports and store the audiogram in the patient's electronic record (EHR). Other findings include the desire for a paperless system and for the integration of all audiology equipment and existing IT databases, with up-to-date? information system involving automated processes.","2372-9198","978-1-5386-6060-7","10.1109/CBMS.2018.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417249","audiology;information system;user requirements","Databases;Information systems;Hearing aids;Artificial intelligence;Auditory system;Ear","","","","12","IEEE","23 Jul 2018","","","IEEE","IEEE Conferences"
"Moroccan sign language recognition based on machine learning","S. M. Abdelouahed; C. Meryem; Y. Ali; A. Abdellah","Computer science department, University Sidi Mohamed Ben Abdallah; Computer science department, University Sidi Mohamed Ben Abdallah; Computer science department, University Sidi Mohamed Ben Abdallah; Physics department, University Sidi Mohamed Ben abdellah","2022 International Conference on Intelligent Systems and Computer Vision (ISCV)","29 Jun 2022","2022","","","1","5","More than 5% of the world's population (466 million people) suffer from a disabling hearing loss: 4 million are children. People with hearing loss usually communicate through spoken language and can benefit from assistive devices such as cochlear implants. However, deaf people have profound hearing loss and use sign language to communicate with others, which involves little or no hearing. To facilitate communication between deaf people and normal people who do not know sign language, we have proposed in this paper a system that allows textual transcription of sign language. The developed system will be able, in a first step, to recognize the sign language alphabet using machine learning and image processing. Simulation results have shown the efficiency of the developed model.","2768-0754","978-1-6654-9558-5","10.1109/ISCV54655.2022.9806116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9806116","Sign language;Machine learning;Deep learning;Image processing;Features extraction;image classification","Shape;Simulation;Sociology;Gesture recognition;Auditory system;Machine learning;Assistive technologies","","1","","21","IEEE","29 Jun 2022","","","IEEE","IEEE Conferences"
"Explainable Machine Learning Prediction for the Academic Performance of Deaf Scholars","N. R. Raji; R. M. S. Kumar; C. L. Biji","Department of Computer Applications, Noorul Islam Centre for Higher Education, Nagercoil, India; Department of Information Technology, Noorul Islam Centre for Higher Education, Nagercoil, India; Department of Analytics, School of Computer Science and Technology, VIT University, Tamil Nadu, Vellore, India","IEEE Access","16 Feb 2024","2024","12","","23595","23612","Deaf and Hard of Hearing (DHH) students encounter obstacles in higher education due to language and communication challenges. Although research aims to improve their academic performance, the potential of Machine Learning (ML) remains underutilized in DHH education. The opacity of ML models further complicates their adoption. This study aims to fill this gap by developing a novel ML-based system with eXplainable AI (XAI), specifically utilizing Local Interpretable Model-Agnostic Explainer (LIME) and Shapley Additive Explainer (SHAP). The objective is twofold: predicting at-risk DHH students and explaining risk factors. Merging ML and XAI, this approach could positively impact DHH students’ educational outcomes. A dataset of 454 records detailing DHH students is collected. To address dataset limitations, synthetic data and SMOTE are used. Students are categorized into three performance levels. The data is modeled with different ML models, transfer models, ensemble models, and combination models. Among the models, the stacked model with XGBoost, ExtraTrees, and Random Forest exhibited better performance with an accuracy of 92.99%. Results highlight the model’s significance, providing insights through XAI into crucial factors affecting academic performance, including communication mode, early intervention, schooling type, and family deafness history. LIME and SHAP values were found to be effective in deriving insights into DHH student performance prediction framework. Communication mode, notably, strongly influences at-risk students. The major contribution of this study is the development of a novel ML-based system and the XAI interpretations whose value lies in its social relevance, guiding stakeholders to enhance DHH scholars’ academic achievements.","2169-3536","","10.1109/ACCESS.2024.3363634","Centre of Excellence for Disability Studies (CEDS), Kerala, India(grant numbers:R21F3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10423667","Artificial intelligence;deaf education;explainability;machine learning","Education;Predictive models;Machine learning;Auditory system;Explainable AI;Stakeholders;Gesture recognition;Artificial intelligence;Deafness","","","","38","CCBYNCND","6 Feb 2024","","","IEEE","IEEE Journals"
"Comparação de Sistemas de Reconhecimento de Voz para Inclusão de Surdos","A. B. Lugli; B. C. Candia; C. A. Ynoguti; J. A. Nascimento Leite; P. B. de Paiva; Y. M. Chiaradia Masselli","Engenharia de Controle e Automação Inatel, Santa Rita do Sapucai, Mg, Brasil; Engenharia de Controle e Automação Inatel, Santa Rita do Sapucai, Mg, Brasil; Engenharia de Controle e Automação Inatel, Santa Rita do Sapucai, Mg, Brasil; Engenharia de Controle e Automação Inatel, Santa Rita do Sapucai, Mg, Brasil; Engenharia de Controle e Automação Inatel, Santa Rita do Sapucai, Mg, Brasil; Engenharia de Controle e Automação Inatel, Santa Rita do Sapucai, Mg, Brasil","2023 15th IEEE International Conference on Industry Applications (INDUSCON)","2 Jan 2024","2023","","","583","584","For decades, the inclusion of deaf and hard of hearing people in education has been a recurring problem with no effective solution. Based on the growth in the field of artificial intelligence, this article presents a study comparing two voice recognition systems for the development of a class subtitling application for accessibility for deaf students. The tests performed were based on the minimum parameters for the best use of these subtitles in real time.","2572-1445","979-8-3503-1418-2","10.1109/INDUSCON58041.2023.10375068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10375068","API;Deaf;Speech Recognition","Industry applications;Education;Speech recognition;Auditory system;Real-time systems;Artificial intelligence","","","","0","IEEE","2 Jan 2024","","","IEEE","IEEE Conferences"
"A Real-Time Convolutional Neural Network Based Speech Enhancement for Hearing Impaired Listeners Using Smartphone","G. S. Bhat; N. Shankar; C. K. A. Reddy; I. M. S. Panahi","Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, USA; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, USA; IC3-AI, Microsoft Corporation, Redmond, WA, USA; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, USA","IEEE Access","24 Jun 2019","2019","7","","78421","78433","This paper presents a Speech Enhancement (SE) technique based on multi-objective learning convolutional neural network to improve the overall quality of speech perceived by Hearing Aid (HA) users. The proposed method is implemented on a smartphone as an application that performs real-time SE. This arrangement works as an assistive tool to HA. A multi-objective learning architecture including primary and secondary features uses a mapping-based convolutional neural network (CNN) model to remove noise from a noisy speech spectrum. The algorithm is computationally fast and has a low processing delay which enables it to operate seamlessly on a smartphone. The steps and the detailed analysis of real-time implementation are discussed. The proposed method is compared with existing conventional and neural network-based SE techniques through speech quality and intelligibility metrics in various noisy speech conditions. The key contribution of this paper includes the realization of CNN SE model on a smartphone processor that works seamlessly with HA. The experimental results demonstrate significant improvements over the state-of-the-art techniques and reflect the usability of the developed SE application in noisy environments.","2169-3536","","10.1109/ACCESS.2019.2922370","National Institute on Deafness and Other Communication Disorders(grant numbers:1R01DC015430-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735823","Convolutional neural network (CNN);speech enhancement (SE);hearing aid (HA);smartphone;real-time implementation;log power spectra (LPS)","Noise measurement;Real-time systems;Speech enhancement;Auditory system;Computer architecture;Convolutional neural networks","","38","","47","OAPA","12 Jun 2019","","","IEEE","IEEE Journals"
"An Automatic Method to Develop Music With Music Segment and Long Short Term Memory for Tinnitus Music Therapy","J. Chen; F. Pan; P. Zhong; T. He; L. Qi; J. Lu; P. He; Y. Zheng","College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; Otorhinolaryngology-Head and Neck Surgery Department, Hearing Center, West China School of Medicine/West China Hospital, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; Otorhinolaryngology-Head and Neck Surgery Department, Hearing Center, West China School of Medicine/West China Hospital, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; Otorhinolaryngology-Head and Neck Surgery Department, Hearing Center, West China School of Medicine/West China Hospital, Sichuan University, Chengdu, China","IEEE Access","10 Aug 2020","2020","8","","141860","141871","Tinnitus is a perception of sound when no external sound is present. It has seriously affected patients' life. Music is an option to relieve tinnitus in clinic, as it can bring enjoyment to listeners. However, existing music used in tinnitus therapies has limited duration, it is usually repetitively played during the long-term treatment and may not be helpful for relaxation. Moreover, individualized preferences of patients are ignored in most cases. Both of them may hinder tinnitus relief. Although existing methods can synthesize specific music that has unlimited duration and is not repetitively played, the synthesized music has defects with pitch mutations and long pitch durations. Moreover, characteristics of these synthesized music have not been confirmed by tinnitus patients. Therefore, this study presents an automatic method to develop the specific music based on music segments from existing music and long short term memory (LSTM). Numerical results indicate that specific music developed in this study not only retains characteristics of original music, but also overcome the defects above. Besides, a total of 30 tinnitus patients and 10 tinnitus-free volunteers participated the auditory experiment. Auditory results are consistent with numerical results and also suggest that tinnitus patients can perceive feelings that are conducive to tinnitus relief after listening preferred music. Therefore, the developed music presents a possible complement to tinnitus treatment in clinic.","2169-3536","","10.1109/ACCESS.2020.3013339","Science and Technology Support Program of Sichuan Province, China(grant numbers:2013GZ1043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153752","LSTM;music segment;tinnitus;unlimited duration","Music;Medical treatment;Auditory system;Market research;Loss measurement;Ear;Multiple signal classification","","11","","47","CCBY","31 Jul 2020","","","IEEE","IEEE Journals"
"A two-stage sign language recognition method focusing on the semantic features of label text","X. Xu; J. Fu","School of Computer Science and Technology, Xi’an University of Posts & Telecommunications, Xi’an, China; School of Computer Science and Technology, Xi’an University of Posts & Telecommunications, Xi’an, China","2024 20th CSI International Symposium on Artificial Intelligence and Signal Processing (AISP)","25 Mar 2024","2024","","","1","5","The ability to recognize sign language is an indispensable technology that plays a crucial role in facilitating communication between individuals who are deaf or hard of hearing. It is of utmost importance to comprehensively understand the nonverbal expressions employed by the hearing impaired. In order to enhance the efficacy of sign language recognition technology, it is imperative to focus on language modeling and improve the utilization of linguistic elements. At present, much attention in sign language recognition techniques that integrate language modeling is directed toward the translation of GLOSS to text in research related to Sign Language Translation (SLT). Our paper, however, proposes a creative approach that involves the linguistic modeling of the corresponding text of sign language during the process of converting signs to GLOSS. Specifically, we have implemented a text correction module that uses a front-mounted sign language recognition module to make preliminary predictions. The corrected GLOSS sequence is then used to obtain the final recognition result with higher accuracy. Our framework was tested on the RWTHPHOENIX-Weather-2014-T dataset and CSL dataset to evaluate its effectiveness in recognizing sign language on a large scale. The experimental results demonstrate that the proposed method significantly enhances the accuracy of the sign language recognition model.","2640-5768","979-8-3503-8394-2","10.1109/AISP61396.2024.10475205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10475205","Continuous Sign Language Recognition;Machine Learning;Language Modeling;Image Processing;Time Series Modeling","Sign language;Adaptation models;Text recognition;Target recognition;Auditory system;Linguistics;Feature extraction","","","","21","IEEE","25 Mar 2024","","","IEEE","IEEE Conferences"
"With You - Indian Sign Language Detection and Alert System","S. Daga; A. Dusane; D. Bobby","Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, India; Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, India; Department of Electronics and Telecommunication Engineering, Vishwakarma Institute of Technology, Pune, India","2024 International Conference on Emerging Smart Computing and Informatics (ESCI)","17 Apr 2024","2024","","","1","5","The global deaf, non-speaking, and hard-of-hearing community relies heavily on sign language to facilitate communication. This paper will delve into the development of a detection system for the Indian Sign Language (ISL) that puts convenience and inclusivity first for this group. To accomplish this goal, the system uses a Long Short-Term Memory (LSTM) artificial neural network. Currently, the system can recognize 6 dynamic signs, 6 emergency signs, and 26 static ISL alphabets. The emergency signals use certain non-verbal cues to automatically dial the designated emergency services number. The emergency's type, the user's IP address, and their location are all included in the message that is delivered. By enabling communication through ISL, this system has been created to provide accessibility for the community of people with hearing disabilities. The artificial neural network enables the system to acquire the capability of learning and identifying various signals. Additionally, the use of emergency indicators can be extremely important in circumstances when verbal communication is impossible as the system can immediately notify emergency services and give them the information they need, to deliver quick aid. Overall, the hearing-impaired people in India may benefit greatly from this system's contributions to their safety and well-being.","","979-8-3503-0661-3","10.1109/ESCI59607.2024.10497366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10497366","Indian Sign Language;LSTM;Machine learning;OpenCV;Sign language detection","Sign language;Learning (artificial intelligence);Auditory system;Assistive technologies;Emergency services;IP networks;Informatics","","","","17","IEEE","17 Apr 2024","","","IEEE","IEEE Conferences"
"Survey on Hand Gestures Recognition for Sign Translation using Artificial Intelligence","R. V; S. P; T. V. V","Department of Computer Science and Engineering, M. Kumarasamy College of Engineering, Karur, Tamilnadu, India; Department of Computer Science and Engineering, M. Kumarasamy College of Engineering, Karur, Tamilnadu, India; Department of Computer Science and Engineering, M. Kumarasamy College of Engineering, Karur, Tamilnadu, India","2024 5th International Conference on Mobile Computing and Sustainable Informatics (ICMCSI)","11 Apr 2024","2024","","","280","287","Sign language recognition is the process of converting hand signals used in the sign format into spoken or written language. It involves analysing and deciphering hand and finger gestures using computer vision and machine learning algorithms, then mapping those actions to corresponding words or phrases. There are various uses for sign language recognition, including facilitating communication between hearing and deaf persons, increasing accessibility for those with hearing loss, and improving sign language interpreting education and training programmes. Training data is diverse, covering different signers, lighting conditions, and backgrounds. Collect data with a variety of signing styles and gestures to improve the model's generalization and generate the sign values as vector format. The way we engage with people who primarily communicate through sign language may change significantly if this technology is used. Sign language can be recognized using a variety of techniques, such as sensor-based techniques, computer vision techniques, and hybrid techniques that combine both. Computer vision algorithms use visual data from cameras to identify hand movements and gestures. Sensorbased methods capture motion and record data by attaching sensors to the hands or fingers. Consequently, it may be possible to investigate various approaches to sign language recognition and predict a better method with a greater accuracy rate.","","979-8-3503-9523-5","10.1109/ICMCSI61536.2024.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493909","Artificial Intelligence (AI);Hand Gestures;Deep Learning;Sign Language;Neural Network","Sign language;Computer vision;Visualization;Training data;Computer architecture;Auditory system;Feature extraction","","","","50","IEEE","11 Apr 2024","","","IEEE","IEEE Conferences"
"Development of ResNet152 UNet++-Based Segmentation Algorithm for the Tympanic Membrane and Affected Areas","T. Kim; K. Oh; J. Kim; Y. Lee; J. Choi","Department of Applied Artificial Intelligence, Major in Bio Artificial Intelligence, Hanyang University, Ansan, Republic of Korea; Department of Otorhinolaryngology-Head and Neck Surgery, Ansan Hospital, Korea University College of Medicine, Ansan, Republic of Korea; Core Research and Development Center, Korea University Ansan Hospital, Ansan, Republic of Korea; Department of Applied Artificial Intelligence, Major in Bio Artificial Intelligence, Hanyang University, Ansan, Republic of Korea; Department of Otorhinolaryngology-Head and Neck Surgery, Ansan Hospital, Korea University College of Medicine, Ansan, Republic of Korea","IEEE Access","12 Jun 2023","2023","11","","56225","56234","Otitis media (OM) is a common disease in childhood that may have aftereffects such as hearing loss. Therefore, early diagnosis and proper treatment are important. However, the diagnostic accuracies of otolaryngology and pediatrics are low, at 73% and 50%, respectively. Therefore, clinical work that supports the early diagnosis of diseases, such as computer-aided diagnostic (CAD) systems, can be helpful. However, CAD systems for diagnosing ear diseases require an automatic tympanic membrane (TM) segmentation model to assist in diagnosis. This is because it is difficult to detect the TM and affected areas in an endoscopic image of the TM owing to irregular lighting. In this study, we propose a ResNet152 UNet++ image segmentation network. The proposed method applies the ResNet152 layer structure to the encoders in the UNet++ model to detect the location of the TM and affected area with high accuracy. Furthermore, the TM and affected regions can be segmented better than when using the previously proposed UNet and UNet++ models. To the best of our knowledge, this study is the first to use a UNet++-based segmentation model to segment TM areas in endoscopic images of the TM and evaluate its performance. The experiments revealed that ResNet152 UNet++ outperforms conventional methods in terms of segmentation of the TM and affected areas.","2169-3536","","10.1109/ACCESS.2023.3281693","Institute of Information and Communications Technology Planning and Evaluation (IITP) Grant funded by the Korea Government, Ministry of Science and ICT (MSIT) through the Artificial Intelligence Convergence Innovation Human Resources Development, Hanyang University ERICA, and the Artificial Intelligence Convergence Research Center, Hanyang University ERICA(grant numbers:RS-2022-00155885,2020-0-01343); National Research Foundation of Korea (NRF) funded by the Korea Government (MSIT)(grant numbers:NRF-2022R1F1A1074999); Korea University Grant and a grant of the Medical Data-Driven Hospital Support Project through the Korea Health Information Service (KHIS), funded by the Ministry of Health and Welfare, Republic of Korea; MSIT, South Korea, through the ICT Challenge and Advanced Network of HRD (ICAN) Program supervised by the IITP(grant numbers:IITP-2022-RS-2022-00156439); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138898","Convolutional neural network;artificial neural network;segmentation;otitis media;computer-aided diagnosis","Image segmentation;Artificial neural networks;Diseases;Computational modeling;Convolutional neural networks;Solid modeling;Medical diagnostic imaging;Computer aided analysis","","","","24","CCBY","31 May 2023","","","IEEE","IEEE Journals"
"Comparison of CNN-based Speech Dereverberation using Neural Vocoder","C. Chun; K. M. Jeon; C. Leem; B. Lee; W. Choi","Dept. of Computer Engineering, Chosun University, Gwangju, South Korea; IntFlow Co., Ltd, AI Convergence Technology Lab, Gwangju, South Korea; IntFlow Co., Ltd, AI Convergence Technology Lab, Gwangju, South Korea; Dept. of Information and Communication Engineering, Chosun University, Gwangju, South Korea; Dept. of Computer Engineering, Chosun University, Gwangju, South Korea","2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)","29 Apr 2021","2021","","","251","254","Reverberation degrades the speech quality and intelligibility, particularly for hearing impaired people. In an automatic speech recognition (ASR) system, a dereverberation technique, which removes reverberation, is widely employed as a pre-processing to increase the performance of the ASR system. In this paper, we compare the performance of the CNN-based dereverberation method by applying various vocoders. The U-Net architecture is employed as the dereverberation technique. WaveGlow, MelGAN, and Griffin Lim are used as vocoders. Such vocoders play a role in converting speech features into speech samples in time domain, and are capable of generating high-quality speech from mel-spectrograms. In order to compare the results, PESQ was measured. As a result, it was confirmed that PESQ was higher than that of the reverberant speech when speech was synthesized with the reverberation removal and vocoder.","","978-1-7281-7638-3","10.1109/ICAIIC51459.2021.9415259","National Research Foundation of Korea; National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415259","Reverberation;speech dereverberation;convolutional neural network;neural vocoder;room impulse response","Time-frequency analysis;Vocoders;Auditory system;Reverberation;Time-domain analysis;Artificial intelligence;Automatic speech recognition","","3","","21","IEEE","29 Apr 2021","","","IEEE","IEEE Conferences"
"Signtific Translator: (The Automated Sign Language Recognition)","M. M; P. P; S. S. A. A; D. S; R. R","CSE Department, Sri Sairam Engineering College, Chennai, India; CSE Department, Sri Sairam Engineering College, Chennai, India; CSE Department, Sri Sairam Engineering College, Chennai, India; CSE Department, Sri Sairam Engineering College, Chennai, India; CSE Department, Sri Sairam Engineering College, Chennai, India","2022 1st International Conference on Computational Science and Technology (ICCST)","14 Feb 2023","2022","","","45","48","People with hearing loss or other limitations frequently communicate using sign language. Sign languages have been used by humans since the beginning of time. The learning curve for sign language is rather low. Signs and symbols were used by early people to communicate. Our goal is to develop a sign language translator with voice help that can translate sign language into text. By using our detection model, anybody who is deaf and has no knowledge of sign language may easily communicate with them. In this essay, we've covered key approaches, strategies, and associated details concerning our model's operation.","","978-1-6654-7655-3","10.1109/ICCST55948.2022.10040280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040280","Sign Language Recognition;Indian Sign Language(ISL);TensorFlow;Machine Learning","Scientific computing;Computational modeling;Symbols;Gesture recognition;Auditory system;Assistive technologies","","1","","5","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Sign language translation based on new continuous sign language dataset","S. Feng; T. Yuan","Technical College for the Deaf, Tianjin University of Technology, Tianjin, China; Technical College for the Deaf, Tianjin University of Technology, Tianjin, China","2022 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)","5 Aug 2022","2022","","","491","494","deaf individuals rely heavily on one another’s use of sign language as a means of communication. In this era, the significance of accessibility has become more and more important. However, for most people with normal hearing, learning sign language is a very difficult thing. Many research teams around the world are using Deep Learning to develop translators for sign language recognition. However, there is a lack of larger and more semantically rich sign language datasets. In this article, we propose a new Chinese sign language dataset- 109 unique sentences from 50 sign language signers and 27,250 clips. On the new Chinese sign language dataset, we further propose a sequence-to-sequence deep learning approach in order to demonstrate how deep learning may continually lower the communication barriers that exist between persons who are deaf and those who have normal hearing.","","978-1-6654-9991-0","10.1109/ICAICA54878.2022.9844468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844468","Dataset;Attention;CSLR;Video to text;BLEU","Deep learning;Training;Semantics;Gesture recognition;Auditory system;Speech recognition;Assistive technologies","","3","","6","IEEE","5 Aug 2022","","","IEEE","IEEE Conferences"
"Recognize Vietnamese Sign Language Using Deep Neural Network","L. Huynh; V. Ngo","Information Technology Department, Ho Chi Minh City University of Education, Ho Chi Minh City, Viet Nam; Information Technology Department, Ho Chi Minh City University of Education, Ho Chi Minh City, Viet Nam","2020 7th NAFOSTED Conference on Information and Computer Science (NICS)","2 Feb 2021","2020","","","191","196","World Health Organization published an article called `Deafness and hearing loss' in March 2020, it said that more than 466 million people in the world lost their hearing ability, and 34 million of them were children. Sign Language has been born and developed for a long time, but its application to communicate has met with many inadequacies and difficulties. Many methods of Computer Vision-based approach gave good results on Sign Language Alphabet Recognition but all of them require the perfect result from background removing step. However, when it comes to real life, removing a complex background is too difficult for any simple background removing algorithms. In this work, our main purpose is to build a model based on deep learning that can recognize Vietnamese Sign Language Alphabet in a complex environment. Results obtained show a robust accuracy of this model in recognizing Vietnamese Sign Language Alphabet.","","978-0-7381-0553-6","10.1109/NICS51282.2020.9335904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335904","VSL;sign language;deep learning;object detection;artificial intelligence","Pediatrics;Assistive technology;Neural networks;Gesture recognition;Auditory system;Organizations;Real-time systems","","1","","21","IEEE","2 Feb 2021","","","IEEE","IEEE Conferences"
"Music education for the deaf and hard of hearing, a literature review to understand the methods used in music teaching","C. D. S. Benites; I. F. Silveira","Faculdade de Computação e Informática, Universidade Presbiteriana Mackenzie, São Paulo, SP; Faculdade de Computação e Informática, Universidade Presbiteriana Mackenzie, São Paulo, SP","2023 18th Iberian Conference on Information Systems and Technologies (CISTI)","15 Aug 2023","2023","","","1","6","This research aims to identify the main scientific productions on the theme of music education for the deaf in various article bases and to find the themes used for teaching music to the deaf. It seeks to analyze them and thus propose a new method using artificial intelligence to meet the needs of the deaf person and make them independent in learning.","2166-0727","978-989-33-4792-8","10.23919/CISTI58278.2023.10211595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10211595","deaf music education;the deaf and the music;teaching theme","Bibliographies;Education;Production;Learning (artificial intelligence);Auditory system;Information systems","","","","0","","15 Aug 2023","","","IEEE","IEEE Conferences"
"Spatiotemporal Convolutions and Video Vision Transformers for Signer-Independent Sign Language Recognition","M. Marais; D. Brown; J. Connan; A. Boby","Department of Computer Science, Rhodes University, Grahamstown, South Africa; Department of Computer Science, Rhodes University, Grahamstown, South Africa; Department of Computer Science, Rhodes University, Grahamstown, South Africa; Department of Computer Science, Rhodes University, Grahamstown, South Africa","2023 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)","24 Aug 2023","2023","","","1","6","Sign language is a vital tool of communication for individuals who are deaf or hard of hearing. Sign language recognition (SLR) technology can assist in bridging the communication gap between deaf and hearing individuals. However, existing SLR systems are typically signer-dependent, requiring training data from the specific signer for accurate recognition. This presents a significant challenge for practical use, as collecting data from every possible signer is not feasible. This research focuses on developing a signer-independent isolated SLR system to address this challenge. The system implements two model variants on the signer-independent datasets: an R(2+ I)D spatiotemporal convolutional block and a Video Vision transformer. These models learn to extract features from raw sign language videos from the LSA64 dataset and classify signs without needing handcrafted features, explicit segmentation or pose estimation. Overall, the R(2+1)D model architecture significantly outperformed the ViViT architecture for signer-independent SLR on the LSA64 dataset. The R(2+1)D model achieved a near-perfect accuracy of 99.53% on the unseen test set, with the ViViT model yielding an accuracy of 72.19 %. Proving that spatiotemporal convolutions are effective at signer-independent SLR.","","979-8-3503-1480-9","10.1109/icABCD59051.2023.10220534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10220534","sign language recognition;signer-independence;vision transformers;spatiotemporal convolutions;video classification;deep learning","Computational modeling;Transfer learning;Training data;Gesture recognition;Computer architecture;Auditory system;Assistive technologies","","","","26","IEEE","24 Aug 2023","","","IEEE","IEEE Conferences"
"Recent Advances in Sign Language Recognition using Deep Learning Techniques","S. E. Panneer; M. Sornam","Department of Computer Applications, D.G.Vaishnav College, Chennai, INDIA; Department of Computer Science, University of Madras, Chennai, INDIA","2022 6th International Conference on Trends in Electronics and Informatics (ICOEI)","24 May 2022","2022","","","1261","1265","Communication is essential to express and receive information, knowledge, ideas, and views among people, but it has been quite a while to be an obstruction for people with hearing and mute disabilities. Though there is sign language to communicate with non-sign people it is difficult for everyone to interpret and understand. As reported by World Health Organization (WHO), five percent of the world population of over 300 million people suffers from hearing disability. Technology has grown rapidly in the last few decades with the presence of deep learning and artificial intelligence, but somehow physically impaired people are not able to get the maximum benefit of it due to lack of awareness, accessibility problems, cost, and other reasons. This survey paper discusses the recent development and assistance provided for signers to communicate with non-signers.","","978-1-6654-8328-5","10.1109/ICOEI53556.2022.9777104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777104","Sign Language;Deep Learning;Artificial Intelligence;Physically Impaired","Deep learning;Costs;Sociology;Gesture recognition;Auditory system;Organizations;Assistive technologies","","3","","27","IEEE","24 May 2022","","","IEEE","IEEE Conferences"
"Per-frame Sign Language Gloss Recognition","C. J. Kim; H. -M. Park","Artificial Intelligence Research Center, Korea Electronics Technology Institute, Seongnam-si, Gyeonggi-do, Republic of Korea; Artificial Intelligence Research Center, Korea Electronics Technology Institute, Seongnam-si, Gyeonggi-do, Republic of Korea","2021 International Conference on Information and Communication Technology Convergence (ICTC)","7 Dec 2021","2021","","","1125","1127","Sign language is a language for communicating among the deaf and hard of hearing people and the hearing people. Sign language has its own grammatical system which is different from spoken or written languages. A sentence of sign language consists of glosses as morphemes, and the meaning of sign language depends on the movements of a body, hands, finger shapes, and facial expressions. Previous methods can translate sign language sentences into a sequence of glosses or written language but are weak for the out-of-vocabulary problem such as variations of word order. This is because the model only considers a sign language sentence as an inseparable sequence, even though the sentence consists of multiple glosses. In this paper, we propose a method that predicting every gloss for each video frame of Korean sign language using transformers. Predicted frame-by-frame gloss information can be used to transform a video of a sign language sentence into a gloss sequence, even if the model has not learned that pattern.","2162-1233","978-1-6654-2383-0","10.1109/ICTC52510.2021.9621167","Institute of Information and communications Technology Planning and evaluation (IITP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9621167","sign language;gloss;human keypoints;transformers","Smoothing methods;Shape;Gesture recognition;Auditory system;Transforms;Assistive technologies;Predictive models","","3","","14","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"English to Indian Sign Language Gloss Structure Translation using Sequence to Sequence Model","M. S. Nair; S. M. Idicula; N. R. Raji; R. Gopal","Department of Computer Science, Cochin University of Science and Technology, Ernakulam, Kerala, India; Department of Artificial Intelligence and Data Science, Muthoot Institute of Technology and Science, Ernakulam, Kerala, India; Degree(HI) National Institute of Speech & Hearing, Trivandrum, Kerala, India; Degree(HI) National Institute of Speech & Hearing, Trivandrum, Kerala, India","2023 9th International Conference on Smart Computing and Communications (ICSCC)","6 Dec 2023","2023","","","560","564","A significant portion of the deaf and hard of hearing population in India uses Indian Sign Language (ISL) for communication. However, they frequently encounter significant communication and educational challenges as a result of the lack of ISL-related resources and technology. In this work, we aim to create a system for translating English text to ISL gloss structure. It can be viewed as a significant step towards ISL generation from normal text. The proposed method uses a seq2seq BiLSTM encoder-LSTM decoder based approach for performing the grammatical transformation of sentences to convert them from English to ISL gloss structure. The evaluation of the system, using BLEU score and ROUGE-L F1 score yielded good results for different batch sizes.","","979-8-3503-1409-0","10.1109/ICSCC59169.2023.10335040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10335040","Indian Sign Language(ISL);ISL gloss;Sequence to Sequence;Machine Translation;Encoder Decoder","Computational modeling;Sociology;Gesture recognition;Auditory system;Assistive technologies;Decoding;Grammar","","","","20","IEEE","6 Dec 2023","","","IEEE","IEEE Conferences"
"A Review For Different Sign Language Recognition Systems","A. Singh; M. Rakhra","School of Computer Science and Engineering, Lovely Professional University, Phagwara, India; School of Computer Science and Engineering, Lovely Professional University, Phagwara, India","2022 4th International Conference on Artificial Intelligence and Speech Technology (AIST)","17 Mar 2023","2022","","","1","6","The fundamental aspects of communication that take place between human beings are exemplified by human language. For people who are deaf or hard of hearing, sign language is the primary mode of communication because the spoken language is inaccessible to those who are hard of hearing. As a result, many people are disabled because of hearing loss. The understanding of sign languages is a particular area of research interest. This study provides an overview and review of hand signals, gestures, and the most important methods utilised to recognise sign languages. The methods for understanding Sign Language are shown and explained. For each method, the accuracy is given. Many researchers have presented their research based on the main categories of these techniques. There are advantages and downsides, or limits associated with each technique. This study should be used as a guide to choose the best model to implement and as a road map for future research. This will help us improve the accuracy of future models and give the sign language community a better way to study how to make a fully video-based translator.","","978-1-6654-9902-6","10.1109/AIST55798.2022.10065037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10065037","Hand-gestures;Face-Gestures;Sign;Sign Language;Sign-Recognition;Feature Extraction;Deep Learning;Convolutional Neural Networks (CNN);Artificial Neural Network (ANN);Support Vector Machine (SVM);Long Short-Term Memory (LSTM);K-Nearest Neighbor (KNN)","Support vector machines;Roads;Neural networks;Gesture recognition;Auditory system;Assistive technologies;Artificial intelligence","","7","","49","IEEE","17 Mar 2023","","","IEEE","IEEE Conferences"
"Consideration of Life Rhythm for Hearing-Dog Robots Searching for User","S. Furuta; T. Nakamura; Y. Iwahori; S. Fukui; M. Kanoh; K. Yamada",Nagoya Institute of Technology; Nagoya Institute of Technology; Chubu University; Aichi University of Education; Chukyo University; Institute of Advanced Media Arts and Sciences,"2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI)","27 Dec 2018","2018","","","102","105","A hearing dog is a sort of assistance dog for hearing-impaired individuals. The physical touch of the dog can alert the individuals to important sounds such as doorbells, alarm clocks, and fire alarms. Although hearing dogs can assist people, there is an insufficient number of them around the world today. As an alternative, a hearing-dog robot has been developed. This robot can move around autonomously to search for a user and notify him or her of important sounds. In this work, we propose an exploring algorithm for the robot that considers past information about the location of the user. Specifically, this algorithm utilizes the user's life rhythm in order to achieve efficient exploring. In our experiments, proposed algorithm showed a shorter time as compared with the algorithm without the user's life rhythm.","2376-6824","978-1-7281-1229-9","10.1109/TAAI.2018.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588487","A Hearing-Dog Robot;Estimation of User Location;Rhythm of Daily Life;Stochastic Explore","Rhythm;Dogs;Auditory system;Probability distribution;Collision avoidance;Mobile robots","","1","","10","IEEE","27 Dec 2018","","","IEEE","IEEE Conferences"
"Brief Review of Recent Researches in Speech Enhancement from Filters to Neural Networks","F. Ge","Information Science and Technology College, Dalian Maritime University, Dalian, China","2020 International Conference on Computing and Data Science (CDS)","9 Dec 2020","2020","","","260","264","According to the World Health Organization, more and more people will suffer from hearing loss in the future. Therefore, there will be greater demand for the output and technology of hearing aids. Under the help of artificial intelligence, the technology of smart hearing aids will also become more intelligent, so that the wearer can get a better experience. This paper mainly studies the related problems of speech signal processing in intelligent digital hearing aids. This article focuses on the speech enhancement in digital hearing aids. First, this work studies the application of filter in speech enhancement technology, mainly introduces the Wiener filter algorithm using the minimum mean square error criterion; the Kalman filter algorithm that can solve discrete signals; spectral subtraction based on multi-window spectrum estimation. Secondly, this paper studies some specific methods of deep learning technology in speech enhancement, mainly introduces DNN-based speech enhancement method, deep learning-based auditory cepstrum coefficient speech enhancement algorithm, and AE-CGAN-based speech enhancement algorithm. Finally, this paper studies the related problems of acoustic scene classification, divides the listening environment into Gaussian white noise, impact noise, and music noise, and explains the solutions for each listening environment.","","978-1-7281-7106-7","10.1109/CDS49703.2020.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9276003","component;formatting;style;styling;insert","FCC;Data science","","2","","44","IEEE","9 Dec 2020","","","IEEE","IEEE Conferences"
"Simple2In1: A Simple Method for Fusing Two Sequences from Different Captioning Systems into One Sequence for a Small-scale Thai Dataset","W. Longjaroen; T. Chay-Intr; K. Funakoshi; A. Chotimongkol; S. Usanavasin","Sirindhorn International Institute of Technology, Thammasat University, Thailand; School of Engineering, Tokyo Institute of Technology, Japan; Institute of Innovative Research, Tokyo Institute of Technology, Japan; National Science and Technology Development Agency, Thailand; Sirindhorn International Institute of Technology, Thammasat University, Thailand","2023 18th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP)","19 Dec 2023","2023","","","1","6","The increasing number of Deaf and Hard of Hearing (DHH) individuals has amplified the need for quality captions, especially in real-time. Previous studies have successfully explored various methods to enhance caption quality, including alignment-based and neural network approaches. While alignment-based methods represent a conventional approach, neural network models are inherently more complex yet offer superior performance. However, these neural models demand large datasets, posing challenges for languages such as Thai with limited datasets available. In this paper, we propose a simple but effective method to improve the quality of Thai captions on a small-scale Thai dataset. Our method utilizes a pre-trained mT5 model to generate a single sequence from two sequences from two different captioning systems. Despite a small dataset and limited input sequences, our method shows potential in improving six evaluation metrics, surpassing all baseline models.","2831-4565","979-8-3503-7121-5","10.1109/iSAI-NLP60301.2023.10355030","Thammasat University; Technology Development; Tokyo Institute of Technology; Royal Society; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10355030","Text Fusion;Captioning Systems;Text Alignment","Measurement;Neural networks;Auditory system;Real-time systems;Natural language processing;Artificial intelligence","","","","25","IEEE","19 Dec 2023","","","IEEE","IEEE Conferences"
"Assessing Closed Captioning Quality Using a Multilayer Perceptron","S. Nam; D. Fels","University of Toronto Toronto, Canada; Ryerson University Toronto, Canada","2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)","8 Nov 2018","2018","","","9","16","Closed Captioning (CC) is a telecommunication service to provide Deaf or Hard of Hearing (D/HOH) audiences the text equivalent of what hearing audiences experience in TV. The quality of CC is often interpreted as an accuracy and assessed in the empirical measure of counting number of errors. Although the regulators necessitate certain rules in the factors of CC quality, the D/HOH community members, who are the primary audiences to the CC, are not completely satisfied. One solution to solve this issue can be including the perspective of D/HOH audience in the assessment process. This research made an attempt to design an automatic quality assessment system for CC using artificial neural networks-multilayer perceptron trained from the D/HOH audience's subjective ratings, and the representative values extracted from the caption file, and the transcript file. As an initial stage of research, the trained model was then compared with other statistical regression models.","","978-1-5386-9555-5","10.1109/AIKE.2018.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527442","Applications of AI;Automated assessment;Simulation;Artificial Intelligence;Machine Learning;Accessible Computing","Delays;Q-factor;Quality assessment;Artificial neural networks;Grammar;Standards;Auditory system","","1","","31","IEEE","8 Nov 2018","","","IEEE","IEEE Conferences"
"A Brief Survey On Dataset And Method Used For Sign Language Detection","S. Aishwarya; M. Sundaram B; B. V","Department of CSE, New Horizon College of Engineering, Bangalore, India; Department of CSE, New Horizon College of Engineering, Bangalore, India; Department of CSE, New Horizon College of Engineering, Bangalore, India","2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)","26 Feb 2024","2023","10","","1","5","Although speech is still a most common form of communication some people have difficulty in listening and speaking. For people with such disabilities, communication poses a big barrier. To solve this issue researcher have been trying to build state of art machine learning and deep learning model so that those people with disabilities can communicate easily, in such a context most of the researchers worked on Indian Sign Language which is one of the most difficult tasks to study because, in contrast to American Sign Language, it is still in its fancy. To perform Indian Sign Language recognition many researchers have built various Artificial Intelligence based models. This paper explains some of the Artificial Intelligence based techniques to solve the problem of communication for those people who have difficulty in listening or speaking.","2687-7767","979-8-3503-8247-1","10.1109/UPCON59197.2023.10434904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10434904","Machine Learning;Sign Language;Deep Learning;computer vision","Surveys;Deep learning;Gesture recognition;Auditory system;Assistive technologies;Speech;Task analysis","","","","14","IEEE","26 Feb 2024","","","IEEE","IEEE Conferences"
"Real-Time Sign Language Recognition using a Multimodal Deep Learning Approach","S. Amutha; N. Shanmukh; A. V. S. R. P. Naidu; P. V. Kumar; G. S. S. Narayana","Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Virudhnagar, Madurai, India; Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Virudhnagar, Madurai, India; Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Virudhnagar, Madurai, India; Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Virudhnagar, Madurai, India; Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Virudhnagar, Madurai, India","2023 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)","4 Aug 2023","2023","","","1","8","Sign language recognition is an important area of research that aims to provide greater access to communication and information for individuals who are deaf or hard of hearing. In this paper, we present a new approach for real-time sign language recognition using a multimodal deep learning approach. The proposed approach integrates video and inertial sensor data for improved recognition accuracy and robustness.The proposed approach uses a convolutional neural network (CNN) to extract features from the video data and a Recurrent neural network (RNN) is used to capture the temporal dynamics of the sign language gestures.Sign language recognition systems use technologies such as computer vision, machine learning, and artificial intelligence to analyze and understand the gestures and movements of the signer. These systems have the potential to improve the quality of life for individuals who are deaf or hard of hearing by providing them with greater access to communication and information in a variety of settings.","","979-8-3503-1590-5","10.1109/ACCAI58221.2023.10199569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10199569","CNN;RNN;inertial sensor data;Deep Learning","Deep learning;Recurrent neural networks;Gesture recognition;Auditory system;Assistive technologies;Streaming media;Feature extraction","","","","15","IEEE","4 Aug 2023","","","IEEE","IEEE Conferences"
"Sign and Machine Language Recognition for Physically Impaired Individuals","S. Jothimani; S. Shruthi; E. D. Tharzanya; S. Hemalatha","Department of Electronics and Communication Engineering, M. Kumarasamy College of Engineering, Karur, Tamil Nadu; Department of Electronics and Communication Engineering, M. Kumarasamy College of Engineering, Karur, Tamil Nadu; Department of Electronics and Communication Engineering, M. Kumarasamy College of Engineering, Karur, Tamil Nadu; Department of Electronics and Communication Engineering, M. Kumarasamy College of Engineering, Karur, Tamil Nadu","2022 3rd International Conference on Electronics and Sustainable Communication Systems (ICESC)","19 Sep 2022","2022","","","1483","1488","Sign language is a type of communication used by people who are deaf or hard of hearing. Disabled People use sign language gestures as a non-verbal communication tool to express their feelings and thoughts to other people. It's tough to converse with those who are deaf or hard of hearing. Deaf and mute persons communicate via hand gesture sign language, which makes it difficult for non-deaf and mute people to understand their language. As a result, technologies that recognise numerous signs and communicate the data to regular people are needed. However, because these ordinary people have a hard time understanding their expressions, experienced sign language experts are required for medical and legal appointments, as well as educational and training sessions. There has been an upsurge in demand for these services in recent years. Other types of services, such as video remote human interpreter using a high-speed Internet connection, have been established, providing an easy-to-use sign language interpreter service that may be utilised and benefited, but with significant limits. In order to address this issue, the artificial intelligence technology can be implemented to analyze user’s hand with finger detection. A novel system has been suggested to design the vision-based system in real time environments. Then the Convolutional Neural Network (CNN) algorithm is used to classify the sign language and provide the label about recognized sign with voice alert.","","978-1-6654-7971-4","10.1109/ICESC54411.2022.9885433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9885433","Sign Language;Deep learning;Impairment people;Convolutional neural network;Region of Interest","Training;Image recognition;Law;Gesture recognition;Speech recognition;Auditory system;Assistive technologies","","","","24","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"Certain Investigation of Various Algorithms to Improvise the Quality of Hearing Aid","K. Priyadharsini; C. Ganesh Babu; J. R. Dinesh Kumar; J. Jony Jas; S. Mano Bharathi; Rajarajan","ECE, Sri Krishna College of Engineering and Technology, Tamilnadu, India; Department of E&I Engg, Bannari Amman Institute of Technology, Sathyamangalam; ECE, Sri Krishna College of Engineering and Technology, Tamilnadu, India; ECE, Sri Krishna College of Engineering and Technology, Tamilnadu, India; ECE, Sri Krishna College of Engineering and Technology, Tamilnadu, India; ECE, Sri Krishna College of Engineering and Technology, Tamilnadu, India","2023 Third International Conference on Smart Technologies, Communication and Robotics (STCR)","22 Jan 2024","2023","1","","1","6","Hearing loss, marked by an inability to hear sounds below a 20 dB threshold, often results from dysfunction in the ear’s organs, associated nerves, or the auditory part of the brain. Deaf individuals encounter direct or indirect discrimination in daily life, as highlighted by National Centre for the Deaf data showing a significant employment gap. Only 48% of deaf individuals are employed, compared to 72% of their hearing counterparts. This discrimination leads to missed opportunities and some deaf individuals leaving jobs due to disability-related biases. After careful study of research papers done by scholars and working with low-end and High-end Hearing aid devices, we decide to start with the analysis of audio signals and implementing the different algorithms in computational tools like MATLAB and Python. Subsequently, we found the quality of hearing aid devices depends on three factors SNR, SI-SNR, and PESQ. Working with different algorithms and modifying some, we achieved better quality hearing aids than existing high-end hearing aids using the SepFormer algorithm. The overall systems shows better results on the signal quality improvements to ensure the HA perforamnce. The Metric GAN+based method has the good variations in SI-SNR of 9.42, PESQ of 3.15. Hence, using the proposed model enhance the quality of HA and supports the dumb people. The quality of the HA is analysed using this proposed mehtod with AI enabled technlogy.","","979-8-3503-7086-7","10.1109/STCR59085.2023.10396954","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10396954","SepFormer;MetricGAN;MetricGAN+;SI-SNR;PESQ","Measurement;Employment;Auditory system;Hearing aids;Mathematical models;Artificial intelligence;Robots","","","","19","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"Data glove-based sign language translation with convolutional neural networks","M. C. Cervera; D. L. Meza; D. Huamanchahua","Department of Mechatronic Engineering, Universidad Continental, Huancayo, Perú; Department of Mechatronic Engineering, Universidad Continental, Huancayo, Perú; Department of Electrical and Mechatronic Engineering, Universidad de Ingeniería y Tecnología - UTEC, Lima, Perú","2022 International Conference on Mechanical, Automation and Electrical Engineering (CMAEE)","17 Mar 2023","2022","","","67","74","This research was carried out because of the communication barriers that currently exist between hearing impaired and hearing people. These barriers hinder their integration into society and affect their interpersonal relationships. The objective of the study was to propose the development of a stationary assistive robot capable of displaying sign language interpretation through the combination of data gloves and the D-CNN and LSTM algorithm to facilitate the communication of hearing-impaired children in Huancayo. The triple diamond research design was used, where the mind map and the lotus diagram were used for the delimitation and definition of the problem. In addition, the IDEF0 technique was used to obtain a structured design of the project system. A morphological matrix was also used to choose the best solution for the problem. The chosen design contemplates the use of an Arduino UNO, flex sensors, accelerometers and gyroscopes for sign detection. The main algorithm consists of the union of a deep convolutional neural network and a LSTM for a correct sign classification module. The proposed design proposes to visualize the conceptual development of the project mentioned above.","","979-8-3503-4657-2","10.1109/CMAEE58250.2022.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10066103","Sign Language Recognition (SLR);Data glove;Convoluting Neural Network (CNN);long short-term memory (LSTM)","Electrical engineering;Flexible printed circuits;Neural networks;Data visualization;Gesture recognition;Auditory system;Diamonds","","","","26","IEEE","17 Mar 2023","","","IEEE","IEEE Conferences"
"Desktop Based Speech Recognition For Hearing And Visually Impaired Using Machine Learning","P. K. Rani; B. Manohari; S. S. Kumar; B. A. Kumar",Sri Krishna College of Engineering and Technology; Sri Krishna College of Engineering and Technology; Sri Krishna College of Engineering and Technology; Sri Krishna College of Engineering and Technology,"2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)","16 Jun 2022","2022","","","702","709","Using Speech acknowledgement with several speakers, a PC-based application is proposed in this study. An informal ID is shown on the PC screen next to a speech-to-message change text yield, indicating that the recognised speaker has been identified. Three features are included in the framework that has been proposed: A client-server model and multi-stringing notion are used to transmit and collect the modified speech over to a message in a client-server model and a client-server model. The system consists of PCs that can transmit and acquire data, a PC programme that connects various devices through WiFi, and a receiver that receives spoken input from the customers. Every one of the functionalities is executed as a PC application with an easy to understand graphical client interface (GUI). There are two modes of operation for the application, one near and one far. Other than in a case when there are speakers within earshot, the customer chooses a location that is either near or remote. Individuals who are deaf or hard of hearing are the primary focus of the proposed structure. Despite the fact that portable hearing aid technology has been much improved, it relies on an obstructed individual's internal, exterior, and center ear, as well as their hearing nerve, to aid in proper hearing. The portable amplifier isn't functional if the nerve of hearing is damaged as well. To assist the meeting impaired, the suggested framework provides Speech as text and recognises the speakers in question. It accordingly forestalls weakness brought about by listening exertion and stress. This prompts expanded use of the debilitated people. The proposed assistive framework likewise causes it feasible for typical individuals who don't know to communicate through signing to address hearing debilitation. This makes the existence of hearing disabled individuals more satisfied which works on their satisfaction.","","978-1-6654-9710-7","10.1109/ICAAIC53929.2022.9793245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793245","Multi-speaker;MFCC;Training;Testing;GMM;Hearing disabled","Client-server systems;Text recognition;Auditory system;Speech recognition;Receivers;Oral communication;Noise measurement","","","","11","IEEE","16 Jun 2022","","","IEEE","IEEE Conferences"
"A Prototype for Mexican Sign Language Recognition and Synthesis in Support of a Primary Care Physician","C. O. Sosa-Jiménez; H. V. Ríos-Figueroa; A. L. Solís-González-Cosío","School of Statistics and Informatics, University of Veracruz (UV), Xalapa, Veracruz, Mexico; Research Institute in Artificial Intelligence, University of Veracruz (UV), Xalapa, Veracruz, Mexico; School of Sciences, National Autonomous University of Mexico (UNAM), Mexico City, Mexico","IEEE Access","12 Dec 2022","2022","10","","127620","127635","Few hearing people know and use Mexican Sign Language (MSL). Consequently, this is the main barrier between people who having total or partial hearing loss and hearing people. This study proposes a system that recognizes and animates in real time a set of signs belonging to the semantic field of general medicine consultation services. Therefore, a linkage between a hearing doctor and a deaf patient can be established in a non-intrusive way and with easy dynamic interaction. Our main contribution is a bidirectional translator system for Mexican Sign Language in the context of primary care health services, in addition to basic signs to fingerspell alphabet and numbers as a complement to provide personal information such as name, age, etc. The recognition module uses a Microsoft Kinect sensor to obtain sign trajectories and images to feed hidden Markov Models (HMMs) for processing sign samples in real time. The experiments showed the recognition of 82 different signs by 22 participants. As a result, accuracy and F1 scores average rates of 99% and 88%, respectively, were obtained.","2169-3536","","10.1109/ACCESS.2022.3226696","National Council of Science and Technology of Mexico (CONACYT)(grant numbers:388930); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9970338","Mexican sign language;depth sensor;dynamic sign language recognition;medicine consultation services;sign language synthesis","Assistive technologies;Gesture recognition;Image recognition;Auditory system;Biomedical imaging;Three-dimensional displays;Cameras","","4","","49","CCBYNCND","5 Dec 2022","","","IEEE","IEEE Journals"
"Next-generation hearing prosthetics","R. Gao; S. Basseas; D. T. Bargiotas; L. H. Tsoukalas","Artificial Intelligence Systems Laboratory (AISL), Purdue University, West Lafayette, IN, USA; NA; Department of Automation, Technological and Educational Institute of Chalkis, Greece; School of Nuclear Engineering, Purdue University","IEEE Robotics & Automation Magazine","2 Apr 2003","2003","10","1","21","25","Neural networks and fuzzy logic are powerful tools for next-generation hearing prosthetics. A neural network, as a function fitter to map the hearing loss to desired gains requirements, provides many benefits over other approaches. The network is able to learn dynamically through experience. It is open and expandable - a physician can easily incorporate new knowledge into the system. Fuzzy logic, on the other hand, is an indispensable tool for the tuning process. It builds a direct and reasonable link between a user's subjective evaluation and the actual required modifications to the gain targets. Again, physicians are free to add new rules to the rule base in reflection of specific needs and patterns. The presented neurofuzzy approach helps hearing prosthetic devices not only in an offline fitting process, but also in online operations. Next-generation hearing prosthetics will be more intelligent than current devices. Hearing aids should be situation-dependent and capable of evolving or adapting. The neurofuzzy approach makes these features possible.","1558-223X","","10.1109/MRA.2003.1191707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1191707","","Neural networks;Fuzzy logic;Next generation networking;Auditory system;Neural prosthesis;Deafness;Neurons;Acoustics;Laboratories;Fitting","","3","","6","IEEE","2 Apr 2003","","","IEEE","IEEE Magazines"
"A real-time interactive nonverbal communication system through semantic feature extraction as an interlingua","Jin Hou; Y. Aoki","Division of Electronics and Information Engineering, Graduate School of Engineering, Hokkaido University, Sapporo, Japan; Division of Electronics and Information Engineering, Graduate School of Engineering, Hokkaido University, Sapporo, Japan","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","14 Jan 2004","2004","34","1","148","154","There has been a growing interest in the use of networked virtual environment (NVE) technology to implement telepresence that allows participants to interact with each other in shared cyberspace. In addition, nonverbal language has attracted increased attention because of its association with more natural human communication, and especially sign languages play an important role for the hearing impaired. This paper proposes a novel real-time nonverbal communication system by introducing an artificial intelligence method into the NVE. We extract semantic information as an interlingua from the input text through natural language processing, and then transmit this semantic feature extraction (SFE) to the three-dimensional (3-D) articulated humanoid models prepared for each client in remote locations. Once the SFE is received, the virtual human is animated by the synthesized SFE. Experiments with Japanese and Chinese sign languages show this system makes the real-time animation of avatars available for the participants when chatting with each other. The communication is more natural since it is not just based on text or predefined gesture icons. This proposed system is suitable for sign language distance training as well.","1558-2426","","10.1109/TSMCA.2003.818461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1259443","","Real time systems;Feature extraction;Handicapped aids;Humans;Animation;Virtual environment;Auditory system;Artificial intelligence;Data mining;Natural language processing","","6","","25","IEEE","14 Jan 2004","","","IEEE","IEEE Journals"
"Design of Intelligent Human-Computer Interaction System for Hard of Hearing and Non-Disabled People","Q. Fu; J. Fu; S. Zhang; X. Li; J. Guo; S. Guo","Tianjin Key Laboratory for Control Theory and Application in Complicated Systems, Tianjin International Joint Research and Development Center, Tianjin University of Technology, Tianjin, China; Intelligent Robot Laboratory, Tianjin University of Technology, Tianjin, China; State Key Laboratory of Robotics and System, Harbin Institute of Technology, Harbin, China; Intelligent Robot Laboratory, Tianjin University of Technology, Tianjin, China; Tianjin Key Laboratory for Control Theory and Application in Complicated Systems, Tianjin International Joint Research and Development Center, Tianjin University of Technology, Tianjin, China; Department of Intelligent Mechanical Systems Engineering, Kagawa University, Takamatsu, Japan","IEEE Sensors Journal","19 Oct 2021","2021","21","20","23471","23479","Since the hard of hearing cannot communicate effectively with the non-disabled, which may cause various inconveniences. As an essential member of a harmonious society, it is particularly urgent to solve their communication problems with non-disabled people. Effective communication between the hard of hearing and the non-disabled has become possible with the continuous development of artificial intelligence. In this paper, an intelligent human-computer interaction system is designed to solve communication inconvenience between the hard of hearing and the non-disabled. This system combines artificial intelligence with wearable devices and classifies gestures with BP neural network, effectively solving the communication problem between the hard of hearing and the non-disabled.","1558-1748","","10.1109/JSEN.2021.3107949","Natural Science Foundation of Tianjin(grant numbers:18JCYBJC43200); Tianjin Key Laboratory for Control Theory and Application in Complicated Systems(grant numbers:TJKL-CTACS-201903); Innovative Cooperation Project of Tianjin Scientific and Technological Support(grant numbers:18PTZWHZ00090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522116","Human-computer interaction;neural networks;gesture recognition;data gloves","Auditory system;Sensors;Bending;Neural networks;Human computer interaction;Gesture recognition;Assistive technology","","10","","30","IEEE","25 Aug 2021","","","IEEE","IEEE Journals"
"A Systematic Review of Machine Learning Approaches for Classifying Indian Sign Language Gestures and Facial Expressions","M. S. Bajwa; G. C. Gandhi","Department of Computer Science and Engineering, Poornima University, Jaipur, Rajasthan, India; Department of Computer Science and Engineering, Poornima University, Jaipur, Rajasthan, India","2022 4th International Conference on Inventive Research in Computing Applications (ICIRCA)","29 Dec 2022","2022","","","848","853","Indian Sign Language is the primary mode of communication known to persons who use Indian Sign Language for people who have hearing or language deficits. Different types of machine learning models are used to broaden the scope of communication for those with impairments and illiteracy. There are numerous machine learning models for analyzing gestures, postures, and facial recognition in Indian Sign Language for single-handed and double-handed signals. The present study on hand gestures, recognition, and translation intends to build an essential foundation for developing a platform to facilitate communication for the s pecially-abled with anyone. Machine learning algorithms generally focus on letter recognition or a few fundamental indicators. Communication is essential for exchanging ideas, thoughts, and feelings. Sign language is a kind of communication that uses hand motions. This is aimed toward those with impairments such as muteness and deafness. Machine learning, a branch of artificial intelligence, will aid in identifying various hand motions and predicting the language created by those inputs based on those inputs[2]. Sign language has a grammar that is unique from and independent of English. When compared to English, SL allows for far more freedom in word order. Tense is marked morphologically on verbs in English, but SL (like many other languages, such as Indian Sign Language) communicates tense lexically using temporal adverbs. The structure of ISL and English differs at the phonological level as well. Signed languages, like spoken languages, include a degree of sublexical structure that includes segments and combinatorial rules; however, phonological elements are manual rather than vocal. The way spatial information is conveyed in English and ISL differs substantially. All Deaf people are illiterate in written English. As an output, the SL text can be produced. SL is just physically executed English, where English and SL share the identical linguistic structure-that one is a straight encoding of the other. Many software designers mistakenly believe that deaf users can always access printed the English language in a user interface. Many designers feel that if auditory information is also supplied as written English, the deaf user's demands will be addressed. Prepositions such as “in,” “on,” and “under” are used to indicate locative information in English, as in many other spoken languages. On the other hand, SL encodes locative and motion information via verbal classifier formulations in which hand shape morphemes define item type, and the location of the hands in signing space schematically depicts the spatial relationship between two things. Thus, English and ASL differ significantly in phonological, morphological, and syntactic areas.","","978-1-6654-9707-7","10.1109/ICIRCA54612.2022.9985747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985747","Machine Learning;Image processing;Computer Vision;Indian Sign Language;Gesture recognition","Image segmentation;Systematics;Shape;Face recognition;Gesture recognition;Machine learning;Assistive technologies","","","","50","IEEE","29 Dec 2022","","","IEEE","IEEE Conferences"
"Intelligent Learning Systems for Inclusive Education in Ghana: Towards an Effective Engagement with Hard of Hearing Students","P. M. Abosi; H. Emereole; D. E. Adjepon-Yamoah","Computer Science and Information Systems Department, Ashesi University, Ghana; Computer Science and Information Systems Department, Ashesi University, Ghana; Computer Science and Information Systems Department, Ashesi University, Ghana","2022 IEEE/IET International Utility Conference and Exposition (IUCE)","29 Mar 2023","2022","","","1","6","This research attempts to address some challenges faced when facilitating an inclusive classroom experience for Deaf/Hard of Hearing students (D/HH) in Ghanaian schools and universities by proposing an intelligent learning system. Hence, this research contributes an Intelligent and Inclusive Learning System (IILS) with two main components: signWithMe subsystem and audio-visual transcription system (AVTS) subsystem.The signWithMe subsystem presents a Ghanaian sign language learning management system (LMS) that is motivated by the limited sign language literacy in Ghana. This subsystem consists of a sign language dictionary, an E-resource, an E-forum, and an E-payment system. It supports inclusive learning of the Ghanaian sign language for both Deaf and Non-Deaf students to interact with the system. This ensures improved diversity and inclusion in our Ghanaian society. Also, the AVTS focuses on improving the teaching and learning experience of Deaf students in Ghanaian schools. The AVTS contributes a novel artificial intelligence approach that uses a counter-checking of Google speech-to-text and lip-reading-to-text transcriptions of the Ghanaian sign language in inclusive university lectures. These two subsystems provide services in line with the mandate of the tenth and fourth Sustainable Development Goal (S.D.G.) of “Reduced Inequality” and “Quality Education” respectively.Finally, the IILS was analysed per the two main components (i.e., signWithMe and AVTS) and are considered to have high level of usability by 20 Deaf and 5 Non-Deaf users.","","978-1-6654-5551-0","10.1109/IUCE55902.2022.10079372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10079372","Inclusive education;Deaf/Hard of Hearing;Sign Language;E-learning system;Text-to-speech;Transcription;Translation","Learning management systems;Electronic learning;Dictionaries;Education;Gesture recognition;Auditory system;Assistive technologies","","","","23","IEEE","29 Mar 2023","","","IEEE","IEEE Conferences"
"Personalization of Hearing Aid Compression by Human-in-the-Loop Deep Reinforcement Learning","N. Alamdari; E. Lobarinas; N. Kehtarnavaz","Electrical and Computer Engineering Department, The University of Texas at Dallas, Richardson, TX, USA; Callier Center for Communication Disorders, The University of Texas at Dallas, Richardson, TX, USA; Electrical and Computer Engineering Department, The University of Texas at Dallas, Richardson, TX, USA","IEEE Access","17 Nov 2020","2020","8","","203503","203515","Existing prescriptive compression strategies used in hearing aid fitting are designed based on gain averages from a group of users which may not be necessarily optimal for a specific user. Nearly half of hearing aid users prefer settings that differ from the commonly prescribed settings. This paper presents a human-in-the-loop deep reinforcement learning approach that personalizes hearing aid compression to achieve improved hearing perception. The developed approach is designed to learn a specific user's hearing preferences in order to optimize compression based on the user's feedbacks. Both simulation and subject testing results are reported. These results demonstrate the proof-of-concept of achieving personalized compression via human-in-the-loop deep reinforcement learning.","2169-3536","","10.1109/ACCESS.2020.3035728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9247199","Personalized audio compression;deep reinforcement learning;human-in-the-loop personalization;personalized hearing aid;hearing aid compression","Fitting;Reinforcement learning;Auditory system;Hearing aids;Testing","","15","","44","CCBY","3 Nov 2020","","","IEEE","IEEE Journals"
"Artificial neural network (ANN) prediction for noise risk assessment in industrial workplace","A. Shamsul; N. B. Ramli","Department of Mechanical Engineering, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Department of Mechanical Engineering, International Islamic University Malaysia, Kuala Lumpur, Malaysia","8th International Conference on Mechatronics Engineering (ICOM 2022)","28 Nov 2022","2022","2022","","143","147","Presently, it is estimated that 16 % of the disabling hearing loss in adults is attributed to occupational noise, ranging from 7 to 21 % in the various countries. With the rise of industrialization, sound pollution is set to increase every 10 years. To alleviate this, the Department of Occupational Safety and Health (DOSH) made it mandatory for industrial workers to undergo annual audiometric testing. However, it is still not enough as hearing loss isn't an annual occasion and hearing damage can occur either instantly or it progresses over time. This could lead to workers prolonging and progressing their hearing damage until the next year. Hence, the purpose of this study is to produce a predictive model to assess the risks of hearing loss for workers in the industrial workplace. This study devised artificial neural network prediction models to predict the level of hearing loss in an employee's left and right ears respectively, based on past audiometric data from The National Institute for Occupational Safety and Health (NIOSH). Once the model is complete, variables such as the sample size, batch size and hidden layers were manipulated to view its effect on the accuracy of the model. The prediction model yielded an overall accuracy of 92% with a range of 78% and 98% when predicting the individual classes of hearing loss and found that increasing certain parameters influences the accuracy of the prediction model. This study utilized 5000 data sample to prove that noise risk assessment can be done in a larger scale using artificial intelligence.","","978-1-83953-806-3","10.1049/icp.2022.2280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964171","","","","","","","","28 Nov 2022","","","IET","IET Conferences"
