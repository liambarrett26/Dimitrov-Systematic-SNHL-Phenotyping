title,abstract,database,database_id,doi
analysis of the audiogram shape in patients with idiopathic sudden sensorineural hearing loss using a cluster analysis,"281. Ear Nose Throat J. 2018 Jul;97(7):E36-E40. doi: 10.1177/014556131809700706.Analysis of the audiogram shape in patients with idiopathic sudden sensorineural hearing loss using a cluster analysis.Watanabe T(1), Suzuki M.Author information:(1)Department of Otolaryngology, Head and Neck Surgery, Oita University Faculty of Medicine, 1-1, Idaigoka, Hasama-machi, Yufu-city, Oita 879-5593, Japan. twatanab@med.oita-u.ac.jp.We performed a cluster analysis to classify the audiogram shape in patients with idiopathic sudden sensorineural hearing loss (ISSNHL). We also investigated whether the audiogram shape is a prognostic indicator in the management of ISSNHL. A total of 115 inpatients with ISSNHL treated between 2001 and 2010 were analyzed. The data collected included age, sex, duration of hearing loss at the time of treatment, and the presence or absence of tinnitus, vertigo, diabetes, nystagmus, and canal paresis. A hierarchical cluster analysis was performed using the hearing threshold for each frequency on audiograms as variables. A logistic regression model was used for the prognostic analysis. The audiogram shape was classified into four clusters: (1) crossing horizontally pattern of all tones; (2) up-sloping pattern of low-tone loss; (3) deaf pattern; and (4) down-sloping pattern of high-tone loss. The age of the patient, presence of canal paresis, and audiogram shape showed statistically significant relationships with hearing improvement. The audiogram shape based on the cluster analysis demonstrated a significant relationship with hearing improvement in patients with ISSNHL. Further studies are needed to elucidate the underlying etiology of each audiogram shape.DOI: 10.1177/014556131809700706",pubmed,30036445,10.1177/014556131809700706
a novel unsupervised spectral clustering for puretone audiograms towards hearing aid filter bank design and initial configurations,"The current practice of adjusting hearing aids (HA) is tiring and time-consuming for both patients and audiologists. Of hearing-impaired people, 40–50% are not satisfied with their HAs. In addition, good designs of HAs are often avoided since the process of fitting them is exhausting. To improve the fitting process, a machine learning (ML) unsupervised approach is proposed to cluster the pure-tone audiograms (PTA). This work applies the spectral clustering (SP) approach to group audiograms according to their similarity in shape. Different SP approaches are tested for best results and these approaches were evaluated by Silhouette, Calinski-Harabasz, and Davies-Bouldin criteria values. Kutools for Excel add-in is used to generate audiograms’ population, annotated using the results from SP, and different criteria values are used to evaluate population clusters. Finally, these clusters are mapped to a standard set of audiograms used in HA characterization. The results indicated that grouping the data in 8 groups or 10 results in ones with high evaluation criteria. The evaluation for population audiograms clusters shows good performance, as it resulted in a Silhouette coefficient >0.5. This work introduces a new concept to classify audiograms using an ML algorithm according to the audiograms’ similarity in shape. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",scopus,2-s2.0-85122034103,10.3390/app12010298
audiometric phenotypes of noiseinduced hearing loss by datadriven cluster analysis and their relevant characteristics,"784. Front Med (Lausanne). 2021 Mar 25;8:662045. doi: 10.3389/fmed.2021.662045. eCollection 2021.Audiometric Phenotypes of Noise-Induced Hearing Loss by Data-Driven Cluster Analysis and Their Relevant Characteristics.Wang Q(1)(2)(3), Qian M(1)(2)(3), Yang L(1)(4), Shi J(1)(4), Hong Y(1)(4), Han K(1)(2)(3), Li C(5), Lin J(5), Huang Z(1)(2)(3)(4), Wu H(1)(2)(3).Author information:(1)Department of Otolaryngology-Head and Neck Surgery, Ninth People's Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, China.(2)Ear Institute, Shanghai Jiao Tong University School of Medicine, Shanghai, China.(3)Shanghai Key Laboratory of Translational Medicine on Ear and Nose Diseases, Shanghai, China.(4)Hearing and Speech Center, Ninth People's Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, China.(5)Network and Information Center, Shanghai Jiao Tong University, Shanghai, China.Background: The definition of notched audiogram for noise-induced hearing loss (NIHL) is presently based on clinical experience, but audiometric phenotypes of NIHL are highly heterogeneous. The data-driven clustering of subtypes could provide refined characteristics of NIHL, and help identify individuals with typical NIHL at diagnosis. Methods: This cross-sectional study initially recruited 12,218 occupational noise-exposed employees aged 18-60 years from two factories of a shipyard in Eastern China. Of these, 10,307 subjects with no history of otological injurie or disease, family history of hearing loss, or history of ototoxic drug use were eventually enrolled. All these subjects completed health behavior questionnaires, cumulative noise exposure (CNE) measurement, and pure-tone audiometry. We did data-driven cluster analysis (k-means clustering) in subjects with hearing loss audiograms (n = 6,599) consist of two independent datasets (n = 4,461 and n = 2,138). Multinomial logistic regression was performed to analyze the relevant characteristics of subjects with different audiometric phenotypes compared to those subjects with normal hearing audiograms (n = 3,708). Results: A total of 10,307 subjects (9,165 males [88.9%], mean age 34.5 [8.8] years, mean CNE 91.2 [22.7] dB[A]) were included, 3,708 (36.0%) of them had completely normal hearing, the other 6,599 (64.0%) with hearing loss audiograms were clustered into four audiometric phenotypes, which were replicable in two distinct datasets. We named the four clusters as the 4-6 kHz sharp-notched, 4-6 kHz flat-notched, 3-8 kHz notched, and 1-8 kHz notched audiogram. Among them, except for the 4-6 kHz flat-notched audiogram which was not significantly related to NIHL, the other three phenotypes with different relevant characteristics were strongly associated with noise exposure. In particular, the 4-6 kHz sharp-notched audiogram might be a typical subtype of NIHL. Conclusions: By data-driven cluster analysis of the large-scale noise-exposed population, we identified three audiometric phenotypes associated with distinct NIHL subtypes. Data-driven sub-stratification of audiograms might eventually contribute to the precise diagnosis and treatment of NIHL.Copyright © 2021 Wang, Qian, Yang, Shi, Hong, Han, Li, Lin, Huang and Wu.DOI: 10.3389/fmed.2021.662045PMCID: PMC8027076",pubmed,33842516,10.3389/fmed.2021.662045
using cluster analysis to classify audiogram shapes references,"The purpose of this study was to design a statistical classification system of audiogram shapes in order to improve and integrate shape recognition across clinical settings. The study included 1633 adult subjects with normal hearing or symmetric sensorineural hearing impairment who underwent pure-tone audiometry between July 2007 and December 2008. K-means cluster analysis was employed to categorize audiometric shapes. Eleven audiogram shapes were identified: rising, flat, peaked 8-kHz dip, 4-kHz dip, 8-kHz dip, mild sloping, severe 8-kHz dip, sloping, abrupt loss, severe sloping, and profound abrupt loss. By using the classification system and nomenclature identified for audiogram shapes as outlined in this study, errors based on personal experiences can be reduced and a consistency can be developed across clinics. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc9&DO=10.3109%2f14992021003796887
datadriven segmentation of audiometric phenotypes across a large clinical cohort,"178. Sci Rep. 2020 Apr 21;10(1):6704. doi: 10.1038/s41598-020-63515-5.Data-driven segmentation of audiometric phenotypes across a large clinical cohort.Parthasarathy A(1)(2), Romero Pinto S(3), Lewis RM(3)(4), Goedicke W(3), Polley DB(3)(5).Author information:(1)Eaton-Peabody Laboratories, Department of Otolaryngology - Head and Neck Surgery, Massachusetts Eye and Ear, Boston, MA, 02114, USA. Aravindakshan_Parthasarathy@meei.harvard.edu.(2)Department of Otolaryngology - Head and Neck Surgery, Harvard Medical School, Boston, MA, 02114, USA. Aravindakshan_Parthasarathy@meei.harvard.edu.(3)Eaton-Peabody Laboratories, Department of Otolaryngology - Head and Neck Surgery, Massachusetts Eye and Ear, Boston, MA, 02114, USA.(4)National Military Audiology and Speech Pathology Center, Walter Reed National Military Medical Center, Bethesda, MD, 20889, USA.(5)Department of Otolaryngology - Head and Neck Surgery, Harvard Medical School, Boston, MA, 02114, USA.Pure tone audiograms are used to assess the degree and underlying source of hearing loss. Audiograms are typically categorized into a few canonical types, each thought to reflect distinct pathologies of the ear. Here, we analyzed 116,400 patient records from our clinic collected over a 24-year period and found that standard categorization left 46% of patient records unclassified. To better account for the full spectrum of hearing loss profiles, we used a Gaussian Mixture Model (GMM) to segment audiograms without any assumptions about frequency relationships, interaural symmetry or etiology. The GMM converged on ten types, featuring varying degrees of high-frequency hearing loss, flat loss, mixed loss, and notched profiles, with predictable relationships to patient age and sex. A separate GMM clustering of 15,380 audiograms from the National Health and Nutrition Examination Survey (NHANES) identified six similar types, that only lacked the more extreme hearing loss configurations observed in our patient cohort. Whereas traditional approaches distill hearing loss configurations down to a few canonical types by disregarding much of the underlying variability, an objective probabilistic model that accounted for all of the data identified an organized, but more heterogenous set of audiogram types that was consistent across two large clinical databases.DOI: 10.1038/s41598-020-63515-5PMCID: PMC7174357",pubmed,32317648,10.1038/s41598-020-63515-5
a novel method for audiogram digitization in audiological reports,"An audiogram records the hearing status, including each hearing threshold at multiple frequencies. While deep learning is gradually maturing in the clinical research approach, audiologists could speed up their diagnostic process with audiogram digitization for handwritten graphs or electronically generated images from instruments. However, given the diversity of audiogram symbols and formats, the existing audiogram digitization model has room for improvement in recognition accuracy. We propose a multi-stage workflow to enhance accuracy by integrating YOLOv5 and the optical character recognition (OCR) model. Our proposed audiogram digitization model could identify all audiogram symbols with an accuracy rate of 98%. We hope that this model could help future research in audiology.",ieee,2169-3536,10.1109/ACCESS.2024.3375362
design and implementation of a portable automated audiometer for hearing classification using machine learning approaches,"Audiometric tests can identify the hearing loss at specific frequencies using the audiogram. The aim and objectives of the study were (i) to develop an automated audiometer for self-diagnosing the hearing ability of the patient; (ii) to extract the features from the acoustic signals and to classify the normal and profound hearing loss patients using different machine learning algorithms; (iii) to validate the hearing loss classification using six-frequency average (6-FA) method based on simple linear regression analysis and machine learning algorithms. The study is conducted among 150 patients, including 75 patients with normal hearing ability and 75 patients with profound hearing loss. The total population of 150 underwent audiometric test both in the soundproof audiometric room and in the normal field environment. Based on the patient response, the intensity and frequency are changed automatically, and the audiogram is plotted by the principle of Artificial Neural Network learning procedures. The overall accuracy produced by classification of normal and profound hearing loss patients using Support Vector Machine (SVM), k-Nearest Neighbor classifier, and Naïve Bayes classifier is 97%, 96%, and 95%, respectively. The results indicated that the SVM classifier outperforms the other two classifiers well. The preliminary audiometric test can be performed remotely and then consulted with an audiologist. Thus, the patient could operate the developed prototype independently and get a consultation from trained medical personnel.  © 2022 National Taiwan University.",scopus,2-s2.0-85132537674,10.4015/S1016237222500351
a novel method for classifying hearing impairment in epidemiological studies of aging the wisconsin agerelated hearing impairment classification scale references,"Purpose: Longitudinal population-based cohort data were used to develop a standardized classification system for age-related hearing impairment using thresholds for frequencies (0.5-8 kHz) typically measured in cohort studies. Method: Audiometric testing data collected in the Epidemiology of Hearing Loss Study from participants (n = 1,369) with four visits (1993-1995, 1998-2000, 2003-2005, and 2009-2010) were included (10,952 audiograms). Cluster analyses (Wald's method) were used to identify audiometric patterns. Maximum allowable threshold values were defined for each cluster to create an ordered scale. Progression was defined as a two-step change. Results: An eight-step scale was developed to capture audiogram shape and severity of hearing impairment. Of the 1,094 participants classified as having normal hearing based on a pure-tone average, only 25% (n = 277) were classified as Level 1 (all thresholds <= 20 dB HL) on the new scale, whereas 17% (n = 182) were Levels 4-6. During the 16-year follow-up, 64.9% of those at Level 1 progressed. There was little regression using this scale. Conclusions: This is the first scale developed from population-based longitudinal cohort data to capture audiogram shape across time. This simple, standardized scale is easy to apply, reduces misclassification of normal hearing, and may be a useful method for identifying risk factors for early, preclinical, age-related changes in hearing. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc19&DO=10.1044%2f2019_AJA-19-00021
a software tool for puretone audiometry classification of audiograms for inclusion of patients in clinical trials english version softwaretool fr reintonaudiometrie klassifizierung von audiogrammen fr den einschluss von patienten in klinische studien englische version,"Objective: Selecting subjects for clinical trials on hearing loss therapies relies on the patient meeting the audiological inclusion criteria. In studies on the treatment of idiopathic sudden sensorineural hearing loss, the patient’s acute audiogram is usually compared with a previous audiogram, the audiogram of the non-affected ear, or a normal audiogram according to an ISO standard. Generally, many more patients are screened than actually fulfill the particular inclusion criteria. The inclusion criteria often require a calculation of pure-tone averages, selection of the most affected frequencies, and calculation of hearing loss differences. Materials and methods: A software tool was developed to simplify and accelerate this inclusion procedure for investigators to estimate the possible recruitment rate during the planning phase of a clinical trial and during the actual study. This tool is Microsoft Excel-based and easy to modify to meet the particular inclusion criteria of a specific clinical trial. The tool was retrospectively evaluated on 100 patients with acute hearing loss comparing the times for classifying automatically and manually. The study sample comprised 100 patients with idiopathic sudden sensorineural hearing loss. Results and conclusion: The age- and sex-related normative audiogram was calculated automatically by the tool and the hearing impairment was graded. The estimated recruitment rate of our sample was quickly calculated. Information about meeting the inclusion criteria was provided instantaneously. A significant reduction of 30 % in the time required for classifying (30 s per patient) was observed. © 2015, The Author(s).",scopus,2-s2.0-84948131830,10.1007/s00106-015-0089-3
audiogram estimation using bayesian active learning,"Two methods for estimating audiograms quickly and accurately using Bayesian active learning were developed and evaluated. Both methods provided an estimate of threshold as a continuous function of frequency. For one method, six successive tones with decreasing levels were presented on each trial and the task was to count the number of tones heard. A Gaussian Process was used for classification and maximum-information sampling to determine the frequency and levels of the stimuli for the next trial. The other method was based on a published method using a Yes/No task but extended to account for lapses. The obtained audiograms were compared to conventional audiograms for 40 ears, 19 of which were hearing impaired. The threshold estimates for the active-learning methods were systematically from 2 to 4 dB below (better than) those for the conventional audiograms, which may indicate a less conservative response criterion (a greater willingness to respond for a given amount of sensory information). Both active-learning methods were able to allow for wrong button presses (due to lapses of attention) and provided accurate audiogram estimates in less than 50 trials or 4 min. For a given level of accuracy, the counting task was slightly quicker than the Yes/No task. © 2018 Acoustical Society of America.",scopus,2-s2.0-85051116924,10.1121/1.5047436
classifying human audiometric phenotypes of agerelated hearing loss from animal models,"131. J Assoc Res Otolaryngol. 2013 Oct;14(5):687-701. doi: 10.1007/s10162-013-0396-x. Epub 2013 Jun 6.Classifying human audiometric phenotypes of age-related hearing loss from animal models.Dubno JR(1), Eckert MA, Lee FS, Matthews LJ, Schmiedt RA.Author information:(1)Department of Otolaryngology-Head and Neck Surgery, Medical University of South Carolina, 135 Rutledge Avenue, MSC 550, Charleston, SC, 29425-5500, USA, dubnojr@musc.edu.Age-related hearing loss (presbyacusis) has a complex etiology. Results from animal models detailing the effects of specific cochlear injuries on audiometric profiles may be used to understand the mechanisms underlying hearing loss in older humans and predict cochlear pathologies associated with certain audiometric configurations (""audiometric phenotypes""). Patterns of hearing loss associated with cochlear pathology in animal models were used to define schematic boundaries of human audiograms. Pathologies included evidence for metabolic, sensory, and a mixed metabolic + sensory phenotype; an older normal phenotype without threshold elevation was also defined. Audiograms from a large sample of older adults were then searched by a human expert for ""exemplars"" (best examples) of these phenotypes, without knowledge of the human subject demographic information. Mean thresholds and slopes of higher frequency thresholds of the audiograms assigned to the four phenotypes were consistent with the predefined schematic boundaries and differed significantly from each other. Significant differences in age, gender, and noise exposure history provided external validity for the four phenotypes. Three supervised machine learning classifiers were then used to assess reliability of the exemplar training set to estimate the probability that newly obtained audiograms exhibited one of the four phenotypes. These procedures classified the exemplars with a high degree of accuracy; classifications of the remaining cases were consistent with the exemplars with respect to average thresholds and demographic information. These results suggest that animal models of age-related hearing loss can be used to predict human cochlear pathology by classifying audiograms into phenotypic classifications that reflect probable etiologies for hearing loss in older humans.DOI: 10.1007/s10162-013-0396-xPMCID: PMC3767874",pubmed,23740184,10.1007/s10162-013-0396-x
how much individualization is required to predict the individual effect of suprathreshold processing deficits assessing plomps distortion component with psychoacoustic detection thresholds and fade references,"Plomp introduced an empirical separation of the increased speech recognition thresholds (SRT) in listeners with a sensorineural hearing loss into an Attenuation (A) component (which can be compensated by amplification) and a non-compensable Distortion (D) component. Previous own research backed up this notion by speech recognition models that derive their SRT prediction from the individual audiogram with or without a psychoacoustic measure of suprathreshold processing deficits. To determine the precision in separating the A and D component for the individual listener with various individual measures and individualized models, SRTs with 40 listeners with a variation in hearing impairment were obtained in quiet, stationary noise, and fluctuating noise (ICRA 5-250 and babble). Both the clinical audiogram and an adaptive, precise sweep audiogram were obtained as well as tone-in-noise detection thresholds at four frequencies to characterize the individual hearing impairment. For predicting the SRT, the FADE-model (which is based on machine learning) was used with either of the two audiogram procedures and optionally the individual tone-in-noise detection thresholds. The results indicate that the precisely measured swept tone audiogram allows for a more precise prediction of the individual SRT in comparison to the clinical audiogram (RMS error of 4.3 dB vs. 6.4 dB, respectively). While an estimation from the precise audiogram and FADE performed equally well in predicting the individual A and D component, the further refinement of including the tone-in-noise detection threshold with FADE led to a slight improvement of prediction accuracy (RMS error of 3.3 dB, 4.6 dB and 1.4 dB, for SRT, A and D component, respectively). Hence, applying FADE is advantageous for scientific purposes where a consistent modeling of different psychoacoustical effects in the same listener with a minimum amount of assumptions is desirable. For clinical purposes, however, a precisely measured audiogram and an estimation of the expected D component using a linear regression appears to be a satisfactory first step towards precision audiology. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc22&DO=10.1016%2fj.heares.2022.108609
autoaudio deep learning for automatic audiogram interpretation,"Hearing loss is the leading human sensory system loss, and one of the leading causes for years lived with disability with significant effects on quality of life, social isolation, and overall health. Coupled with a forecast of increased hearing loss burden worldwide, national and international health organizations have urgently recommended that access to hearing evaluation be expanded to meet demand. The objective of this study was to develop 'AutoAudio' – a novel deep learning proof-of-concept model that accurately and quickly interprets diagnostic audiograms. Adult audiogram reports representing normal, conductive, mixed and sensorineural morphologies were used to train different neural network architectures. Image augmentation techniques were used to increase the training image set size. Classification accuracy on a separate test set was used to assess model performance. The architecture with the highest out-of-training set accuracy was ResNet-101 at 97.5%. Neural network training time varied between 2 to 7 h depending on the depth of the neural network architecture. Each neural network architecture produced misclassifications that arose from failures of the model to correctly label the audiogram with the appropriate hearing loss type. The most commonly misclassified hearing loss type were mixed losses. Re-engineering the process of hearing testing with a machine learning innovation may help enhance access to the growing worldwide population that is expected to require audiologist services. Our results suggest that deep learning may be a transformative technology that enables automatic and accurate audiogram interpretation.",cinahl,1485598,10.1007/s10916-020-01627-1
mining audiograms to improve the interpretability of automated audiometry measurements,"Many people with hearing loss are unaware of it and do not seek benefit from available interventions such as hearing aids. This is in part due to the limited accessibility to qualified hearing healthcare providers in developing and developed countries alike. Automated audiometry, which has gained in popularity amidst the torrent of advances in telemedicine and mobile health, makes it possible to deliver basic hearing tests to remote or otherwise underserved communities at low cost. While this technology makes it possible to perform hearing assessments outside of a sound booth, many individuals administering the test are non-specialists, and thus, have a limited ability to interpret audiometric measurements and to make tailored recommendations. In this paper, we present the first steps towards the development of a flexible, supervised learning approach for the classification of audiograms in terms of their shape, severity and symmetry. More specifically, we outline our approach to building a set of non-redundant, annotation-ready audiograms from a much larger dataset. In addition, we present a Rapid Audiogram Annotation Environment (RAAE) designed specifically for the collection of audiogram annotations from a large community of expert audiologists. Preliminary results indicate that annotations provided through our environment are consistent leading to low intra-coder variability. Data gathered through the RAAE will form the basis of learning algorithms to help non-experts make better decisions from audiometric data.",ieee,,10.1109/MeMeA.2018.8438746
robust machine learning method for imputing missing values in audiograms collected in children,"29. Int J Audiol. 2022 Jan;61(1):66-77. doi: 10.1080/14992027.2021.1884909. Epub 2021 Feb 27.Robust machine learning method for imputing missing values in audiograms collected in children.Pitathawatchai P(1), Chaichulee S(2), Kirtsreesakul V(1).Author information:(1)Department of Otolaryngology Head and Neck Surgery, Faculty of Medicine, Prince of Songkla University, Hat Yai, Thailand.(2)Institute of Biomedical Engineering, Department of Biomedical Sciences and Biomedical Engineering, Faculty of Medicine, Prince of Songkla University, Hat Yai, Thailand.OBJECTIVE: To assess the accuracy and reliability of a machine learning (ML) algorithm for predicting the full audiograms of hearing-impaired children relative to the common approach (CA).DESIGN: Retrospective study.STUDY SAMPLE: There were 206 audiograms included from 206 children with sensorineural hearing loss. Nested cross-validation was used for evaluating the performance of the CA and ML. Six audiogram prediction simulations were performed in which either one or two thresholds across 0.5-4 kHz from complete audiograms in the dataset were labelled. Missing thresholds at the remaining frequencies were then predicted using the CA and ML in each simulation. The accuracy of the ML algorithm was determined by comparing the median average absolute threshold differences between the CA and ML using Wilcoxon signed-rank test. The reliability between runs of the ML was also assessed with Cronbach's alphas.RESULTS: The median average absolute threshold differences in ML (5-8 dBHL) were statistically significantly lower than those in CA (6.25-10 dBHL) in all six simulations (p value < 0.05). The ML algorithm was also found to be reliable to predict the audiograms in all six simulations (α > 0.9).CONCLUSION: Using the ML to predict the children's audiograms was reliable and more accurate than using the CA.DOI: 10.1080/14992027.2021.1884909",pubmed,33641573,10.1080/14992027.2021.1884909
investigation of hearing loss in elderly vertigo and dizziness patients in the past 10 years references,"Background: Vertigo and hearing loss are both prevalent in the elderly. This study retrospectively analyzed hearing test results from elderly patients experiencing vertigo and dizziness at ENT outpatient over a 10-year period, in order to study the patterns of hearing loss in this patient population. Methods: Nine thousand three hundred eighty four patients over 50 years old underwent retrospective collection and screening of outpatient diagnosis, pure tone audiometry, acoustic immittance measurement (tympanogram) and auditory brainstem response (ABR) test. The patient's audiograms are divided into 7 subtypes according to a set of fixed criteria. Meanwhile, K-Means clustering analysis method was used to classify the audiogram. Results: The Jerger classification of tympanogram in elderly patients with vertigo and dizziness showed the majority falling under type A. The leading audiogram shapes were flat (27.81% in right ear and 26.89% in left ear), high-frequency gently sloping (25.97% in right ear and 27.34% in left ear), and high-frequency steeply sloping (21.60% in right ear and 22.53% in left ear). Meniere's disease (MD; 30.87%), benign recurrent vertigo (BRV; 19.07%), and benign paroxysmal positional vertigo (BPPV; 15.66%) were the most common etiologies in elderly vestibular diseases. We observed statistically significant differences in hearing thresholds among these vestibular diseases (P < 0.001). K-Means clustering analysis suggested that the optimal number of clusters was three, with sample sizes for the three clusters being 2,747, 2,413, and 4,139, respectively. The ANOVA statistical results of each characteristic value showed P < 0.001. Conclusion: The elderly patients often have mild to moderate hearing loss as a concomitant symptom with vertigo. Female patients have better hearing thresholds than males. The dominant audiometric shapes in this patient population were flat, high-frequency gently sloping, and high-frequency steeply sloping according to a set of fixed criteria. This study highlights the need for tailored strategies in managing hearing loss in elderly patients with vertigo and dizziness. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc22&DO=10.3389%2ffnagi.2023.1225786
data mining of audiology patient records factors influencing the choice of hearing aid type,"Background: This paper describes the analysis of a database of over 180,000 patient records, collected from over 23,000 patients, by the hearing aid clinic at James Cook University Hospital in Middlesbrough, UK. These records consist of audiograms (graphs of the faintest sounds audible to the patient at six different pitches), categorical data (such as age, gender, diagnosis and hearing aid type) and brief free text notes made by the technicians. This data is mined to determine which factors contribute to the decision to fit a BTE (worn behind the ear) hearing aid as opposed to an ITE (worn in the ear) hearing aid.Methods: From PCA (principal component analysis) four main audiogram types are determined, and are related to the type of hearing aid chosen. The effects of age, gender, diagnosis, masker, mould and individual audiogram frequencies are combined into a single model by means of logistic regression. Some significant keywords are also discovered in the free text fields by using the chi-squared (χ(2)) test, which can also be used in the model. The final model can act a decision support tool to help decide whether an individual patient should be offered a BTE or an ITE hearing aid.Results: The final model was tested using 5-fold cross validation, and was able to replicate the decisions of audiologists whether to fit an ITE or a BTE hearing aid with precision in the range 0.79 to 0.87.Conclusions: A decision support system was produced to predict the type of hearing aid which should be prescribed, with an explanation facility explaining how that decision was arrived at. This system should prove useful in providing a ""second opinion"" for audiologists.",cinahl,14726947,10.1186/1472-6947-12-S1-S6
using machine learning and the national health and nutrition examination survey to classify individuals with hearing loss,"739. Front Digit Health. 2021 Aug 18;3:723533. doi: 10.3389/fdgth.2021.723533. eCollection 2021.Using Machine Learning and the National Health and Nutrition Examination Survey to Classify Individuals With Hearing Loss.Ellis GM(1), Souza PE(1)(2).Author information:(1)Department of Communication Sciences and Disorders, Northwestern University, Evanston, IL, United States.(2)Knowles Hearing Center, Evanston, IL, United States.Even before the COVID-19 pandemic, there was mounting interest in remote testing solutions for audiology. The ultimate goal of such work was to improve access to hearing healthcare for individuals that might be unable or reluctant to seek audiological help in a clinic. In 2015, Diane Van Tasell patented a method for measuring an audiogram when the precise signal level was unknown (patent US 8,968,209 B2). In this method, the slope between pure-tone thresholds measured at 2 and 4 kHz is calculated and combined with questionnaire information in order to reconstruct the most likely audiograms from a database of options. An approach like the Van Tasell method is desirable because it is quick and feasible to do in a patient's home where exact stimulus levels are unknown. The goal of the present study was to use machine learning to assess the effectiveness of such audiogram-estimation methods. The National Health and Nutrition Examination Survey (NHANES), a database of audiologic and demographic information, was used to train and test several machine learning algorithms. Overall, 9,256 cases were analyzed. Audiometric data were classified using the Wisconsin Age-Related Hearing Impairment Classification Scale (WARHICS), a method that places hearing loss into one of eight categories. Of the algorithms tested, a random forest machine learning algorithm provided the best fit with only a few variables: the slope between 2 and 4 kHz; gender; age; military experience; and self-reported hearing ability. Using this method, 54.79% of the individuals were correctly classified, 34.40% were predicted to have a milder loss than measured, and 10.82% were predicted to have a more severe loss than measured. Although accuracy was low, it is unlikely audibility would be severely affected if classifications were used to apply gains. Based on audibility calculations, underamplification still provided sufficient gain to achieve ~95% correct (Speech Intelligibility Index ≥ 0.45) for sentence materials for 88% of individuals. Fewer than 1% of individuals were overamplified by 10 dB for any audiometric frequency. Given these results, this method presents a promising direction toward remote assessment; however, further refinement is needed before use in clinical fittings.Copyright © 2021 Ellis and Souza.DOI: 10.3389/fdgth.2021.723533PMCID: PMC8521948",pubmed,34713189,10.3389/fdgth.2021.723533
a software tool for puretone audiometry classification of audiograms for inclusion of patients in clinical trials english version,"262. HNO. 2016 Mar;64 Suppl 1:S1-6. doi: 10.1007/s00106-015-0089-3.A software tool for pure‑tone audiometry. Classification of audiograms for inclusion of patients in clinical trials. English version.Rahne T(1), Buthut F(2), Plößl S(2), Plontke SK(2).Author information:(1)Department of Otorhinolaryngology, University Hospital Halle (Saale), Ernst-Grube-Str. 40, 06120, Halle (Saale), Germany. torsten.rahne@uk-halle.de.(2)Department of Otorhinolaryngology, University Hospital Halle (Saale), Ernst-Grube-Str. 40, 06120, Halle (Saale), Germany.OBJECTIVE: Selecting subjects for clinical trials on hearing loss therapies relies on the patient meeting the audiological inclusion criteria. In studies on the treatment of idiopathic sudden sensorineural hearing loss, the patient's acute audiogram is usually compared with a previous audiogram, the audiogram of the non-affected ear, or a normal audiogram according to an ISO standard. Generally, many more patients are screened than actually fulfill the particular inclusion criteria. The inclusion criteria often require a calculation of pure-tone averages, selection of the most affected frequencies, and calculation of hearing loss differences.MATERIALS AND METHODS: A software tool was developed to simplify and accelerate this inclusion procedure for investigators to estimate the possible recruitment rate during the planning phase of a clinical trial and during the actual study. This tool is Microsoft Excel-based and easy to modify to meet the particular inclusion criteria of a specific clinical trial. The tool was retrospectively evaluated on 100 patients with acute hearing loss comparing the times for classifying automatically and manually. The study sample comprised 100 patients with idiopathic sudden sensorineural hearing loss.RESULTS AND CONCLUSION: The age- and sex-related normative audiogram was calculated automatically by the tool and the hearing impairment was graded. The estimated recruitment rate of our sample was quickly calculated. Information about meeting the inclusion criteria was provided instantaneously. A significant reduction of 30 % in the time required for classifying (30 s per patient) was observed.DOI: 10.1007/s00106-015-0089-3PMCID: PMC4819485",pubmed,26607156,10.1007/s00106-015-0089-3
standard audiograms for koreans derived through hierarchical clustering using data from the korean national health and nutrition examination survey 20092012,"471. Sci Rep. 2019 Mar 6;9(1):3675. doi: 10.1038/s41598-019-40300-7.Standard Audiograms for Koreans Derived through Hierarchical Clustering Using Data from the Korean National Health and Nutrition Examination Survey 2009-2012.Chang YS(1), Yoon SH(2), Kim JR(2), Baek SY(3), Cho YS(2), Hong SH(4), Kim S(3), Moon IJ(5).Author information:(1)Department of Otorhinolaryngology - Head and Neck Surgery, Korea University College of Medicine, Korea University Ansan Hospital, Ansan, Republic of Korea.(2)Department of Otorhinolaryngology-Head and Neck Surgery, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul, Korea.(3)Statistics and Data Center, Research Institute for Future Medicine, Samsung Medical Center, Seoul, Korea.(4)Department of Otorhinolaryngology-Head and Neck Surgery, Samsung Changwon Hospital, Sungkyunkwan University School of Medicine, Seoul, Korea.(5)Department of Otorhinolaryngology-Head and Neck Surgery, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul, Korea. moonij@skku.edu.Assessments of standardized region/population-specific audiological characteristics are needed for provision of effective rehabilitative services through reducing costs associated with hearing aids. This study aims to propose a set of standard audiograms representing the Korean population that were derived by analyzing data from the 2009-2012 Korea National Health and Nutrition Examination Survey (KNHANES), a nationwide epidemiologic study conducted by Korean government organizations. Standard audiograms were derived by applying a hierarchical clustering method from recorded audiologic data that were obtained independently at 6 frequencies for each ear: 0.5, 1.0, 2.0, 3.0, 4.0, and 6.0 kHz (in dB HL). To derive the optimal number of clusters of the desired standard audiograms, cubic clustering criterion, pseudo-F-, and pseudo-t2-statistics were calculated. These analyses resulted in 29 clusters representing a standard audiogram of the South Korean population. Eighteen of the clusters represented normal hearing audiograms (73.11%), while 11 represented hearing-impaired (HI) standard audiograms (27.89%). Of the 11 HI audiograms, 7 were defined as flat-type (17.81%), while the remaining 4 were defined as sloping-type (9.08%). In conclusion, 29 audiograms representing standard audiograms for the Korean population have been derived using KNHANES data. Improved understanding of the characteristics of each cluster may be helpful for development of more personalized, fixed-setting hearing aids.DOI: 10.1038/s41598-019-40300-7PMCID: PMC6403394",pubmed,30842521,10.1038/s41598-019-40300-7
applications of machine learning methods in retrospective studies on hearing,"Hearing healthcare professionals rely on the audiograms produced through pure tone audiometry, among other tests, to diagnose and treat hearing loss. Researchers also rely on audiograms to study the prevalence of hearing loss in various populations. Notably, due to the available test time, intraoctave frequencies are not often recorded, even though they can contribute to certain diagnoses. Previous work has proposed the imputation of these thresholds using a simple average of neighboring thresholds. In this work, we present an alternative approach for addressing missing intra-octave thresholds that relies on a $\pmb{k}$ -nearest neighbors algorithm and show that accuracy can be slightly improved using a data-driven approach to imputation. We also present a Gaussian mixture model-based approach to flagging atypical or potentially unreliable audiograms to produce high quality datasets. Our method allows the imputation of intra-octave thresholds with an accuracy no worse than simple averaging. For the more challenging 6000 Hz threshold, our method appears to be particularly effective. Overall, our method allows for improved presentation of complete audiogram datasets. © 2018 IEEE.",scopus,2-s2.0-85060239928,10.1109/LSC.2018.8572268
fast continuous audiogram estimation using machine learning,"277. Ear Hear. 2015 Nov-Dec;36(6):e326-35. doi: 10.1097/AUD.0000000000000186.Fast, Continuous Audiogram Estimation Using Machine Learning.Song XD(1), Wallace BM, Gardner JR, Ledbetter NM, Weinberger KQ, Barbour DL.Author information:(1)1Laboratory of Sensory Neuroscience and Neuroengineering, Department of Biomedical Engineering, Washington University in St. Louis, St. Louis, Missouri, USA; 2Program in Audiology and Communication Sciences, Washington University in St. Louis, St. Louis, Missouri, USA; and 3Department of Computer Science & Engineering, Washington University in St. Louis, St. Louis, Missouri, USA.OBJECTIVES: Pure-tone audiometry has been a staple of hearing assessments for decades. Many different procedures have been proposed for measuring thresholds with pure tones by systematically manipulating intensity one frequency at a time until a discrete threshold function is determined. The authors have developed a novel nonparametric approach for estimating a continuous threshold audiogram using Bayesian estimation and machine learning classification. The objective of this study was to assess the accuracy and reliability of this new method relative to a commonly used threshold measurement technique.DESIGN: The authors performed air conduction pure-tone audiometry on 21 participants between the ages of 18 and 90 years with varying degrees of hearing ability. Two repetitions of automated machine learning audiogram estimation and one repetition of conventional modified Hughson-Westlake ascending-descending audiogram estimation were acquired by an audiologist. The estimated hearing thresholds of these two techniques were compared at standard audiogram frequencies (i.e., 0.25, 0.5, 1, 2, 4, 8 kHz).RESULTS: The two threshold estimate methods delivered very similar estimates at standard audiogram frequencies. Specifically, the mean absolute difference between estimates was 4.16 ± 3.76 dB HL. The mean absolute difference between repeated measurements of the new machine learning procedure was 4.51 ± 4.45 dB HL. These values compare favorably with those of other threshold audiogram estimation procedures. Furthermore, the machine learning method generated threshold estimates from significantly fewer samples than the modified Hughson-Westlake procedure while returning a continuous threshold estimate as a function of frequency.CONCLUSIONS: The new machine learning audiogram estimation technique produces continuous threshold audiogram estimates accurately, reliably, and efficiently, making it a strong candidate for widespread application in clinical and research audiometry.DOI: 10.1097/AUD.0000000000000186PMCID: PMC4709018",pubmed,26258575,10.1097/AUD.0000000000000186
a machine learning based clustering protocol for determining hearing aid initial configurations from puretone audiograms,"779. Interspeech. 2019 Sep;2019:2325-2329. doi: 10.21437/interspeech.2019-3091.A Machine Learning Based Clustering Protocol for Determining Hearing Aid Initial Configurations from Pure-Tone Audiograms.Belitz C(1), Ali H(1), Hansen JHL(1).Author information:(1)CRSS: Center for Robust Speech Systems, The University of Texas at Dallas, Richardson, TX, USA.Of the nearly 35 million people in the USA who are hearing impaired, only an estimated 25% use hearing aids (HA). A good number of HAs are prescribed but not used partially because of the time to convergence for best operation between the audiologist and user. To improve HA retention, it is suggested that a machine learning (ML) protocol could be established which improves initial HA configurations given a user's pure-tone audiogram. This study examines a ML clustering method to predict the best initial HA fitting from a corpus of over 90,000 audiogram-fitting pairs collected from hearing centers throughout the USA. We first examine the final HA comfort targets to determine a limited number of preset configurations using several multi-dimensional clustering methods (Birch, Ward, and k-means). The goal is to reduce the amount of adjustments between the centroid, selected as a fitting configuration to represent the cluster, and the final HA configurations. This may be used to reduce the adjustment cycles for HAs or as preset starting configurations for personal sound amplification products (PSAPs). Using various classification methods, audiograms are mapped to a limited number of potential preset configurations. Finally, the average adjustment between the preset fitting targets and the final fitting targets is examined.DOI: 10.21437/interspeech.2019-3091PMCID: PMC8299699",pubmed,34307641,10.21437/interspeech.2019-3091
determination of patients hearing sensitivity using data mining techniques,"The aim of this paper is to analyse and classify an audiometric dataset belongs to 200 patients using data mining techniques. The classification task consists of the classification the data of each patient as belonging to one out of four categories (conductive type, sensorineural hearing loss, mixed type or normal). These classifications show the diverse kinds of hearing misfortune relying upon which part of the hearing way is influenced. The used dataset provides sensitivity test values of the patient's hearing sense and these tests are mostly done by an audiologist using the audiometer experiments. The audiometric tests are used to define the patient's hearing threshold level at several frequencies. The specialists often try to locate the problem of hearing path where exactly, so this characterizes the hearing problem as having a place with one of the classified groups. There are several new data mining algorithms applied in this research to classify patient hearing system. This automated system will help medical specialists to make decision. Experimental results show that Artificial Immune Recognition System (AIRS), Pruned Tree J48 and Random Forest (RF) techniques have given very promising classification results (between 99-100% accuracy) for hearing loss data set. We have better accuracy results than other traditional machine learning methods for hearing dataset. © 2018 IEEE.",scopus,2-s2.0-85070203066,10.1109/ICAICT.2018.8746924
online machine learning audiometry,"2. Ear Hear. 2019 Jul/Aug;40(4):918-926. doi: 10.1097/AUD.0000000000000669.Online Machine Learning Audiometry.Barbour DL(1), Howard RT(1)(2), Song XD(1), Metzger N(1), Sukesan KA(1)(3), DiLorenzo JC(1)(3), Snyder BRD(1), Chen JY(1), Degen EA(1), Buchbinder JM(1)(2), Heisey KL(1).Author information:(1)Laboratory of Sensory Neuroscience and Neuroengineering, Department of Biomedical Engineering, Washington University in St. Louis, Missouri, USA.(2)Program in Audiology and Communication Sciences, Department of Otolaryngology, Washington University School of Medicine, St. Louis, Missouri, USA.(3)Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, Missouri, USA.OBJECTIVES: A confluence of recent developments in cloud computing, real-time web audio and machine learning psychometric function estimation has made wide dissemination of sophisticated turn-key audiometric assessments possible. The authors have combined these capabilities into an online (i.e., web-based) pure-tone audiogram estimator intended to empower researchers and clinicians with advanced hearing tests without the need for custom programming or special hardware. The objective of this study was to assess the accuracy and reliability of this new online machine learning audiogram method relative to a commonly used hearing threshold estimation technique also implemented online for the first time in the same platform.DESIGN: The authors performed air conduction pure-tone audiometry on 21 participants between the ages of 19 and 79 years (mean 41, SD 21) exhibiting a wide range of hearing abilities. For each ear, two repetitions of online machine learning audiogram estimation and two repetitions of online modified Hughson-Westlake ascending-descending audiogram estimation were acquired by an audiologist using the online software tools. The estimated hearing thresholds of these two techniques were compared at standard audiogram frequencies (i.e., 0.25, 0.5, 1, 2, 4, 8 kHz).RESULTS: The two threshold estimation methods delivered very similar threshold estimates at standard audiogram frequencies. Specifically, the mean absolute difference between threshold estimates was 3.24 ± 5.15 dB. The mean absolute differences between repeated measurements of the online machine learning procedure and between repeated measurements of the Hughson-Westlake procedure were 2.85 ± 6.57 dB and 1.88 ± 3.56 dB, respectively. The machine learning method generated estimates of both threshold and spread (i.e., the inverse of psychometric slope) continuously across the entire frequency range tested from fewer samples on average than the modified Hughson-Westlake procedure required to estimate six discrete thresholds.CONCLUSIONS: Online machine learning audiogram estimation in its current form provides all the information of conventional threshold audiometry with similar accuracy and reliability in less time. More importantly, however, this method provides additional audiogram details not provided by other methods. This standardized platform can be readily extended to bone conduction, masking, spectrotemporal modulation, speech perception, etc., unifying audiometric testing into a single comprehensive procedure efficient enough to become part of the standard audiologic workup.DOI: 10.1097/AUD.0000000000000669PMCID: PMC6476703",pubmed,30358656,10.1097/AUD.0000000000000669
datadriven approach for auditory profiling and characterization of individual hearing loss,"61. Trends Hear. 2018 Jan-Dec;22:2331216518807400. doi: 10.1177/2331216518807400.Data-Driven Approach for Auditory Profiling and Characterization of Individual Hearing Loss.Sanchez Lopez R(1), Bianchi F(1), Fereczkowski M(1), Santurette S(1)(2), Dau T(1).Author information:(1)1 Hearing Systems Group, Department of Electrical Engineering, Technical University of Denmark, Kongens Lyngby, Denmark.(2)2 Department of Otorhinolaryngology, Head and Neck Surgery and Audiology, Rigshospitalet, Copenhagen, Denmark.Pure-tone audiometry still represents the main measure to characterize individual hearing loss and the basis for hearing-aid fitting. However, the perceptual consequences of hearing loss are typically associated not only with a loss of sensitivity but also with a loss of clarity that is not captured by the audiogram. A detailed characterization of a hearing loss may be complex and needs to be simplified to efficiently explore the specific compensation needs of the individual listener. Here, it is hypothesized that any listener's hearing profile can be characterized along two dimensions of distortion: Type I and Type II. While Type I can be linked to factors affecting audibility, Type II reflects non-audibility-related distortions. To test this hypothesis, the individual performance data from two previous studies were reanalyzed using an unsupervised-learning technique to identify extreme patterns in the data, thus forming the basis for different auditory profiles. Next, a decision tree was determined to classify the listeners into one of the profiles. The analysis provides evidence for the existence of four profiles in the data. The most significant predictors for profile identification were related to binaural processing, auditory nonlinearity, and speech-in-noise perception. This approach could be valuable for analyzing other data sets to select the most relevant tests for auditory profiling and propose more efficient hearing-deficit compensation strategies.DOI: 10.1177/2331216518807400PMCID: PMC6236853",pubmed,30384803,10.1177/2331216518807400
dynamically masked audiograms with machine learning audiometry supplement ,,base,aefe6dba96ce365277c0aebe4de3e8eba9332e400686217b6eb41907fa11277e,
classification of hearing aids into feature profiles using hierarchical latent class analysis applied to a large dataset of hearing aids,"317. Ear Hear. 2020 Nov/Dec;41(6):1619-1634. doi: 10.1097/AUD.0000000000000410.Classification of Hearing Aids Into Feature Profiles Using Hierarchical Latent Class Analysis Applied to a Large Dataset of Hearing Aids.Lansbergen S(1), Dreschler WA.Author information:(1)Department of Clinical and Experimental Audiology, Amsterdam University Medical Center, University of Amsterdam, Amsterdam, the Netherlands.OBJECTIVES: We developed a framework for objectively comparing hearing aids, independent of brand, type, or product family. This was done using a large dataset of commercially available hearing aids. To achieve this, we investigated which hearing aid features are suitable for comparison, and are also relevant for the rehabilitation of hearing impairment. To compare hearing aids objectively, we distinguished populations of hearing aids based on a set of key hearing aid features. Finally, we describe these hearing aid subpopulations so that these could potentially be used as a supporting tool for the selection of an appropriate hearing aid.DESIGN: In this study, we used technical (meta-)data from 3911 hearing aids (available on the Dutch market in March 2018). The dataset contained about 50 of the most important characteristics of a hearing aid. After cleaning and handling the data via a well-defined knowledge discovery in database procedure, a total 3083 hearing aids were included. Subsequently, a set of well-defined key hearing aid features were used as input for further analysis. The data were split into an in-the-ear style hearing aid subset and a behind-the-ear style subset, for separate analyses. The knowledge discovery in databases procedure was also used as an objective guiding tool for applying an exploratory cluster analysis to expose subpopulations of hearing aids within the dataset. The latter was done using Latent Class Tree Analysis, which is an extension to the better-known Latent Class Analysis clustering method: with the important addition of a hierarchical structure.RESULTS: A total of 10 hearing aid features were identified as relevant for audiological rehabilitation: compression, sound processing, noise reduction (NR), expansion, wind NR, impulse (noise) reduction, active feedback management, directionality, NR environments, and ear-to-ear communication. These features had the greatest impact on results yielded by the Latent Class Tree cluster analysis. At the first level in the hierarchical cluster model, the two subpopulations of hearing aids could be divided into 3 main branches, mainly distinguishable by the overall availability or technology level of hearing aid features. Higher-level results of the cluster analysis yielded a set of mutually exclusive hearing aid populations, called modalities. In total, nine behind-the-ear and seven in-the-ear modalities were found. These modalities were characterized by particular profiles of (complex) interplay between the selected key features. A technical comparison of features (e.g., implementation) is beyond the scope of this research.CONCLUSIONS: Combining a large dataset of hearing aids with a probabilistic hierarchical clustering method enables analysis of hearing aid characteristics which extends beyond product families and manufacturers. Furthermore, this study found that the resulting hearing aid modalities can be thought of as a generic alternative to the manufacturer-dependent proprietary ""concepts,"" and could potentially aid the selection of an appropriate hearing aid for technical rehabilitation. This study is in line with a growing need for justification of hearing aid selection and the increasing demand for evidence-based practice.DOI: 10.1097/AUD.0000000000000410PMCID: PMC7722464",pubmed,33136637,10.1097/AUD.0000000000000410
improving puretone audiometry using probabilistic machine learning classificationdp   2018,"Hearing loss is a critical public health concern, affecting hundreds millions of people worldwide and dramatically impacting quality of life for affected individuals. While treatment techniques have evolved in recent years, methods for assessing hearing ability have remained relatively unchanged for decades. The standard clinical procedure is the modified Hughson-Westlake procedure, an adaptive pure-tone detection task that is typically performed manually by audiologists, costing millions of collective hours annually among healthcare professionals. In addition to the high burden of labor, the technique provides limited detail about an individual's hearing ability, estimating only detection thresholds at a handful of pre-defined pure-tone frequencies (a threshold audiogram). An efficient technique that produces a detailed estimate of the audiometric function, including threshold and spread, could allow for better characterization of particular hearing pathologies and provide more diagnostic value. Parametric techniques exist to efficiently estimate multidimensional psychometric functions, but are ill-suited for estimation of audiometric functions because these functions cannot be easily parameterized. The Gaussian process is a compelling machine learning technique for inference of nonparametric multidimensional functions using binary data. The work described in this thesis utilizes Gaussian process classification to build an automated framework for efficient, high-resolution estimation of the full audiometric function, which we call the machine learning audiogram (MLAG). This Bayesian technique iteratively computes a posterior distribution describing its current belief about detection probability given the current set of observed pure tones and detection responses. The posterior distribution can be used to provide a current point estimate of the psychometric function as well as to select an informative query point for the next stimulus to be provided to the listener. The Gaussian process covariance function encodes correlations between variables, reflecting prior beliefs on the system; MLAG uses a composite linear/squared exponential covariance function that enforces monotonicity with respect to intensity but only smoothness with respect to frequency for the audiometric function. This framework was initially evaluated in human subjects for threshold audiogram estimation. 2 repetitions of MLAG and 1 repetition of manual clinical audiometry were conducted in each of 21 participants. Results indicated that MLAG both agreed with clinical estimates and exhibited test-retest reliability to within accepted clinical standards, but with significantly fewer tone deliveries required compared to clinical methods while also providing an effectively continuous threshold estimate along frequency. This framework's ability to evaluate full psychometric functions was then evaluated using simulated experiments. As a feasibility check, performance for estimating unidimensional psychometric functions was assessed and directly compared to inference using standard maximum-likelihood probit regression; results indicated that the two methods exhibited near identical performance for estimating threshold and spread. MLAG was then used to estimate 2-dimensional audiometric functions constructed using existing audiogram phenotypes. Results showed that this framework could estimate both threshold and spread of the full audiometric function with high accuracy and reliability given a sufficient sample count; non-active sampling using the Halton set required between 50--100 queries to reach clinical reliability, while active sampling strategies reduced the required number to around 20--30, with Bayesian active leaning by disagreement exhibiting the best performance of the tested methods. Overall, MLAG's accuracy, reliability, and high degree of detail make it a promising... (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc17&AN=2018-09133-026
artificial intelligencebased hearing loss detection using acoustic threshold and speech perception level,"Hearing loss detection using automated audiometers and artificial intelligence methods has gained increasing attention in recent years. The proposed work aims: (a) to design an automated audiometer to diagnose hearing ability and to evaluate hearing intensity for healthy and profound hearing loss patients within 250 Hz to 8 kHz, (b) to compare the proposed automated audiometer with a conventional audiometer when estimating auditory perception level using pure tone and speech audiometers, and (c) to use the machine learning algorithms to classify hearing loss and normal subjects based on the selected features extracted from speech signals. Participants in the study included 50 healthy individuals and 50 patients with profound hearing loss. In the proposed hardware unit, the transmitted pure-tone signal and the speech signal stimulus are controlled automatically instead of being controlled manually. Using a digital potentiometer, a pure-tone audiometer can be automatically calibrated by varying the frequency and intensity of the generated tones according to the users’ responses. During speech audiometric measurements, pre-recorded speech and repeated speech signals are analyzed to estimate speech recognition threshold (SRT) and word recognition score (WRS). The designed audiometer plots the audiogram automatically, estimating SRT and WRS, and classifying the subject as normal or hearing impaired. This study demonstrates the feasibility of using Machine Learning to predict hearing impairment in patients. A support vector machine, a random forest, and an AdaBoost model produced accuracy rates of 98%, 96%, and 96%, respectively, when identifying normal and hearing loss subjects. The proposed audiometer system is miniaturized, portable, and user-friendly in comparison to conventional audiometers. Consequently, the prototype would make it possible for subjects to conduct their own audiometric tests independently and send the results along with their audiogram to a trained medical professional to receive advice. © 2023, King Fahd University of Petroleum & Minerals.",scopus,2-s2.0-85159655682,10.1007/s13369-023-07927-1
audioprofile surfaces the 21st century audiogram,"Objective: To present audiometric data in 3 dimensions by considering age as an addition dimension. Methods: Audioprofile surfaces (APSs) were fitted to a set of audiograms by plotting each measurement of an audiogram as an independent point in 3 dimensions with the x, y, and z axes representing frequency, hearing loss in dB, and age, respectively. Results: Using the Java-based APS viewer as a standalone application, APSs were pre-computed for 34 loci. By selecting APSs for the appropriate genetic locus, a clinician can compare this APS-generated average surface to a specific patient s audiogram. Conclusion: Audioprofile surfaces provide an easily interpreted visual representation of a person s hearing acuity relative to others with the same genetic cause of hearing loss. Audioprofile surfaces will support the generation and testing of sophisticated hypotheses to further refine our understanding of the biology of hearing. © The Author(s) 2015.",scopus,2-s2.0-84966784030,10.1177/0003489415614863
audiometric profiles and patterns of benefit a datadriven analysis of subjective hearing difficulties and handicaps references,"Abstract Objective Hearing rehabilitation attempts to compensate for auditory dysfunction, reduce hearing difficulties and minimise participation restrictions that can lead to social isolation. However, there is no systematic approach to assess the quality of the intervention at an individual level that might help to evaluate the need of further hearing rehabilitation in the hearing care clinic. Design A data-driven analysis on subjective data reflecting hearing disabilities and handicap was chosen to explore ""benefit patterns"" as a result of rehabilitation in different audiometric groups. The method was based on (1) dimensionality reduction; (2) stratification; (3) archetypal analysis; (4) clustering; (5) item importance estimation. Study sample 572 hearing-aid users completed questionnaires of hearing difficulties (speech, spatial and qualities hearing scale; SSQ) and hearing handicap (HHQ). Results The data-driven approach revealed four benefit profiles that were different for each audiometric group. The groups with low degree of high-frequency hearing loss (HLHF) showed a priority for rehabilitating hearing handicaps, whereas the groups with HLHF > 50 dB HL showed a priority for improvements in speech understanding. Conclusions The patterns of benefit and the stratification approach might guide the clinical intervention strategy and improve the efficacy and quality of service in the hearing care clinic. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc20&DO=10.1080%2f14992027.2021.1905890
evaluation of a novel speechinnoise test for hearing screening classification performance and transducers characteristics,"One of the current gaps in teleaudiology is the lack of methods for adult hearing screening viable for use in individuals of unknown language and in varying environments. We have developed a novel automated speech-in-noise test that uses stimuli viable for use in non-native listeners. The test reliability has been demonstrated in laboratory settings and in uncontrolled environmental noise settings in previous studies. The aim of this study was: (i) to evaluate the ability of the test to identify hearing loss using multivariate logistic regression classifiers in a population of 148 unscreened adults and (ii) to evaluate the ear-level sound pressure levels generated by different earphones and headphones as a function of the test volume. The multivariate classifiers had sensitivity equal to 0.79 and specificity equal to 0.79 using both the full set of features extracted from the test as well as a subset of three features (speech recognition threshold, age, and number of correct responses). The analysis of the ear-level sound pressure levels showed substantial variability across transducer types and models, with earphones levels being up to 22 dB lower than those of headphones. Overall, these results suggest that the proposed approach might be viable for hearing screening in varying environments if an option to self-adjust the test volume is included and if headphones are used. Future research is needed to assess the viability of the test for screening at a distance, for example by addressing the influence of user interface, device, and settings, on a large sample of subjects with varying hearing loss.  © 2013 IEEE.",scopus,2-s2.0-85112658406,10.1109/JBHI.2021.3100368
sentence recognition prediction for hearingimpaired listeners in stationary and fluctuation noise with fade empowering the attenuation and distortion concept by plomp with a quantitative processing model,"347. Trends Hear. 2016 Sep 7;20:2331216516655795. doi: 10.1177/2331216516655795.Sentence Recognition Prediction for Hearing-impaired Listeners in Stationary and Fluctuation Noise With FADE: Empowering the Attenuation and Distortion Concept by Plomp With a Quantitative Processing Model.Kollmeier B(1), Schädler MR(2), Warzybok A(2), Meyer BT(2), Brand T(2).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, Germany birger.kollmeier@uni-oldenburg.de.(2)Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, Germany.To characterize the individual patient's hearing impairment as obtained with the matrix sentence recognition test, a simulation Framework for Auditory Discrimination Experiments (FADE) is extended here using the Attenuation and Distortion (A+D) approach by Plomp as a blueprint for setting the individual processing parameters. FADE has been shown to predict the outcome of both speech recognition tests and psychoacoustic experiments based on simulations using an automatic speech recognition system requiring only few assumptions. It builds on the closed-set matrix sentence recognition test which is advantageous for testing individual speech recognition in a way comparable across languages. Individual predictions of speech recognition thresholds in stationary and in fluctuating noise were derived using the audiogram and an estimate of the internal level uncertainty for modeling the individual Plomp curves fitted to the data with the Attenuation (A-) and Distortion (D-) parameters of the Plomp approach. The ""typical"" audiogram shapes from Bisgaard et al with or without a ""typical"" level uncertainty and the individual data were used for individual predictions. As a result, the individualization of the level uncertainty was found to be more important than the exact shape of the individual audiogram to accurately model the outcome of the German Matrix test in stationary or fluctuating noise for listeners with hearing impairment. The prediction accuracy of the individualized approach also outperforms the (modified) Speech Intelligibility Index approach which is based on the individual threshold data only.© The Author(s) 2016.DOI: 10.1177/2331216516655795PMCID: PMC5017573",pubmed,27604782,10.1177/2331216516655795
auditory and nonauditory contributions for unaided speech recognition in noise as a function of hearing aid use,"20GM109023) and the National Institute on Deafness and Other Communication Disorders (R01DC008318 and L30DC017300). The funding sources had no role in the design and conduct of the study; in the collection, analysis, and interpretation of the data; or in the decision to submit the article for publication; or in the preparation, review, or approval of the article. There are no conflicts of interest, financial, or otherwise.614. Front Psychol. 2017 Feb 21;8:219. doi: 10.3389/fpsyg.2017.00219. eCollection 2017.Auditory and Non-Auditory Contributions for Unaided Speech Recognition in Noise as a Function of Hearing Aid Use.Gieseler A(1), Tahden MA(1), Thiel CM(2), Wagener KC(3), Meis M(3), Colonius H(1).Author information:(1)Cluster of Excellence 'Hearing4all', University of OldenburgOldenburg, Germany; Cognitive Psychology Lab, Department of Psychology, University of OldenburgOldenburg, Germany.(2)Cluster of Excellence 'Hearing4all', University of OldenburgOldenburg, Germany; Biological Psychology Lab, Department of Psychology, University of OldenburgOldenburg, Germany.(3)Cluster of Excellence 'Hearing4all', University of OldenburgOldenburg, Germany; Hörzentrum Oldenburg GmbHOldenburg, Germany.Differences in understanding speech in noise among hearing-impaired individuals cannot be explained entirely by hearing thresholds alone, suggesting the contribution of other factors beyond standard auditory ones as derived from the audiogram. This paper reports two analyses addressing individual differences in the explanation of unaided speech-in-noise performance among n = 438 elderly hearing-impaired listeners (mean = 71.1 ± 5.8 years). The main analysis was designed to identify clinically relevant auditory and non-auditory measures for speech-in-noise prediction using auditory (audiogram, categorical loudness scaling) and cognitive tests (verbal-intelligence test, screening test of dementia), as well as questionnaires assessing various self-reported measures (health status, socio-economic status, and subjective hearing problems). Using stepwise linear regression analysis, 62% of the variance in unaided speech-in-noise performance was explained, with measures Pure-tone average (PTA), Age, and Verbal intelligence emerging as the three most important predictors. In the complementary analysis, those individuals with the same hearing loss profile were separated into hearing aid users (HAU) and non-users (NU), and were then compared regarding potential differences in the test measures and in explaining unaided speech-in-noise recognition. The groupwise comparisons revealed significant differences in auditory measures and self-reported subjective hearing problems, while no differences in the cognitive domain were found. Furthermore, groupwise regression analyses revealed that Verbal intelligence had a predictive value in both groups, whereas Age and PTA only emerged significant in the group of hearing aid NU.DOI: 10.3389/fpsyg.2017.00219PMCID: PMC5318449",pubmed,28270784,10.3389/fpsyg.2017.00219
auditory steadystate response measurement in evaluating hearing loss milder than moderate to severe level,"Background: Auditory steady-state responses (ASSR) is an objective method of hearing examination in clinic in recent years. ASSR has the frequency specificity as compared with previous auditory brainstem responses (ABR). Objective: To investigate the accuracy of ASSR in objective hearing assessment. Design: A case-control observation. Setting: Department of Otorhinolaryngology, the First Affiliated Hospital of Sun Yat-sen University. Participants: The subjects in the normal hearing group were the 21 undergraduates (42 ears) were enrolled, they all had not any symptoms of ear disease, without history of noise exposure and disease of vestibule system, and they were normal in otoscopy. The outpatients and inpatients with neurosensory deafness were selected from the Department of Otorhinolaryngology, the First Affiliated Hospital of Sun Yat-sen University. All the children cases worn hearing aids, and had the speech ability, and cooperated in the examination. The main types included 6 ears of sudden deafness, 8 ears of presbycusis, and 20 ears of neurosensory deafness due to other unknown causes. Central lesions were excluded by MR examination, and all the patients agreed with the enrollment. The results of pure-tone audiometry were all flat or descending audiogram. According to the severity of hearing damage, the patients were divided into mild deafness group (13 ears), moderate deafness group (9 ears) and moderate-to-severe deafness group (12 ears). Methods: 1 The pure-tone audiometry was performed at the frequencies of 0.125-8 000 Hz in a sound insulation room. The auditory threshold grades of the subjects with normal hearing all accorded with the standards of GB-7583-87 expected value distribution. The average value of air-conduction auditory thresholds of pure-tone audiometry at the frequencies of 0.5, 1, 2 and 4 kHz was calculated. 2 ASSR measurement was performed with the synchronous stimulation pattern in a sound and electromagnetic shielding room, including 8 points for both ears of the same stimulation intensity and the carrier frequency tones of 0.5, 1, 2 and 4 kHz respectively. 3 ABR examination was performed by click sounds with sparse waves in a sound and electromagnetic shielding room, and insert earphones were used. The threshold results were judged according to the minimal stimulation sound intensity of the distinguishable V wave. 4 The results of pure-tone audiometry were compared with those of ABR examination, and the results of ASSR measurement in different hearing groups were processed with analysis of variance, multi-classification discrimination based Bayes standard and q test. Main outcome measures: The thresholds of pure-tone audiometry, ASSR measurement and ABR examination, and the correct rate analyzed by the multi-classification discrimination based Bayes standard were mainly observed. Results: The indexes of the 42 ears in the normal hearing group, 13, 9 and 12 ears in the mild, moderate and moderate-to-severe deafness groups were all involved in the analysis of results. 1 The ABR values were accorded with the actual hearing levels, and the closest to the ASSR thresholds at 1-2 kHz; ASSR reflected induction rates at different frequencies were gradually decreased with the aggravation of hearing damage, and that at each frequency varied with the changes of hearing level, the induction rates of ASSR responses were all 100% for the subjects with normal hearing and patients with mild deafness, but those for the patients with moderate and moderate-to-severe deafness were decreased (0.5 kHz: 77.8%, 92.8%; 4 kHz: 88.9%, 85.7%). At different frequencies, the ASSR thresholds in the moderate-to-severe deafness group were significantly higher than those in the normal hearing group (P < 0.05). The ASSR thresholds at 0.5 and 4 kHz in the moderate-to-severe deafness group were significantly higher than those in the mild deafness group (P < 0.05). The ASSR threshold at 2 kHz in the mild deafness group was significantly higher than that in the normal hearing group (P < 0.05). The ASSR thresholds at 4 kHz in the moderate-to-severe deafness group were significantly higher than those in the normal hearing group and mild deafness group. 2 The incorrect discriminations of actual pure-tone audiometry were analyzed with the interactive clustering discriminant analysis of ASSR measurement and actual pure-tone audiometry, and the results showed that the correct rate of discrimination was 100% in the normal hearing group; Only 1 of the 12 cases in the mild deafness group was incorrectly judged, and the correct rate was 92%; Only 1 of the 19 cases in the moderate deafness group was incorrectly judged, and the correct rate was 89%; the correct rate in the moderate-to-severe deafness group was 83%. Conclusion: The results of ASSR measurement can detect the incorrect discrimination of objective hearing condition by taking the results of pure-tone audiometry as the standards. ASSR has an acceptable accuracy for deafness higher than mild level in estimating objective hearing, and it has a better prospect of application in practice.",scopus,2-s2.0-33846055705,
data driven machine learning model for audiometric threshold classification,"Hearing loss is defined as the inability to hear partially or completely, in one or both the ears. It is present in people of all age groups. The continuous exposure to noise in today's world, aging and congenital defects are leading causes of hearing loss. Hearing loss can be present in new born as a result of maternal infections during pregnancy, complications after birth and head trauma. This study will develop a model to estimate the degree of Hearing loss of a sample set of people in the 18-22 age group. The hearing loss was calculated based on the intensity threshold values that was generated by the Smartphone mobile application-based hearing test [1] [2]. This threshold value was compared with the standard audiometric table to classify the sample set into two groups. Support Vector Machine (SVM) was used for building the binary classification model. The Support Vector Machine searches for an optimum hyperplane to classify the two groups. It uses the extreme points, termed as support vectors, to create the hyperplane. The hyperplane is created so as to maximize the margin, which is the distance between the hyperplane and the support vectors. The support vector machine algorithm supports different kernels for building a model. Three different kernels - Linear kernel, Polynomial kernel and Radial Basis function were used with three different training set sizes - 80% training size, 75% training size and 70% training size to select a model of high accuracy. The model with the highest accuracy was tested, and the confusion matrix of the test set data is obtained to verify the results. A classification report provides the values of Precision, Recall and F1 score to assess the quality of the model developed.  © 2022 IEEE.",scopus,2-s2.0-85150260862,10.1109/I4C57141.2022.10057711
the implementation of the kmedoid clustering for grouping hearing loss function on excessive smartphone use,"During the current pandemic, smartphones have become a means of learning for all students in Indonesia, including high school students. Students use smartphones to send assignments, learn via video calls, and conduct online exams. The prolonged use of smartphones, from the beginning of learning hours in the morning to study hours in the evening, has a terrible impact on the ear health of high school students in Padang. Excessive smartphone use caused a decrease in the student's hearing function. Therefore, this study aims to group the audiometry results of high school students in Padang who have a hearing loss function. The audiogram result is only performed as the result of a frequency test of the subject's hearing in both the left and right ear. Conventionally, an otolaryngologist concluded the final decision of hearing loss ability. This research proposed an automatic classification of audiometry results using machine learning methods. The K-Medoids clustering was selected to classify the audiometry data in this research. Of 210 audiometry data, 91 data is confirmed by an otolaryngologist as valid data. By using the K-Medoids clustering, 93 data is classified into Normal hearing, Mild Hearing loss, and Moderate Hearing loss. The proposed model successfully grouped the audiometry data into three categories. The confusion matrix is applied to measure the model performance, which has 28,3% accuracy, 64,3% precision, and 21,4% recall. © 2023, Politeknik Negeri Padang. All rights reserved.",scopus,2-s2.0-85181944438,10.30630/joiv.7.4.1873
audiogram digitization tool for audiological reports,"Multiple private and public insurers compensate workers whose hearing loss can be directly attributed to excessive exposure to noise in the workplace. The claim assessment process is typically lengthy and requires significant effort from human adjudicators who must interpret hand-recorded audiograms, often sent via fax or equivalent. In this work, we present a solution developed in partnership with the Workplace Safety Insurance Board of Ontario to streamline the adjudication process. We present a flexible and open-source audiogram digitization algorithm capable of automatically extracting the hearing thresholds from a scanned or faxed audiology report as a proof-of-concept. The algorithm extracts most thresholds within 5 dB accuracy, allowing to substantially lessen the time required to convert an audiogram into digital format in a semi-supervised fashion, and is a first step towards the automation of the adjudication process. The source code for the digitization algorithm and a desktop-based implementation of our NIHL annotation portal is publicly available on GitHub https://github.com/GreenCUBIC/AudiogramDigitization.",ieee,2169-3536,10.1109/ACCESS.2022.3215972
a data driven concept to choose the optimal ci electrode array15th international conference on cochlear implants and other implantable auditory technologies 27th jun 2018  30th jun 2018 antwerp belgium,"Background: A Cochlear Implantat (CI) is a successful therapy in the treatment of total or severe hearing loss. Various electrodes from different manufacturers exist and often the patients choose the manufacturer in advance. The surgeons decide, based on their subjective experience, which electrode type and length to implant. It is known that the cochlea anatomy varies in length and shape among the population. There is a large range in the hearing outcomes for CI patients that is not yet sufficiently well understood and we don't have a theoretical model for the optimal electrode position. Methods: Therefore, we suggest a data driven approach: In an ongoing retrospective investigation of more than 670 patients with MED-EL FLEX electrodes, we manually evaluate the pre- and postoperative radiological images in terms of the individual cochlea anatomy (cochlear duct length, lateral wall helix) and the exact post-op location of all inserted electrode contacts. Moreover, speech understanding, residual hearing, medical background and individual history of hearing loss were taken into account. After considerable efforts to normalize, clean, and link all the mentioned information, we can apply and compare different supervised machine learning methods like decision trees, Gaussian processes, neural networks, or logistic regression in order to predict the post-operative speech performance and the residual hearing. For the learning of these models, the different post-op hearing and speech tests have to be combined into a single number to compare the success of implantations. Furthermore, all the pre-operative data have to be mapped to define ""similar"" patients. Conclusion: Now, it is possible to tap the hidden knowledge of a large patient database: Suggesting the optimal electrode and insertion depth that retrospectively led to the best hearing outcome for patients with similar anatomy and residual hearing. We present the conceptual benefits and limitations of such a system.",cinahl,2083389X,
agerelated decline of speech perception references,"Hearing loss is one of the most common disorders worldwide. It affects communicative abilities in all age groups. However, it is well known that elderly people suffer more frequently from hearing loss. Two different model approaches were employed: A generalised linear model and a random forest regression model were used to quantify the relationship between pure-tone hearing loss, age, and speech perception. Both models were applied to a large clinical data set of 19,801 ears, covering all degrees of hearing loss. They allow the estimation of age-related decline in speech recognition for different types of audiograms. Our results show that speech scores depend on the specific type of hearing loss and life decade. We found age effects for all degrees of hearing loss. A deterioration in speech recognition of up to 25 percentage points across the whole life span was observed for constant pure-tone thresholds. The largest decrease was 10 percentage points per life decade. This age-related decline in speech recognition cannot be explained by elevated hearing thresholds as measured by pure-tone audiometry. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc21&DO=10.3389%2ffnagi.2022.891202
a flexible datadriven audiological patient stratification method for deriving auditory profiles,"743. Front Neurol. 2022 Sep 15;13:959582. doi: 10.3389/fneur.2022.959582. eCollection 2022.A flexible data-driven audiological patient stratification method for deriving auditory profiles.Saak S(1)(2), Huelsmeier D(1)(2), Kollmeier B(1)(2)(3)(4), Buhl M(1)(2).Author information:(1)Medical Physics, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany.(2)Cluster of Excellence Hearing4all, Carl von Ossietky Universität Oldenburg, Oldenburg, Germany.(3)Hörzentrum Oldenburg gGmbH, Oldenburg, Germany.(4)Hearing Speech and Audio Technology, Fraunhofer Institute of Digital Media Technology (IDMT), Oldenburg, Germany.For characterizing the complexity of hearing deficits, it is important to consider different aspects of auditory functioning in addition to the audiogram. For this purpose, extensive test batteries have been developed aiming to cover all relevant aspects as defined by experts or model assumptions. However, as the assessment time of physicians is limited, such test batteries are often not used in clinical practice. Instead, fewer measures are used, which vary across clinics. This study aimed at proposing a flexible data-driven approach for characterizing distinct patient groups (patient stratification into auditory profiles) based on one prototypical database (N = 595) containing audiogram data, loudness scaling, speech tests, and anamnesis questions. To further maintain the applicability of the auditory profiles in clinical routine, we built random forest classification models based on a reduced set of audiological measures which are often available in clinics. Different parameterizations regarding binarization strategy, cross-validation procedure, and evaluation metric were compared to determine the optimum classification model. Our data-driven approach, involving model-based clustering, resulted in a set of 13 patient groups, which serve as auditory profiles. The 13 auditory profiles separate patients within certain ranges across audiological measures and are audiologically plausible. Both a normal hearing profile and profiles with varying extents of hearing impairments are defined. Further, a random forest classification model with a combination of a one-vs.-all and one-vs.-one binarization strategy, 10-fold cross-validation, and the kappa evaluation metric was determined as the optimal model. With the selected model, patients can be classified into 12 of the 13 auditory profiles with adequate precision (mean across profiles = 0.9) and sensitivity (mean across profiles = 0.84). The proposed approach, consequently, allows generating of audiologically plausible and interpretable, data-driven clinical auditory profiles, providing an efficient way of characterizing hearing deficits, while maintaining clinical applicability. The method should by design be applicable to all audiological data sets from clinics or research, and in addition be flexible to summarize information across databases by means of profiles, as well as to expand the approach toward aided measurements, fitting parameters, and further information from databases.Copyright © 2022 Saak, Huelsmeier, Kollmeier and Buhl.DOI: 10.3389/fneur.2022.959582PMCID: PMC9520582",pubmed,36188360,10.3389/fneur.2022.959582
metabolic and functional correlates of agerelated hearing loss advanced mri findings and rehabilitation perspectives of the central auditory pathways,"BACKGROUND: Presbycusis is the hearing loss (HL) determined by aging mechanisms affecting the inner ear. Auditory cortical hypoperfusion has been shown in the early phases of presbycusis, suggesting a regionally selective metabolic vulnerability secondary to peripheral loss of function. In this study, HL patients were stratified according to the audiogram profiles to possibly enable a finer regional characterization of cortical perfusion changes within the primary auditory cortex. METHODS: Sixty-two HL patients (age range: 47-78 years, PTA <50dB) and thirty-two normal hearing (NH) subjects (age-range 48-78 years) were enrolled in a 3 Tesla MRI study. Two clusters of HL patients were identified, and labeled as low-loss-high-slope (LLHS) and high-losslow-slope (HLLS), according to the audiogram centro-types from an independent data set of fifty-five HL patients (age range: 45-80 years, PTA<50dB). Pseudo-continuous arterial spin labeling (ASL) and T1-weighted MRI were performed to derive cerebral blood flow (CBF) maps and to assess group-level perfusion changes within the primary auditory cortex. RESULT S: The comparison of CBF maps of all HL patients vs. NH controls confirmed a statistically significant CBF reduction in a compact region encompassing the right transverse temporal gyrus. The separate comparisons of LLHS and HLLS patients vs. NH subjects resulted in different localizations of the hypoperfusion region along the transverse temporal gyrus. CONCLUSIONS: Presbycusis is associated with perfusion reductions in the right primary auditory cortex with a different spatial pattern according to the audiogram profile. The observed heterogeneity of central auditory perfusion patterns may underlie different pathogenetic aspects and clinical forms of presbycusis.  © 2021 EDIZIONI MINERVA MEDICA.",scopus,2-s2.0-85117308051,10.23736/S2724-6302.21.02375-6
eeg based detection of conductive and sensorineural hearing loss using artificial neural networks,"In this paper, a simple method has been proposed to distinguish the normal and abnormal hearing subjects (conductive or sensorineural hearing loss) using acoustically stimulated EEG signals. Auditory Evoked Potential (AEP) signals are unilaterally recorded with monaural acoustical stimulus from the normal and abnormal hearing subjects with conductive or sensorineural hearing loss. Spectral power and spectral entropy features of gamma rhythms are extracted from the recorded AEP signals. The extracted features are applied to machine-learning algorithms to categorize the AEP signal dynamics into their hearing threshold states (normal hearing, abnormal hearing) of the subjects. Feed forward and feedback neural network models are employed with gamma band features and their performances are analyzed in terms of specificity, sensitivity and classification accuracy for the left and right ears across 9 subjects. The maximum classification accuracy of the developed neural network was observed as 96.75 per cent in discriminating the normal and hearing loss (conductive or sensorineural) subjects. From the neural network models, it has been inferred that network models were able to classify the normal hearing and abnormal hearing subjects with conductive or sensorineural hearing loss. Further, this study proposed a feature band-score index to explore the feasibility of using fewer electrode channels to detect the type of hearing loss for newborns, infants, and multiple handicaps, person who lacks verbal communication and behavioral response to the auditory stimulation.",scopus,2-s2.0-84880972968,10.4156/jnit.vol4.issue3.24
automated hearing impairment diagnosis using machine learning,"Approximately 700 million people will suffer from disabling hearing loss by 2050. Underdeveloped and developing countries, which encompass a considerable proportion of people with incapacitating hearing impairment, have a sparse number of audiologists and otolaryngologists. The lack of specialists leaves most hearing impairments undiagnosed for a long time. In this paper, we propose an automated hearing impairment diagnosis software - based on machine learning - to support audiologists and otolaryngologists in accurately and efficiently diagnosing and classifying hearing loss. We present the design, implementation, and performance analysis of the automated hearing impairment diagnosis software, which consists of two modules: a hearing test Data Generation Module and a Machine Learning Model. The Data Generation Module produces a diverse and exhaustive dataset for training and evaluating the Machine Learning Model. By employing multiclass and multi-label classification techniques to learn from the hearing test data, the model can instantaneously predict the type, degree, and configuration of hearing loss with high accuracy. Our proposed Machine Learning Model demonstrates propitious results with a prediction time of 634 ms, a log-loss reduction rate of 98.48%, and macro and micro precisions of 100% - showing the model's applicability to assist audiologists and otolaryngologists in rapidly and accurately classifying the type, degree, and configuration of hearing loss.  © 2022 IEEE.",scopus,2-s2.0-85133968110,10.1109/IETC54973.2022.9796707
effects of personalizing hearingaid parameter settings using a realtime machinelearning approach,"In most hearing-aid fittings, amplification is prescribed by a fitting rationale that uses the audiogram as the main input. This approach may fail in situations where the user's listening intention deviates from that assumed by the rationale. This shortcoming motivated a new commercially available method to self-adjust hearing-aid parameters while in a specific situation. The method is based on machine-learning algorithms that estimate the setting that optimizes user satisfaction based on user preferences in paired comparisons of parameter settings. We present results from a lab study where 20 participants with hearing loss used the method to adjust hearing-aid gain in 12 different sound scenarios with respect to three different sound attributes, and subsequently, in a double-blind assessment, compared the adjusted settings with the prescribed settings. The results showed a benefit of the method on basic audio quality. A large spread in the gain adjustments was observed, suggesting the need for more personalized settings of hearing aids. We also present anonymous user data gathered during real-life use of the method, which indicate when and why the method is used. We compare these data to general investigations of listeners' auditory reality and suggest clinical and rehabilitative implications of using the method. © 2019 Proceedings of the International Congress on Acoustics. All rights reserved.",scopus,2-s2.0-85099331086,10.18154/RWTH-CONV-239132
applicability of subcortical eeg metrics of synaptopathy to older listeners with impaired audiograms,"260. Hear Res. 2019 Sep 1;380:150-165. doi: 10.1016/j.heares.2019.07.001. Epub 2019 Jul 2.Applicability of subcortical EEG metrics of synaptopathy to older listeners with impaired audiograms.Garrett M(1), Verhulst S(2).Author information:(1)Medizinische Physik and Cluster of Excellence ""Hearing4all"", Department of Medical Physics and Acoustics, University of Oldenburg, Oldenburg, Germany. Electronic address: markus.garrett@uol.de.(2)Hearing Technology @ WAVES, Department of Information Technology, Ghent University, Ghent, Belgium; Medizinische Physik and Cluster of Excellence ""Hearing4all"", Department of Medical Physics and Acoustics, University of Oldenburg, Oldenburg, Germany. Electronic address: s.verhulst@ugent.be.Emerging evidence suggests that cochlear synaptopathy is a common feature of sensorineural hearing loss, but it is not known to what extent electrophysiological metrics targeting synaptopathy in animals can be applied to people, such as those with impaired audiograms. This study investigates the applicability of subcortical electrophysiological measures associated with synaptopathy, i.e., auditory brainstem responses (ABRs) and envelope following responses (EFRs), to older participants with high-frequency sloping audiograms. The outcomes of this study are important for the development of reliable and sensitive synaptopathy diagnostics in people with normal or impaired outer-hair-cell function. Click-ABRs at different sound pressure levels and EFRs to amplitude-modulated stimuli were recorded, as well as relative EFR and ABR metrics which reduce the influence of individual factors such as head size and noise floor level on the measures. Most tested metrics showed significant differences between the groups and did not always follow the trends expected from synaptopathy. Age was not a reliable predictor for the electrophysiological metrics in the older hearing-impaired group or young normal-hearing control group. This study contributes to a better understanding of how electrophysiological synaptopathy metrics differ in ears with healthy and impaired audiograms, which is an important first step towards unravelling the perceptual consequences of synaptopathy.Copyright © 2019 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2019.07.001",pubmed,31306930,10.1016/j.heares.2019.07.001
elospheres intelligibility prediction model for the clarity prediction challenge 2022,"This paper describes and evaluates the ELO-SPHERES project sentence intelligibility model for the Clarity Prediction Challenge 2022. The aim of the model is to make predictions of the intelligibility of enhanced speech to hearing impaired listeners. Input to the model are binaural processed audio of short sentences generated in a simulated noisy and reverberant environment together with the original source audio. Output of the model is a prediction of the intelligibility of each sentence in terms of percentage words correct for a known hearing-impaired listener characterized by a pure-tone audiogram. Models are evaluated in terms of the root mean squared error of prediction. We approached this problem in three stages: (i) evaluation of the influences of the scene metadata on scores, (ii) construction of classifiers for estimation of scene metadata from audio, and (iii) training a non-linear regression model on the challenge data and evaluation using 5-fold cross validation. On the test data, a baseline system using only the standard short-time objective intelligibility metric on the better ear achieved a RMS prediction error of 27%, while our model that also took into account given and estimated scene data achieved an RMS error of 22%. Copyright © 2022 ISCA.",scopus,2-s2.0-85140088307,10.21437/Interspeech.2022-10521
auditory brainstem response data preprocessing method for the automatic classification of hearing loss patients,"Auditory brainstem response (ABR) is the response of the brain stem through the auditory nerve. The ABR test is a method of testing for loss of hearing through electrical signals. Basically, the test is conducted on patients such as the elderly, the disabled, and infants who have difficulty in communication. This test has the advantage of being able to determine the presence or absence of objective hearing loss by brain stem reactions only, without any communication. This paper proposes the image preprocessing process required to construct an efficient graph image data set for deep learning models using auditory brainstem response data. To improve the performance of the deep learning model, we standardized the ABR image data measured on various devices with different forms. In addition, we applied the VGG16 model, a CNN-based deep learning network model developed by a research team at the University of Oxford, using preprocessed ABR data to classify the presence or absence of hearing loss and analyzed the accuracy of the proposed method. This experimental test was performed using 10,000 preprocessed data, and the model was tested with various weights to verify classification learning. Based on the learning results, we believe it is possible to help set the criteria for preprocessing and the learning process in medical graph data, including ABR graph data. © 2023 by the authors.",scopus,2-s2.0-85178909011,10.3390/diagnostics13233538
assessing the heterogeneity of complaints related to tinnitus and hyperacusis from an unsupervised machine learning approach an exploratory study,"Introduction: Subjective tinnitus (ST) and hyperacusis (HA) are common auditory symptoms that may become incapacitating in a subgroup of patients who thereby seek medical advice. Both conditions can result from many different mechanisms, and as a consequence, patients may report a vast repertoire of associated symptoms and comorbidities that can reduce dramatically the quality of life and even lead to suicide attempts in the most severe cases. The present exploratory study is aimed at investigating patients' symptoms and complaints using an in-depth statistical analysis of patients' natural narratives in a real-life environment in which, thanks to the anonymization of contributions and the peer-to-peer interaction, it is supposed that the wording used is totally free of any self-limitation and self-censorship.Methods: We applied a purely statistical, non-supervised machine learning approach to the analysis of patients' verbatim exchanged on an Internet forum. After automated data extraction, the dataset has been preprocessed in order to make it suitable for statistical analysis. We used a variant of the Latent Dirichlet Allocation (LDA) algorithm to reveal clusters of symptoms and complaints of HA patients (topics). The probability of distribution of words within a topic uniquely characterizes it. The convergence of the log-likelihood of the LDA-model has been reached after 2,000 iterations. Several statistical parameters have been tested for topic modeling and word relevance factor within each topic.Results: Despite a rather small dataset, this exploratory study demonstrates that patients' free speeches available on the Internet constitute a valuable material for machine learning and statistical analysis aimed at categorizing ST/HA complaints. The LDA model with K = 15 topics seems to be the most relevant in terms of relative weights and correlations with the capability to individualizing subgroups of patients displaying specific characteristics. The study of the relevance factor may be useful to unveil weak but important signals that are present in patients' narratives.Discussion/conclusion: We claim that the LDA non-supervised approach would permit to gain knowledge on the patterns of ST- and HA-related complaints and on patients' centered domains of interest. The merits and limitations of the LDA algorithms are compared with other natural language processing methods and with more conventional methods of qualitative analysis of patients' output. Future directions and research topics emerging from this innovative algorithmic analysis are proposed.",cinahl,14203030,10.1159/000504741
longitudinal changes in audiometric phenotypes of agerelated hearing loss,"68. J Assoc Res Otolaryngol. 2017 Apr;18(2):371-385. doi: 10.1007/s10162-016-0596-2. Epub 2016 Nov 9.Longitudinal Changes in Audiometric Phenotypes of Age-Related Hearing Loss.Vaden KI Jr(1), Matthews LJ(2), Eckert MA(2), Dubno JR(3).Author information:(1)Hearing Research Program, Department of Otolaryngology-Head and Neck Surgery, Medical University of South Carolina, 135 Rutledge Avenue, MSC 550, Charleston, SC, 29425-5500, USA. vaden@musc.edu.(2)Hearing Research Program, Department of Otolaryngology-Head and Neck Surgery, Medical University of South Carolina, 135 Rutledge Avenue, MSC 550, Charleston, SC, 29425-5500, USA.(3)Hearing Research Program, Department of Otolaryngology-Head and Neck Surgery, Medical University of South Carolina, 135 Rutledge Avenue, MSC 550, Charleston, SC, 29425-5500, USA. dubnojr@musc.edu.Presbyacusis, or age-related hearing loss, can be characterized in humans as metabolic and sensory phenotypes, based on patterns of audiometric thresholds that were established in animal models. The metabolic phenotype is thought to result from deterioration of the cochlear lateral wall and reduced endocochlear potential that decreases cochlear amplification and produces a mild, flat hearing loss at lower frequencies coupled with a gradually sloping hearing loss at higher frequencies. The sensory phenotype, resulting from environmental exposures such as excessive noise or ototoxic drugs, involves damage to sensory and non-sensory cells and loss of the cochlear amplifier, which produces a 50-70 dB threshold shift at higher frequencies. The mixed metabolic + sensory phenotype exhibits a mix of lower frequency, sloping hearing loss similar to the metabolic phenotype, and steep, higher frequency hearing loss similar to the sensory phenotype. The current study examined audiograms collected longitudinally from 343 adults 50-93 years old (n = 686 ears) to test the hypothesis that metabolic phenotypes increase with increasing age, in contrast with the sensory phenotype. A Quadratic Discriminant Analysis (QDA) was used to classify audiograms from each of these ears as (1) Older-Normal, (2) Metabolic, (3) Sensory, or (4) Metabolic + Sensory phenotypes. Although hearing loss increased systematically with increasing age, audiometric phenotypes remained stable for the majority of ears (61.5 %) over an average of 5.5 years. Most of the participants with stable phenotypes demonstrated matching phenotypes for the left and right ears. Audiograms were collected over an average period of 8.2 years for ears with changing audiometric phenotypes, and the majority of those ears transitioned to a Metabolic or Metabolic + Sensory phenotype. These results are consistent with the conclusion that the likelihood of metabolic presbyacusis increases with increasing age in middle to older adulthood.DOI: 10.1007/s10162-016-0596-2PMCID: PMC5352606",pubmed,27830350,10.1007/s10162-016-0596-2
evaluation of machine learning algorithms and explainability techniques to detect hearing loss from a speechinnoise screening test references,"Purpose: The aim of this study was to analyze the performance of multivariate machine learning (ML) models applied to a speech-in-noise hearing screening test and investigate the contribution of the measured features toward hearing loss detection using explainability techniques. Method: Seven different ML techniques, including transparent (i.e., decision tree and logistic regression) and opaque (e.g., random forest) models, were trained and evaluated on a data set including 215 tested ears (99 with hearing loss of mild degree or higher and 116 with no hearing loss). Post hoc explainability techniques were applied to highlight the role of each feature in predicting hearing loss. Results: Random forest (accuracy = .85, sensitivity = .86, specificity = .85, precision = .84) performed, on average, better than decision tree (accuracy = .82, sensitivity = .84, specificity = .80, precision = .79). Support vector machine, logistic regression, and gradient boosting had similar performance as random forest. According to post hoc explainability analysis on models generated using random forest, the features with the highest relevance in predicting hearing loss were age, number and percentage of correct responses, and average reaction time, whereas the total test time had the lowest relevance. Conclusions: This study demonstrates that a multivariate approach can help detect hearing loss with satisfactory performance. Further research on a bigger sample and using more complex ML algorithms and explainability techniques is needed to fully investigate the role of input features (including additional features such as risk factors and individual responses to low-/high-frequency stimuli) in predicting hearing loss. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc22&DO=10.1044%2f2022_AJA-21-00194
identifying genetic risk variants associated with noiseinduced hearing loss based on a novel strategy for evaluating individual susceptibility,"Background: The overall genetic profile for noise-induced hearing loss (NIHL) remains elusive. Herein we proposed a novel machine learning (ML) based strategy to evaluate individual susceptibility to NIHL and identify the underlying genetic risk variants based on a subsample of participants with extreme phenotypes. Methods: Five features (age, sex, cumulative noise exposure [CNE], smoking, and alcohol drinking status) of 5,539 shipbuilding workers from large cross-sectional surveys were included in four ML classification models to predict their hearing levels. The area under the curve (AUC) and prediction accuracy were exploited to evaluate the performance of the models. Based on the prediction error of the ML models, the NIHL-susceptible group (n=150) and NIHL-resistant group (n=150) with a paradoxical relationship between hearing levels and features were separately screened, to identify the underlying variants associated with NIHL risk using whole-exome sequencing (WES). Subsequently, candidate risk variants were validated in an additional replication cohort (n=2108), followed by a meta-analysis. Results: With 10-fold cross-validation, the performances of the four ML models were robust and similar, with average AUCs and accuracies ranging from 0.783 to 0.798 and 73.7% to 73.8%, respectively. The phenotypes of the NIHL-susceptible and NIHL-resistant groups were significantly different (all p<0.001). After WES analysis and filtering, 12 risk variants contributing to NIHL susceptibility were identified and replicated. The meta-analyses showed that the A allele of CDH23 rs41281334 (odds ratio [OR]=1.506, 95% confidence interval [CI]=1.106-2.051) and the C allele of WHRN rs12339210 (OR=3.06, 95% CI=1.398-6.700) were significantly associated with increased risk of NIHL after adjustment for confounding factors. Conclusions: This study revealed two genetic variants in CDH23 rs41281334 and WHRN rs12339210 that associated with NIHL risk, based on a promising approach for evaluating individual susceptibility using ML models. © 2021",scopus,2-s2.0-85109364112,10.1016/j.heares.2021.108281
automated hearing impairment diagnosis using machinelearning an opensource software development undergraduate research project,"Approximately 700 million people will have disabling hearing loss by 2050. Underdeveloped and developing countries, which encompass a considerable proportion of people with disabling hearing impairment, have a sparse number of audiologists and otolaryngologists. The lack of specialists leaves most hearing impairments undiagnosed for a long time, resulting in negative societal and economic impacts. In this article, we propose an automated hearing impairment diagnosis software—based on machine learning—to support audiologists and otolaryngologists in accurately and efficiently diagnosing and classifying hearing loss. We present the design, implementation, and performance analysis of an open-source automated hearing impairment diagnosis software, which consists of two modules: a hearing test data-generation module and a machine-learning model. The data-generation module produces a diverse and exhaustive data set for training and evaluating the machine-learning model. By employing multiclass and ultilabel classification techniques to learn from the hearing test data, the model can swiftly predict the type, degree, and configuration of hearing loss with high reliability. Our proposed machine-learning model demonstrates promising results with a prediction time of 634 ms, a log-loss reduction rate of 0.9848 and accuracy, precision, recall, and f1-score of 1.0000—showing the model's applicability to assist audiologists and otolaryngologists in rapidly and accurately classifying the type, degree, and configuration of hearing loss. In addition to the technical contributions, this article also highlights the importance of involving undergraduate students in open-source software development projects which have a direct impact on improving the quality of human life. © 2024 Wiley Periodicals LLC.",scopus,2-s2.0-85186599282,10.1002/cae.22724
common configurations of realear aided response targets prescribed by nalnl2 for older adults with mildtomoderate hearing loss references,"Purpose: This study investigates common real-ear aided response (REAR) configurations prescribed by the NAL-NL2 algorithm for older adults with hearing loss. Method: A data set that is representative of the older adult U.S. population with mild-to-moderate sensorineural hearing loss was constructed from the audiometric data of 934 adults (aged 55-85 years) from the National Health and Nutrition Examination Survey years 1999-2012. Two clustering approaches were implemented to generate common REAR configurations for eight frequencies (0.25, 0.5, 1, 2, 3, 4, 6, and 8 kHz) at three input levels (55, 65, and 75 dB SPL). (a) In the REAR-based clustering approach, the National Health and Nutrition Examination Survey audiograms were first converted to REAR targets and then clustered to generate common REAR configurations. (b) In the audiogram-based clustering approach, the audiograms were first clustered into common hearing loss profiles and then converted to REAR configurations. The trade-off between the number of available REAR configurations and the percentage of the U.S. population whose hearing loss could be fit by at least one of them (i.e., percent coverage) was evaluated. Hearing loss fit was defined as less than +/- 5-dB difference between an individual's REAR targets and those of the clustered REAR configuration. Results: Percent coverage increases with the number of available REAR configurations, with four configurations resulting in 75% population coverage. Overall, REAR-based clustering yielded 5 percentage points better coverage on average compared to audiogram-based clustering. Conclusions: The common REAR configurations can be used for programming the gain frequency responses in preconfigured over-the-counter hearing aids and provide clinically appropriate amplification settings for older adults with mild-to-moderate hearing loss. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc19&DO=10.1044%2f2020_AJA-20-00025
speech intelligibility in quiet and noise in people wearing hearing aids a big data analysisxxxv world congress of audiology april 1013 2022 warsaw poland,"Objectives: Over 1.5 billion of the world's population require rehabilitation to address their 'disabling' hearing loss. Whereas hearing aids improve speech recognition in quiet, their performance in noisy environment have not been clearly evaluated. Here, we investigate the benefit of wearing hearing aid using a data base of 30,000 subjects. Specifically, we compared aided speech performance of subjects in quiet and noisy backgrounds using deep learning algorithms. Material and methods: Subjects from 40 to 109 years old (78±7 years old, mean±SD) with a bilateral symmetric hearing loss were included in the study. Patients were fitted between 2017 and 2021 with standardized protocols realized in 700 Amplifon hearing care centers distributed all over France. Hearing assessments were performed using a pure tone audiometry (PTA) from 125 Hz to 8 kHz. Speech-in-quiet (dissyllabic words presented in free field) and a speech-in-noise recognition tests (HINT sentences in free field, masking noise at 65 dB HL, adaptative procedure) were performed to assess the aided speech performance of subjects. The data were anonymized, centralized, and stored on secure data server. Data processing was carried out using R statistical software. Results: The 30,000 patients were partitioned (k-means algorithm, Euclidean distance, 1000 iterations) in 200 clusters according to their pure-tone audiogram. A positive and significant linear relationship was found between aided pure tone threshold in quiet and the mean PTA threshold at frequencies 0.5, 1, 2, and 4 kHz. Positive linear relationship was also found between aided speech performance in quiet and mean PTA threshold. When evaluating the aided speech performance in noise, the correlation was reduced especially in subjects with a mean PTA threshold below 50 dB HL (i.e. mild and moderate hearing loss). Note that hearing aids provide a significant benefit in noise for patients with moderately severe and severe hearing loss (mean PTA threshold above 50 dB HL). Conclusions: Our results show that the use of hearing aids improves speech intelligibility both in quiet and noisy environments for people with moderately severe to severe hearing loss. For people with mild to moderate hearing loss, hearing aids improve mostly the speech intelligibility in quiet.",cinahl,2083389X,
comparison of machine learning models to classify auditory brainstem responses recorded from children with auditory processing disorder,"Introduction: Auditory brainstem responses (ABRs) offer a unique opportunity to assess the neural integrity of the peripheral auditory nervous system in individuals presenting with listening difficulties. ABRs are typically recorded and analyzed by an audiologist who manually measures the timing and quality of the waveforms. The interpretation of ABRs requires considerable experience and training, and inappropriate interpretation can lead to incorrect judgments about the integrity of the system. Machine learning (ML) techniques may be a suitable approach to automate ABR interpretation and reduce human error. Objectives: The main objective of this paper was to identify a suitable ML technique to automate the analysis of ABR responses recorded as a part of the electrophysiological testing in the Auditory Processing Disorder clinical test battery. Methods: ABR responses recorded during routine clinical assessment from 136 children being evaluated for auditory processing difficulties were analyzed using several common ML algorithms: Support Vector Machines (SVM), Random Forests (RF), Decision Trees (DT), Gradient Boosting (GB), Extreme Gradient Boosting (Xgboost), and Neural Networks (NN). A variety of signal feature extraction techniques were used to extract features from the ABR waveforms as inputs to the ML algorithms. Statistical significance testing and confusion matrices were used to identify the most robust model capable of accurately identifying neurological abnormalities present in ABRs. Results: Clinically significant features in the time-frequency representation of the signal were identified. The ML model trained using the Xgboost algorithm was identified as the most robust model with an accuracy of 92% compared to other models. Conclusion: The findings of the present study demonstrate that it is possible to develop accurate ML models to automate the process of analyzing ABR waveforms recorded at suprathreshold levels. There is currently no ML-based application to screen children with listening difficulties. Therefore, it is expected that this work will be translated into an evaluation tool that can be used by audiologists in the clinic. Furthermore, this work may aid future researchers in exploring ML paradigms to improve clinical test batteries used by audiologists in achieving accurate diagnoses. © 2021 Elsevier B.V.",scopus,2-s2.0-85099787713,10.1016/j.cmpb.2021.105942
individual differences in auditory brainstem response wave characteristics relations to different aspects of peripheral hearing loss,"231. Trends Hear. 2016 Nov 11;20:2331216516672186. doi: 10.1177/2331216516672186.Individual Differences in Auditory Brainstem Response Wave Characteristics: Relations to Different Aspects of Peripheral Hearing Loss.Verhulst S(1)(2), Jagadeesh A(3), Mauermann M(3), Ernst F(3).Author information:(1)Cluster of Excellence Hearing4all and Medizinische Physik, Department of Medical Physics and Acoustics, Oldenburg University, Oldenburg, Germany s.verhulst@ugent.be.(2)Department of Information Technology, Ghent University, Technologiepark, Zwijnaarde, Belgium.(3)Cluster of Excellence Hearing4all and Medizinische Physik, Department of Medical Physics and Acoustics, Oldenburg University, Oldenburg, Germany.Little is known about how outer hair cell loss interacts with noise-induced and age-related auditory nerve degradation (i.e., cochlear synaptopathy) to affect auditory brainstem response (ABR) wave characteristics. Given that listeners with impaired audiograms likely suffer from mixtures of these hearing deficits and that ABR amplitudes have successfully been used to isolate synaptopathy in listeners with normal audiograms, an improved understanding of how different hearing pathologies affect the ABR source generators will improve their sensitivity in hearing diagnostics. We employed a functional model for human ABRs in which different combinations of hearing deficits were simulated and show that high-frequency cochlear gain loss steepens the slope of the ABR Wave-V latency versus intensity and amplitude versus intensity curves. We propose that grouping listeners according to a ratio of these slope metrics (i.e., the ABR growth ratio) might offer a way to factor out the outer hair cell loss deficit and maximally relate individual differences for constant ratios to other peripheral hearing deficits such as cochlear synaptopathy. We compared the model predictions to recorded click-ABRs from 30 participants with normal or high-frequency sloping audiograms and confirm the predicted relationship between the ABR latency growth curve and audiogram slope. Experimental ABR amplitude growth showed large individual differences and was compared with the Wave-I amplitude, Wave-V/I ratio, or the interwaveI-W latency in the same listeners. The model simulations along with the ABR recordings suggest that a hearing loss profile depicting the ABR growth ratio versus the Wave-I amplitude or Wave-V/I ratio might be able to differentiate outer hair cell deficits from cochlear synaptopathy in listeners with mixed pathologies.© The Author(s) 2016.DOI: 10.1177/2331216516672186PMCID: PMC5117250",pubmed,27837052,10.1177/2331216516672186
associations between agerelated hearing loss cognitive impairment and multiple chronic conditions in a group care setting,"Purpose: The purpose of this study was to explore the relationships between hearing loss, cognitive status, and a range of health outcomes over a period of 2 years in a sample of older adults who are enrolled in Program of All-Inclusive Care for the Elderly, which is a Medicare/Medicaid beneficiary program for individuals who are nursing home eligible but living in the community at time of enrollment. Method: The sample (N = 144) includes a diverse (47% White/non-Hispanic, 35% Black/African American, and 16% Latin/Hispanic) group of adults ranging from 55 to 93 years old. We used medical chart data to measure respondents' cognitive and health status, including chronic conditions and hospital use. Hearing status was measured once at the beginning of the 2-year review period. We used logistic regression and negative binomial hurdle models for analyses. We used latent class analysis (LCA) to explore the extent to which respondents cluster into a set of ""health profiles"" characterized by their hearing, cognitive status, and health conditions. Results: We found that hearing loss is weakly associated with heart disease and diabetes and associated with cerebrovascular disease and falls; cognitive impairment is also associated with cerebrovascular disease and the number of falls. LCA indicates that respondents cluster into a variety of health profiles with a consistent pairing of hearing loss and depression. Conclusions: The results are largely consistent with associations reported in epidemiological studies that include age-related hearing loss. Of particular interest in this study is the LCA that suggested that all of the profiles associated with a high likelihood of hearing loss included a high risk of depression. The co-occurrence of these two factors highlights the need to identify and treat hearing loss in older adults, especially as part of the treatment plan for individuals with depressive symptoms.",cinahl,10924388,10.1044/2023_JSLHR-23-00067
presbycusis phenotypes form a heterogeneous continuum when ordered by degree and configuration of hearing loss,"372. Hear Res. 2010 Jun 1;264(1-2):10-20. doi: 10.1016/j.heares.2010.02.001. Epub 2010 Feb 6.Presbycusis phenotypes form a heterogeneous continuum when ordered by degree and configuration of hearing loss.Allen PD(1), Eddins DA.Author information:(1)Department of Neurobiology and Anatomy, University of Rochester School of Medicine and Dentistry, 601 Elmwood Avenue, Box 603, Rochester, NY 14642, USA. paul_allen@urmc.rochester.eduMany reports have documented age-by-frequency increases in average auditory thresholds in various human populations. Despite this, the prevalence of different patterns of hearing loss in presbycusis remains uncertain. We examined 'presbycusis phenotypes' in a database of 960 subjects (552 female, 408 male, 18-92 years) that each had 30 measures of peripheral hearing sensitivity: pure tone audiograms for left and right ears from 0.25 to 8 kHz and DPOAE for each ear with F(mean)=1-6.4 kHz. Surprisingly, the hearing phenotypes did not naturally separate into discrete classes of presbycusis. Principal component (PC) analysis revealed that two principal components account for 74% of the variance among the 30 measures of hearing. The two components represent the overall degree (PC1) and configuration of loss (Flat vs. Sloping; PC2) and the phenotypes form a continuum when plotted against them. A heuristic partitioning of this continuum produced classes of presbycusis that vary in their degree of Sloping or Flat hearing loss, suggesting that the previously reported sub-types of presbycusis arise from the categorical segregation of a continuous and heterogeneous distribution. Further, most phenotypes lie intermediate to the extremes of either Flat or Sloping loss, indicating that if audiometric configuration does predict presbycusis etiology, then a mixed origin is the most prevalent.Copyright 2010 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2010.02.001PMCID: PMC2868118",pubmed,20144701,10.1016/j.heares.2010.02.001
influence of the interest operators in the detection of spontaneous reactions to the sound,"Hearing plays a key role in our social participation and daily activities. In health, hearing loss in one of the most common conditions, so its diagnosis and monitoring is highly important. The standard test for the evaluation of hearing is the pure tone audiometry, which is a behavioral test that requires a proper interaction and communication between the patient and the audiologist. This need of understanding is which makes this test unworkable when dealing with patients with severe cognitive decline or other communication disorders. In these particular cases, the audiologist base the evaluation in the detection of spontaneous facial reaction that may indicate auditory perception. With the aim of supporting the audiologist, a screening method that analyzes video sequences and seeks for eye gestural reactions was proposed. In this paper, a comprehensive survey about one of the crucial steps of the methodology is presented. This survey determines the optimal configuration for all of them, and evaluates in detail their combination with different classification techniques. The obtained results provide a global vision of the suitability of the different interest operators. © Springer International Publishing Switzerland 2015.",scopus,2-s2.0-84950273177,10.1007/978-3-319-25210-0_21
machine learning models for the hearing impairment prediction in workers exposed to complex industrial noise a pilot study,"164. Ear Hear. 2019 May/Jun;40(3):690-699. doi: 10.1097/AUD.0000000000000649.Machine Learning Models for the Hearing Impairment Prediction in Workers Exposed to Complex Industrial Noise: A Pilot Study.Zhao Y(1), Li J(1), Zhang M(2), Lu Y(1), Xie H(2), Tian Y(1), Qiu W(3).Author information:(1)Key Laboratory for Biomedical Engineering of Ministry of Education, Collaborative Innovation Center for Diagnosis and Treatment of Infectious Diseases, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China.(2)Institute of Environmental and Occupational Health, Zhejiang Provincial Center for Disease Control and Prevention, Hangzhou, China.(3)Auditory Research Laboratory, State University of New York at Plattsburgh, New York, USA.OBJECTIVES: To demonstrate the feasibility of developing machine learning models for the prediction of hearing impairment in humans exposed to complex non-Gaussian industrial noise.DESIGN: Audiometric and noise exposure data were collected on a population of screened workers (N = 1,113) from 17 factories located in Zhejiang province, China. All the subjects were exposed to complex noise. Each subject was given an otologic examination to determine their pure-tone hearing threshold levels and had their personal full-shift noise recorded. For each subject, the hearing loss was evaluated according to the hearing impairment definition of the National Institute for Occupational Safety and Health. Age, exposure duration, equivalent A-weighted SPL (LAeq), and median kurtosis were used as the input for four machine learning algorithms, that is, support vector machine, neural network multilayer perceptron, random forest, and adaptive boosting. Both classification and regression models were developed to predict noise-induced hearing loss applying these four machine learning algorithms. Two indexes, area under the curve and prediction accuracy, were used to assess the performances of the classification models for predicting hearing impairment of workers. Root mean square error was used to quantify the prediction performance of the regression models.RESULTS: A prediction accuracy between 78.6 and 80.1% indicated that the four classification models could be useful tools to assess noise-induced hearing impairment of workers exposed to various complex occupational noises. A comprehensive evaluation using both the area under the curve and prediction accuracy showed that the support vector machine model achieved the best score and thus should be selected as the tool with the highest potential for predicting hearing impairment from the occupational noise exposures in this study. The root mean square error performance indicated that the four regression models could be used to predict noise-induced hearing loss quantitatively and the multilayer perceptron regression model had the best performance.CONCLUSIONS: This pilot study demonstrated that machine learning algorithms are potential tools for the evaluation and prediction of noise-induced hearing impairment in workers exposed to diverse complex industrial noises.DOI: 10.1097/AUD.0000000000000649PMCID: PMC6493679",pubmed,30142102,10.1097/AUD.0000000000000649
pure tone hearing profiles in children with otitis media with effusion references,"Introduction: Otitis media with effusion (OME) is a common middle ear disease in children. The associated conductive hearing loss is a major concern for hearing health professionals. The aim of the present study was to describe the configuration of pure tone audiograms of children with OME and to design a statistical stratification algorithm to facilitate hearing loss profiling in children with OME. Methods: School age children with OME were recruited. Bone and air conduction thresholds were obtained using standard procedures. Hierarchical cluster analysis was employed to determine audiometric profile groups. The Mandarin Hearing in Noise Test was used to measure sentence perception in children for cluster analysis validity assessment. Results: Ninety-seven children (164 ears) aged between 72 months and 153 months were examined. Air conduction thresholds averaged for 500 Hz, 1000 Hz and 2000 Hz were in the range of 8.3-53.3 dB HL with a mean of 26.8 dB HL. Bone conduction thresholds were found to be influenced by middle ear pathology with a maximal elevation at 2000 Hz of 25 dB HL. Four audiometric profiles were identified. Cluster 1 contained 54 ears (32.9%) with normal or near normal hearing, Clusters 2 contained 37 ears (22.6%) with mild hearing loss, Cluster 3 included 48 ears (29.3%) and Cluster 4 included 25 ears (15.2%) with moderate hearing loss. Stability and validity of the four-cluster profiling procedure was examined and established with satisfactory results. Conclusions: OME in children is associated with pure tone hearing thresholds ranging from normal to moderate hearing loss. The hierarchical clustering algorithm proved useful as a novel means of profiling hearing loss in children with OME and may assist in identifying affected children at greater risk of auditory disadvantage. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc17&DO=10.1080%2f09638288.2017.1290698
prevalence of hearing impairment by gender and audiometric configuration results from the national health and nutrition examination surgery 19992004 and the keokuk county rural health study 19941998,"Purpose: This study describes the most common audiometric configurations and the prevalence of these configurations among adults (ages 20 to 69) in the noninstitutionalized population of the United States and in a sample of residents of a rural county in Iowa.Research Design: This was a cross-sectional population-based study.Study Sample: Estimates generalizing to the noninstitutionalized population of the United States were based on National Health and Nutrition Examination Survey (NHANES) data collected from 2819 women and 2525 men between 1999 and 2004. Estimates from the rural county were based on Keokuk County Rural Health Study (KCRHS) data collected from 892 women and 750 men between 1994 and 1998.Data Collection and Analysis: Cluster analyses (k-means) were used to divide participants into groups including maximally similar bilateral air conduction audiograms. Separate cluster analyses were conducted for each gender. For NHANES data, prevalence and error estimates were obtained using sample weights intended to provide data generalizing to the noninstitutionalized population of the United States within this age range.Results: The hierarchical structure of audiometric configurations revealed that approximately 25% of women and 50% of men aged 20 to 69 in the noninstitutionalized population of the United States were best described by a configuration consistent with a marked hearing impairment in at least one frequency. Hearing impairments were more common among participants in the KCRHS. Gently sloping configurations of hearing impairment were dominant among women, while configurations featuring a greater slope were dominant among men. There was a greater variety of audiometric configurations in men than women.Conclusions: In addition to their descriptive value, these data can be used to inform future studies of risk factors and progression of hearing loss, and to improve the generalizability of studies involving rehabilitative options for people with hearing impairment.",cinahl,10500545,10.3766/jaaa.19.9.3
automated audiometer for home based health care based on mobile app,"Hearing deficiency is one of the most important issues that people experience these days. This hearing loss is assessed using an audiometer. An audiometer is an instrument that tests the efficacy or hearing capacity of the ear. In this proposed study, an Android app is created along with a bone conduction device that functions as an audiometer for bone conduction. The aim and objectives of the proposed study are as follows: i) to create an automatic bone conduction system for self-diagnosing a patient's hearing ability; ii) to use pure tone audiometry to assess auditory thresholds at various frequencies and classify the normal and hearing loss patients with different machine learning classifiers. The right and left ear features of normal and hearing loss patients were given as the input to the SVM, k-NN and Naive Bayes classifier. The Naive bayes classifier produced 100% accuracy and outperformed well compared to the SVM (98%) and k-NN classifier (98%). The developed mobile app would be used to conduct the self-audiometry test at home and reduce the expense of audiometry tests.  © 2023 Author(s).",scopus,2-s2.0-85159950202,10.1063/5.0126375
hearing loss in children after tympanic membrane perforation cluster analysis of 27 cases,"Introduction: Tympanic membrane perforation (TMP) may be caused by several factors but commonly leads to conductive hearing loss. This study aims to characterize the profiles of hearing loss in pediatric patients with TMPs. Material and methods: A retrospective analysis of the medical charts of 27 patients was conducted. Otoscopy of the TM was done and pure tone audiometry was used to assess hearing loss. Cluster analysis was applied to evaluate the profiles of hearing loss and to find possible relations between profiles of hearing loss and the location of the perforation on the TM. Results: Cluster analysis revealed three types of hearing loss. The mean hearing loss in cluster 1 (6 cases) was above 30 dB, mainly as the result of perforation after chronic otitis media. Hearing loss in clusters 2 (9 cases) and 3 (12 cases) was less than 30 dB. In cluster 2 the perforation was mostly located in the posterior quadrants, while in cluster 3 it was most commonly in the inferior quadrants. In clusters 2 and 3, perforation was usually caused by slap of the open hand, injury, or past ventilation tube. Conclusions: Three different profiles (clusters) of hearing loss resulting from TMP were identified. Force of injury, etiology of the injury, and inflammation produce different sizes of perforations. Conductive hearing loss increases with perforation size and is independent of TM location. In general, hearing loss classification methods have the potential to improve diagnostic procedures, surgery, and rehabilitation of patients with TMPs.",cinahl,2083389X,10.17430/JHS.2022.12.4.5
initial symptoms and retrospective evaluation of prognosis in menieres disease,"Clinical studies on an initial symptom and a long-term course of vertigo and hearing impairment and retrospective evaluation of the prognosis were performed in Meniere's disease. One hundred and fifty-one patients (67 males and 84 females) with Meniere's disease were treated in the Neuro-otological clinic, Kitasato University Hospital from 1990 to 1995. Ages ranged from 17 to 77 years (mean 47.3 years) at the onset of the disease when the first vertigo attack occurred. There were 106 (70.1%) in their 30s, 40s and 50s, and 28 (18.5%) aged 60 years or over. Seventy-eight patients visited the clinic within one year of the onset of the disease, but the mean interval was 4 years and 5 months (the longest was 25 years). The mean duration time for the follow-up studies from the time of their first visit to the hospital was 2 years and 5 months. The bilateral ears were invaded in 19 patients (12.6%) and the mean length of their time course was 9 years and 10 months which is longer than the length in unilateral cases. Several important key points for diagnosis of Meniere's disease were investigated in 28 of the 151 cases who had been followed up successfully over a relatively long time course (the mean follow-up time was 7 years and 3 months). Fluctuated or stational cochlear signs, such as tinnitus, hearing impairment and/or fullness in the ear, had started prior to the onset of the first vertigo attack in 17 (61%) of 28 cases. Vertigo without cochlear sign appearing at the onset and cochlear signs were combined later in six (21%) of the 28 cases. Only five (18%) of the 28 cases had vertigo combined with a cochlear sign simultaneously at the onset of the disease. The affected ear was on the left in 15 cases and on the right in seven of 22 unilateral cases. In six bilateral cases the left ear was the first to be invaded in four out of six cases. The interval between the first and second attacks was over 1 year in six of the 28 cases and over 6 months in 10 of the 28 cases. Nine out of the 28 patients had recurrence of vertigo attacks during the first month and five of the nine had a cluster of attacks in the first month. Our study of 28 patients over a long time course revealed eight patients (28.6%) free from the disease. These patients had no recurrence of vertigo for more than 2 years after their last attack, and sixteen (57.1%) of the 28 patients had no recurrence of vertigo for more than 1 year. However, a long period of relief time of more than 2 years in 11 of the 28 patients and a period of more than I year was noticed in 16 of the 28 patients. Hearing levels at the middle and low frequencies in the firrst hearing test were compared with the last test. The mean of hearing levels changed from 38.1 to 36.2 dB after 2 years and I month in six cases with the right ear affected and from 34.1 to 45.3 dB after 5 years and 3 months in 15 cases with the left ear affected, but in seven cases with bilateral diseased ears the hearing in both ears became worse, from 25.5 to 57.1 dB in the right ear and from 30.5 to 53.6 dB in the left ear during a period of more than 10 years. These clinical findings should be utilized for diagnosis at the onset of Meniere's disease to determine the interval for observation in order to evaluate the efficacy of treatment.",scopus,2-s2.0-0029692037,10.3109/00016489609124348
exploring factors that contribute to the success of rehabilitation with hearing aids,"166. Ear Hear. 2023 Nov-Dec 01;44(6):1514-1525. doi: 10.1097/AUD.0000000000001393. Epub 2023 Jun 9.Exploring Factors That Contribute to the Success of Rehabilitation With Hearing Aids.Lansbergen SE(1), Versfeld N(2), Dreschler WA(1).Author information:(1)Department(s), Clinical and Experimental Audiology, Amsterdam UMC, University of Amsterdam, Amsterdam, The Netherlands.(2)Otolaryngology Head and Neck Surgery, Ear and Hearing, Amsterdam UMC, Vrije Universiteit Amsterdam, Amsterdam Public Health Research Institute, Amsterdam, Boelelaan, The Netherlands.OBJECTIVES: Hearing aids are an essential and important part of hearing rehabilitation. The combination of technical data on hearing aids and individual rehabilitation needs can give insight into the factors that contribute to the success of rehabilitation. This study sets out to investigate if different subgroups of (comparable) hearing aids lead to differences in the success of rehabilitation, and whether these differences vary between different domains of auditory functioning.DESIGN: This study explored the advantages of including patient-reported outcome measures (PROMs) in the process of purchasing new hearing aids in a large sample of successful hearing aid users. Subject data were obtained from 64 (commercial) hearing aid dispensers and 10 (noncommercial) audiological centers in the Netherlands. The PROM was a 32-item questionnaire and was used to determine the success of rehabilitation using hearing aids by measuring auditory disability over time. The items were mapped on six domains of auditory functioning: detection, discrimination, localization, speech in quiet, speech in noise, and noise tolerance, encompassing a variety of daily-life listening situations. Hearing aids were grouped by means of cluster analysis, resulting in nine subgroups. In total, 1149 subjects were included in this study. A general linear model was used to model the final PROM results. Model results were analyzed via a multifactor Analysis of Variance. Post hoc analyses provided detailed information on model variables.RESULTS: Results showed a strong statistically significant effect of hearing aids on self-perceived auditory functioning in general. Clinically relevant differences were found for auditory domains including detection, speech in quiet, speech in noise, and localization. There was only a small, but significant, effect of the different subgroups of hearing aids on the final PROM results, where no differences were found between the auditory domains. Minor differences were found between results obtained in commercial and noncommercial settings, or between novice and experienced users. Severity of Hearing loss, age, gender, and hearing aid style (i.e., behind-the-ear versus receiver-in-canal type) did not have a clinically relevant effect on the final PROM results.CONCLUSIONS: The use of hearing aids has a large positive effect on self-perceived auditory functioning. There was however no salient effect of the different subgroups of hearing aids on the final PROM results, indicating that technical properties of hearing aids only play a limited role in this respect. This study challenges the belief that premium devices outperform basic ones, highlighting the need for personalized rehabilitation strategies and the importance of evaluating factors contributing to successful rehabilitation for clinical practice.Copyright © 2023 The Authors. Ear & Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health, Inc.DOI: 10.1097/AUD.0000000000001393PMCID: PMC10583950",pubmed,37792897,10.1097/AUD.0000000000001393
cognitively inspired feature extraction and speech recognition for automated hearing loss testing,"Hearing loss, a partial or total inability to hear, is one of the most commonly reported disabilities. A hearing test can be carried out by an audiologist to assess a patient’s auditory system. However, the procedure requires an appointment, which can result in delays and practitioner fees. In addition, there are often challenges associated with the unavailability of equipment and qualified practitioners, particularly in remote areas. This paper presents a novel idea that automatically identifies any hearing impairment based on a cognitively inspired feature extraction and speech recognition approach. The proposed system uses an adaptive filter bank with weighted Mel-frequency cepstral coefficients for feature extraction. The adaptive filter bank implementation is inspired by the principle of spectrum sensing in cognitive radio that is aware of its environment and adapts to statistical variations in the input stimuli by learning from the environment. Comparative performance evaluation demonstrates the potential of our automated hearing test method to achieve comparable results to the clinical ground truth, established by the expert audiologist’s tests. The overall absolute error of the proposed model when compared with the expert audiologist test is less than 4.9 dB and 4.4 dB for the pure tone and speech audiometry tests, respectively. The overall accuracy achieved is 96.67% with a hidden Markov model (HMM). The proposed method potentially offers a second opinion to audiologists, and serves as a cost-effective pre-screening test to predict hearing loss at an early stage. In future work, authors intend to explore the application of advanced deep learning and optimization approaches to further enhance the performance of the automated testing prototype considering imperfect datasets with real-world background noise. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",scopus,2-s2.0-85061590715,10.1007/s12559-018-9607-4
using machine learning to predict sensorineural hearing loss,,base,0450550955d30e032e8f8b74552d710d13dd4a8de0b7c0826e5d2778e65705df,
feasibility of constructing a unilateral sudden sensorineural hearing loss machine learning classification model based on diffusion tensor imaging ,"Objective: To explore the feasibility of constructing a machine learning classification model for unilateral sudden sensorineural hearing loss (SSHL) patients and normal controls based on diffusion tensor imaging. Methods: Prospective collection of 84 patients with untreated SSHL were recruited from the otolaryngology department of the Union Hospital of Tongji Medical College of Huazhong University of Science and Technology between June 2013 to May 2015 as the SSHL group. Meanwhile, a total of 63 healthy volunteers who were no any ear disease history, and the hearing function were confirmed with pure tone audiometry, were collected as the control group. All subjects underwent a brain DTI scan. The data were divided into the training set and validation set according to the ratio of 7 to 3, that was, the training set contained 58 cases of SSHL patients and 44 control groups, and the validation set included 26 cases of SSHL patients and 19 control groups. A vector which included the DTI parameters such as fractional anisotropy, mean diffusivity, axial diffusivity and radial diffusivity was constructed with the software R. The LASSO regression of machine learning method was used to perform feature dimensionality reduction and construct a classification model. The training set samples were used to map the nomogram based on the multivariate logistic analysis method, the validation set and the AUC were used to evaluate the prediction ability of the nomogram, and the calibration curve was used to evaluate the model. Results: From the 200 feature vectors including the fractional anisotropy (FA), mean diffusivity (MD), axial diffusivity (AD), and radial diffusivity (RD) values of each brain region, after each dimension reduction process, a total of six features were retained, which were the MD of left superior corona radiate and right superior fronto-occipital fasciculus, the AD of the body of corpus callosum, and the RD of left inferior cerebellar peduncle, left superior corona radiate and right posterior limb of internal capsule. The six features of patients with unilateral SSHL were higher than the control group, and the difference was statistically significant (P<0.05). Based on this, a two-class model is constructed and a nomogram is drawn. The sensitivity, specificity, accuracy and AUC of the training set were 93.1% (54/58), 72.7% (32/44), 84.3% (86/102) and 0.854, respectively; the sensitivity, specificity, accuracy and AUC of validation set were 80.8% (21/26), 84.2% (16/19), 82.2% (37/45), 0.870, respectively. Nomogram could significantly improve the classification efficiency of the control group and patients, and the model with the LASSO method showed a higher prediction curve than other models. Conclusions: The machine learning classification model based on DTI metrics can effectively distinguish patients with unilateral sudden sensorineural deafness from healthy control people. Copyright © 2019 by the Chinese Medical Association.",scopus,2-s2.0-85076413394,10.3760/cma.j.issn.1005-1201.2019.09.010
preschool hearing screening program first evidence studyiii international congress of apta associacao portuguesa de audiologistas october 1415 2022 batalha portugal,"Background: According to the World Health Organization in 2020, 34 million children around the world have deafness or hearing loss, in which 60% of cases can be prevented. This study intends to identify hearing changes in preschoolers who attend the cluster of schools in the Centro Hospitalar Entre Douro e Vouga, CHEDV's region and to emphasize the value of preschool hearing evaluation in order to prevent long-term effects on the child's overall development. Currently, this type of screening is not carried out in our country. Material and methods: 46 children (92 ears), aged between 4 and 6 years old, were evaluated by Santa Maria da Feira school educators using anamnesis, otoscopy, tympanogram, pure-tone air-condution at 0.5-4 kHz and vocal audiometry. Children who displayed changes in these assessments were alerted to the assistant doctor and directed to an ENT consultation. Results: We found that 28% of the ears tested had mild hearing loss and 3% had moderated hearing loss, according to BIAP 02/1. In 57% of the instances (40% type C and 17% type B), the tympanogram appeared altered, meaning that the middle ear was altered in more than half of the examined ears. Additionally, 20 of 46 children (43.47% of the cases) were found to have hearing disorders. Conclusions: The high prevalence of children with hearing disorders found emphasizes the significance of the Pre-School Hearing Screening in the hearing assessment of pre-school aged children to minimize the detrimental effects of hearing on their development, cognition, communication, and interpersonal relationships. It is important to note that such initiatives highlight the necessity to design hearing health education programs in addition to the diagnosis and treatment of middle ear disorders.",cinahl,2083389X,
is having hearing loss fundamentally different multigroup structural equation modeling of the effect of cognitive functioning on speech identification,"148. Ear Hear. 2022 Sep-Oct 01;43(5):1437-1446. doi: 10.1097/AUD.0000000000001196. Epub 2022 Jan 4.Is Having Hearing Loss Fundamentally Different? Multigroup Structural Equation Modeling of the Effect of Cognitive Functioning on Speech Identification.Marsja E(1)(2), Stenbäck V(1)(2), Moradi S(1)(2)(3), Danielsson H(1)(2), Rönnberg J(1)(2).Author information:(1)Division of Disability Research, Department of Behavioral Sciences and Learning, Linköping University, Linköping, Sweden.(2)Linnaeus CENTRE HEAD, The Swedish Institute for Disability research, Linköping University, Linköping, Sweden.(3)Faculty of Health and Social Sciences, Department of Health, Social and Welfare Studies, University of South-Eastern Norway, Porsgrunn, Norway.OBJECTIVES: Previous research suggests that there is a robust relationship between cognitive functioning and speech-in-noise performance for older adults with age-related hearing loss. For normal-hearing adults, on the other hand, the research is not entirely clear. Therefore, the current study aimed to examine the relationship between cognitive functioning, aging, and speech-in-noise, in a group of older normal-hearing persons and older persons with hearing loss who wear hearing aids.DESIGN: We analyzed data from 199 older normal-hearing individuals (mean age = 61.2) and 200 older individuals with hearing loss (mean age = 60.9) using multigroup structural equation modeling. Four cognitively related tasks were used to create a cognitive functioning construct: the reading span task, a visuospatial working memory task, the semantic word-pairs task, and Raven's progressive matrices. Speech-in-noise, on the other hand, was measured using Hagerman sentences. The Hagerman sentences were presented via an experimental hearing aid to both normal hearing and hearing-impaired groups. Furthermore, the sentences were presented with one of the two background noise conditions: the Hagerman original speech-shaped noise or four-talker babble. Each noise condition was also presented with three different hearing processing settings: linear processing, fast compression, and noise reduction.RESULTS: Cognitive functioning was significantly related to speech-in-noise identification. Moreover, aging had a significant effect on both speech-in-noise and cognitive functioning. With regression weights constrained to be equal for the two groups, the final model had the best fit to the data. Importantly, the results showed that the relationship between cognitive functioning and speech-in-noise was not different for the two groups. Furthermore, the same pattern was evident for aging: the effects of aging on cognitive functioning and aging on speech-in-noise were not different between groups.CONCLUSION: Our findings revealed similar cognitive functioning and aging effects on speech-in-noise performance in older normal-hearing and aided hearing-impaired listeners. In conclusion, the findings support the Ease of Language Understanding model as cognitive processes play a critical role in speech-in-noise independent from the hearing status of elderly individuals.Copyright © 2022 The Authors. Ear & Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health, Inc.DOI: 10.1097/AUD.0000000000001196",pubmed,34983896,10.1097/AUD.0000000000001196
diagnosis of auditory function in infants with covid19 part i,"Purpose. Identify the possible influence of the SARS-CoV-2 virus on the auditory function of children of the first year of life. Materials and methods. An analysis of the auditory function of 100 children born to mothers who had undergone COVID-19 during pregnancy and had recovered independently during the first year of life, as well as epicrises of case histories of the mothers themselves and their children, was carried out. The work was retrospective prospective and consisted of 2 stages. At the first stage, discharge epicrises of case histories of children and their mothers were studied, and at the second stage, audiological examination was carried out for all children by the method of auditory induced potentials, otoacoustic emission and tympanometry. The data we obtained were processed by various statistical methods using Cohonen artificial neural network-based clustering and a rigid clustering procedure by age at the time of examination, followed by analysis of the resulting clusters. Results. Analysis of auditory function at the time of treatment showed that there was no record of induced otoacoustic emission in 27 children, of whom 10 were full-term and 17 premature. In 17 of the 27 children, 17% of the total number of children, sensorineural stiffness was detected, namely, bilateral in 12 children, unilateral in one child and bilateral deafness in 4 children. The cluster analysis revealed worse parameters of induced otoacoustic emission and auditory induced potentials in the group where mothers had COVID-19 during pregnancy than in the group where the children themselves were sick. Conclusion. The cases of children with sensorineural hearing loss identified in our work indirectly, as well as the results of cluster analysis, may indicate a possible direct effect of the SARS-CoV-2 virus on certain structures of the auditory analyzer. © 2024, Professionalnye Izdaniya. All rights reserved.",scopus,2-s2.0-85189626743,10.34883/PI.2024.14.1.033
mri screening in vestibular schwannoma a deep learningbased analysis of clinical and audiometric data,"736. Otol Neurotol Open. 2023 Mar 9;3(1):e028. doi: 10.1097/ONO.0000000000000028. eCollection 2023 Mar.MRI Screening in Vestibular Schwannoma: A Deep Learning-based Analysis of Clinical and Audiometric Data.Kortebein S(1), Gu S(2), Dai K(1), Zhao E(1), Riska K(1), Kaylie D(1), Hoa M(2).Author information:(1)Department of Head and Neck Surgery and Communication Sciences, Duke University School of Medicine, Durham, NC.(2)Auditory Development and Restoration Program, NIDCD Otolaryngology Surgeon-Scientist Program, Division of Intramural Research, NIDCD/NIH, Bethesda, MD.OBJECTIVE: To find a more objective method of assessing which patients should be screened for a vestibular schwannoma (VS) with magnetic resonance imaging (MRI) using a deep-learning algorithm to assess clinical and audiometric data.MATERIALS AND METHODS: Clinical and audiometric data were collected for 592 patients who received an audiogram between January 2015 and 2020 at Duke University Health Center with and without VS confirmed by MRI. These data were analyzed using a deep learning-based analysis to determine if the need for MRI screening could be assessed more objectively with adequate sensitivity and specificity.RESULTS: Patients with VS showed slightly elevated, but not statistically significant, mean thresholds compared to those without. Tinnitus, gradual hearing loss, and aural fullness were more common in patients with VS. Of these, only the presence of tinnitus was statistically significant. Several machine learning algorithms were used to incorporate and model the collected clinical and audiometric data, but none were able to distinguish ears with and without confirmed VS. When tumor size was taken into account the analysis was still unable to distinguish a difference.CONCLUSIONS: Using audiometric and clinical data, deep learning-based analyses failed to produce an adequately sensitive and specific model for the detection of patients with VS. This suggests that a specific pattern of audiometric asymmetry and clinical symptoms may not necessarily be predictive of the presence/absence of VS to a level that clinicians would be comfortable forgoing an MRI.Copyright © 2023 The Authors. Published by Wolters Kluwer Health, Inc. on behalf of Otology & Neurotology, Inc.DOI: 10.1097/ONO.0000000000000028PMCID: PMC10950172",pubmed,38516318,10.1097/ONO.0000000000000028
study of presbycusis and single microphone noise reduction techniques for hearing aids,"Age-Related hearing loss or Presbycusis is a slow, progressive hearing loss and affects both ears equally. First signs of hearing loss are inability to understand speech in noisy environments. This study includes the changes in the response of auditory filter and the hearing thresholds due to the masking sounds for elderly with hearing impaired. The hearing aid designer should come across this study before designing a suitable device not just only amplifying the sounds. The dynamic range of hearing thresholds especially for hearing impaired changes due to Masking. Therefore, to understand the speech for hearing impaired requires high SNRs due to broadening of auditory filter compared to normal hearing. To improve the SNR, it is challenging to select best noise reduction technique among plenty. This study presents number of SMNR methods to get clean speech by estimating noise from noisy speech. It is also discusses the combination of the techniques and the idea behind the techniques. © 2018 IEEE.",scopus,2-s2.0-85071021820,10.1109/ICCIC.2018.8782402
ways of rehabilitation of hearingimpaired persons revalidatiemogelijkheden voor slechthorenden,"Hearing impairment is a symptom, not a diagnosis. A number of types of hearing impairment can be distinguished. The self-reported hearing problems cluster around six hearing factors, the most important of which are speech understanding in noise and localisation of a sound source. For these capabilities equivalent functioning of both ears is important. The general practitioner can determine diagnosis and severity of the impairment using rather simple tools. When the cause of the impairment cannot be reduced in a proper way an adaptation of the sound to the impaired ear will be indicated. This can be arranged by either an ENT specialist or a centre for audiology. The selection of a proper hearing aid requires expertise and particular attention for the complaints. It is of the utmost importance that the hearing-impaired person can try out the effects of the hearing aid in daily circumstances for some weeks. If the patient, members of the family or the prescriber are not satisfied with the results, supplementary help is required for example training in communication skills or special devices.",scopus,2-s2.0-0032501756,
certain investigation of various algorithms to improvise the quality of hearing aid,"Hearing loss, marked by an inability to hear sounds below a 20 dB threshold, often results from dysfunction in the ear’s organs, associated nerves, or the auditory part of the brain. Deaf individuals encounter direct or indirect discrimination in daily life, as highlighted by National Centre for the Deaf data showing a significant employment gap. Only 48% of deaf individuals are employed, compared to 72% of their hearing counterparts. This discrimination leads to missed opportunities and some deaf individuals leaving jobs due to disability-related biases. After careful study of research papers done by scholars and working with low-end and High-end Hearing aid devices, we decide to start with the analysis of audio signals and implementing the different algorithms in computational tools like MATLAB and Python. Subsequently, we found the quality of hearing aid devices depends on three factors SNR, SI-SNR, and PESQ. Working with different algorithms and modifying some, we achieved better quality hearing aids than existing high-end hearing aids using the SepFormer algorithm. The overall systems shows better results on the signal quality improvements to ensure the HA perforamnce. The Metric GAN+based method has the good variations in SI-SNR of 9.42, PESQ of 3.15. Hence, using the proposed model enhance the quality of HA and supports the dumb people. The quality of the HA is analysed using this proposed mehtod with AI enabled technlogy.",ieee,,10.1109/STCR59085.2023.10396954
predicting cochlear implants score with the aid of reconfigured long shortterm memory,"A surgical procedure namely the Cochlear implantation aims in fitting the electronic device the cochlear implant. This electronic device helps person with moderate to severe hearing loss. It becomes very important to treat children with auditory deprivation much earlier, since it prohibits their language development skill too. This research aims to develop a model that can be used to guide Cochlear Implants (CI) programming for new patients in the children of 5 to 10 ages using visual cross modal data obtained from previously programmed patients. The cohort chosen is bilateral congenitally deaf children. This age group is selected since their language development is affected due to their auditory deprivations. The design is based on obtaining the analysis of cross modal plasticity using the visual evoked potential. AI based techniques, which is formed using the patient database. The goal is to use patients, real time database collected from the children and observe if it is likely to discover patterns in the data that can predict something about future patients. The resolution would be a program that can discover factors for the auditory deprived. The objective of this work is to apply Long Short-Term Memory (LSTM) network based Artificial Intelligence (AI) model to discover the unknown pattern. LSTM is suited to classify, process and predict time series given time lags of unknown duration. Relative insensitivity to gap length gives an advantage to LSTM over alternative RNNs. To augment an additional performance, the investigation comprises Enhanced Swarm based Crow Search Optimization (ESCSO) to identify optimal weights. The results exhibit the dominance of suggested ESCSO based LSTM technique over other techniques. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",scopus,2-s2.0-85138578989,10.1007/s11042-022-13812-0
how can ehealth meet the hearing and communication needs of adults with hearing impairment and their significant others a group concept mapping study,"Objectives: To seek the perspectives of key stakeholders regarding: (1) how eHealth could help meet the hearing and communication needs of adults with hearing impairment and their significant others; and (2) how helpful each aspect of eHealth would be to key stakeholders personally. Design: Group concept mapping, a mixed-methods participatory research method, was used to seek the perspectives of key stakeholders: adults with hearing impairment (n = 39), significant others (n = 28), and hearing care professionals (n = 56). All participants completed a short online survey before completing one or more of the following activities: brainstorming, sorting, and rating. Brainstorming required participants to generate ideas in response to the focus prompt, ""One way I would like to use information and communication technologies to address the hearing and communication needs of adults with hearing loss and their family and friends is to."" The sorting task required participants to sort all statements into groups that made sense to them. Finally, the rating task required participants to rate each of the statements according to ""How helpful would this idea be to you?"" using a 5-point Likert scale. Hierarchical cluster analysis was applied to the ""sorting"" data to develop a cluster map using the Concept Systems software. The ""rating"" data were subsequently analyzed at a cluster level and an individual-item level using descriptive statistics. Differences in cluster ratings between stakeholder groups were examined using Kruskal-Wallis tests. Results: Overall, 123 statements were generated by participants in response to the focus prompt and were included in subsequent analyses. Based on the ""sorting"" data and hierarchical cluster analysis, a seven-cluster map was deemed to be the best representation of the data. Three key themes emerged from the data, including using eHealth to (1) Educate and Involve Others; (2) Support Aural Rehabilitation; and (3) Educate About and Demonstrate the Impacts of Hearing Impairment and Benefits of Hearing Rehabilitation. Overall median rating scores for each cluster ranged from 3.97 (educate and involve significant others) to 3.44 (empower adults with hearing impairment to manage their hearing impairment from home). Conclusions: These research findings demonstrate the broad range of clinical applications of eHealth that have the capacity to support the implementation of patient- and family-centered hearing care, with self-directed educational tools and resources typically being rated as most helpful. Therefore, eHealth appears to be a viable option for enabling a more biopsychosocial approach to hearing healthcare and educating and involving significant others in the hearing rehabilitation process without adding more pressure on clinical time. More research is needed to inform the subsequent development of eHealth interventions, and it is recommended that health behavior change theory be adhered to for such interventions. © 2022 Lippincott Williams and Wilkins. All rights reserved.",scopus,2-s2.0-85120701675,10.1097/AUD.0000000000001097
hearing loss prediction using machine learning approaches contributions limitations and issues,"Hearing, one of the five basic human senses is the ability to perceive sounds and give meaning to them. Hearing loss is a significant health problem affecting children and adults and is growing exponentially. There is a lack of knowledge regarding hearing loss despite enough awareness, resulting in detection and treatment delays. The need for detection at an early stage is significant so that people can take necessary precautions given the limited options for treatment. This paper aims to survey machine learning-based hearing loss prediction. We investigate datasets, machine learning methods, and their outcomes. We also discuss the constraints, difficulties, and intended future works. Based on the results of this survey, we have a greater understanding of the problem's complexity, the obstacles to developing a better system, and the scope of the research, which has led us to concentrate our efforts in the future on analysing data from newborns, infants, and young children. © 2022 IEEE.",scopus,2-s2.0-85145441672,10.1109/GCAT55367.2022.9972110
investigating the relation between minimum masking levels and hearing thresholds for tinnitus subtyping,"Heterogeneity of tinnitus imposes a challenge for its treatment. Identifying tinnitus subtypes might help to establish individualized diagnosis and therapies. The minimum masking level (MML) is a clinical tool defined as the minimum intensity of a masking sound required to cover tinnitus. Understanding the differences among masking patterns in patients could facilitate the task of subtyping tinnitus. Here, we studied the variability of hearing thresholds and MMLs among patients with tinnitus to identify tinnitus subgroups. A population of 366 consecutive patients from a specialized tinnitus clinic were included in the analysis. Hearing thresholds and MMLs were determined for octave frequencies from 0.25 to 8 kHz, as well as for 3 and 6 kHz. Subjects were divided into two groups according to whether their tinnitus was maskable (M, 329 subjects) or non-maskable (NM, 37 subjects). Hearing thresholds and tinnitus loudness did not differ significantly between both groups. The dimensionality of the data was reduced by means of principal component analysis (PCA), and the largest resulting components were used for clustering the data. The cluster analysis resulted in five clusters with differences in tinnitus pitch, lateralization, hearing thresholds and MML, as well as on age and gender. Clusters differed in contours of hearing thresholds and MML, describing patterns of low or high thresholds in combination with low or high MML. The clustering solution presented a low silhouette value (0.45), implying that the clustering is weak and could be artificial. The analysis pointed out the diversity across tinnitus patients. Our results suggest that there might be a continuum of patients' characteristics rather than discrete subgroups. © 2021 Elsevier B.V.",scopus,2-s2.0-85106621473,10.1016/bs.pbr.2021.04.011
expert system for aiding the diagnosis in hearing screening expertensystem zur diagnoseunterstutzung bei horprufungen,"For expert systems intended to aid diagnosis, a structure with five levels is proposed. These levels are the original area, the parameter and a reduced parameter layer, the classification and the final-decision layer. On the basis of this structures, an expert system was developed specifically for neonatal hearing screening with transitory evoked otoacoustic emissions (TEOAE). In a second step, this system was investigated for its suitability to classify emissions, regardless of patient age. For the comparison measurements in 252 mainly adult patients, some with an acquired hearing impairment, were used. To adapt the pass/fail decision to the extended evaluation criteria, the false classifications from a first run with the new data were used for training. Thereafter, the expert system, working with a wider data basis, classified the new data with a sensitivity that was increased by 4.8 % to 97.2 %, and a 2.0 % improvement in specificity to 95.5 % when classifying new data, These results, together with those of 97.3 % and 94.3 % achieved with exclusively neonatal TEOAE classification, clearly show the advantage of the expert system structures chosen, and document evidence of the practical applicability of the method.; For expert systems intended to aid diagnosis, a structure with five levels is proposed. These levels are the original area, the parameter and a reduced parameter layer, the classification and the final-decision layer. On the basis of this structures, an expert system was developed specifically for neonatal hearing screening with transitory evoked otoacoustic emissions (TEOAE). In a second step, this system was investigated for its suitability to classify emissions, regardless of patient age. For the comparison measurements in 252 mainly adult patients, some with an acquired hearing impairment, were used. To adapt the pass/fail decision to the extended evaluation criteria, the false classifications from a first run with the new data were used for training. Thereafter, the expert system, working with a wider data basis, classified the new data with a sensitivity that was increased by 4.8% to 97.2%, and a 2.0% improvement in specificity to 95.5% when classifying new data. These results, together with those of 97.3% and 94.3% achieved with exclusively neonatal TEOAE classification, clearly show the advantage of the expert system structures chosen, and document evidence of the practical applicability of the method.",scopus,2-s2.0-0034283180,10.1515/bmte.2000.45.9.248
a review and a framework of variables for defining and characterizing tinnitus subphenotypes,"Tinnitus patients can present with various characteristics, such as those related to the tinnitus perception, symptom severity, and pattern of comorbidities. It is speculated that this phenotypic heterogeneity is associated with differences in the underlying pathophysiology and personal reaction to the condition. However, there is as yet no established protocol for tinnitus profiling or subtyping, hindering progress in treatment development. This review summarizes data on variables that have been used in studies investigating phenotypic differences in subgroups of tinnitus, including variables used to both define and compare subgroups. A PubMed search led to the identification of 64 eligible articles. In most studies, variables for subgrouping were chosen by the researchers (hypothesis-driven approach). Other approaches included application of unsupervised machine-learning techniques for the definition of subgroups (data-driven), and subgroup definition based on the response to a tinnitus treatment (treatment response). A framework of 94 variable concepts was created to summarize variables used across all studies. Frequency statistics for the use of each variable concept are presented, demonstrating those most and least commonly assessed. This review highlights the high dimensionality of tinnitus heterogeneity. The framework of variables can contribute to the design of future studies, helping to decide on tinnitus assessment and subgrouping. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",scopus,2-s2.0-85097165031,10.3390/brainsci10120938
sensitivity and specificity of a method for diagnosis of military noiseinduced hearing loss,"144. J Acoust Soc Am. 2021 Jan;149(1):62. doi: 10.1121/10.0002977.Sensitivity and specificity of a method for diagnosis of military noise-induced hearing loss.Moore BCJ(1), von Gablenz P(2).Author information:(1)Department of Psychology, University of Cambridge, Downing Street, Cambridge, CB2 3EB, United Kingdom.(2)Institute of Hearing Technology and Audiology and Cluster of Excellence ""Hearing4All"", Oldenburg, Germany.Moore [(2020). J. Acoust. Soc. Am. 148, 884-894] proposed a method for the diagnosis of hearing loss produced by noise exposure during military service (denoted M-NIHL) based on the audiogram. This letter characterizes the sensitivity and specificity of the method, based on 116 ears of men claiming compensation for M-NIHL and 244 ears of an age-matched non-noise-exposed control group of men screened to match the noise-exposed group in age, absence of conductive hearing loss, no history of ear diseases, and asymmetry across ears ≤10 dB. The sensitivity was 0.97 and the specificity was 0.67, giving a discriminability index d' of 2.3.DOI: 10.1121/10.0002977",pubmed,33514161,10.1121/10.0002977
data glovebased sign language translation with convolutional neural networks,"This research was carried out because of the communication barriers that currently exist between hearing impaired and hearing people. These barriers hinder their integration into society and affect their interpersonal relationships. The objective of the study was to propose the development of a stationary assistive robot capable of displaying sign language interpretation through the combination of data gloves and the D-CNN and LSTM algorithm to facilitate the communication of hearing-impaired children in Huancayo. The triple diamond research design was used, where the mind map and the lotus diagram were used for the delimitation and definition of the problem. In addition, the IDEF0 technique was used to obtain a structured design of the project system. A morphological matrix was also used to choose the best solution for the problem. The chosen design contemplates the use of an Arduino UNO, flex sensors, accelerometers and gyroscopes for sign detection. The main algorithm consists of the union of a deep convolutional neural network and a LSTM for a correct sign classification module. The proposed design proposes to visualize the conceptual development of the project mentioned above.",ieee,,10.1109/CMAEE58250.2022.00020
hearing aid treatment for patients with mixed hearing loss part ii speech recognition in comparison to direct acoustic cochlear stimulation,"266. Audiol Neurootol. 2020;25(3):133-142. doi: 10.1159/000504285. Epub 2020 Jan 31.Hearing Aid Treatment for Patients with Mixed Hearing Loss. Part II: Speech Recognition in Comparison to Direct Acoustic Cochlear Stimulation.Wardenga N(1)(2), Snik AFM(3), Kludt E(4)(5), Waldmann B(6), Lenarz T(4)(5), Maier H(4)(5).Author information:(1)Cluster of Excellence Hearing4all, Hannover, Germany, wardenga.nina@mh-hannover.de.(2)Department of Otolaryngology, Hannover Medical School, Hannover, Germany, wardenga.nina@mh-hannover.de.(3)Department of Biophysics, Radboud University, Nijmegen, The Netherlands.(4)Cluster of Excellence Hearing4all, Hannover, Germany.(5)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.(6)Cochlear Deutschland GmbH and Co. KG, Hannover, Germany.BACKGROUND: The conventional therapy for severe mixed hearing loss is middle ear surgery combined with a power hearing aid. However, a substantial group of patients with severe mixed hearing loss cannot be treated adequately with today's state-of-the-art (SOTA) power hearing aids, as predicted by the accompanying part I of this publication, where we compared the available maximum power output (MPO) and gain from technical specifications to requirements for optimum benefit using a common fitting rule. Here, we intended to validate the theoretical assumptions from part I experimentally in a mixed hearing loss cohort fitted with SOTA power hearing aids. Additionally, we compared the results with an implantable hearing device that circumvents the impaired middle ear, directly stimulating the cochlea, as this might be a better option.OBJECTIVES: Speech recognition outcomes obtained from patients with severe mixed hearing loss supplied acutely with a SOTA hearing aid were studied to validate the outcome predictions as described in part I. Further, the results obtained with hearing aids were compared to those in direct acoustic cochlear implant (DACI) users.MATERIALS AND METHODS: Twenty patients (37 ears with mixed hearing loss) were provided and fitted with a SOTA power hearing aid. Before and after an acclimatization period of at least 4 weeks, word recognition scores (WRS) in quiet and in noise were studied, as well as the speech reception threshold in noise (SRT). The outcomes were compared retrospectively to a second group of 45 patients (47 ears) using the DACI device. Based on the severity of the mixed hearing loss and the available gain and MPO of the SOTA hearing aid, the hearing aid and DACI users were subdivided into groups with prediction of sufficient, partially insufficient, or very insufficient hearing aid performance.RESULTS: The patients with predicted adequate SOTA hearing aid performance indeed showed the best WRS in quiet and in noise when compared to patients with predicted inferior outcomes. Insufficient hearing aid performance at one or more frequencies led to a gradual decrease in hearing aid benefit, validating the criteria used here and in the accompanying paper. All DACI patients showed outcomes at the same level as the adequate hearing aid performance group, being significantly better than those of the groups with inadequate hearing aid performance. Whereas WRS in quiet and noise were sensitive to insufficient gain or output, showing significant differences between the SOTA hearing aid and DACI groups, the SRT in noise was less sensitive.CONCLUSIONS: Limitations of outcomes in mixed hearing loss individuals due to insufficient hearing aid performance can be accurately predicted by applying a commonly used fitting rule and the 35-dB dynamic range rule on the hearing aid specifications. Evidently, when outcomes in patients with mixed hearing loss using the most powerful hearing aids are insufficient, bypassing the middle ear with a powerful active middle ear implant or direct acoustic implant can be a promising alternative treatment.The Author(s). Published by S. Karger AG, Basel.DOI: 10.1159/000504285PMCID: PMC7265759",pubmed,32007992,10.1159/000504285
patterns in the social representation of hearing loss across countries how do demographic factors influence this representation,"This study aims to understand patterns in the social representation of hearing loss reported by adults across different countries and explore the impact of different demographic factors on response patterns. The study used a cross-sectional survey design. Data were collected using a free association task and analysed using qualitative content analysis, cluster analysis and chi-square analysis. The study sample included 404 adults (18 years and over) in the general population from four countries (India, Iran, Portugal and UK). The cluster analysis included 380 responses out of 404 (94.06%) and resulted in five clusters. The clusters were named: (1) individual aspects; (2) aetiology; (3) the surrounding society; (4) limitations and (5) exposed. Various demographic factors (age, occupation type, education and country) showed an association with different clusters, although country of origin seemed to be associated with most clusters. The study results suggest that how hearing loss is represented in adults in general population varies and is mainly related to country of origin. These findings strengthen the argument about cross-cultural differences in perception of hearing loss, which calls for a need to make necessary accommodations while developing public health strategies about hearing loss. © 2018, © 2018 British Society of Audiology, International Society of Audiology, and Nordic Audiological Society. Published by Informa UK Limited, trading as Taylor & Francis Group.",scopus,2-s2.0-85057085145,10.1080/14992027.2018.1516894
hearing aids,,cinahl,10745734,
ensemble filters with harmonize psosvm algorithm for optimal hearing disorder prediction,"Discovering a hearing disorder at an earlier intervention is critical for reducing the effects of hearing loss and the approaches to increase the remaining hearing ability can be implemented to achieve the successful development of human communication. Recently, the explosive dataset features have increased the complexity for audiologists to decide the proper treatment for the patient. In most cases, data with irrelevant features and improper classifier parameters causes a crucial influence on the audiometry system in terms of accuracy. This is due to the dependent processes of these two, where the classification accuracy performance could be worsened if both processes are conducted independently. Although the filter algorithm is capable of eliminating irrelevant features, it still lacks the ability to consider feature reliance and results in a poor selection of significant features. Improper kernel parameter settings may also contribute to poor accuracy performance. In this paper, an ensemble filters feature selection based on Information Gain (IG), Gain Ratio (GR), Chi-squared (CS), and Relief-F (RF) with harmonize optimization of Particle Swarm Optimization (PSO) and Support Vector Machine (SVM) is presented to mitigate these problems. Ensemble filters are utilized so that the initial top dominant features relevant for classification can be considered. Then, PSO and SVM are optimized simultaneously to achieve the optimal solution. The results on a standard Audiology dataset show that the proposed method produces 96.50% accuracy with optimal solution compared to classical SVM, which signifies the proposed method is effective in handling high dimensional data for hearing disorder prediction. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",scopus,2-s2.0-85147287898,10.1007/s00521-023-08244-2
artificial neural network ann prediction for noise risk assessment in industrial workplace,"Presently, it is estimated that 16 % of the disabling hearing loss in adults is attributed to occupational noise, ranging from 7 to 21 % in the various countries. With the rise of industrialization, sound pollution is set to increase every 10 years. To alleviate this, the Department of Occupational Safety and Health (DOSH) made it mandatory for industrial workers to undergo annual audiometric testing. However, it is still not enough as hearing loss isn't an annual occasion and hearing damage can occur either instantly or it progresses over time. This could lead to workers prolonging and progressing their hearing damage until the next year. Hence, the purpose of this study is to produce a predictive model to assess the risks of hearing loss for workers in the industrial workplace. This study devised artificial neural network prediction models to predict the level of hearing loss in an employee's left and right ears respectively, based on past audiometric data from The National Institute for Occupational Safety and Health (NIOSH). Once the model is complete, variables such as the sample size, batch size and hidden layers were manipulated to view its effect on the accuracy of the model. The prediction model yielded an overall accuracy of 92% with a range of 78% and 98% when predicting the individual classes of hearing loss and found that increasing certain parameters influences the accuracy of the prediction model. This study utilized 5000 data sample to prove that noise risk assessment can be done in a larger scale using artificial intelligence. © 2022 IET Conference Proceedings. All rights reserved.",scopus,2-s2.0-85174650568,10.1049/icp.2022.2280
automatic user preferences selection of smart hearing aid using bioaid,"162. Sensors (Basel). 2022 Oct 20;22(20):8031. doi: 10.3390/s22208031.Automatic User Preferences Selection of Smart Hearing Aid Using BioAid.Siddiqui HUR(1), Saleem AA(1), Raza MA(1), Zafar K(1), Russo R(2), Dudley S(3).Author information:(1)Institute of Computer Science, Khawaja Fareed University of Engineering and Information Technology, Rahim Yar Khan 64200, Pakistan.(2)Department of Brain and Behavioral Sciences, University of Pavia, 27100 Pavia, Italy.(3)School of Engineering, London South Bank University, London SE1 0AA, UK.Noisy environments, changes and variations in the volume of speech, and non-face-to-face conversations impair the user experience with hearing aids. Generally, a hearing aid amplifies sounds so that a hearing-impaired person can listen, converse, and actively engage in daily activities. Presently, there are some sophisticated hearing aid algorithms available that operate on numerous frequency bands to not only amplify but also provide tuning and noise filtering to minimize background distractions. One of those is the BioAid assistive hearing system, which is an open-source, freely available downloadable app with twenty-four tuning settings. Critically, with this device, a person suffering with hearing loss must manually alter the settings/tuning of their hearing device when their surroundings and scene changes in order to attain a comfortable level of hearing. However, this manual switching among multiple tuning settings is inconvenient and cumbersome since the user is forced to switch to the state that best matches the scene every time the auditory environment changes. The goal of this study is to eliminate this manual switching and automate the BioAid with a scene classification algorithm so that the system automatically identifies the user-selected preferences based on adequate training. The aim of acoustic scene classification is to recognize the audio signature of one of the predefined scene classes that best represent the environment in which it was recorded. BioAid, an open-source biological inspired hearing aid algorithm, is used after conversion to Python. The proposed method consists of two main parts: classification of auditory scenes and selection of hearing aid tuning settings based on user experiences. The DCASE2017 dataset is utilized for scene classification. Among the many classifiers that were trained and tested, random forests have the highest accuracy of 99.7%. In the second part, clean speech audios from the LJ speech dataset are combined with scenes, and the user is asked to listen to the resulting audios and adjust the presets and subsets. A CSV file stores the selection of presets and subsets at which the user can hear clearly against the scenes. Various classifiers are trained on the dataset of user preferences. After training, clean speech audio was convolved with the scene and fed as input to the scene classifier that predicts the scene. The predicted scene was then fed as input to the preset classifier that predicts the user's choice for preset and subset. The BioAid is automatically tuned to the predicted selection. The accuracy of random forest in the prediction of presets and subsets was 100%. This proposed approach has great potential to eliminate the tedious manual switching of hearing assistive device parameters by allowing hearing-impaired individuals to actively participate in daily life by automatically adjusting hearing aid settings based on the acoustic scene.DOI: 10.3390/s22208031PMCID: PMC9610183",pubmed,36298382,10.3390/s22208031
a model of speech recognition for hearingimpaired listeners based on deep learning,"27. J Acoust Soc Am. 2022 Mar;151(3):1417. doi: 10.1121/10.0009411.A model of speech recognition for hearing-impaired listeners based on deep learning.Roßbach J(1), Kollmeier B(2), Meyer BT(1).Author information:(1)Communication Acoustics and Cluster of Excellence Hearing4all, Carl von Ossietzky University, D-26111 Oldenburg, Germany.(2)Medical Physics and Cluster of Excellence Hearing4all, Carl von Ossietzky University, D-26111 Oldenburg, Germany.Automatic speech recognition (ASR) has made major progress based on deep machine learning, which motivated the use of deep neural networks (DNNs) as perception models and specifically to predict human speech recognition (HSR). This study investigates if a modeling approach based on a DNN that serves as phoneme classifier [Spille, Ewert, Kollmeier, and Meyer (2018). Comput. Speech Lang. 48, 51-66] can predict HSR for subjects with different degrees of hearing loss when listening to speech embedded in different complex noises. The eight noise signals range from simple stationary noise to a single competing talker and are added to matrix sentences, which are presented to 20 hearing-impaired (HI) listeners (categorized into three groups with different types of age-related hearing loss) to measure their speech recognition threshold (SRT), i.e., the signal-to-noise ratio with 50% word recognition rate. These are compared to responses obtained from the ASR-based model using degraded feature representations that take into account the individual hearing loss of the participants captured by a pure-tone audiogram. Additionally, SRTs obtained from eight normal-hearing (NH) listeners are analyzed. For NH subjects and three groups of HI listeners, the average SRT prediction error is below 2 dB, which is lower than the errors of the baseline models.DOI: 10.1121/10.0009411",pubmed,35364918,10.1121/10.0009411
functional anatomy of the human cochlear nerve and its role in microvascular decompressions for tinnitus,"781. Neurosurgery. 2004 Feb;54(2):381-8; discussion 388-90. doi: 10.1227/01.neu.0000103420.53487.79.Functional anatomy of the human cochlear nerve and its role in microvascular decompressions for tinnitus.De Ridder D(1), Ryu H, Møller AR, Nowé V, Van de Heyning P, Verlooy J.Author information:(1)Department of Neurosurgery and Otorhinolaryngology, University Hospital Antwerp, Wilrijkstraat 10, 2650 Edegem, Antwerp, Belgium. dirk.de.ridder@uza.beOBJECTIVE: The functional anatomy (i.e., tonotopy) of the human cochlear nerve is unknown. A better understanding of the tonotopy of the central nervous system segment of the cochlear nerve and of the pathophysiology of tinnitus might help to ameliorate the disappointing results obtained with microvascular decompressions in patients with tinnitus.METHODS: We assume that vascular compression of the cochlear nerve can induce a frequency-specific form of hearing loss and that when the nerve is successfully decompressed, this hearing loss can recuperate. Thirty-one patients underwent a microvascular decompression of the vestibulocochlear nerve for vertigo or tinnitus. Preoperative audiograms were subtracted from postoperative audiograms, regardless of the surgical result with regard to the tinnitus and vertigo, because the hearing improvement could be the only sign of the vascular compression. The frequency of maximal improvement was then correlated to the site of vascular compression. A tonotopy of the cochlear nerve was thus obtained.RESULTS: A total of 18 correlations can be made between the site of compression and postoperative maximal hearing improvement frequency when 5-dB hearing improvement is used as threshold, 13 when 10-dB improvement is used as threshold. A clear distribution can be seen, with clustering of low frequencies at the posterior and inferior side of the cochlear nerve, close to the brainstem, and close to the root exit zone of the facial nerve. High frequencies are distributed closer to the internal acoustic meatus and more superiorly along the posterior aspect of the cochlear nerve.CONCLUSION: The tonotopic organization of the cisternal segment of the cochlear nerve has an oblique rotatory structure as a result of the rotatory course of the cochlear nerve in the posterior fossa. Knowledge of this tonotopic organization of the auditory nerve in its cisternal course might benefit surgeons who perform microvascular decompression operations for the vestibulocochlear compression syndrome, especially in the treatment of unilateral severe tinnitus.DOI: 10.1227/01.neu.0000103420.53487.79",pubmed,14744285,10.1227/01.neu.0000103420.53487.79
cry analisys of deaf and normal hearing zeroto twoyear old childrens anlisis del llanto en nios hipoacsicos y normoyentes de 0 a 2 aos de edad,"Infant crying is a complex phenomenon that implies several functions: breathing, action of laryngeal and supra-laryngeal muscles under the control of the neurovegetative systems of the brainstem, and the limbic system, and the association of cortical areas and the cerebellum. Although it is a communication system different to babbling and language, it is related with the future development of phonation. Cry analysis provides information about the neurophysiologic and psychological states of newborns and the identification of perinatal abnormalities. It is necessary to discuss the subject extensively because there are new data on situations such as laringomalacia, congenital hypothyroidism, deafness and sleep apnea that seem to be associated to infantile crying behaviors. Infant cries can be analyzed as behavioral conditions (hunger, anger and pain cries) allows knowing of mother-child relationship or the effect under diverse cultural conditions, such as stress, emotional deprivation or illness. A spectrographic analysis of the cries may identify several characteristics: threshold, latency, duration of phonation, maximum and minimum of the fundamental frequency (F0), occurrence and maximum pitch of shift, gliding, melody, biphonation, bifurcation, noise concentration, quality of the voice, double harmonic break, glottal plosives, vibratos, melody types, F0 stability and inspiratory stridor. To date, it has not been possible to establish alteration patterns. The best studied variables are F0, its harmonics and the duration of each emission; it is accepted that F0 varies between 400 and 600 Hz, during 1.4 ± 0.6 s. Under such approaches, diverse alterations and risk factors have been studied: congenital alterations, malnutrition, sudden death, maternal exposition to drugs, prematurely born babies or perinatal asphyxia and disturbances of the central nervous system. Authors have reported F0 equal or less than 300 Hz in cases of sudden death or with high frequencies, near the 1000 Hz in the Cri du chat syndrome, perinatal asphyxia and other cases who died suddenly. During the cry, there is an increase of intra-abdominal pressure, heart rate and blood pressure, reduction of oxygen saturation, increase of the intra cranial-pressure, beginning of stress reactions, depletion of the energy anf oxygen reserves, such as the found in the Valsalva's maneuver. Every event of prolonged cries implies alteration of the breathing control like a Hering-Breuer reflex. Considering that some authors have proposed early vocalizations are a good predictor of deafness, in a previous paper we reported the characteristics of the cry of 20 deaf neonates. However, we were not able to demonstrate differences when comparing them with normal hearing neonates and infants, using only parametric methods. Still, we decided to go further and investigate the quality of infant cries of deaf neonates and infants. Material and methods. Twenty zero-to two-year old cases were studied; they were deaf children of both sexes; all cases were included in a follow-up program on the Human Communication Department of the National Institute of Perinatology of Mexico and were compared with 20 normal hearing children. We recorded Brain Stem Evoked Auditory Responses (BEAR) and cry recording using a digital Sony recorder during the physical exploration. We analyzed the frequency (Hz) and duration of the espiratory cries, the duration of inspiration between two cry emissions and the characteristics of the spectrogram. Quantitative analysis. The usual estimates of means and standard variation were obtained and they were compared with one way analysis of variance. We organized typologies of frequency by means of cluster techniques (Ward method). The distribution of the duration of the periods of crying and silence was explored with a contingency tables. Qualitative analysis. Two standardized observers visually analyzed all the cries to determine any variation of F0 and of harmonic frequencies. Whenever a variation of F0 was observed, we obtained maximum and minimum frequencies, as well as average duration of each cry emission. The procedure was validated by means of the graphic comparison with a Fourier's analysis. Results. Mean duration of cries in the deaf group was 0.5845 ± 0.6150 s (range 0.08-5.2 s), while in the group of normal hearing cases was 0.5387 ± 0.2631 (range 0.06-1.75 s). From the deaf group, five cases had very prolonged duration of cries, without statistical significance. The mean duration of the inspiration was 0.3962 ± 0.2326, with a range of 0.06 to 1.75 in the deaf group and of 0.4083 ± 0.1854, with a range of 0.21 at 0.96, in the controls, without difference among groups. There was no correlation between the time of espiratory cry and that of the inspiration. Three cry topologies were organized: one of shorter duration (mean 0.30 s), with 111 spectrograms, an intermediate one (mean 0.73) with 85 spectrograms and one of prolonged duration (mean 4.5 s) with spectrograms of three cases. Three topologies of the inspiratory period were obtained: one of short periods (mean 0.33 s), with 171 spectrograms, one of intermediate duration (mean 0.80 s) with 18 spectrograms and one of prolonged duration (mean 1.60 s) with three cases. There were no statistical differences of tipologies between the deaf groups and normal hearing cases. On the qualitative analysis of cries, we came across several variations which are interpreted as abnormalities: vibratos, poor melodic control, loss of fundamental frequencies, harmonic limited production, plosives, gliding, bi phonation, and a loss of intensity at end of cry emissions. These changes were also observed on the control cases, but only in a very limited number. Discussion. Cry spectrogram analysis are non invasive indicators of the neonate's neurophysiologic organization. Although cry duration varies in healthy newborns, the accepted variation for a normal range is 1.1 to 2.8 s, with standard deviations around 0.6 s. Consistent differences have not been demonstrated between risk and control groups. However, abnormal cases such as Down syndrome or severe asphyxia have very short cries, whereas on the Cri du chat syndrome the duration of cries is prolonged. Extended cries imply cardiac and respiratory risks which have been associated with later outcomes as development retardation and sudden death. There are also some questions to solve, such as the regulation and control of cry, starting from breathing mechanisms or from a sensorial afferent, mediated by hearing. The deaf infants are constituted in a study model, considering that the auditory afference is suppressed and the control of the cry is restricted to the breathing environment. In the studied spectrograms, the duration of the cry was within reported normal limits by other authors, inasmuch in the normal hearing control cases as in the deaf, except the dissident cases, but without these reaching statistical significance. Further research of brainstem function is needed for the abnormal cases with prolonged cry periods, since such cries are interpreted as an alteration of the breathing reflexes of Hering-Breuer, which might have a pathological meaning in the sense of the sob's spasm or even more severe risk factors as sleep apnea and even sudden death. The qualitative analysis in the deaf individuals demonstrated a poor quality and unstable character of melodic control, with a smaller number of harmonics. The deaf cases lost the relationship between the fundamental frequencies and their harmonics, mainly because of the participation of supraglottic structures that modulate pitch and due to the poorness of melodic control, either for monotony or due to the impossibility of returning to a normal pattern, following variations such as vibrato, plosives or noise concentration. In the cases of prolonged cries, starting from the third second, the sound intensity tends to diminish and the harmonics are lost, perhaps due to a decrease of the subglottal pressure of phonation. This finding supports the auditory control of crying related to breathing mechanisms. Conclusions. In preliminary terms, by means of the melodic analysis of the spectrograms, differences are demonstrated between the cries of the deaf and of the normal hearing cases. The increase of the complexity of the melody of the cry, or their poverty, are indicative of the neuromuscular function and they may support the evaluation of phonation before language development. The study of the spectrograms of deaf individuals does not constitute an element for the detection or for diagnosis since, to date, estimators of sensibility or of specificity have not been established, but they constitute a support for its integral evaluation, with the possibility of evaluating and of improving therapeutic rehabilitation.",scopus,2-s2.0-33846830014,
auditory brainstem response classification a hybrid model using time and frequency features,"211. Artif Intell Med. 2007 May;40(1):1-14. doi: 10.1016/j.artmed.2006.07.001. Epub 2006 Aug 22.Auditory brainstem response classification: a hybrid model using time and frequency features.Davey R(1), McCullagh P, Lightbody G, McAllister G.Author information:(1)Department of Language and Communication Science, City University, Northampton Square, London EC1V 0HB, UK.OBJECTIVE: The auditory brainstem response (ABR) is an evoked response obtained from brain electrical activity when an auditory stimulus is applied to the ear. An audiologist can determine the threshold level of hearing by applying stimuli at reducing levels of intensity, and can also diagnose various otological, audiological, and neurological abnormalities by examining the morphology of the waveform and the latencies of the individual waves. This is a subjective process requiring considerable expertise. The aim of this research was to develop software classification models to assist the audiologist with an automated detection of the ABR waveform and also to provide objectivity and consistency in this detection.MATERIALS AND METHODS: The dataset used in this study consisted of 550 waveforms derived from tests using a range of stimulus levels applied to 85 subjects ranging in hearing ability. Each waveform had been classified by a human expert as 'response=Yes' or 'response=No'. Individual software classification models were generated using time, frequency and cross-correlation measures. Classification employed both artificial neural networks (NNs) and the C5.0 decision tree algorithm. Accuracies were validated using six-fold cross-validation, and by randomising training, validation and test datasets.RESULTS: The result was a two stage classification process whereby strong responses were classified to an accuracy of 95.6% in the first stage. This used a ratio of post-stimulus to pre-stimulus power in the time domain, with power measures at 200, 500 and 900Hz in the frequency domain. In the second stage, outputs from time, frequency and cross-correlation classifiers were combined using the Dempster-Shafer method to produce a hybrid model with an accuracy of 85% (126 repeat waveforms).CONCLUSION: By combining the different approaches a hybrid system has been created that emulates the approach used by an audiologist in analysing an ABR waveform. Interpretation did not rely on one particular feature but brought together power and frequency analysis as well as consistency of subaverages. This provided a system that enhanced robustness to artefacts while maintaining classification accuracy.DOI: 10.1016/j.artmed.2006.07.001",pubmed,16930965,10.1016/j.artmed.2006.07.001
hearing distribution of idiopathic bilateral sensorineural hearing loss,"476. Nihon Jibiinkoka Gakkai Kaiho. 1993 Jan;96(1):18-23. doi: 10.3950/jibiinkoka.96.18.[Hearing distribution of idiopathic bilateral sensorineural hearing loss].[Article in Japanese]Hirayama M(1).Author information:(1)Department of Otolaryngology, Kitasato University, Sagamihara.We investigated the mode of progression of idiopathic bilateral sensorineural hearing loss diagnosed in patients seen in the Hearing Loss Clinic at the Department of Otolaryngology of Kitasato University Hospital. Entered into the study were 105 patients whose courses could be observed for more than 3 years. Audiograms were taken 1069 times in these 105 patients and were examined with regard to the distribution of hearing levels by frequency. Idiopathic bilateral sensorineural hearing loss was divided into three stages from the aspect of the time of hearing change stages I, II and III. Hearing clustering points of the respective stages were compared with each other. Proceeding from the peak of stage I to that of stage II was found to take place at the same peak as that of stage III. Another peak hearing clustering point was noted at s.o..DOI: 10.3950/jibiinkoka.96.18",pubmed,8459305,10.3950/jibiinkoka.96.18
interest operator analysis for automatic assessment of spontaneous gestures in audiometries,"Hearing loss is a common disease which affects a large percentage of the population. Hearing loss may have a negative impact on health, social participation, and daily activities, so its diagnosis and monitoring is indeed important. The audiometric tests related to this diagnosis are constrained when the patient suffers from some form of cognitive impairment. In these cases, audilogist must try to detect particular facial reactions that may indicate auditory perception. With the aim of supporting the audiologist in this evaluation, a screening method that analyzes video sequences and seeks for facial reactions within the eye area was proposed. In this research, a comprehensive survey of one of the most relevent steps of this methodology is presented. This survey considers different alternatives for the detection of the interest points and the classsification techniques. The provided results allow to determine the most suitable configuration for this domain.",scopus,2-s2.0-84902329254,10.5220/0004926102210229
bayesian puretone audiometry through active learning under informed priors,"763. Front Digit Health. 2021 Aug 13;3:723348. doi: 10.3389/fdgth.2021.723348. eCollection 2021.Bayesian Pure-Tone Audiometry Through Active Learning Under Informed Priors.Cox M(1), de Vries B(1)(2).Author information:(1)Signal Processing Systems Group, Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, Netherlands.(2)GN Hearing, Eindhoven, Netherlands.Pure-tone audiometry-the process of estimating a person's hearing threshold from ""audible"" and ""inaudible"" responses to tones of varying frequency and intensity-is the basis for diagnosing and quantifying hearing loss. By taking a probabilistic modeling approach, both optimal tone selection (in terms of expected information gain) and hearing threshold estimation can be derived through Bayesian inference methods. The performance of probabilistic model-based audiometry methods is directly linked to the quality of the underlying model. In recent years, Gaussian process (GP) models have been shown to provide good results in this context. We present methods to improve the efficiency of GP-based audiometry procedures by improving the underlying model. Instead of a single GP, we propose to use a GP mixture model that can be conditioned on side-information about the subject. The underlying idea is that one can typically distinguish between different types of hearing thresholds, enabling a mixture model to better capture the statistical properties of hearing thresholds among a population. Instead of modeling all hearing thresholds by a single GP, a mixture model allows specific types of hearing thresholds to be modeled by independent GP models. Moreover, the mixing coefficients can be conditioned on side-information such as age and gender, capturing the correlations between age, gender, and hearing threshold. We show how a GP mixture model can be optimized for a specific target population by learning the parameters from a data set containing annotated audiograms. We also derive an optimal tone selection method based on greedy information gain maximization, as well as hearing threshold estimation through Bayesian inference. The proposed models are fitted to a data set containing roughly 176 thousand annotated audiograms collected in the Nordic countries. We compare the predictive accuracies of optimized mixture models of varying sizes with that of an optimized single-GP model. The usefulness of the optimized models is tested in audiometry simulations. Simulation results indicate that an optimized GP mixture model can significantly outperform an optimized single-GP model in terms of predictive accuracy, and leads to significant increases the efficiency of the resulting Bayesian audiometry procedure.Copyright © 2021 Cox and de Vries.DOI: 10.3389/fdgth.2021.723348PMCID: PMC8521968",pubmed,34713188,10.3389/fdgth.2021.723348
machine learning based detection of hearing loss using auditory perception responses,"Hearing loss or hearing impairment is the primary reason of deafness throughout the world. Hearing impairment can occur to one or both the ears. If hearing loss is identified in time, it can be minimized by practicing specific precautions. In this paper, we investigate the likelihood of detection of hearing loss through auditory system responses. Auditory perception and human age are highly interrelated. Likewise, detecting a significant gap within the real age and the estimated age, the hearing loss can easily be identified. Our proposed system for human age estimation has promising results with a Root Mean Square Error (RMSE) value of 4.1 years, and classification performance efficiency for hearing loss is 94%, showing the applicability of our approach for detection of hearing loss. © 2019 IEEE.",scopus,2-s2.0-85084794921,10.1109/SITIS.2019.00034
beyond audiometric phenotype toward a differential diagnosis for presbycusisdp   2015,"Age-related hearing loss (ARHL; presbycusis) results from degeneration of neural and/or cochlear structures. A taxonomy distinguishing presbycusis subtypes according to site of lesion was originally proposed by linking audiometric results to histopathological findings. In most cases, the pathology is complex and audiometry and word recognition scores (WRS) are insufficient to identify the location(s) of pathologies along the auditory pathway, often referred to as the site(s) of lesion. Several sophisticated tests of auditory function, with some specifically designed to inspect cochlear or neural status (e.g., distortion product otoacoustic emissions [DPOAEs] and the auditory brainstem response [ABR]) are available today but not in routine use to distinguish between presbycutic subtypes. Because there is no pre-mortem method in place to identify contributing pathologies and their relative dominance in individual cases of presbycusis, the goal of the present study is to improve differential diagnosis in the hope of providing individualized therapeutics for those suffering ARHL. In order to determine candidacy and dosing for these treatments, specific diagnoses will become crucial. This dissertation thus systematically explores an extensive test battery composed of behavioral (audiometry and speech testing) and physiological (ABR, DPOAEs, and electrocochleography) assays of auditory function in presbycutic ears. We compare these responses to data from normal hearing subjects who serve as a reference group. Quadratic (f2-f 1) DPOAEs are incorporated as a potential means of gaining supplementary information regarding cochlear function. As f2-f1 DPOAEs have not been studied extensively, our initial experiments were focused on determining optimal stimulus parameters to elicit them. Results show narrow stimulus frequency ratios (1.14) and moderately high stimulus levels (70 dB SPL) are ideal. We initially set out to uncover a method of diagnosis for ARHL that improves upon the ""gold standard"" (audiometry and WRS). Two analytical strategies (principal component and hierarchical cluster analyses) were used to evaluate various phenotyping strategies. The results provide a potential solution, revealing the feasibility of a much more detailed diagnosis of presbycusis subtypes even in this limited data set. In the future, these phenotyping techniques should be applied on an epidemiological scale and ultimately, might inform appropriate treatment methodologies for each presbycusis subtype. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc14&AN=2015-99020-424
genderspecific hearing loss in german adults aged 18 to 84 years compared to usamerican and current european studies,"149. PLoS One. 2020 Apr 23;15(4):e0231632. doi: 10.1371/journal.pone.0231632. eCollection 2020.Gender-specific hearing loss in German adults aged 18 to 84 years compared to US-American and current European studies.von Gablenz P(1), Hoffmann E(2), Holube I(1).Author information:(1)Institute of Hearing Technology and Audiology, Jade University of Applied Sciences and Cluster of Excellence ""Hearing4all"", Oldenburg, Germany.(2)Department of Audiology, Aalen University of Applied Sciences, Aalen, Germany.INTRODUCTION: From an epidemiological point of view, the increase of pure-tone hearing thresholds as one aspect of biological ageing is moderated by societal factors. Since health policies refer to empirical findings, it is reasonable to replicate population-based hearing surveys and to compare estimates for different birth cohorts from the same regions or, conversely, for the same birth cohorts from different regions.METHODS: We pooled data from two independent cross-sectional German studies conducted between 2008 and 2012 and including 3105 adults. The increase of thresholds, the prevalence and risk of hearing impairment (HI) by age and gender were compared to results reported for European and US-American studies that were carried out at about the same time. Since these studies differed with regard to the age limits, the statistical approaches and, importantly, their definitions of HI, data adjustments were performed to enable the comparison.RESULTS: Overall, 15.5% of the participants in the German studies showed a pure-tone average at 0.5, 1, 2, and 4 kHz in the better ear (PTA) greater than 25 dB HL and 8.6% had a PTA of at least 35 dB HL. Based on one-to-one comparisons, the German estimates demonstrated a good agreement to a large Dutch study and with some reservations to a Swedish study, but considerable differences to US-American results. Comprehensive comparisons of the within-study gender differences showed that age-related HI was less and the gender gap was markedly smaller in Europe compared to the US due to the lower HI in males found in the European studies.CONCLUSION: Discrepancies in measurement procedures, conditions, and equipment that complicate the comparison of absolute HI estimates across studies play no or only a marginal role when comparing relative estimates. Hence, the gender gap differences reviewed in this analysis possibly stem from societal conditions that distinguish societies commonly labeled modern industrialized western countries.DOI: 10.1371/journal.pone.0231632PMCID: PMC7179866",pubmed,32324766,10.1371/journal.pone.0231632
positive pressure therapy for mnires disease,"Background: Ménière's disease is a condition that causes recurrent episodes of vertigo, associated with hearing loss and tinnitus. It is often treated with medication, but different interventions are sometimes used. Positive pressure therapy is a treatment that creates small pressure pulses, generated by a pump that is attached to tubing placed in the ear canal. It is typically used for a few minutes, several times per day. The underlying cause of Ménière's disease is unknown, as is the way in which this treatment may work. The efficacy of this intervention at preventing vertigo attacks, and their associated symptoms, is currently unclear. Objectives: To evaluate the benefits and harms of positive pressure therapy versus placebo or no treatment in people with Ménière's disease. Search methods: The Cochrane ENT Information Specialist searched the Cochrane ENT Register; CENTRAL; Ovid MEDLINE; Ovid Embase; Web of Science; ClinicalTrials.gov; ICTRP and additional sources for published and unpublished trials. The date of the search was 14 September 2022. Selection criteria: We included randomised controlled trials (RCTs) and quasi-RCTs in adults with a diagnosis of Ménière's disease comparing positive pressure therapy with either placebo or no treatment. We excluded studies with follow-up of less than three months. Data collection and analysis: We used standard Cochrane methods. Our primary outcomes were: 1) improvement in vertigo (assessed as a dichotomous outcome - improved or not improved), 2) change in vertigo (assessed as a continuous outcome, with a score on a numerical scale) and 3) serious adverse events. Our secondary outcomes were: 4) disease-specific health-related quality of life, 5) change in hearing, 6) change in tinnitus and 7) other adverse effects. We considered outcomes reported at three time points: 3 to < 6 months, 6 to ≤ 12 months and > 12 months. We used GRADE to assess the certainty of evidence for each outcome. Main results: We included three studies with a total of 238 participants, all of which compared positive pressure using the Meniett device to sham treatment. The duration of follow-up was a maximum of four months. Improvement in vertigo. A single study assessed whether participants had an improvement in the frequency of their vertigo whilst using positive pressure therapy, therefore we are unable to draw meaningful conclusions from the results. Change in vertigo. Only one study reported on the change in vertigo symptoms using a global score (at 3 to < 6 months), so we are again unable to draw meaningful conclusions from the numerical results. All three studies reported on the change in the frequency of vertigo. The summary effect showed that people receiving positive pressure therapy had, on average, 0.84 fewer days per month affected by vertigo (95% confidence interval from 2.12 days fewer to 0.45 days more; 3 studies; 202 participants). However, the evidence on the change in vertigo frequency was of very low certainty, therefore there is great uncertainty in this estimate. Serious adverse events. None of the included studies provided information on the number of people who experienced serious adverse events. It is unclear whether this is because no adverse events occurred, or whether they were not assessed and reported. Authors' conclusions: The evidence for positive pressure therapy for Ménière's disease is very uncertain. There are few RCTs that compare this intervention to placebo or no treatment, and the evidence that is currently available from these studies is of low or very low certainty. This means that we have very low confidence that the effects reported are accurate estimates of the true effect of these interventions. Consensus on the appropriate outcomes to measure in studies of Ménière's disease is needed (i.e. a core outcome set) in order to guide future studies in this area and enable meta-analyses of the results. This must include appropriate consideration of the potential harms of treatment, as well as the benefits. Copyright © 2023 The Authors. Cochrane Database of Systematic Reviews published by John Wiley & Sons, Ltd. on behalf of The Cochrane Collaboration.",scopus,2-s2.0-85148772618,10.1002/14651858.CD015248.pub2
hearing conservation program for agricultural students shortterm outcomes from a clusterrandomized trial with planned longterm followup references,"Objectives : (1) To conduct a contemporary analysis of historical data on short-term efficacy of a 3-year hearing conservation program conducted from 1992 to 1996 in Wisconsin, USA, with 753 high school students actively involved in farm work; (2) to establish procedures for assessment of hearing loss for use in a recently funded follow-up of this same hearing conservation program cohort. Methods : We analyzed a pragmatic cluster-randomized controlled trial, with schools as the unit of randomization. Thirty-four rural schools were recruited and randomized to intervention or control. The intervention included classroom instruction, distribution of hearing protection devices, direct mailings, noise level assessments, and yearly audiometric testing. The control group received the audiometric testing. Results : Students exposed to the hearing conservation program reported more frequent use of hearing protection devices, but there was no evidence of reduced levels of noise-induced hearing loss (NIHL). Conclusion : Our analysis suggests that, since NIHL is cumulative, a 3-year study was likely not long enough to evaluate the efficacy of this intervention. While improvements in reported use of hearing protection devices were noted, the lasting impact of these behaviors is unknown and the finding merits corroboration by longer term objective hearing tests. A follow-up study of the cohort has recently been started. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc8&DO=10.1016%2fj.ypmed.2009.09.020
development of a predictive model for individualized hearing aid benefit,"Objectives To develop a model to predict individualized hearing aid benefit. To provide interpretations of model predictions on global and individual levels. Methods We compiled a data set of patients with hearing loss who trialed hearing aids and completed the Client Oriented Scale of Improvement (COSI) questionnaire, a validated patient-reported outcome measure of hearing aid benefit. Features included demographic, medical, and audiological measures. The outcome was the COSI score for change in listening ability with hearing aids, scaled from 1 to 5. Model development was performed using fivefold cross-validation repeated three times with hyperparameter tuning. Model performance was assessed using the root mean squared error (RMSE) of the COSI scores. Model interpretation was performed using Shapley Additive Explanations. Results The data set comprised 1,286 patients across 3,523 listening situations. The best performing model was random forest with an RMSE of 0.80, found to be significantly better than the next best model (eXtreme gradient boosting with RMSE of 0.85, p < 0.01). The most important features in predicting hearing aid benefit were shorter duration of hearing aid use, higher pure-tone average in the better hearing ear, and younger age. Conclusion We have developed a predictive model for hearing aid benefit that can also provide individualized explanations of model predictions. Predictive modeling could be a useful tool in assessing a patient's candidacy and predicted benefit from hearing aids.  © Wolters Kluwer Health, Inc. All rights reserved.",scopus,2-s2.0-85144094074,10.1097/MAO.0000000000003739
who ear and hearing disorders survey in four provinces in china,"Objective: To investigate the population based prevalence of ear diseases and hearing impairment in Jiangsu, Sichuan, Guizhou and Jilin Provinces in China, develop strategies to provide scientific data for the global database and to draw up prevention and intervention strategies. Methods: Using the WHO Ear and Hearing Disorders Survey Protocol and the probability proportion to size (PPS) sampling technique, 30,733 residents were targeted for investigation in 150 clusters in four provinces. Every subject had an ear examination and pure tone audiometry. Definitions of disabling hearing loss and the classification of hearing impairment used were in accordance with WHO recommendations. Results: Among 30,733 targeted residents, 29,246 individuals (95.2%) participated in the survey. One thousand, three hundred and sixty individuals (4.4%) were absent; 127 individuals (0.4%) refused. The prevalences of hearing impairment and disabling hearing impairment were 14.2% and 5.2% of investigated individuals, respectively: 9.1% of the sample had a mild hearing loss, 3.8% a moderate degree of hearing loss, 1.1% a severe and 0.3% a profound hearing loss. Using data from the fifth population census in China (2000), we calculated the standardized rates of hearing impairment and hearing disability in our study to be 11.7% and 4.4%, respectively. There was a significant difference in the prevalence between males and females, urban and rural dwellers, as well as for different ages. The prevalence of ear diseases was 6.5% of investigated individuals: the standardized rate was 5.9%; 0.2% of investigated individuals had auricle malformation, 2.2% impacted cerumen, 0.2% otitis externa, 0.3% fungi, 0.1% foreign body, 0.1% acute otitis media, 0.9% chronic suppurative otitis media, 1.8% serous otitis media and 1.3% dry perforation of tympanic membrane. Overall, 8.0% of investigated persons were assessed to be likely to benefit from hearing aids, while 4.0% of persons needed medication, 0.1% language/speech rehabilitation, 1.5% non-urgent surgery and 0.9% other treatment. Conclusions: The high prevalence of hearing impairment and disability is a heavy burden on social development and also hinders normal family life. The government and society as a whole should show more concern about these problems. Strategies for prevention and intervention should be focused on less developed regions, rural areas, aging people and non-infectious conditions. Hearing aids services, medication, professional education and training are particularly important in developing countries. © 2011 Informa Healthcare.",scopus,2-s2.0-81255167092,10.3109/1651386X.2011.631285
an artificial intelligence hearing aid based on twolevel neural network,"Hearing aids have become an indispensable part of the lives of some hearing-impaired people. Traditional hearing aids will be adjusted according to the personal hearing curve and allowing patients to avoid noise-induced harm. However, there is no sound classification or intelligent noise reduction, which cannot meet the higher demand for hearing aids. This paper designed a hearing aid based on a two-level neural network, and the Urbansound8K data set was used to train the neural network. It can simulate the human auditory attention mechanism and intelligently control the output volume. At the same time, the noise reduction model is used to perform corresponding noise reduction processing on different speech streams. Experimental results show that the hearing aid can differentially amplify various sounds in different scenes. The noise part of the sound heard by the user will be suppressed to a certain extent, which can improve the comfort of long-term wearing.  © 2021 IEEE.",scopus,2-s2.0-85124792281,10.1109/IDAACS53288.2021.9660975
predicting the hearing outcome in sudden sensorineural hearing loss via machine learning models,,base,9351be112ef3ba4a47ea91bd0fde47f144b80b20c796e61557766d0af8340db0,
intelligibility and audibility at hearing aids output in noisy environment class 1 and class 2 performance comparisonxxxv world congress of audiology april 1013 2022 warsaw poland,"Objectives: Speech intelligibility in noise is a major problem for the hearing impaired even with hearing aids. The hearing aid manufacturers' challenge is to propose signal processing strategies that allow speech to emerge in complex sound environments. For this, options such as adaptive directionality of microphone systems and noise reduction are used. The most recent hearing aids use the source localization technique or artificial intelligence to detect useful signals (speech) from those caused by nuisance (noise). Once located, noises are reduced to allow the speech to emerge. Class 1 hearing aids -low-end products- regulated in the 100% health system in France have such treatment options that improve intelligibility in noise. Class 2 hearing aids -high-end products- according to the manufacturers, involve more elaborate algorithms and for a greater number of frequency channels. The objective of this study is to evaluate these options effectiveness for the two hearing aids classes. Material: The hearing simulation platform of the hearing aid training center of the Faculty of Pharmacy in the University of Lorraine (France) has been developed to reproduce complex sound environments and to analyze the output signals of hearing aids placed on the artificial head's ears. Methods: This platform allows to evaluate the Signal to Noise Ratio (SNR) indicator at the hearing aids output, for different sound environments -several incidences of the voice signal and different spatialization configurations of the noise- and for several input SNR (6 to 9 dB) using the method of Hagerman and Olofsson (Acta Acustica, 2004). Results: The estimated output SNR as a function of frequency for different sound environments is compared to input SNR values. This highlights differences between classes of hearing aids for five different manufacturers, in terms of speech emergence. Conclusions: This work allows to discuss the comparative performance of Class 1 and 2 devices and the algorithmic strategies of hearing aids manufacturers. It shows that all the Class 1 hearing aids even improve SNR for noisy environments. The Class 2 performance is not systematically superior to those of Class 1, depending on the manufacturer.",cinahl,2083389X,
effects of directional hearing aid settings on different laboratory measures of spatial awareness perception,"738. Audiol Res. 2018 Nov 21;8(2):215. doi: 10.4081/audiores.2018.215. eCollection 2018 Oct 2.Effects of directional hearing aid settings on different laboratory measures of spatial awareness perception.Lundbeck M(1)(2), Grimm G(1)(2), Hohmann V(1)(2), Bramsløw L(3), Neher T(4).Author information:(1)Medizinische Physik and Cluster of Excellence ""Hearing4all"", Oldenburg University, Oldenburg, Germany.(2)HörTech gGmbH, Oldenburg, Germany.(3)Eriksholm Research Centre, Snekkersten, Denmark.(4)Institute of Clinical Research, University of Southern Denmark, Odense, Denmark.Hearing loss can negatively influence the spatial hearing abilities of hearing-impaired listeners, not only in static but also in dynamic auditory environments. Therefore, ways of addressing these deficits with advanced hearing aid algorithms need to be investigated. In a previous study based on virtual acoustics and a computer simulation of different bilateral hearing aid fittings, we investigated auditory source movement detectability in older hearing- impaired (OHI) listeners. We found that two directional processing algorithms could substantially improve the detectability of left-right and near-far source movements in the presence of reverberation and multiple interfering sounds. In the current study, we carried out similar measurements with a loudspeaker-based setup and wearable hearing aids. We fitted a group of 15 OHI listeners with bilateral behind-the-ear devices that were programmed to have three different directional processing settings. Apart from source movement detectability, we assessed two other aspects of spatial awareness perception. Using a street scene with up to five environmental sound sources, the participants had to count the number of presented sources or to indicate the movement direction of a single target signal. The data analyses showed a clear influence of the number of concurrent sound sources and the starting position of the moving target signal on the participants' performance, but no influence of the different hearing aid settings. Complementary artificial head recordings showed that the acoustic differences between the three hearing aid settings were rather small. Another explanation for the lack of effects of the tested hearing aid settings could be that the simulated street scenario was not sufficiently sensitive. Possible ways of improving the sensitivity of the laboratory measures while maintaining high ecological validity and complexity are discussed.DOI: 10.4081/audiores.2018.215PMCID: PMC6275462",pubmed,30581544,10.4081/audiores.2018.215
learningbased referencefree speech quality assessment for normal hearing and hearing impaired applicationsdp   2023,"Accurate speech quality measures are highly attractive and beneficial in the design, finetuning, and benchmarking of speech processing algorithms, devices, and communication systems. Switching from narrowband telecommunication to wideband telephony is a change within the telecommunication industry which provides users with better speech quality experience but introduces a number of challenges in speech processing. Noise is the most common distortion on audio signals and as a result there have been a lot of studies on developing high performance noise reduction algorithms. Assistive hearing devices are designed to decrease communication difficulties for people with loss of hearing. As the algorithms within these devices become more advanced, it becomes increasingly crucial to develop accurate and robust quality metrics to assess their performance. Objective speech quality measurements are more attractive compared to subjective assessments as they are cost-effective and subjective variability is eliminated. Although there has been extensive research on objective speech quality evaluation for narrowband speech, those methods are unsuitable for wideband telephony. In the case of hearing-impaired applications, objective quality assessment is challenging as it has to be capable of distinguishing between desired modifications which make signals audible and undesired artifacts. In this thesis a model is proposed that allows extracting two sets of features from the distorted signal only. This approach which is called reference-free (nonintrusive) assessment is attractive as it does not need access to the reference signal. Although this benefit makes nonintrusive assessments suitable for real-time applications, more features need to be extracted and smartly combined to provide comparable accuracy as intrusive metrics. Two feature vectors are proposed to extract information from distorted signals and their performance is examined in three studies. In the first study, both feature vectors are trained on various portions of a noise reduction database for normal hearing applications. In the second study, the same investigation is performed on two sets of databases acquired through several hearing aids. Third study examined the generalizability of the proposed metrics on benchmarking four wireless remote microphones in a variety of environmental conditions. Machine learning techniques are deployed for training the models in the three studies. The studies show that one of the feature sets is robust when trained on different portions of the data from different databases and it also provides good quality prediction accuracy for both normal hearing and hearing-impaired applications. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc22&AN=2023-10936-251
decoding of auditory imagination activity based on machine learning methods,"The decoding of brain signals has important research value and is also full of challenges. In recent years, the fMRI technology is more and more widely used in the brain signals decoding because of its high spatial resolution and non-invasive characteristic. The existing brain region template provides a powerful tool for brain region decoding research, and the obtained functional connectivity matrix quantifies the correlation between brain regions. The development of pattern recognition technology has created a favorable technical foundation for the study of fMRI data, including a variety of mature machine learning methods such as support vector machine. In this paper, the fMRI data of 24 healthy subjects was measured when they performed the auditory imagination under different sound and scene conditions, then the functional connectivity matrices of brain regions were obtained and to be classified by several pattern recognition methods, such as optimized support vector machine, naive Bayes, and logistic regression. At last, the classification methods were compared according to the classification accuracy. Compared with related works, Methods of data acquisition and the accuracy of the results are better than previous job. The method can help psychologists and neuroscientists perform the brain signal decoding with high efficiency and quality. At the same time, the research is helpful to reveal the neural mechanism of auditory imagination and auditory perception, deepen our understanding of human brain auditory information processing, help the computer to simulate human hearing, and use the correlation of brain signals to help the deaf and mute people to carry out related activities, which has good social benefits.  © 2022 IEEE.",scopus,2-s2.0-85152190146,10.1109/ICFTIC57696.2022.10075201
prognosis of idiopathic sudden sensorineural hearing loss the nomogram perspective,"Objective: The aim of this study is to create a nomogram for accurately predicting the prognosis of idiopathic sudden sensorineural hearing loss (ISSNHL) and provide a reference for clinical treatment. Methods: Three hundred and twenty-three patients with ISSNHL were admitted from September 2014 to November 2020. The clinical data were retrospectively reviewed. Prognostic factors for ISSNHL were assessed based on univariate and multivariate logistic regression analysis and used to create a nomogram. Nomogram performance in terms of predictive and discriminatory ability was evaluated by calculating the concordance index (C-index) and generating calibration plots. Results: The overall hearing improvement rate was 41.4%, comprising complete recovery (13.3%), marked recovery (17.0%), and slight recovery (11.1%). Multivariate logistic regression analysis showed that age, symptoms of vertigo, interval between onset and treatment, low-density lipoprotein, and type of hearing loss were independent predictors of ISSNHL. A nomogram based on these 5 factors had a C index of 0.798 (95% confidence interval 0.750-0.845). Conclusions: Age, vertigo, interval between onset and treatment, low-density lipoprotein level, and type of hearing loss are closely associated with hearing recovery. The nomogram may enable prediction of the prognosis of ISSNHL and facilitate clinical decision-making. © The Author(s) 2022.",scopus,2-s2.0-85124081061,10.1177/00034894221075114
a novel method to determine the maximum output of individual patients for an active transcutaneous bone conduction implant using clinical routine data,"465. Ear Hear. 2024 Jan-Feb 01;45(1):219-226. doi: 10.1097/AUD.0000000000001415. Epub 2023 Aug 15.A Novel Method to Determine the Maximum Output of Individual Patients for an Active Transcutaneous Bone Conduction Implant Using Clinical Routine Data.Ghoncheh M(1), Busch S(1)(2), Lenarz T(1)(2), Maier H(1)(2).Author information:(1)Department of Otolaryngology and Institute of Audioneurotechnology (VIANNA), Hannover Medical School, Hannover, Germany.(2)Cluster of Excellence ""Hearing4all"", Hannover Medical School, Hannover, Germany.OBJECTIVES: The maximum output provided by a bone conduction (BC) device is one of the main factors that determines the success when treating patients with conductive or mixed hearing loss. Different approaches such as sound pressure measurements using a probe microphone in the external auditory canal or a surface microphone on the forehead have been previously introduced to determine the maximum output of active transcutaneous BC devices that are not directly accessible after implantation. Here, we introduce a method to determine the maximum output hearing level (MOHL) of a transcutaneous active BC device using patients' audiometric data.DESIGN: We determined the maximum output in terms of hearing level MOHL (dB HL) of the Bonebridge using the audiometric and direct BC threshold of the patient together with corresponding force levels at hearing threshold and the maximum force output of the device. Seventy-one patients implanted with the Bonebridge between 2011 and 2020 (average age 45 ± 19 years ranging from 5 to 84 years) were included in this study. The analyses of MOHLs were performed by (1) dividing patients into two groups with better or worse average audiometric BC threshold (0.5, 1, 2, 4 kHz), on the ipsilateral side or (2) by separating the MOHLs based on better or worse frequency-by-frequency specific audiometric BC thresholds on the ipsilateral (implanted) side.RESULTS: When using a frequency-by-frequency analysis obtained average ipsilateral MOHLs were in the range between 51 and 73 dB HL for frequencies from 0.5 to 6 kHz in the group with better audiometric BC threshold on the ipsilateral ears. The average contralateral MOHLs in the group with better contralateral hearing were in the range from 43 to 67 dB HL. The variability of the data was approximately 6 to 11 dB (SDs) across measured frequencies (0.5 to 6 kHz). The average MOHLs were 4 to 8 dB higher across frequencies in the group with better audiometric BC threshold on the ipsilateral ears than in the group with better audiometric BC threshold on the contralateral ears. The differences between groups were significant across measured frequencies ( t test; p < 0.05).CONCLUSIONS: Our proposed method demonstrates that the individual frequency-specific MOHL on the ipsilateral and contralateral side of individual patients with a transcutaneous BC device can be determined mainly using direct and audiometric BC threshold data of the patients from clinical routine. The average MOHL of the implant was found 4 to 8 dB higher on the ipsilateral (implanted) side than on the contralateral side.Copyright © 2023 The Authors. Ear & Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health, Inc.DOI: 10.1097/AUD.0000000000001415PMCID: PMC10718211",pubmed,37580866,10.1097/AUD.0000000000001415
classification of background noises for hearingaid applications,"462. J Acoust Soc Am. 1995 Jan;97(1):461-70. doi: 10.1121/1.412274.Classification of background noises for hearing-aid applications.Kates JM(1).Author information:(1)Center for Research in Speech and Hearing Sciences, City University of New York, Graduate Center, New York 10036.A background-noise classification procedure is being developed for hearing-aid applications, wherein the hearing-aid response would be adjusted in response to changes in the noise environment. The classification procedure is based on measuring four signal features giving the fluctuations of the signal envelope and the mean frequency and low- and high-frequency slopes of the average spectrum. A more complicated procedure, based on determining the envelope modulation spectra in auditory critical bands, was also investigated and was found to offer no advantages over the simpler procedure. The accuracy of the classification procedure was determined for eleven everyday background noises under optimal conditions where the training and test noise sequences were different portions of the same short noise recording. A cluster analysis was used to determine the similarities among the feature vectors for the noises, and when the noises are grouped into a reduced number of clusters the noise-classification accuracy using the four features exceeds 90%.DOI: 10.1121/1.412274",pubmed,7860827,10.1121/1.412274
prevalence and factors associated with hearing loss and hearing aid use in korean elders,"Background: This study examined hearing loss prevalence and hearing aid usage rates among Korean elders by comparing the differences between those with and without hearing loss, and between those who used and did not use hearing aids. Methods: This study was based on data collected during the Korean National Health and Nutrition Examination Survey V (2010-2012). The study sample consisted of 5,447 Koreans aged ≥60 years who received a hearing assessment. Hearing loss was measured using a pure tone audiometry test and classified according to the World Health Organization's criteria. Hearing aid use was assessed by self-report. Multiple logistic regression analyses were performed to determine the associations between hearing loss, hearing aid use, and related variables. Results: Hearing loss was found in 16.8% of the elders and only 15.9% of them used a hearing aid. Male (95% CI: 1.27-2.15), tinnitus (95% CI: 1.58-2.32), dizziness (95% CI: 1.05-1.73), and occupational noise exposure (95% CI: 1.32-2.38) were the variables most strongly associated with hearing loss after multivariate adjustment. Tinnitus (95% CI: 1.34-4.13) and occupational noise exposure (95% CI: 1.01-5.02) were strongly associated with hearing aid use after multivariate adjustment. Conclusion: More than half of South Korean elders aged ≥60 and older have hearing loss but the rate of hearing aid use is very low. An aural public health program should address modifiable risk factors, such as tinnitus and noise exposure, and non-modifiable risk factors associated with hearing loss in the elderly.",cinahl,22516085,
ontologybased prediction of cochlear implantation outcome using crossmodal plasticity analysis,"Cochlear implantation is a surgical procedure by which an electronic medical device, namely a cochlear implant, is fitted to the individual who has the challenge of hearing. It takes the function of the damaged inner ear, the cochlea. The hearing impaired may not benefit from this procedure because of cross-modal plasticity. This work aims to study the implantee’s visual modal adaptation by analyzing the visual evoked potential. The proposed methodology analyses the existence of cross-modal reorganization in the auditory cortex of bilateral prelingually deaf children after cochlear implantation using visual evoked potential. Fifty healthy, 50 prelingually, deaf children 50 cochlear implanted were considered as a cohort of the Visual evoked potential. The evoked potential recorded using pattern reversal stimulation. The amplitude and latency of N75, P100, and N145 components show a significant difference in normal, cochlear, and deaf. The early diagnosis of hearing impairment demands the patients and doctors to make a series of decisions for the betterment of the implantee in an accelerated manner through traditional database methodology implemented for making decisions, analyzing, interpreting, processing of data is so difficult in the conventional system. Medical knowledge represented for the computers to analyze the inferred data and to make the decisions. Ontology is the most potent tool to encode medical data semantically. A fuzzy ontology-based decision support system built to predict the cochlear implantation outcome. The ontology created using Protégé software tool and the decision taken using Jena and pellet reasoner. A fuzzy-based prediction model designed using a fuzzy interface system to estimate the categories of auditory performance (CAP) test reliable indicators of cochlear implantation success. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",scopus,2-s2.0-85083982542,10.1007/s12652-020-02011-0
fieldtesting of a rapid survey method to assess the prevalence and causes of hearing loss in gaoan jiangxi province china,"724. Arch Public Health. 2020 Mar 6;78:16. doi: 10.1186/s13690-020-0398-1. eCollection 2020.Field-testing of a rapid survey method to assess the prevalence and causes of hearing loss in Gao'an, Jiangxi province, China.Bright T(1), Shan X(2), Xu J(2), Liang J(2), Xiao B(3), Ensink R(4), Mactaggart I(1), Polack S(1), Yip JLY(1).Author information:(1)1International Centre for Evidence in Disability, London School of Hygiene & Tropical Medicine, Keppel St, London, WC1 E7HT United Kingdom.(2)Gao'an City People's Hospital, Gao'an, Jiangxi China.(3)3Zhongshan Opthalmic Centre, Sun Yatsen University, Guangzhou, Guangdong China.(4)4Department of Oto-rhino-laryngology, Gelre Hospitals, Zutphen, The Netherlands.BACKGROUND: The Rapid Assessment of Hearing Loss (RAHL) survey protocol aims to measure the prevalence and causes of hearing loss in a low cost and rapid manner, to inform planning of ear and hearing services. This paper reports on the first field-test of the RAHL in Gao'an County, Jiangxi Province, China. This study aimed to 1) To report on the feasibility of RAHL; 2) report on the estimated prevalence and causes of hearing loss in Gao'an.METHODS: A cross-sectional population-based survey was conducted in September-October 2018. Forty-seven clusters in Gao'an County were selected using probability-proportionate-to-size sampling. Within clusters, compact segment sampling was conducted to select 30 people aged 50+. A questionnaire was completed covering sociodemographics, hearing health, and risk factors. Automated pure-tone audiometry was completed for all participants, using smartphone-based audiometry (hearTest), at 0.5, 1, 2, 4 kHz (kHz). All participants had their ears examined by an Ear Nose and Throat (ENT) doctor, using otoscopy, and probable causes of hearing loss assigned. Prevalence estimates were age and sex standardised to the Jiangxi population. Feasibility of a cluster size of 30 was examined by assessing the response rate, and the proportion of clusters completed in 1 day.RESULTS: 1344 of 1421 eligible participants completed the survey (94.6%). 100% of clusters were completed in 1 day. The survey was completed in 4.5 weeks. The prevalence of moderate or greater hearing loss (pure-tone average of 0.5, 1, 2, 4 kHz of > = 41dBHL in the better ear) was 16.3% (95% CI = 14.3, 18.5) and for any level of hearing loss (pure-tone average of > = 26dBHL in the better ear) the prevalence was 53.2% (95% CI = 49.2, 57.1). The majority of hearing loss was due to acquired sensorineural causes (91.7% left; 92.1% right). Overall 54.0% of the population aged 50+ (108,000 people) are in need of diagnostic audiology services, 3.4% were in need of wax removal (7000 people), and 4.8% were in need of surgical services (9500 people). Hearing aid coverage was 0.4%.CONCLUSION: The RAHL survey protocol is feasible, demonstrated through the number of people examined per day, and the high response rate. The survey was completed in a much shorter period than previous all-age surveys in China. Some remaining challenges included assignment of causes of probable sensorineural loss. The data obtained from this survey can be used to scale-up hearing services in Gao'an.© The Author(s). 2020.DOI: 10.1186/s13690-020-0398-1PMCID: PMC7059708",pubmed,32166026,10.1186/s13690-020-0398-1
morphological features of human reissners membrane,"Light and electronmicroscopic investigations of Reissner's membrane were undertaken on 10 cochleae from 6 patients with normal hearing for their age. The membrane consisted of two layers, an epithelium and a mesothelium separated by a basement membrane. The mesothelium was formed by a single thin layer which was intermittently discontinuous. The melanocytes were localized on the mesothelial side of the basement membrane. Their number was 2-4 times greater in the upper half of the basal turn and in the middle turn than elsewhere. The epithelium was much thicker and had more irregular features than the mesothelium. It was composed of two types of epithelial cells, flat and rounded. The flat cells were more regular in shape than the rounded cells and they were mainly distributed in the middle and apical turns. Judging from their structure they were in a resting state. The rounded cells covered a smaller area than the flat ones and had numerous microvilli. They assumed three different shapes, cuboidal, spindle-form and spherical and were arranged in four different patterns, namely bands, strands, whorls and clusters. The rounded cells were the most active according to the composition of the cytoplasm and dominated the cell population in the hook and the lower half of the basal turn where the age-related sensorineural degeneration is most apparent. © 1993 Informa UK Ltd All rights reserved: reproduction in whole or part not permitted.",scopus,2-s2.0-0027266923,10.3109/00016489309135817
model based on learning needs of children with auditory impairment,"This paper presents a model based on the needs of children with an auditory impairment, in which the dual research lines of Human Computer Interaction and Artificial Intelligence are employed in the design of intelligent interactive systems able to meet the requirements of the user. In following a philosophy of user-centered design, different characteristics of children with hearing disabilities are identified, along with AI techniques that could be applied in the model. The main issues involved in designing a user profile and the techniques used in order to create the process of adapting the system to the user are also discussed. © Springer International Publishing Switzerland 2016.",scopus,2-s2.0-84978878009,10.1007/978-3-319-39910-2_30
multivariate classification of mild and moderate hearing loss using a speechinnoise test for hearing screening at a distance,"In the area of smartphone-based hearing screening, the number of speech-in-noise tests available is growing rapidly. However, the available tests are typically based on a univariate classification approach, for example using the speech recognition threshold (SRT) or the number of correct responses. There is still lack of multivariate approaches to screen for hearing loss (HL). Moreover, all the screening methods developed so far do not assess the degree of HL, despite the potential importance of this information in terms of patient education and clinical follow-up. The aim of this study was to characterize multivariate approaches to identify mild and moderate HL using a recently developed, validated speech-in-noise test for hearing screening at a distance, namely the WHISPER (Widespread Hearing Impairment Screening and PrEvention of Risk) test. The WHISPER test is automated, minimally dependent on the listeners’ native language, it is based on an optimized, efficient adaptive procedure, and it uses a multivariate approach. The results showed that age and SRT were the features with highest performance in identifying mild and moderate HL, respectively. Multivariate classifiers using all the WHISPER features achieved better performance than univariate classifiers, reaching an accuracy equal to 0.82 and 0.87 for mild and moderate HL, respectively. Overall, this study suggested that mild and moderate HL may be discriminated with high accuracy using a set of features extracted from the WHISPER test, laying the ground for the development of future self-administered speech-in-noise tests able to provide specific recommendations based on the degree of HL. © 2023, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.",scopus,2-s2.0-85151054296,10.1007/978-3-031-28663-6_7
classification of eegbased auditory evoked potentials using entropybased features and machine learning techniques,"Hearing loss is a prevalent impairment that disrupts interactions with others and individuals' learning abilities. Immediate and accurate diagnosis of hearing loss using Electroencephalogram (EEG) signals, particularly Auditory Evoked Potentials (AEP), is considered the most effective approach to address this issue. The AEP signals, generated in the cerebral cortex in response to auditory stimuli, serve as the most reliable method for diagnosing deafness. This study introduces a novel approach for detecting hearing ability through the classification of EEG-AEP signals. The current experiment makes use of a publicly available dataset that contains AEP responses from 16 people who responded to auditory stimuli on either the left or right side. Sample Entropy is employed to extract the feature, capturing the complex temporal dynamics of the EEG signals. Four popular machine learning-based classifiers, namely Support Vector Machines (SVM), K-Nearest Neighbors (KNN), Random Forest (RF), and Logistic Regression (LR), are utilized for classification purposes. The results indicate that SVM achieves the highest classification accuracy of 99.37% with subject-4 and the average accuracy of 90.74% is achieved with all subjects. This finding shows the effectiveness of Sample Entropy as a feature extraction technique for characterizing AEPs and highlights the potential of SVM as a robust classifier for the accurate identification of auditory stimuli localization. The accuracy achieved in this study indicates a promising direction for the development of reliable and non-invasive methods for hearing-related diagnoses.  © 2023 IEEE.",scopus,2-s2.0-85179138618,10.1109/ICICT4SD59951.2023.10303426
prevalence characteristics and treatment patterns of hearing difficulty in the united states,"751. JAMA Otolaryngol Head Neck Surg. 2018 Jan 1;144(1):65-70. doi: 10.1001/jamaoto.2017.2223.Prevalence, Characteristics, and Treatment Patterns of Hearing Difficulty in the United States.Mahboubi H(1), Lin HW(1), Bhattacharyya N(2).Author information:(1)Division of Neurotology and Skull Base Surgery, Department of Otolaryngology-Head and Neck Surgery, University of California, Irvine.(2)Department of Otology & Laryngology, Harvard Medical School, Boston, Massachusetts.IMPORTANCE: Hearing loss is one of the most prevalent chronic conditions in the United States and has been associated with negative physical, social, cognitive, economic, and emotional consequences. Despite the high prevalence of hearing loss, substantial gaps in the utilization of amplification options, including hearing aids and cochlear implants (CI), have been identified.OBJECTIVE: To investigate the contemporary prevalence, characteristics, and patterns of specialty referral, evaluation, and treatment of hearing difficulty among adults in the United States.DESIGN, SETTING, AND PARTICIPANTS: A cross-sectional analysis of responses from a nationwide clustered representative sample of adults who participated in the 2014 National Health Interview Survey and responded to the hearing module questions was carried out.MAIN OUTCOMES AND MEASURES: Data regarding demographics as well as self-reported hearing status, functional hearing, laterality, onset, and primary cause of the hearing loss were collected. In addition, specific data regarding hearing-related clinician visits, hearing tests, referrals to hearing specialist, and utilization of hearing aids and CIs were analyzed.RESULTS: Among 239.6 million adults, 40.3 million (16.8%) indicated their hearing was less than ""excellent/good,"" ranging from ""a little trouble hearing"" to ""deaf."" The mean (SD) age of participants was 47 (0.2) years with 48.2% being men and 51.8% women. Approximately 48.8 million (20.6%) had visited a physician for hearing problems in the preceding 5 years. Of these, 32.6% were referred to an otolaryngologist and 27.3% were referred to an audiologist. Functional hearing was reported as the ability to hear ""whispering"" or ""normal voice"" (225.4 million; 95.5%), to ""only hear shouting"" (8.0 million; 3.4%), and ""not appreciating shouting"" (2.8 million; 1.1%). Among the last group, 5.3% were recommended to have a CI, of which 22.1% had received one. Of the adults who indicated their hearing from ""a little trouble hearing"" to being ""deaf,"" 12.9 million (32.2%) had never seen a clinician for hearing problems and 11.1 million (28.0%) had never had their hearing tested.CONCLUSIONS AND RELEVANCE: There are considerable gaps between self-reported hearing loss and patients receiving medical evaluation and recommended treatments for hearing loss. Improved awareness regarding referrals to otolaryngologists and audiologists as well as auditory rehabilitative options among clinicians may improve hearing loss care.DOI: 10.1001/jamaoto.2017.2223PMCID: PMC5833589",pubmed,29167904,10.1001/jamaoto.2017.2223
field test of the rapid assessment of hearing loss survey protocol in ntcheu district malawi references,"Objective: (1) To test the feasibility of the Rapid Assessment of Hearing Loss (RAHL) survey protocol in Malawi (Ntcheu); (2) To estimate the prevalence and probable causes of hearing loss (adults 50 +). Design: Cross-sectional population-based survey. Study sample: Clusters (n = 38) were selected using probability-proportionate-to-size-sampling. Within each cluster, 30 people aged 50 + were selected using compact-segment-sampling. All participants completed smartphone-based audiometry (hearTest). Prevalence was estimated using WHO definitions (PTA of thresholds 0.5, 1, 2, 4 kHz in the better ear of > 25 dB HL (any) and > 40 dB HL (>= moderate)). Otoscopy and questionnaire were used to assess probable causes. Participants with hearing loss and/or ear disease were asked about care-seeking and barriers. Results: Four teams completed the survey in 24 days. 1080 of 1153 (93.7%) participants were examined. The median time to complete the protocol was 24 min/participant. Prevalence of hearing loss was 35.9% (95% CI = 31.6-40.2) (any level); and 10.0% (95% CI = 7.9-12.5) (>= moderate). The majority was classified as probable sensorineural. Nearly one third of people (30.9%) needed diagnostic audiology services and possible hearing aid fitting. Hearing aid coverage was < 1%. Lack of perceived need was a key barrier. Conclusion: The RAHL is simple, fast and provides information about the magnitude and probable causes of hearing loss to plan services. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc19&DO=10.1080%2f14992027.2020.1739764
construction of hearing percentiles in women with nonconstant variance from the linear mixedeffects model,"482. Stat Med. 1997 Nov 15;16(21):2475-88. doi: 10.1002/(sici)1097-0258(19971115)16:21<2475::aid-sim669>3.0.co;2-d.Construction of hearing percentiles in women with non-constant variance from the linear mixed-effects model.Morrell CH(1), Pearson JD, Brant LJ, Gordon-Salant S.Author information:(1)Mathematical Sciences Department, Loyola College, Baltimore, MD 21210, USA.Current age-specific reference standards for adult hearing thresholds are primarily cross-sectional in nature and vary in the degree of screening of the reference sample for noise-induced hearing loss and other hearing problems. We develop methods to construct age-specific percentiles for longitudinal data that have been modelled using the linear mixed-effects model. We apply these methods to construct percentiles of hearing level using data from a carefully screened sample of women from the Baltimore Longitudinal Study of Aging. However, the variation in the residuals and random effects from the linear mixed-effects model does not remain constant with age and frequency of the stimulus tone. In addition, the distribution of the hearing levels is not symmetric about the mean. We develop a number of methods to use the output from the linear mixed-effects model to construct percentiles that do not have constant variance. We use a transformation of the hearing levels to provide for skewness in the final percentile curves. The change in the variation of the residuals and random effects is modelled as a function of beginning age and frequency and we use this variance function to construct the hearing percentiles. We present a number of approaches. First, we use the absolute values of the population residuals to model the total deviation about the mean as a function of beginning age and frequency. Second, we model the standard deviation in the person-specific (cluster) residuals as well as the standard deviation in the estimated random effects. Finally, we use weighted least squares with the regressions on the absolute cluster residuals and absolute estimated random effects where the weights are the reciprocal of the standard deviations of their estimates.DOI: 10.1002/(sici)1097-0258(19971115)16:21<2475::aid-sim669>3.0.co;2-d",pubmed,9364655,10.1002/(sici)1097-0258(19971115)16:21<2475::aid-sim669>3.0.co;2-d
individual aided speechrecognition performance and predictions of benefit for listeners with impaired hearing employing fade,"101. Trends Hear. 2020 Jan-Dec;24:2331216520938929. doi: 10.1177/2331216520938929.Individual Aided Speech-Recognition Performance and Predictions of Benefit for Listeners With Impaired Hearing Employing FADE.Schädler MR(1), Hülsmeier D(1), Warzybok A(1), Kollmeier B(1).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg.The benefit in speech-recognition performance due to the compensation of a hearing loss can vary between listeners, even if unaided performance and hearing thresholds are similar. To accurately predict the individual performance benefit due to a specific hearing device, a prediction model is proposed which takes into account hearing thresholds and a frequency-dependent suprathreshold component of impaired hearing. To test the model, the German matrix sentence test was performed in unaided and individually aided conditions in quiet and in noise by 18 listeners with different degrees of hearing loss. The outcomes were predicted by an individualized automatic speech-recognition system where the individualization parameter for the suprathreshold component of hearing loss was inferred from tone-in-noise detection thresholds. The suprathreshold component was implemented as a frequency-dependent multiplicative noise (mimicking level uncertainty) in the feature-extraction stage of the automatic speech-recognition system. Its inclusion improved the root-mean-square prediction error of individual speech-recognition thresholds (SRTs) from 6.3 dB to 4.2 dB and of individual benefits in SRT due to common compensation strategies from 5.1 dB to 3.4 dB. The outcome predictions are highly correlated with both the corresponding observed SRTs (R2 = .94) and the benefits in SRT (R2 = .89) and hence might help to better understand-and eventually mitigate-the perceptual consequences of as yet unexplained hearing problems, also discussed in the context of hidden hearing loss.DOI: 10.1177/2331216520938929PMCID: PMC7493243",pubmed,32924797,10.1177/2331216520938929
interventions to prevent occupational noiseinduced hearing loss,"1. Cochrane Database Syst Rev. 2017 Jul 7;7(7):CD006396. doi: 
10.1002/14651858.CD006396.pub4.

Interventions to prevent occupational noise-induced hearing loss.

Tikka C(1), Verbeek JH, Kateman E, Morata TC, Dreschler WA, Ferrite S.

Author information:
(1)Cochrane Work Review Group, Finnish Institute of Occupational Health, PO Box 
310, Kuopio, Finland, 70101.

Update of
    Cochrane Database Syst Rev. 2012 Oct 17;10:CD006396.

BACKGROUND: This is the second update of a Cochrane Review originally published 
in 2009. Millions of workers worldwide are exposed to noise levels that increase 
their risk of hearing disorders. There is uncertainty about the effectiveness of 
hearing loss prevention interventions.
OBJECTIVES: To assess the effectiveness of non-pharmaceutical interventions for 
preventing occupational noise exposure or occupational hearing loss compared to 
no intervention or alternative interventions.
SEARCH METHODS: We searched the CENTRAL; PubMed; Embase; CINAHL; Web of Science; 
BIOSIS Previews; Cambridge Scientific Abstracts; and OSH UPDATE to 3 October 
2016.
SELECTION CRITERIA: We included randomised controlled trials (RCT), controlled 
before-after studies (CBA) and interrupted time-series (ITS) of non-clinical 
interventions under field conditions among workers to prevent or reduce noise 
exposure and hearing loss. We also collected uncontrolled case studies of 
engineering controls about the effect on noise exposure.
DATA COLLECTION AND ANALYSIS: Two authors independently assessed study 
eligibility and risk of bias and extracted data. We categorised interventions as 
engineering controls, administrative controls, personal hearing protection 
devices, and hearing surveillance.
MAIN RESULTS: We included 29 studies. One study evaluated legislation to reduce 
noise exposure in a 12-year time-series analysis but there were no controlled 
studies on engineering controls for noise exposure. Eleven studies with 3725 
participants evaluated effects of personal hearing protection devices and 17 
studies with 84,028 participants evaluated effects of hearing loss prevention 
programmes (HLPPs). Effects on noise exposure Engineering interventions 
following legislationOne ITS study found that new legislation in the mining 
industry reduced the median personal noise exposure dose in underground coal 
mining by 27.7 percentage points (95% confidence interval (CI) -36.1 to -19.3 
percentage points) immediately after the implementation of stricter legislation. 
This roughly translates to a 4.5 dB(A) decrease in noise level. The intervention 
was associated with a favourable but statistically non-significant downward 
trend in time of the noise dose of -2.1 percentage points per year (95% CI -4.9 
to 0.7, 4 year follow-up, very low-quality evidence). Engineering intervention 
case studiesWe found 12 studies that described 107 uncontrolled case studies of 
immediate reductions in noise levels of machinery ranging from 11.1 to 19.7 
dB(A) as a result of purchasing new equipment, segregating noise sources or 
installing panels or curtains around sources. However, the studies lacked 
long-term follow-up and dose measurements of workers, and we did not use these 
studies for our conclusions. Hearing protection devicesIn general hearing 
protection devices reduced noise exposure on average by about 20 dB(A) in one 
RCT and three CBAs (57 participants, low-quality evidence). Two RCTs showed 
that, with instructions for insertion, the attenuation of noise by earplugs was 
8.59 dB better (95% CI 6.92 dB to 10.25 dB) compared to no instruction (2 RCTs, 
140 participants, moderate-quality evidence). Administrative controls: 
information and noise exposure feedbackOn-site training sessions did not have an 
effect on personal noise-exposure levels compared to information only in one 
cluster-RCT after four months' follow-up (mean difference (MD) 0.14 dB; 95% CI 
-2.66 to 2.38). Another arm of the same study found that personal noise exposure 
information had no effect on noise levels (MD 0.30 dB(A), 95% CI -2.31 to 2.91) 
compared to no such information (176 participants, low-quality evidence). 
Effects on hearing loss Hearing protection devicesIn two studies the authors 
compared the effect of different devices on temporary threshold shifts at 
short-term follow-up but reported insufficient data for analysis. In two CBA 
studies the authors found no difference in hearing loss from noise exposure 
above 89 dB(A) between muffs and earplugs at long-term follow-up (OR 0.8, 95% CI 
0.63 to 1.03 ), very low-quality evidence). Authors of another CBA study found 
that wearing hearing protection more often resulted in less hearing loss at very 
long-term follow-up (very low-quality evidence). Combination of interventions: 
hearing loss prevention programmesOne cluster-RCT found no difference in hearing 
loss at three- or 16-year follow-up between an intensive HLPP for agricultural 
students and audiometry only. One CBA study found no reduction of the rate of 
hearing loss (MD -0.82 dB per year (95% CI -1.86 to 0.22) for a HLPP that 
provided regular personal noise exposure information compared to a programme 
without this information.There was very-low-quality evidence in four very 
long-term studies, that better use of hearing protection devices as part of a 
HLPP decreased the risk of hearing loss compared to less well used hearing 
protection in HLPPs (OR 0.40, 95% CI 0.23 to 0.69). Other aspects of the HLPP 
such as training and education of workers or engineering controls did not show a 
similar effect.In three long-term CBA studies, workers in a HLPP had a 
statistically non-significant 1.8 dB (95% CI -0.6 to 4.2) greater hearing loss 
at 4 kHz than non-exposed workers and the confidence interval includes the 4.2 
dB which is the level of hearing loss resulting from 5 years of exposure to 85 
dB(A). In addition, of three other CBA studies that could not be included in the 
meta-analysis, two showed an increased risk of hearing loss in spite of the 
protection of a HLPP compared to non-exposed workers and one CBA did not.
AUTHORS' CONCLUSIONS: There is very low-quality evidence that implementation of 
stricter legislation can reduce noise levels in workplaces. Controlled studies 
of other engineering control interventions in the field have not been conducted. 
There is moderate-quality evidence that training of proper insertion of earplugs 
significantly reduces noise exposure at short-term follow-up but long-term 
follow-up is still needed.There is very low-quality evidence that the better use 
of hearing protection devices as part of HLPPs reduces the risk of hearing loss, 
whereas for other programme components of HLPPs we did not find such an effect. 
The absence of conclusive evidence should not be interpreted as evidence of lack 
of effectiveness. Rather, it means that further research is very likely to have 
an important impact.

DOI: 10.1002/14651858.CD006396.pub4
PMCID: PMC6353150",pubmed,28685503,10.1002/14651858.CD006396.pub4
increased atherogenic index in the general hearing loss population,"Purpose. The purpose of this study was to evaluate the association of hearing loss with atherogenic index (AI) in the general population. Methods. A multistage study using cluster random sampling method was conducted in the Zhejiang province from 2016 to 2018. Pure-tone air-conduction hearing thresholds were measured at frequencies of 0.125.8kHz for each subject. After obtaining their consent, all participants were asked to provide their own plasma lipid data. Results. A total of 3,414 eligible participants were included, 1,765 (51.7%) were men and 1,649 (48.3%) were women and 1,113 (32.6%) had hearing loss. Ridge regression showed increased AI in subjects with hearing loss. The subgroup with the highest quartile of AI, presenting the highest risk of hearing loss as compared to the lowest quartile, comprised young and middle-aged women. Further analysis revealed that the AI in people with different categories of hearing loss was higher than that in the normal population, except for those with (extremely) severe hearing loss. Moreover, the young and middle-aged women exhibited the most significant correlations between AI and hearing loss. © 2020 Huai Zhang et al.",scopus,2-s2.0-85095881563,10.1515/med-2020-0003
application of data mining to big data acquired in audiology principles and potential,"9. Trends Hear. 2018 Jan-Dec;22:2331216518776817. doi: 10.1177/2331216518776817.Application of Data Mining to ""Big Data"" Acquired in Audiology: Principles and Potential.Mellor JC(1), Stone MA(2)(3), Keane J(1)(4).Author information:(1)1 School of Computer Science, University of Manchester, UK.(2)2 Manchester Centre for Audiology and Deafness, University of Manchester, UK.(3)3 Manchester Academic Health Sciences Centre, University of Manchester, UK.(4)4 Manchester Institute of Biotechnology, University of Manchester, UK.Comment in    10.1177/2331216518773632.The ubiquity and cheapness of miniature low-power sensors, digital processing, and large amounts of storage contained in small packages has heralded the ability to acquire large amounts of data about systems during their course of operation. The size and complexity of the data sets so generated have colloquially been labeled ""big data."" The computer science field of ""data mining"" has arisen with the purpose of extracting meaning from such data, expressly looking for patterns that not only link historic observations but also predict future behavior. This overview article considers the process, techniques, and interpretation of data mining, with specific focus on its application in audiology. Modern hearing instruments contain data-logging technology to record data separate from the audio stream, such as the acoustic environments in which the device was being used and how the signal processing was consequently operating. Combined with details about the patient, such as the audiogram, the variety of data generated lends itself to a data mining approach. To date, reports of the use and interpretation of these data have been mostly constrained to questions such as looking for changes in patterns of daily use, or the degree and direction of volume control manipulation as the patient's experience with a hearing aid changes. In this, and an accompanying results paper, the practical applications of some data mining techniques are described as applied to a large data set of examples of real-world device usage, as supplied by a hearing aid manufacturer.DOI: 10.1177/2331216518776817PMCID: PMC6022814",pubmed,29848183,10.1177/2331216518776817
lifestyle and dietary interventions for mnires disease,"Background: Ménière's disease is a condition that causes recurrent episodes of vertigo, associated with hearing loss and tinnitus. Lifestyle or dietary modifications (including reducing the amount of salt or caffeine in the diet) are sometimes suggested to be of benefit for this condition. The underlying cause of Ménière's disease is unknown, as is the way in which these interventions may work. The efficacy of these different interventions at preventing vertigo attacks, and their associated symptoms, is currently unclear. Objectives: To evaluate the benefits and harms of lifestyle and dietary interventions versus placebo or no treatment in people with Ménière's disease. Search methods: The Cochrane ENT Information Specialist searched the Cochrane ENT Register; Central Register of Controlled Trials (CENTRAL); Ovid MEDLINE; Ovid Embase; Web of Science; ClinicalTrials.gov; ICTRP and additional sources for published and unpublished trials. The date of the search was 14 September 2022. Selection criteria: We included randomised controlled trials (RCTs) and quasi-RCTs in adults with Ménière's disease comparing any lifestyle or dietary intervention with either placebo or no treatment. We excluded studies with follow-up of less than three months, or with a cross-over design (unless data from the first phase of the study could be identified). Data collection and analysis: We used standard Cochrane methods. Our primary outcomes were: 1) improvement in vertigo (assessed as a dichotomous outcome - improved or not improved), 2) change in vertigo (assessed as a continuous outcome, with a score on a numerical scale) and 3) serious adverse events. Our secondary outcomes were: 4) disease-specific health-related quality of life, 5) change in hearing, 6) change in tinnitus and 7) other adverse effects. We considered outcomes reported at three time points: 3 to < 6 months, 6 to ≤ 12 months and > 12 months. We used GRADE to assess the certainty of evidence for each outcome. Main results: We included two RCTs, one related to diet, and the other related to fluid intake and sleep. In a Swedish study, 51 participants were randomised to receive 'specially processed cereals' or standard cereals. The specially processed cereals are thought to stimulate the production of anti-secretory factor - a protein that reduces inflammation and fluid secretion. Participants received the cereals for three months. The only outcome reported by this study was disease-specific health-related quality of life. The second study was conducted in Japan. The participants (223) were randomised to receive abundant water intake (35 mL/kg/day), or to sleep in darkness (in an unlit room for six to seven hours per night), or to receive no intervention. The duration of follow-up was two years. The outcomes assessed were 'improvement in vertigo' and hearing. As these studies considered different interventions we were unable to carry out any meta-analysis, and for almost all outcomes the certainty of the evidence was very low. We are unable to draw meaningful conclusions from the numerical results. Authors' conclusions: The evidence for lifestyle or dietary interventions for Ménière's disease is very uncertain. We did not identify any placebo-controlled RCTs for interventions that are frequently recommended for those with Ménière's disease, such as salt restriction or caffeine restriction. We identified only two RCTs that compared a lifestyle or dietary intervention to placebo or no treatment, and the evidence that is currently available from these studies is of low or very low certainty. This means that we have very low confidence that the effects reported are accurate estimates of the true effect of these interventions. Consensus on the appropriate outcomes to measure in studies of Ménière's disease is needed (i.e. a core outcome set) in order to guide future studies in this area and enable meta-analyses of the results. This must include appropriate consideration of the potential harms of treatment, as well as the benefits. Copyright © 2023 The Authors. Cochrane Database of Systematic Reviews published by John Wiley & Sons, Ltd. on behalf of The Cochrane Collaboration.",scopus,2-s2.0-85148976101,10.1002/14651858.CD015244.pub2
cochlear implantation in postlingual deaf adults is timesensitive towards positive outcome clinical utility of advanced machine learning techniques15th international conference on cochlear implants and other implantable auditory technologies 27th jun 2018  30th jun 2018 antwerp belgium,"Objectives: We investigated the effects of preoperative factors on outcomes of CIs in postlingually deaf adults using a general linear model (GLM) and a nonlinear Regression Forest Regression (RFR) model. Study Design: Postoperative monosyllabic word recognition scores (WRS) served as the dependent variable to predict. Predictors included duration of deafness, duration of auditory deprivation (duration of deafness without hearing aid use), age at implantation, preoperative hearing threshold and monosyllabic WRS in quiet. Patients: Postlingually deaf adults (n = 120) who received CI, which was fully inserted, without any inner ear abnormalities or combined disabilities and with follow-up of more than 2 years. Methods: The prediction accuracy was evaluated with the mean absolute error (MAE) as well as the Pearson's correlation coefficient between the true WRS and predicted WRS. To determine the importance of predictors, we measured increase in the MAE when a given variable was omitted in the regression model relative to when it was included. We used a leave-one-out cross-validation to avoid bias related to inclusion of the test data into the training procedure. Results: The fitting of GLMs resulted in prediction performance with correlation coefficient r=0.7 and MAE of 15.6+/-9.5). On the other hand, the RFR machine learning yielded superior prediction performance to the GLM with r=0.96 and MAE of 6.1+/-4.7 (t=9.8; p<0.00001). Computation of the importance showed that the contribution of DAD to the prediction was the highest (MAE increase when omitted: 12.1), followed by duration of deafness (8.6) and AgeCI (8.3). In a subsequent analysis, a subgroup of patients with DAD ≤ 10 years showed higher postCI WRS and smaller variation than those with DAD > 10 years. Conclusion: The current study on clinical utility of machine learning on auditory outcomes of CIs in postlingually deaf adults demonstrated that an advanced nonlinear classifier yields a highly accurate prediction ability with an error of +/-6 in WRS. Our finding also suggests that CI should be implemented no later than a sensitive period (10 years) after deafness to lead to successful outcome. Finally, our machine learning technique has the potential for patient counseling and predicting benefit from CI to patients.",cinahl,2083389X,
prevalence and audiological profiles of gjb2 mutations in a large collective of hearing impaired patients,"220. Hear Res. 2016 Mar;333:77-86. doi: 10.1016/j.heares.2016.01.006. Epub 2016 Jan 15.Prevalence and audiological profiles of GJB2 mutations in a large collective of hearing impaired patients.Burke WF(1), Warnecke A(2), Schöner-Heinisch A(3), Lesinski-Schiedat A(4), Maier H(2), Lenarz T(2).Author information:(1)Department of Otolaryngology, Hannover Medical School, Hannover, Germany; Cluster of Excellence, Hearing4All, Germany. Electronic address: burke.william@mh-hannover.de.(2)Department of Otolaryngology, Hannover Medical School, Hannover, Germany; Cluster of Excellence, Hearing4All, Germany.(3)Institute for Human Genetics, Hannover Medical School, Hannover, Germany.(4)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.Mutations in the GJB2 gene are known to represent the commonest cause of hereditary and congenital hearing loss. In this study, a complete sequencing of the GJB2 gene in a cohort of 506 patients from a single, large cochlear implant program in Europe was performed. Audiological testing for those patients who could actively participate was performed using pure tone audiometry (PTA). Those unable to undergo PTA were measured using click-auditory brainstem response (ABR). Data analysis was performed to determine genotype-phenotype correlations of the mutational status vs. audiological profiles and vs. age at the time of presentation. An overall prevalence of biallelic mutations of 13.4% was found for the total collective. When subsets of younger patients were examined, the prevalence increased to 27% of those up to age 18 and 35% of those up to age 5 at the time of testing, respectively. This increase was found to be highly significant (p < 0.001). Analysis of the mean PTA thresholds revealed a strong correlation between allele combination status and mean PTA (p = 0.021). The prevalence of simple heterozygotes was found to be approximately 10.1%, which is around 3.3 times the value expected in the general population. As GJB2 follows a recessive pattern of inheritance, the question arises as to why such a large fraction of simple heterozygotes was observed among the hearing impaired patients included in this study.Copyright © 2016 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2016.01.006",pubmed,26778469,10.1016/j.heares.2016.01.006
a novel approach for classifying native chinese and malay speaking persons according to cortical auditory evoked responses,"303. J Int Adv Otol. 2019 Apr;15(1):87-93. doi: 10.5152/iao.2019.4553.A Novel Approach for Classifying Native Chinese and Malay Speaking Persons According to Cortical Auditory Evoked Responses.Ibrahim IA(1), Ting HN(1), Moghavvemi M(2).Author information:(1)Department of Electrical Engineering, University of Malaya School of Engineering, Kuala Lumpur, Malaysia;Department of Biomedical Engineering, University of Malaya School of Engineering, Kuala Lumpur, Malaysia.(2)Center of Research in Applied Electronics (CRAE), University of Malaya School of Engineering, Kuala Lumpur, Malaysia;University of Science and Culture, Tehran, Iran.OBJECTIVES: This study uses a new approach for classifying the human ethnicity according to the auditory brain responses (electroencephalography [EEG] signals) with a high level of accuracy. Moreover, the study presents three different algorithms used to classify the human ethnicity using auditory brain responses. The algorithms were tested on Malays and Chinese as a case study.MATERIALS AND METHODS: The EEG signal was used as a brain response signal, which was evoked by two auditory stimuli (Tones and Consonant Vowels stimulus). The study was carried out on Malaysians (Malay and Chinese) with normal hearing and with hearing loss. A ranking process for the subjects' EEG data and the nonlinear features was used to obtain the maximum classification accuracy.RESULTS: The study formulated the classification of Normal Hearing Ethnicity Index and Sensorineural Hearing Loss Ethnicity Index. These indices classified the human ethnicity according to brain auditory responses by using numerical values of response signal features. Three classification algorithms were used to verify the human ethnicity. Support Vector Machine (SVM) classified the human ethnicity with an accuracy of 90% in the cases of normal hearing and sensorineural hearing loss (SNHL); the SVM classified with an accuracy of 84%.CONCLUSION: The classification indices categorized or separated the human ethnicity in both hearing cases of normal hearing and SNHL with high accuracy. The SVM classifier provided a good accuracy in the classification of the auditory brain responses. The proposed indices might constitute valuable tools for the classification of the brain responses according to the human ethnicity.DOI: 10.5152/iao.2019.4553PMCID: PMC6483426",pubmed,30924771,10.5152/iao.2019.4553
predicting speech discrimination scores from puretone thresholdsa machine learningbased approach using data from 12697 subjects,"122. PLoS One. 2021 Dec 31;16(12):e0261433. doi: 10.1371/journal.pone.0261433. eCollection 2021.Predicting speech discrimination scores from pure-tone thresholds-A machine learning-based approach using data from 12,697 subjects.Kim H(1)(2), Park J(3), Choung YH(1)(2), Jang JH(1)(2), Ko J(3).Author information:(1)Ajou University Hospital, Suwon, South Korea.(2)Department of Otolaryngology, School of Medicine, Ajou University, Suwon, South Korea.(3)School of Integrated Technology, College of Engineering, Yonsei University, Seoul, South Korea.Diagnostic tests for hearing impairment not only determines the presence (or absence) of hearing loss, but also evaluates its degree and type, and provides physicians with essential data for future treatment and rehabilitation. Therefore, accurately measuring hearing loss conditions is very important for proper patient understanding and treatment. In current-day practice, to quantify the level of hearing loss, physicians exploit specialized test scores such as the pure-tone audiometry (PTA) thresholds and speech discrimination scores (SDS) as quantitative metrics in examining a patient's auditory function. However, given that these metrics can be easily affected by various human factors, which includes intentional (or accidental) patient intervention, there are needs to cross validate the accuracy of each metric. By understanding a ""normal"" relationship between the SDS and PTA, physicians can reveal the need for re-testing, additional testing in different dimensions, and also potential malingering cases. For this purpose, in this work, we propose a prediction model for estimating the SDS of a patient by using PTA thresholds via a Random Forest-based machine learning approach to overcome the limitations of the conventional statistical (or even manual) methods. For designing and evaluating the Random Forest-based prediction model, we collected a large-scale dataset from 12,697 subjects, and report a SDS level prediction accuracy of 95.05% and 96.64% for the left and right ears, respectively. We also present comparisons with other widely-used machine learning algorithms (e.g., Support Vector Machine, Multi-layer Perceptron) to show the effectiveness of our proposed Random Forest-based approach. Results obtained from this study provides implications and potential feasibility in providing a practically-applicable screening tool for identifying patient-intended malingering in hearing loss-related tests.DOI: 10.1371/journal.pone.0261433PMCID: PMC8719684",pubmed,34972151,10.1371/journal.pone.0261433
association of grm7 variants with different phenotype patterns of agerelated hearing impairment in an elderly male han chinese population,"394. PLoS One. 2013 Oct 11;8(10):e77153. doi: 10.1371/journal.pone.0077153. eCollection 2013.Association of GRM7 variants with different phenotype patterns of age-related hearing impairment in an elderly male Han Chinese population.Luo H(1), Yang T, Jin X, Pang X, Li J, Chai Y, Li L, Zhang Y, Zhang L, Zhang Z, Wu W, Zhang Q, Hu X, Sun J, Jiang X, Fan Z, Huang Z, Wu H.Author information:(1)Department of Otolaryngology Head and Neck Surgery, Xinhua Hospital, Shanghai JiaoTong University School of Medicine, Shanghai, China ; Ear Institute, Shanghai JiaoTong University School of Medicine, Shanghai, China ; Department of Otolaryngology Head and Neck Surgery, RenJi Hospital, Shanghai JiaoTong University School of Medicine, Shanghai, China.Several single nucleotide polymorphisms (SNPs) of the Glutamate metabotrophic receptor 7 gene (GRM7) have recently been identified by the genome-wide association study (GWAS) as potentially playing a role in susceptibility to age-related hearing impairment (ARHI), however this has not been validated in the Han Chinese population. The aim of this study was to determine if these SNPs are also associated with ARHI in an elderly male Han Chinese population. In this case-control candidate genes association study, a total of 982 men with ARHI and 324 normal-hearing controls subjects were studied. Using K-means cluster analysis, four audiogram shape subtypes of ARHI were identified in the case group: ''flat shape (FL)'', ''sloping shape (SL)'', ''2-4 kHz abrupt loss (AL) shape'' and ''8 kHz dip (8D) shape''. Results suggested that the SNP rs11928865 (A>T) of GRM7 was significantly associated with ARHI after adjusting for non-genetic factors (p = 0.000472, OR = 1.599, 95%CI = 1.229~2.081). Furthermore, frequency of TT genotype (rs11928865) were significant higher in the SL subgroup and AL subgroup with compared to controls group (p = 9.41E-05, OR = 1.945, 95%CI = 1.393~2.715; p = 0.000109, OR = 1.915, 95%CI = 1.378~2.661 adjusted, respectively) after Bonferroni correction. However, there wasn't significant difference in the frequency of the TT genotype between cases in the FL subgroup or the 8D subgroup with when compared with controls. Results of the current study suggest that, in an elderly male Han Chinese population, GRM7 SNP rs11928865 (TT) occurs more frequently in ARHI patients with SL and AL phenotype patterns.DOI: 10.1371/journal.pone.0077153PMCID: PMC3795658",pubmed,24146964,10.1371/journal.pone.0077153
a deep learning approach to predict conductive hearing loss in patients with otitis media with effusion using otoscopic images,"Importance: Otitis media with effusion (OME) is one of the most common causes of acquired conductive hearing loss (CHL). Persistent hearing loss is associated with poor childhood speech and language development and other adverse consequence. However, to obtain accurate and reliable hearing thresholds largely requires a high degree of cooperation from the patients. Objective: To predict CHL from otoscopic images using deep learning (DL) techniques and a logistic regression model based on tympanic membrane features. Design, Setting, and Participants: A retrospective diagnostic/prognostic study was conducted using 2790 otoscopic images obtained from multiple centers between January 2015 and November 2020. Participants were aged between 4 and 89 years. Of 1239 participants, there were 209 ears from children and adolescents (aged 4-18 years [16.87%]), 804 ears from adults (aged 18-60 years [64.89%]), and 226 ears from older people (aged >60 years, [18.24%]). Overall, 679 ears (54.8%) were from men. The 2790 otoscopic images were randomly assigned into a training set (2232 [80%]), and validation set (558 [20%]). The DL model was developed to predict an average air-bone gap greater than 10 dB. A logistic regression model was also developed based on otoscopic features. Main Outcomes and Measures: The performance of the DL model in predicting CHL was measured using the area under the receiver operating curve (AUC), accuracy, and F1 score (a measure of the quality of a classifier, which is the harmonic mean of precision and recall; a higher F1 score means better performance). In addition, these evaluation parameters were compared to results obtained from the logistic regression model and predictions made by three otologists. Results: The performance of the DL model in predicting CHL showed the AUC of 0.74, accuracy of 81%, and F1 score of 0.89. This was better than the results from the logistic regression model (ie, AUC of 0.60, accuracy of 76%, and F1 score of 0.82), and much improved on the performance of the 3 otologists; accuracy of 16%, 30%, 39%, and F1 scores of 0.09, 0.18, and 0.25, respectively. Furthermore, the DL model took 2.5 seconds to predict from 205 otoscopic images, whereas the 3 otologists spent 633 seconds, 645 seconds, and 692 seconds, respectively. Conclusions and Relevance: The model in this diagnostic/prognostic study provided greater accuracy in prediction of CHL in ears with OME than those obtained from the logistic regression model and otologists. This indicates great potential for the use of artificial intelligence tools to facilitate CHL evaluation when CHL is unable to be measured.  © 2022 American Medical Association. All rights reserved.",scopus,2-s2.0-85130704347,10.1001/jamaoto.2022.0900
exploration of a physiologicallyinspired hearingaid algorithm using a computer model mimicking impaired hearing,"323. Int J Audiol. 2016;55(6):346-57. doi: 10.3109/14992027.2015.1135352. Epub 2016 Feb 26.Exploration of a physiologically-inspired hearing-aid algorithm using a computer model mimicking impaired hearing.Jürgens T(1)(2), Clark NR(2)(3), Lecluyse W(2)(4), Meddis R(2).Author information:(1)a Medizinische Physik, Forschungszentrum Neurosensorik and Cluster of Excellence 'Hearing4all', Carl-von-Ossietzky Universität Oldenburg , Oldenburg , Germany .(2)b Department of Psychology , University of Essex , Colchester , UK .(3)c Mimi Hearing Technologies GmbH , Berlin , Germany .(4)d Department of Children, Young People and Education , University Campus Suffolk , Ipswich , UK.OBJECTIVE: To use a computer model of impaired hearing to explore the effects of a physiologically-inspired hearing-aid algorithm on a range of psychoacoustic measures.DESIGN: A computer model of a hypothetical impaired listener's hearing was constructed by adjusting parameters of a computer model of normal hearing. Absolute thresholds, estimates of compression, and frequency selectivity (summarized to a hearing profile) were assessed using this model with and without pre-processing the stimuli by a hearing-aid algorithm. The influence of different settings of the algorithm on the impaired profile was investigated. To validate the model predictions, the effect of the algorithm on hearing profiles of human impaired listeners was measured.STUDY SAMPLE: A computer model simulating impaired hearing (total absence of basilar membrane compression) was used, and three hearing-impaired listeners participated.RESULTS: The hearing profiles of the model and the listeners showed substantial changes when the test stimuli were pre-processed by the hearing-aid algorithm. These changes consisted of lower absolute thresholds, steeper temporal masking curves, and sharper psychophysical tuning curves.CONCLUSION: The hearing-aid algorithm affected the impaired hearing profile of the model to approximate a normal hearing profile. Qualitatively similar results were found with the impaired listeners' hearing profiles.DOI: 10.3109/14992027.2015.1135352",pubmed,26918797,10.3109/14992027.2015.1135352
automatic prediction of conductive hearing loss using video pneumatic otoscopy and deep learning algorithm,"75. Ear Hear. 2022 Sep-Oct 01;43(5):1563-1573. doi: 10.1097/AUD.0000000000001217. Epub 2022 Mar 29.Automatic Prediction of Conductive Hearing Loss Using Video Pneumatic Otoscopy and Deep Learning Algorithm.Byun H(1)(2), Park CJ(3)(2), Oh SJ(4), Chung MJ(3)(4)(5), Cho BH(4)(6), Cho YS(7).Author information:(1)Department of Otorhinolaryngology-Head and Neck Surgery, Hanyang University College of Medicine, Hanyang University Medical Center, Seoul, South Korea.(2)These authors contributed equally to this work.(3)Department of Digital Health, SAIHST, Sungkyunkwan University, Seoul, South Korea.(4)Medical AI Research Center, Samsung Medical Center, Seoul, South Korea.(5)Department of Radiology, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul, South Korea.(6)Department of Medical Device Management and Research, SAIHST, Sungkyunkwan University, Seoul, South Korea.(7)Department of Otorhinolaryngology-Head and Neck Surgery, Sungkyunkwan University School of Medicine, Samsung Medical Center, Seoul, South Korea.OBJECTIVES: Diseases of the middle ear can interfere with normal sound transmission, which results in conductive hearing loss. Since video pneumatic otoscopy (VPO) findings reveal not only the presence of middle ear effusions but also dynamic movements of the tympanic membrane and part of the ossicles, analyzing VPO images was expected to be useful in predicting the presence of middle ear transmission problems. Using a convolutional neural network (CNN), a deep neural network implementing computer vision, this preliminary study aimed to create a deep learning model that detects the presence of an air-bone gap, conductive component of hearing loss, by analyzing VPO findings.DESIGN: The medical records of adult patients who underwent VPO tests and pure-tone audiometry (PTA) on the same day were reviewed for enrollment. Conductive hearing loss was defined as an average air-bone gap of more than 10 dB at 0.5, 1, 2, and 4 kHz on PTA. Two significant images from the original VPO videos, at the most medial position on positive pressure and the most laterally displaced position on negative pressure, were used for the analysis. Applying multi-column CNN architectures with individual backbones of pretrained CNN versions, the performance of each model was evaluated and compared for Inception-v3, VGG-16 or ResNet-50. The diagnostic accuracy predicting the presence of conductive component of hearing loss of the selected deep learning algorithm used was compared with experienced otologists.RESULTS: The conductive hearing loss group consisted of 57 cases (mean air-bone gap = 25 ± 8 dB): 21 ears with effusion, 14 ears with malleus-incus fixation, 15 ears with stapes fixation including otosclerosis, one ear with a loose incus-stapes joint, 3 cases with adhesive otitis media, and 3 ears with middle ear masses including congenital cholesteatoma. The control group consisted of 76 cases with normal hearing thresholds without air-bone gaps. A total of 1130 original images including repeated measurements were obtained for the analysis. Of the various network architectures designed, the best was to feed each of the images into the individual backbones of Inception-v3 (three-column architecture) and concatenate the feature maps after the last convolutional layer from each column. In the selected model, the average performance of 10-fold cross-validation in predicting conductive hearing loss was 0.972 mean areas under the curve (mAUC), 91.6% sensitivity, 96.0% specificity, 94.4% positive predictive value, 93.9% negative predictive value, and 94.1% accuracy, which was superior to that of experienced otologists, whose performance had 0.773 mAUC and 79.0% accuracy on average. The algorithm detected over 85% of cases with stapes fixations or ossicular chain problems other than malleus-incus fixations. Visualization of the region of interest in the deep learning model revealed that the algorithm made decisions generally based on findings in the malleus and nearby tympanic membrane.CONCLUSIONS: In this preliminary study, the deep learning algorithm created to analyze VPO images successfully detected the presence of conductive hearing losses caused by middle ear effusion, ossicular fixation, otosclerosis, and adhesive otitis media. Interpretation of VPO using the deep learning algorithm showed promise as a diagnostic tool to differentiate conductive hearing loss from sensorineural hearing loss, which would be especially useful for patients with poor cooperation.Copyright © 2022 The Authors. Ear & Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health, Inc.DOI: 10.1097/AUD.0000000000001217",pubmed,35344974,10.1097/AUD.0000000000001217
using a decision tree approach to determine hearing aid ownership in older adults,"The main clinical intervention for older adults with hearing loss is the provision of hearing aids. However, uptake and usage in this population have historically been reported as low. The aim of this study was to understand the hearing loss characteristics, from measured audiometric hearing loss and self-perceived hearing handicap, that contribute to the decision of hearing aid ownership. A total of 2833 adults aged 50+ years, of which 329 reported hearing aid ownership, were involved with a population-based survey with audiometric hearing assessments. Classification and regression tree (CART) analysis was used to classify hearing aid ownership from audiometric measurements and hearing disability outcomes. An overall accuracy of 92.5% was found for the performance of the CART analysis in predicting hearing aid ownership from hearing loss characteristics. By including hearing disability, sensitivity for predicting hearing aid ownership increased by up to 40% compared with just audiometric hearing loss measurements alone. A decision tree approach that considers both objectively measured hearing loss and self-perceived hearing disability, could facilitate a more tailored and personalised approach for determining hearing aid needs in the older population. Without intervention, older adults with hearing loss are at higher risk of cognitive decline and higher rates of depression, anxiety, social isolation. The provision of hearing aids can compensate hearing function, however, uptake and usage have been reported as low. Using a more precise cut-off from audiometric measures and self-perceived hearing disability scores could facilitate a tailored and personalised approach to screen and identify older adults for hearing aid needs.",cinahl,9638288,10.1080/09638288.2022.2087761
a longitudinal study of hearing level aggravation due to aging,"540. Nihon Jibiinkoka Gakkai Kaiho. 1996 Apr;99(4):558-66. doi: 10.3950/jibiinkoka.99.558.[A longitudinal study of hearing level aggravation due to aging].[Article in Japanese]Ono Y(1).Author information:(1)Department of Oto-rhino-laryngology, School of Medicine, Kitasato University, Sagamihara.It has been reported that hearing aggravation due to aging is predominantly in the high frequency range and that there are rapid and slow phase of aggravation in hearing level. In the present study, the pattern of aggravation of hearing level due to aging was investigated in 387 adults of over 35 years of age who had visited the Kitasato Health Science Center for a regular health check-up annually for more than 5 years, and were diagnosed as having neither external of middle ear diseases nor hearing impairment of obvious origin at their first visit. The subjects were divided into 8 groups according to their ages in increments of 5 years. Their audiograms were obtained annually, and the results were used to obtain the distribution of the hearing level at each test frequency. All of the subjects were examined for individual changes in audiograms and those who showed 20 dB or more aggravation of hearing in a 1-year period without subsequent improvement were defined as having a rapid phase of aggravation. There were clustering points in hearing distribution at around 30 dB and 60 dB at the test frequency of 8 kHz in those subjects showing a rapid phase of aggravation. Similar clustering points were also noted in those subjects who showed gradual aggravation of 20 dB or more in a 5-year period and who had 20 dB or more aggravation in one year but showed later improvement. As for the test frequencies lower than 4 kHz, there appeared to be a clustering point at around 30 dB. The incidence of the rapid phase of aggravation was then determined in each group, in order to investigate the relationship between aging and the appearance of the rapid phase of aggravation. The rapid phase was already noted in the youngest age group (range, 35-39 years), while the incidence gradually increased up to the 50-54-year group and stayed at a constant level in the 55-69-year groups. The incidence markedly decreased thereafter. The results suggest that hearing aggravation due to aging does not occur at any particular age. Rather, the hearing aggravation appeared to be closely related to the hearing level, and to manifest itself when the hearing level approaches 30 dB or 60 dB.DOI: 10.3950/jibiinkoka.99.558",pubmed,8683366,10.3950/jibiinkoka.99.558
application of the mathematical model for prognosis in the rehabilitation of children after cochlear implantation ispolzovanie matematicheskoi modeli dlya opredeleniya prognoza reabilitatsii u detei posle kokhlearnoi implantatsii,"Despite the variety of etiological factors, cochlear implantation (CI) remains the only effective method for the rehabilitation of the patients presenting with total deafness. The aim of this study was the enhancement of the efficiency of selection of the candidates for CI, the improvement of the quality of rehabilitation of the patients with cochlear implants, and the determination of the prognostic criteria for clinical trials.; PATIENTS AND METHODS: (CI). The decision-making support system (DMSS) based on the artificial neural networks (ANNs) has been created to enhance the efficiency of rehabilitation of the patients with cochlear implants and increase the effectiveness of the selection of candidates for cochlear implantation. The results of the children's rehabilitation after CI have been analyzed by using a mathematical model of artificial neural networks (Kohonen layer). The basis for the assessment of ANNs was formed by the results of the observations of audioverbal perception in 110 patients aged from 6 months to 17 years. The initial data were the average values obtained with the use of the Russian-language version of the Nottingham children's implant profile's test T1 - T3. The testing was performed before CI and 3, 6, 12, 18, and 24 months after it.; MAIN RESULTS: The work yielded the four-cluster data structure. It made it possible to estimate the effectiveness of the clinical trials in selected classes depending on the etiology of the disease, the age of the patients, and their experience with the application of hearing aids. The reliable estimation of the dynamics of auditory perception at the stage of rehabilitation and prognosis of the outcomes of CI made it possible to take additional preventive and therapeutic measures in the combination with complementary psychological and educational procedures.; Цель исследования - повышение эффективности отбора кандидатов на кохлеарную имплантацию (КИ), улучшение качества реабилитации пациентов с имплантами, определение прогностических критериев КИ. Проведен анализ результатов реабилитации детей после КИ с использованием математической модели ИНС (слоя Кохонена). В основу построения ИНС легли результаты оценки развития слухоречевого восприятия у 110 пациентов в возрасте от 6 мес до 17 лет. Тестирование пациентов проводилось до КИ и через 3, 6, 12, 18 и 24 мес после проведенной КИ с помощью русскоязычной версии батареи тестов EARS. Проведенная работа позволила получить четырехкластерную структуру данных, оценить эффективность КИ в выделенных классах в зависимости от этиологии заболевания, возраста, наличия опыта слухопротезирования. На этапе реабилитации произведены достоверная оценка динамики развития слухового восприятия и прогноз результатов КИ, что дало возможность своевременно применить дополнительные методы лечения в сложных случаях.",scopus,2-s2.0-85021851753,10.17116/otorino201681647-50
validation of a cochlear implant patientspecific model of the voltage distribution in a clinical setting,"774. Front Bioeng Biotechnol. 2016 Nov 23;4:84. doi: 10.3389/fbioe.2016.00084. eCollection 2016.Validation of a Cochlear Implant Patient-Specific Model of the Voltage Distribution in a Clinical Setting.Nogueira W(1), Schurzig D(1), Büchner A(1), Penninger RT(1), Würfel W(1).Author information:(1)Department of Otolaryngology, Cluster of Excellence ""Hearing4all"", Medical University Hannover , Hannover , Germany.Cochlear Implants (CIs) are medical implantable devices that can restore the sense of hearing in people with profound hearing loss. Clinical trials assessing speech intelligibility in CI users have found large intersubject variability. One possibility to explain the variability is the individual differences in the interface created between electrodes of the CI and the auditory nerve. In order to understand the variability, models of the voltage distribution of the electrically stimulated cochlea may be useful. With this purpose in mind, we developed a parametric model that can be adapted to each CI user based on landmarks from individual cone beam computed tomography (CBCT) scans of the cochlea before and after implantation. The conductivity values of each cochlea compartment as well as the weighting factors of different grounding modes have also been parameterized. Simulations were performed modeling the cochlea and electrode positions of 12 CI users. Three models were compared with different levels of detail: a homogeneous model (HM), a non-patient-specific model (NPSM), and a patient-specific model (PSM). The model simulations were compared with voltage distribution measurements obtained from the backward telemetry of the 12 CI users. Results show that the PSM produces the lowest error when predicting individual voltage distributions. Given a patient-specific geometry and electrode positions, we show an example on how to optimize the parameters of the model and how to couple it to an auditory nerve model. The model here presented may help to understand speech performance variability and support the development of new sound coding strategies for CIs.DOI: 10.3389/fbioe.2016.00084PMCID: PMC5120131",pubmed,27933290,10.3389/fbioe.2016.00084
prevalence of hearing loss in northern and southern germany,"57. HNO. 2017 Aug;65(Suppl 2):130-135. doi: 10.1007/s00106-016-0318-4.Prevalence of hearing loss in Northern and Southern Germany.[Article in English]von Gablenz P(1), Hoffmann E(2), Holube I(3).Author information:(1)Institute of Hearing Technology and Audiology, Jade University of Applied Sciences and Cluster of Excellence ""Hearing4All"", Oldenburg, Germany. petra.vongablenz@jade-hs.de.(2), Kempten (Allgäu), Germany.(3)Institute of Hearing Technology and Audiology, Jade University of Applied Sciences and Cluster of Excellence ""Hearing4All"", Oldenburg, Germany.BACKGROUND: The HÖRSTAT study conducted in Northwest Germany found hearing impairment in approximately 16% of adults when applying the World Health Organization (WHO) criterion. However, the robustness of extrapolations to a national level might be questioned, as the epidemiological data were collected on a regional level.METHODS: Independently from HÖRSTAT, the ""Hearing in Germany"" study examined adult hearing in Aalen, a town located in Southwest Germany. Both cross-sectional studies were based on stratified random samples from the general population. The average pure-tone threshold shift at 0.5, 1, 2, and 4 kHz (PTA4), the prevalence of hearing impairment (WHO criterion: PTA4 in the better ear >25), and hearing aid uptake were compared. Data from the Aalen and HÖRSTAT studies were pooled (n = 3105) to extrapolate to the prevalence and the degree of hearing impairment for the years 2015, 2020, and 2025.RESULTS: Both studies yielded very similar results for PTA4. Weighted for official population statistics, the prevalence of hearing impairment according to the WHO criterion is 16.2% in adults, thus affecting 11.1 million persons in Germany. Owing to demographic changes, the prevalence is expected to increase in the medium term by around 1% per 5‑year period. With a similar degree of hearing loss, hearing aid provision differs from place to place.CONCLUSION: When adjusted for gender and age to the European Standard Population, the prevalence of hearing impairment observed both in HÖRSTAT and the Aalen sample is considerably lower than reported for international studies. Since the analysis refers to cross-sectional data only, possible cohort effects are not considered in the prevalence projection.DOI: 10.1007/s00106-016-0318-4",pubmed,28477091,10.1007/s00106-016-0318-4
rehabilitation possibilities for hearingimpaired subjects,"544. Ned Tijdschr Geneeskd. 1998 Jan 10;142(2):63-7.[Rehabilitation possibilities for hearing-impaired subjects].[Article in Dutch]Kapteyn TS(1).Author information:(1)Academisch Ziekenhuis Vrije Universiteit, afd. KNO, Amsterdam.Hearing impairment is a symptom, not a diagnosis. A number of types of hearing impairment can be distinguished. The self-reported hearing problems cluster around six hearing factors, the most important of which are speech understanding in noise and localisation of a sound source. For these capabilities equivalent functioning of both ears is important. The general practitioner can determine diagnosis and severity of the impairment using rather simple tools. When the cause of the impairment cannot be reduced in a proper way an adaptation of the sound to the impaired ear will be indicated. This can be arranged by either an ENT specialist or a centre for audiology. The selection of a proper hearing aid requires expertise and particular attention for the complaints. It is of the utmost importance that the hearing-impaired person can try out the effects of the hearing aid in daily circumstances for some weeks. If the patient, members of the family or the prescriber are not satisfied with the results, supplementary help is required, for example training in communication skills or special devices.",pubmed,9556995,
artificial intelligence applications in otology a state of the art review,"Objective: Recent advances in artificial intelligence (AI) are driving innovative new health care solutions. We aim to review the state of the art of AI in otology and provide a discussion of work underway, current limitations, and future directions. Data Sources: Two comprehensive databases, MEDLINE and EMBASE, were mined using a directed search strategy to identify all articles that applied AI to otology. Review Methods: An initial abstract and title screening was completed. Exclusion criteria included nonavailable abstract and full text, language, and nonrelevance. References of included studies and relevant review articles were cross-checked to identify additional studies. Conclusion: The database search identified 1374 articles. Abstract and title screening resulted in full-text retrieval of 96 articles. A total of N = 38 articles were retained. Applications of AI technologies involved the optimization of hearing aid technology (n = 5; 13% of all articles), speech enhancement technologies (n = 4; 11%), diagnosis and management of vestibular disorders (n = 11; 29%), prediction of sensorineural hearing loss outcomes (n = 9; 24%), interpretation of automatic brainstem responses (n = 5; 13%), and imaging modalities and image-processing techniques (n = 4; 10%). Publication counts of the included articles from each decade demonstrated a marked increase in interest in AI in recent years. Implications for Practice: This review highlights several applications of AI that otologists and otolaryngologists alike should be aware of given the possibility of implementation in mainstream clinical practice. Although there remain significant ethical and regulatory challenges, AI powered systems offer great potential to shape how healthcare systems of the future operate and clinicians are key stakeholders in this process. © American Academy of Otolaryngology–Head and Neck Surgery Foundation 2020.",scopus,2-s2.0-85086237068,10.1177/0194599820931804
reception of environmental sounds through cochlear implants,"469. Ear Hear. 2005 Feb;26(1):48-61. doi: 10.1097/00003446-200502000-00005.Reception of environmental sounds through cochlear implants.Reed CM(1), Delhorne LA.Author information:(1)Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, USA.OBJECTIVE: The objective of this study was to measure the performance of persons with cochlear implants on a test of environmental-sound reception.DESIGN: The reception of environmental sounds was studied using a test employing closed sets of 10 sounds in each of four different settings (General Home, Kitchen, Office, and Outside). The participants in the study were 11 subjects with cochlear implants. Identification testing was conducted under each of the four closed sets of stimuli using a one-interval, 10-alternative, forced-choice procedure. The data were summarized in terms of overall percent correct identification scores and information transfer (IT) in bits. Confusion patterns were described using a hierarchical-clustering analysis. In addition, individual performance on the environmental-sound task was related to the ability to recognize isolated words through the cochlear implant alone.RESULTS: Levels of performance were similar across the four stimulus sets. Mean scores across subjects ranged from 45.3% correct (and IT of 1.5 bits) to 93.8% correct (and IT of 3.1 bits). Performance on the environmental-sound identification test was roughly related to NU-6 word recognition ability. Specifically, those subjects with word scores greater than 34% correct performed at levels of 80 to 94% on environmental-sound recognition, whereas subjects with word scores less than 34% had greater difficulty on the task. Results of the hierarchical clustering analysis, conducted on two groups of subjects (a high-performing [HP] group and a low-performing [LP] group), indicated that confusions were confined to three or four specific stimuli for the HP subjects and that larger clusters of confused stimuli were observed in the data of the LP group. Signals with distinct temporal-envelope characteristics were easily perceived by all subjects, and confused items tended to share similar overall durations and temporal envelopes.CONCLUSIONS: Temporal-envelope cues appear to play a large role in the identification of environmental sounds through cochlear implants. The finer distinctions made by the HP group compared with the LP group may be related to a better ability both to resolve temporal differences and to use gross spectral cues. These findings are qualitatively consistent with patterns of confusions observed in the reception of speech segments through cochlear implants.DOI: 10.1097/00003446-200502000-00005",pubmed,15692304,10.1097/00003446-200502000-00005
validation of selfreported hearing loss among multiethnic community dwelling older adults in malaysia,"Introduction: Little is known about the prevalence of hearing loss and the usefulness of self-report hearing loss among older adults in Malaysia. Aim: We conducted a population-based study to investigate the prevalence of self-reported hearing problem and its relationship with audiometric hearing thresholds in older adults in Selangor, Malaysia. We also investigated demographic factors that were associated with the self-reported hearing loss. Materials and Methods: The participants were recruited from Selangor using a multi-stage clustered sampling involving 324 participants aged between 60 to 88-year-old (68.3±5.9 years). All participants underwent a face-to-face interview and pure tone audiometry. Self-reported hearing loss was obtained using three questions. Results: The prevalence of self-reported hearing problems was 53.4%. This prevalence did not differ significantly among age group, gender, race and education level (p>0.05). Univariate and logistic regression analyses found that tinnitus and Pure Tone Average (PTA) of at least moderate hearing loss at 0.5 kHz to 4 kHz contributed significantly to the likelihood of self-reported hearing problem. Participants with tinnitus and participants with PTA at least moderate hearing loss at 0.5 kHz to 4 kHz were twice as likely to report hearing problem than their counterparts. The questions yielded poor sensitivity in identifying at least mild loss and moderate sensitivity for at least moderate hearing loss. Conclusion: The present study highlights the need for a more effective self-report inventory or audiometry instrument that is less sensitive to background noise to better estimate hearing loss prevalence among adults in Malaysia. © 2017, Journal of Clinical and Diagnostic Research. All rights reserved.",scopus,2-s2.0-85032025654,10.7860/JCDR/2017/28144.10756
predicting cochlear dead regions in patients with hearing loss through a machine learningbased approach a preliminary study,"247. PLoS One. 2019 Jun 3;14(6):e0217790. doi: 10.1371/journal.pone.0217790. eCollection 2019.Predicting cochlear dead regions in patients with hearing loss through a machine learning-based approach: A preliminary study.Chang YS(1)(2), Park H(3), Hong SH(4), Chung WH(5), Cho YS(5), Moon IJ(5).Author information:(1)Department of Otorhinolaryngology-Head and Neck Surgery, Korea University College of Medicine, Korea University Ansan Hospital, Ansan, Republic of Korea.(2)Department of Digital Health, SAIHST, Sungkyunkwan University, Seoul, Republic of Korea.(3)Hearing Research Laboratory, Samsung Medical Center, Seoul, Republic of Korea.(4)Department of Otorhinolaryngology-Head and Neck Surgery, Samsung Changwon Hospital, Sungkyunkwan University School of Medicine, Changwon, Republic of Korea.(5)Department of Otorhinolaryngology-Head and Neck Surgery, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul, Republic of Korea.We propose a machine learning (ML)-based model for predicting cochlear dead regions (DRs) in patients with hearing loss of various etiologies. Five hundred and fifty-five ears from 380 patients (3,770 test samples) diagnosed with sensorineural hearing loss (SNHL) were analyzed. A threshold-equalizing noise (TEN) test was applied to detect the presence of DRs. Data were collected on sex, age, side of the affected ear, hearing loss etiology, word recognition scores (WRS), and pure-tone thresholds at each frequency. According to the cause of hearing loss as diagnosed by the physician, we categorized the patients into six groups: 1) SNHL with unknown etiology; 2) sudden sensorineural hearing loss (SSNHL); 3) vestibular schwannoma (VS); 4) Meniere's disease (MD); 5) noise-induced hearing loss (NIHL); or 6) presbycusis or age-related hearing loss (ARHL). To develop a predictive model, we performed recursive partitioning and regression for classification, logistic regression, and random forest. The overall prevalence of one or more DRs in test ears was 20.36% (113 ears). Among the 3,770 test samples, the overall frequency-specific prevalence of DR was 6.7%. WRS, pure-tone thresholds at each frequency, disease type (VS or MD), and frequency information were useful for predicting DRs. Sex and age were not associated with detecting DRs. Based on these results, we suggest possible predictive factors for determining the presence of DRs. To improve the predictive power of the model, a more flexible model or more clinical features, such as the duration of hearing loss or risk factors for developing DRs, may be needed.DOI: 10.1371/journal.pone.0217790PMCID: PMC6546232",pubmed,31158267,10.1371/journal.pone.0217790
data from past patients used to streamline adjustment of levels for cochlear implant for new patients,"Cochlear implant technology gives deaf people the ability to sense sound and speech. It consists of an electrode array inserted (implanted) in the cochlea of the ear and an external device that wirelessly connects to the electrodes. Each electrode stimulates different areas of the auditory nerve based of what frequency they represent. The sensitivity of auditory nerve fibers varies from patient to patient so upper and lower limits for each electrode must be set for each patient. Currently, this is undertaken by an operator adjusting the levels while the patient gives oral feedback. This process is both time consuming and challenging in that many patients are young children who are only partly able to provide feedback. At Oslo University Hospital, data has been collected on adjustment levels and response measurements from a number of former patients (158 have been used in this project). In this paper, we consider to what extent it is possible to predict values for new patients by using various machine learning techniques on data from previous patients. Although it is not possible to achieve fully automatic adjustments, the experiments show that a good starting point can be provided for manual adjustment. Further, the work has also shown which electrodes are most important to measure to automatically predict levels of other electrodes. © 2016 IEEE.",scopus,2-s2.0-85015980253,10.1109/SSCI.2016.7850063
hearing disability measured by the speech spatial and qualities of hearing scale in clinically normalhearing and hearingimpaired middleaged persons and disability screening by means of a reduced ssq the ssq5,"398. Ear Hear. 2012 Sep-Oct;33(5):615-6. doi: 10.1097/AUD.0b013e31824e0ba7.Hearing disability measured by the speech, spatial, and qualities of hearing scale in clinically normal-hearing and hearing-impaired middle-aged persons, and disability screening by means of a reduced SSQ (the SSQ5).Demeester K(1), Topsakal V, Hendrickx JJ, Fransen E, van Laer L, Van Camp G, Van de Heyning P, van Wieringen A.Author information:(1)Department of Otolaryngology, University and University Hospital of Antwerp, Belgium. kelly.demeester@ua.ac.beOBJECTIVES: : The goals of the present study were twofold: in the first part, the prevalence and profile of hearing disability in healthy, middle-aged persons were determined by the speech, spatial, and qualities of hearing scale (SSQ). In the second part of this study, the number of SSQ items was reduced to five to make this questionnaire available for routine usage in clinical settings and for screening purposes.METHODS: : SSQ responses derived from 103 normal-hearing 18- to 25-year-old persons were compared with the SSQ responses of 24 clinically normal-hearing (all thresholds between 125 and 8000 Hz ≤25 dB HL) and 109 healthy, 55- to 65-year-old persons with age-related hearing impairment to determine the prevalence and profile of hearing disability. The 45 items of the SSQ were reduced to five by cluster analyses and binary logistic regression analyses. The robustness of this five-item version (SSQ5) was determined in three control populations: an adult 25- to 55-year-old population (n = 159), an ENT-patient population (n = 60), and a population of hearing aid candidates (n = 50). The feasibility of the SSQ5 for screening was compared with the feasibility of the simple question ""Do you have hearing loss?"" by determining, respectively, the sensitivity, specificity, and maximum achievable discriminatory power for predicting hearing status according to speech-in-noise performance.RESULTS: : Prevalence numbers showed data of healthy, middle-aged persons with significant disability, despite minimal impairment (25%) versus data of middle-aged persons with significant impairment and nevertheless, minimal disability (61%). The profile of hearing disability seemed similar in all normal-hearing and hearing-impaired subgroups (i.e., most problems with understanding speech especially in noise conditions, and least problems with sound quality). Compared with the single question: ""Do you have hearing loss?"" the use of the SSQ5 had 37% more maximum discriminatory power for determining hearing status category based on speech-in-noise performance in 55- to 65-year-old persons. In addition, the SSQ5 seemed robust in adult populations of different ages (89.6% correlation between the answers of the SSQ5 and SSQ45), as well as in ENT-patient populations (93.7% correlation) and hearing aid candidate populations (79.2% correlation).CONCLUSIONS: : The results of this study suggest that disability measures and measures for hearing impairment cannot replace each other, but are complementary. Therefore, it is advised to implement both disability measures and impairment measures in screening and referral policies for hearing loss. To get a first impression of hearing disability, our results suggest that it is useful to ask five disability questions (SSQ5) instead of one general question like ""Do you have hearing loss?""DOI: 10.1097/AUD.0b013e31824e0ba7",pubmed,22568994,10.1097/AUD.0b013e31824e0ba7
towards personalized and optimized fitting of cochlear implants,"679. Front Neurosci. 2023 Jul 13;17:1183126. doi: 10.3389/fnins.2023.1183126. eCollection 2023.Towards personalized and optimized fitting of cochlear implants.Van Opstal AJ(1), Noordanus E(1).Author information:(1)Donders Centre for Neuroscience, Section Neurophysics, Radboud University, Nijmegen, Netherlands.A cochlear implant (CI) is a neurotechnological device that restores total sensorineural hearing loss. It contains a sophisticated speech processor that analyzes and transforms the acoustic input. It distributes its time-enveloped spectral content to the auditory nerve as electrical pulsed stimulation trains of selected frequency channels on a multi-contact electrode that is surgically inserted in the cochlear duct. This remarkable brain interface enables the deaf to regain hearing and understand speech. However, tuning of the large (>50) number of parameters of the speech processor, so-called ""device fitting,"" is a tedious and complex process, which is mainly carried out in the clinic through 'one-size-fits-all' procedures. Current fitting typically relies on limited and often subjective data that must be collected in limited time. Despite the success of the CI as a hearing-restoration device, variability in speech-recognition scores among users is still very large, and mostly unexplained. The major factors that underly this variability incorporate three levels: (i) variability in auditory-system malfunction of CI-users, (ii) variability in the selectivity of electrode-to-auditory nerve (EL-AN) activation, and (iii) lack of objective perceptual measures to optimize the fitting. We argue that variability in speech recognition can only be alleviated by using objective patient-specific data for an individualized fitting procedure, which incorporates knowledge from all three levels. In this paper, we propose a series of experiments, aimed at collecting a large amount of objective (i.e., quantitative, reproducible, and reliable) data that characterize the three processing levels of the user's auditory system. Machine-learning algorithms that process these data will eventually enable the clinician to derive reliable and personalized characteristics of the user's auditory system, the quality of EL-AN signal transfer, and predictions of the perceptual effects of changes in the current fitting.Copyright © 2023 Van Opstal and Noordanus.DOI: 10.3389/fnins.2023.1183126PMCID: PMC10372492",pubmed,37521701,10.3389/fnins.2023.1183126
towards personalized auditory models predicting individual sensorineural hearingloss profiles from recorded human auditory physiology,"102. Trends Hear. 2021 Jan-Dec;25:2331216520988406. doi: 10.1177/2331216520988406.Towards Personalized Auditory Models: Predicting Individual Sensorineural Hearing-Loss Profiles From Recorded Human Auditory Physiology.Keshishzadeh S(1), Garrett M(2), Verhulst S(1).Author information:(1)Hearing Technology @ WAVES, Department of Information Technology, Ghent University, Belgium.(2)Medizinische Physik and Cluster of Excellence Hearing4all, Department of Medical Physics and Acoustics, University of Oldenburg, Oldenburg, Germany.Over the past decades, different types of auditory models have been developed to study the functioning of normal and impaired auditory processing. Several models can simulate frequency-dependent sensorineural hearing loss (SNHL) and can in this way be used to develop personalized audio-signal processing for hearing aids. However, to determine individualized SNHL profiles, we rely on indirect and noninvasive markers of cochlear and auditory-nerve (AN) damage. Our progressive knowledge of the functional aspects of different SNHL subtypes stresses the importance of incorporating them into the simulated SNHL profile, but has at the same time complicated the task of accomplishing this on the basis of noninvasive markers. In particular, different auditory-evoked potential (AEP) types can show a different sensitivity to outer-hair-cell (OHC), inner-hair-cell (IHC), or AN damage, but it is not clear which AEP-derived metric is best suited to develop personalized auditory models. This study investigates how simulated and recorded AEPs can be used to derive individual AN- or OHC-damage patterns and personalize auditory processing models. First, we individualized the cochlear model parameters using common methods of frequency-specific OHC-damage quantification, after which we simulated AEPs for different degrees of AN damage. Using a classification technique, we determined the recorded AEP metric that best predicted the simulated individualized cochlear synaptopathy profiles. We cross-validated our method using the data set at hand, but also applied the trained classifier to recorded AEPs from a new cohort to illustrate the generalizability of the method.DOI: 10.1177/2331216520988406PMCID: PMC7871356",pubmed,33526004,10.1177/2331216520988406
predicting hearing recovery following treatment of idiopathic sudden sensorineural hearing loss with machine learning models,"Purpose: Idiopathic sudden sensorineural hearing loss (ISSHL) is an emergency otological disease, and its definite prognostic factors remain unclear. This study applied machine learning methods to develop a new ISSHL prognosis prediction model. Materials and methods: This retrospective study reviewed the medical data of 244 patients who underwent combined intratympanic and systemic steroid treatment for ISSHL at a tertiary referral center between January 2015 and October 2019. We used 35 variables to predict hearing recovery based on Siegel's criteria. In addition to performing an analysis based on the conventional logistic regression model, we developed prediction models with five machine learning methods: least absolute shrinkage and selection operator, decision tree, random forest (RF), support vector machine, and boosting. To compare the predictive ability of each model, the accuracy, precision, recall, F-score, and the area under the receiver operator characteristic curves (ROC-AUC) were calculated. Results: Former otological history, ear fullness, delay between symptom onset and treatment, delay between symptom onset and intratympanic steroid injection (ITSI), and initial hearing thresholds of the affected and unaffected ears differed significantly between the recovery and non-recovery groups. While the RF method (accuracy: 72.22%, ROC-AUC: 0.7445) achieved the highest predictive power, the other methods also featured relatively good predictive power. In the RF model, the following variables were identified to be important for hearing-recovery prediction: delay between symptom onset and ITSI or the initial treatment, initial hearing levels of the affected and non-affected ears, body mass index, and a previous history of hearing loss. Conclusions: The machine learning models predictive of hearing recovery following treatment for ISSHL showed superior predictive power relative to the conventional logistic regression method, potentially allowing for better patient treatment outcomes. © 2020 Elsevier Inc.",scopus,2-s2.0-85098985673,10.1016/j.amjoto.2020.102858
development of an artificial intelligence based occupational noise induced hearing loss early warning system for mine workers,"Introduction: Occupational Noise Induced Hearing Loss (ONIHL) is one of the most prevalent conditions among mine workers globally. This reality is due to mine workers being exposed to noise produced by heavy machinery, rock drilling, blasting, and so on. This condition can be compounded by the fact that mine workers often work in confined workspaces for extended periods of time, where little to no attenuation of noise occurs. The objective of this research work is to present a preliminary study of the development of a hearing loss, early monitoring system for mine workers. Methodology: The system consists of a smart watch and smart hearing muff equipped with sound sensors which collect noise intensity levels and the frequency of exposure. The collected information is transferred to a database where machine learning algorithms namely the logistic regression, support vector machines, decision tree and Random Forest Classifier are used to classify and cluster it into levels of priority. Feedback is then sent from the database to a mine worker smart watch based on priority level. In cases where the priority level is extreme, indicating high levels of noise, the smart watch vibrates to alert the miner. The developed system was tested in a mock mine environment consisting of a 67 metres tunnel located in the basement of a building whose roof top represents the “surface” of a mine. The mock-mine shape, size of the tunnel, steel-support infrastructure, and ventilation system are analogous to deep hard-rock mine. The wireless channel propagation of the mock-mine is statistically characterized in 2.4–2.5 GHz frequency band. Actual underground mine material was used to build the mock mine to ensure it mimics a real mine as close as possible. The system was tested by 50 participants both male and female ranging from ages of 18 to 60 years. Results and discussion: Preliminary results of the system show decision tree had the highest accuracy compared to the other algorithms used. It has an average testing accuracy of 91.25% and average training accuracy of 99.79%. The system also showed a good response level in terms of detection of noise input levels of exposure, transmission of the information to the data base and communication of recommendations to the miner. The developed system is still undergoing further refinements and testing prior to being tested in an actual mine. Copyright © 2024 Madahana, Ekoru, Sebothoma and Khoza-Shangase.",scopus,2-s2.0-85189209485,10.3389/fnins.2024.1321357
lexical tone recognition with an artificial neural network,"171. Ear Hear. 2008 Jun;29(3):326-35. doi: 10.1097/AUD.0b013e3181662c42.Lexical tone recognition with an artificial neural network.Zhou N(1), Zhang W, Lee CY, Xu L.Author information:(1)School of Hearing, Speech and Language Sciences, Ohio University, Athens, Ohio 45701, USA.OBJECTIVES: Tone production is particularly important for communicating in tone languages such as Mandarin Chinese. In the present study, an artificial neural network was used to recognize tones produced by adult native speakers. The purposes of the study were (1) to test the sensitivity of the neural network to speaker variation typically in adult speaker groups, (2) to evaluate two normalization procedures to overcome the effects of speaker variation, and (3) to compare tone recognition performance of the neural network with that of the human listeners.DESIGN: A feedforward multilayer neural network was used. Twenty-nine adult native Mandarin Chinese speakers were recruited to record tone samples. The F0 contours of the vowel part of the 1044 monosyllabic words recorded were extracted using an autocorrelation method. Samples from the F0 contours were used as inputs to the neural network. The efficacy of the neural network was first tested by varying the number of inputs and the number of neurons in the hidden layer from 1 to 16. The sensitivity of the neural network to speaker variation was tested by (1) using the raw F0 data from speech tokens of a number of randomly drawn speakers that varied from 1 to 29, (2) using the raw F0 data from speech tokens of either male-only or female-only speakers, and (3) using two sets of normalized F0 data (i.e., tone 1-based normalization and first-order derivative) from speech tokens from a number of randomly drawn speakers that varied from 1 to 29. The recognition performance of the neural network under several experimental conditions was compared with the corresponding recognition performance of 10 normal-hearing, native Mandarin Chinese speaking adult listeners.RESULTS: Three inputs and four hidden neurons were found to be sufficient for the neural network to perform at about 85% correct using speech samples without normalization. The performance of the neural network was affected by variation across speakers particularly between genders. Using the tone 1-based normalization procedure, the performance of the neural network improved significantly. The recognition accuracy of the neural network as a whole or for each tone was comparable with that of the human listeners.CONCLUSIONS: The neural network can be used to evaluate the tone production of Mandarin Chinese speaking adults with human listener-like recognition accuracy. The tone 1-based normalization procedure improves the performance of the neural network to human listener-like accuracy. The success of our neural network in recognizing tones from multiple speakers supports its utility for evaluating tone production. Further testing of the neural network with hearing-impaired speakers might reveal its potential use for clinical evaluation of tone production.DOI: 10.1097/AUD.0b013e3181662c42PMCID: PMC2562432",pubmed,18453884,10.1097/AUD.0b013e3181662c42
perceptual features for robust speech recognition,"Automatic speech recognition (ASR) broadly encompasses the recognition of human speech by a machine or by some artificial intelligence. The recognition process should be robust, that is, it should accurately recognise the spoken word in the presence of speaker variabilities, word perplexities, and speech corrupted by noise which are introduced during transmission and in the communication channels itself. Research in the past several decades has produced speech processing techniques like the short-time Fourier transform (STFT), the linear prediction (LP) and autoregressive (AR) methods, and the mel-frequency cepstral coefficients (MFCC), which have contributed significantly to robust speech recognition. The ability of the human auditory system to recognize speech in adverse and noisy conditions has motivated speech researchers to include features of human perception in speech recognition systems. Particularly in the early 1980s, several computational models of the auditory periphery based on physiological measurements of the response on individual fibres of the auditory nerve were proposed [1],[2],[3]. These ""cochlear models"" only provided marginal improvements at higher computational costs when applied to speech recognition. As a result, a decline in the interest in auditory models was observed until computing resources were able to meet the intensive computational requirements of such models. In recent years, there has been a resurgence in perceptual speech processing after research provided evidence that it may lead to improved recognition performances [12],[13],[14]. This chapter describes several psychoacoustic properties ofthe peripheral auditorysys-tem applied to a speech recognition front-end. Dynamic behaviour of the auditory nerves are incorporated in speech parametrization utilizing temporal processing, so that time domain information as appropriate time constants are incorporated in speech parameterization. A simplified method of synaptic adaptation as determined by psychoacoustic observations in an auditory nerve is described. It utilizes a high pass infinite impulse response (IIR) temporal filter to enhance the signal onsets and the subsequent dynamic and the steady-state characteristics [15],[16]. Speech features are extracted in the temporal mode utilizing a zero-crossing algorithm [5]. The two-tone suppression as observed in the non-linear response of the basilar membrane is described in a zero-crossing auditory front-end using a temporal companding strategy [18],[17]. This may introduce asymmetric gain control without degrading the spectral contrast. The word recognitions are evaluated by continuous density hidden Markov models and are shown to provide improvements over conventional parameterizations in clean and noise conditions. Some ofthese perceptual algorithms may also benefit people with sensorineural hearing loss and may be implemented in hearing aids and cochlear implants for the hearing impaired through VLSI implementations [18]. ©2010 Nova Science Publishers, Inc. All rights reserved.",scopus,2-s2.0-84893005470,
the relative contribution of cochlear synaptopathy and reduced inhibition to agerelated hearing impairment for people with normal audiograms,"160. Trends Hear. 2023 Jan-Dec;27:23312165231213191. doi: 10.1177/23312165231213191.The Relative Contribution of Cochlear Synaptopathy and Reduced Inhibition to Age-Related Hearing Impairment for People With Normal Audiograms.Gómez-Álvarez M(1)(2), Johannesen PT(1)(2), Coelho-de-Sousa SL(1)(2), Klump GM(3), Lopez-Poveda EA(1)(2)(4).Author information:(1)Instituto de Neurociencias de Castilla y León, Universidad de Salamanca, Salamanca, Spain.(2)Instituto de Investigación Biomédica de Salamanca, Universidad de Salamanca, Salamanca, Spain.(3)Department of Neuroscience and Cluster of Excellence ""Hearing4all"", Carl von Ossietzky University of Oldenburg, Oldenburg, Germany.(4)Departamento de Cirugía, Facultad de Medicina, Universidad de Salamanca, Salamanca, Spain.Older people often show auditory temporal processing deficits and speech-in-noise intelligibility difficulties even when their audiogram is clinically normal. The causes of such problems remain unclear. Some studies have suggested that for people with normal audiograms, age-related hearing impairments may be due to a cognitive decline, while others have suggested that they may be caused by cochlear synaptopathy. Here, we explore an alternative hypothesis, namely that age-related hearing deficits are associated with decreased inhibition. For human adults (N = 30) selected to cover a reasonably wide age range (25-59 years), with normal audiograms and normal cognitive function, we measured speech reception thresholds in noise (SRTNs) for disyllabic words, gap detection thresholds (GDTs), and frequency modulation detection thresholds (FMDTs). We also measured the rate of growth (slope) of auditory brainstem response wave-I amplitude with increasing level as an indirect indicator of cochlear synaptopathy, and the interference inhibition score in the Stroop color and word test (SCWT) as a proxy for inhibition. As expected, performance in the auditory tasks worsened (SRTNs, GDTs, and FMDTs increased), and wave-I slope and SCWT inhibition scores decreased with ageing. Importantly, SRTNs, GDTs, and FMDTs were not related to wave-I slope but worsened with decreasing SCWT inhibition. Furthermore, after partialling out the effect of SCWT inhibition, age was no longer related to SRTNs or GDTs and became less strongly related to FMDTs. Altogether, results suggest that for people with normal audiograms, age-related deficits in auditory temporal processing and speech-in-noise intelligibility are mediated by decreased inhibition rather than cochlear synaptopathy.DOI: 10.1177/23312165231213191PMCID: PMC10644751",pubmed,37956654,10.1177/23312165231213191
why do hearing aids fail to restore normal auditory perception,"Hearing loss is a widespread condition that is linked to declines in quality of life and mental health. Hearing aids remain the treatment of choice, but, unfortunately, even state-of-the-art devices provide only limited benefit for the perception of speech in noisy environments. While traditionally viewed primarily as a loss of sensitivity, hearing loss is also known to cause complex distortions of sound-evoked neural activity that cannot be corrected by amplification alone. This Opinion article describes the effects of hearing loss on neural activity to illustrate the reasons why current hearing aids are insufficient and to motivate the use of new technologies to explore directions for improving the next generation of devices. © 2018 Elsevier Ltd",scopus,2-s2.0-85044530331,10.1016/j.tins.2018.01.008
cluster analysis of auditory and vestibular test results in definite menires disease,"219. Laryngoscope. 2011 Aug;121(8):1810-7. doi: 10.1002/lary.21844.Cluster analysis of auditory and vestibular test results in definite Menière's disease.Montes-Jovellar L(1), Guillen-Grima F, Perez-Fernandez N.Author information:(1)Department of Otorhinolaryngology, Clínica Universidad de Navarra, University Hospital and Medical School, University of Navarra, Navarra, Spain.OBJECTIVES/HYPOTHESIS: To determine whether patients with Menière's disease can be grouped into distinct subtypes based on a cluster analysis of distinct disease parameters.STUDY DESIGN: Prospective study at a tertiary center associated with a university hospital.METHODS: The study included 153 patients diagnosed with unilateral definite Menière's disease. The main variables employed were taken from auditory, vestibular, posturographic, and disability assessments.RESULTS: A four-cluster solution best fitted the data. Each cluster represented a distinct patient profile. Cluster 1 patients (13.1%) were the eldest, with the worst hearing bilaterally and good vestibular function but with a significant postural impact and a low level of disability. Cluster 2 patients (41.2%) were the least affected in all the parameters that were close to normal. Cluster 3 patients (34.6%) were the most affected, experiencing frequent and intense vertigo attacks, and they were visually dependent. Cluster 4 patients (11.1%) had strong asymmetric hearing between both ears and the most uncompensated vestibular deficit; they were moderately disabled.CONCLUSIONS: We have identified four distinct profiles of patients with definite Menière's disease that we consider as ""mildly active elderly,"" ""mildly active young,"" ""active compensated,"" and ""active uncompensated."" We have demonstrated that only in a restricted population of patients can the American Academy of Otolaryngology-Head and Neck Surgery staging system provide analysis of subtypes of the disease.Copyright © 2011 The American Laryngological, Rhinological, and Otological Society, Inc.DOI: 10.1002/lary.21844",pubmed,21792974,10.1002/lary.21844
optimizing agerelated hearing risk predictions an advanced machine learning integration with hhies,"Objectives: The elderly are disproportionately affected by age-related hearing loss (ARHL). Despite being a well-known tool for ARHL evaluation, the Hearing Handicap Inventory for the Elderly Screening version (HHIE-S) has only traditionally been used for direct screening using self-reported outcomes. This work uses a novel integration of machine learning approaches to improve the predicted accuracy of the HHIE-S tool for ARHL in older adults. Methods: We employed a dataset that was gathered between 2016 and 2018 and included 1,526 senior citizens from several Taipei City Hospital branches. 80% of the data were used for training (n = 1220) and 20% were used for testing (n = 356). XGBoost, Gradient Boosting, and LightGBM were among the machine learning models that were only used and assessed on the training set. In order to prevent data leakage and overfitting, the Light Gradient Boosting Machine (LGBM) model—which had the greatest AUC of 0.83 (95% CI 0.81–0.85)—was then only used on the holdout testing data. Results: On the testing set, the LGBM model showed a strong AUC of 0.82 (95% CI 0.79–0.86), far outperforming conventional techniques. Notably, several HHIE-S items and age were found to be significant characteristics. In contrast to traditional HHIE research, which concentrates on the psychological effects of hearing loss, this study combines cutting-edge machine learning techniques—specifically, the LGBM classifier—with the HHIE-S tool. The incorporation of SHAP values enhances the interpretability of the model's predictions and provides a more comprehensive comprehension of the significance of various aspects. Conclusions: Our methodology highlights the great potential that arises from combining machine learning with validated hearing evaluation instruments such as the HHIE-S. Healthcare practitioners can anticipate ARHL more accurately thanks to this integration, which makes it easier to intervene quickly and precisely. © 2023, The Author(s).",scopus,2-s2.0-85179959771,10.1186/s13040-023-00351-z
machine learning prediction of objective hearing loss with demographics clinical factors and subjective hearing status,"59. Otolaryngol Head Neck Surg. 2023 Sep;169(3):504-513. doi: 10.1002/ohn.288. Epub 2023 Feb 9.Machine Learning Prediction of Objective Hearing Loss With Demographics, Clinical Factors, and Subjective Hearing Status.Gathman TJ(1)(2), Choi JS(3), Vasdev RMS(1)(2), Schoephoerster JA(1), Adams ME(3).Author information:(1)School of Medicine, University of Minnesota, Minnesota, Minneapolis, USA.(2)Department of Biomedical Engineering, University of Minnesota, Minneapolis, Minnesota, USA.(3)Department of Otolaryngology, University of Minnesota, Minneapolis, Minnesota, USA.OBJECTIVE: Hearing loss (HL) is highly prevalent, yet underrecognized and underdiagnosed. Lack of standardized screening, awareness, cost, and access to hearing testing present barriers to HL identification. To facilitate prescreening and selection of patients who warrant audiometric evaluation, we developed a machine learning (ML) model to predict speech-frequency pure-tone average (PTA).STUDY DESIGN: Cross-sectional study.SETTING: National Health and Nutrition Examination Survey (NHANES).METHODS: The cohort included 8918 adults (≥20 years) who completed audiometric testing with NHANES (2012-2018). The primary outcome measure was the prediction of better hearing ear speech-frequency PTA. Relevant predictors included demographics, medical conditions, and subjective assessment of hearing. Supervised ML with a tree-based architecture was used. Regression performance was determined by the mean absolute error (MAE) with binary classification assessed with area under the receiver operating characteristic curve (AUC).RESULTS: Using the full set of predictors, the test set MAE between the ML-predicted and actual PTA was 5.29 dB HL (95% confidence interval [CI]: 4.97-5.61). The 5 most influential predictors of higher PTA were increased age, worse subjective hearing, male gender, increased body mass index, and history of smoking. The 5-factor abbreviated model performed comparably to the extended feature set with MAE 5.36 (95% CI: 5.03-5.69) and AUC for PTA > 25 dB HL of 0.92 (95% CI: 0.90-0.94).CONCLUSION: The ML model was able to predict PTA with patient demographics, clinical factors, and subjective hearing status. ML-based prediction may be used to identify individuals who could benefit most from audiometric evaluation.© 2023 The Authors. Otolaryngology-Head and Neck Surgery published by Wiley Periodicals LLC on behalf of American Academy of Otolaryngology-Head and Neck Surgery Foundation.DOI: 10.1002/ohn.288",pubmed,36758959,10.1002/ohn.288
marathi speech intelligibility enhancement using iams based neurofuzzy classifier approach for hearing aid users,"Globally, 1.6 billion individuals suffered from hearing disability in 2019. According to the World Health Organization, by 2050, the number of people with hearing impairments will rise to 2.5 billion. Speech perception in noisy surroundings is a challenge for hearing aid users. This study aimed to design a novel methodology to improve the speech recognition ability of hearing aid users from various backgrounds. To improve speech enhancement, we propose a discrete cosine transform (DCT)-based improved amplitude-magnitude spectrogram (I-AMS) algorithm with a fuzzy classifier. First, the I-AMS approach disintegrates speech signals containing noise into time-frequency units and eliminates the noise present in the signal. Next, the time frequency units (t-f units), modulation frequency (fm), and centre frequency (fc) are extracted from the denoised signal. A neuro-fuzzy classifier was used to classify the background speech environment into three different classes. The proposed I-AMS algorithm was tested, achieved improvements in terms of sensitivity (+1.02%) and accuracy (+11.80%). Speech denoising revealed a 1.27% improvement in speech recognition performance.",ieee,2169-3536,10.1109/ACCESS.2022.3223365
predicting the impact of otof gene missense variants on auditory neuropathy spectrum disorder,"243. Int J Mol Sci. 2023 Dec 7;24(24):17240. doi: 10.3390/ijms242417240.Predicting the Impact of OTOF Gene Missense Variants on Auditory Neuropathy Spectrum Disorder.Dmitriev DA(1), Shilov BV(1), Polunin MM(2), Zadorozhny AD(1), Lagunin AA(1)(3).Author information:(1)Department of Bioinformatics, Medico-Biological Faculty, Pirogov Russian National Research Medical University, Moscow 117997, Russia.(2)Department of Otorhinolaryngology, Faculty of Pediatrics, Pirogov Russian National Research Medical University, Moscow 117997, Russia.(3)Department of Bioinformatics, Institute of Biomedical Chemistry, Moscow 119121, Russia.Auditory neuropathy spectrum disorder (ANSD) associated with mutations of the OTOF gene is one of the common types of sensorineural hearing loss of a hereditary nature. Due to its high genetic heterogeneity, ANSD is considered one of the most difficult hearing disorders to diagnose. The dataset from 270 known annotated single amino acid substitutions (SAV) related to ANSD was created. It was used to estimate the accuracy of pathogenicity prediction using the known (from dbNSFP4.4) method and a new one. The new method (ConStruct) for the creation of the protein-centric classification model is based on the use of Random Forest for the analysis of missense variants in exons of the OTOF gene. A system of predictor variables was developed based on the modern understanding of the structure and function of the otoferlin protein and reflecting the location of changes in the tertiary structure of the protein due to mutations in the OTOF gene. The conservation values of nucleotide substitutions in genomes of 100 vertebrates and 30 primates were also used as variables. The average prediction of balanced accuracy and the AUC value calculated by the 5-fold cross-validation procedure were 0.866 and 0.903, respectively. The model shows good results for interpreting data from the targeted sequencing of the OTOF gene and can be implemented as an auxiliary tool for the diagnosis of ANSD in the early stages of ontogenesis. The created model, together with the results of the pathogenicity prediction of SAVs via other known accurate methods, were used for the evaluation of a manually created set of 1302 VUS related to ANSD. Based on the analysis of predicted results, 16 SAVs were selected as the new most probable pathogenic variants.DOI: 10.3390/ijms242417240PMCID: PMC10743402",pubmed,38139069,10.3390/ijms242417240
auditory perception and language functional imaging of speech sensitive auditory cortex perception auditive et langage imagerie fonctionnelle du cortex auditif sensible au langage,"Since the description of cortical deafness, it has been known that the superior temporal cortex is bilaterally involved in the initial stages of language auditory perception but the precise anatomical limits and the function of this area remain debated. Here we reviewed more than 40 recent papers of positron emission tomography and functional magnetic resonance imaging related to language auditory perception, and we performed a meta-analysis of the localization of the peaks of activation in the Talairach's space. We found 8 studies reporting word versus non-word listening contrasts with 54 activation peaks in the temporal lobes. These peaks clustered in a bilateral and well-limited area of the temporal superior cortex, which is here operationally defined as the speech sensitive auditory cortex. This area is more than 4cm long, located in the superior temporal gyrus and the superior temporal sulcus, both anterior and posterior to Heschl's gyrus. It do not include the primary auditory cortex nor the ascending part of the planum temporale. The speech sensitive auditory cortex is not activated by pure tones, environmental sounds, or attention directed toward elementary components of a sound such as intensity, pitch, or duration, and thus has some specificity for speech signals. The specificity is not perfect, since we found a number of non-speech auditory stimuli activating the speech sensitive auditory cortex. Yet the latter studies always involve auditory perception mechanisms which are also relevant to speech perception either at the level of primitive auditory scene analysis processes, or at the level of specific schema-based recognition processes. The dorsal part of the speech sensitive auditory cortex may be involved in primitive scene analysis processes, whereas distributed activation of this area may contribute to the emergence of a broad class of « voice » schemas and of more specific « speech sche- mas/phonetic modules » related to different languages. In addition, this area is activated by language-related lip movement, suggesting that a multimodal integration of the auditory and the visual information relevant in speech perception occurs at this level. Finally, there is a task- related top-down modulation of the pattern of activation of the speech sensitive auditory cortex which may reflect the fact that the different parts of this structure are connected to different down-stream cortical regions involved in the neural processing of different types of tasks.",scopus,2-s2.0-0034795894,
evaluation of the bimodal benefit in a large cohort of cochlear implant subjects using a contralateral hearing aid,"455. Otol Neurotol. 2014 Oct;35(9):e240-4. doi: 10.1097/MAO.0000000000000529.Evaluation of the bimodal benefit in a large cohort of cochlear implant subjects using a contralateral hearing aid.Illg A(1), Bojanowicz M, Lesinski-Schiedat A, Lenarz T, Büchner A.Author information:(1)*Department of Otolaryngology, Hannover Medical School, Hannover, Germany; and †Cluster of Excellence, Hearing4All, Medizinische Hochschule Hannover, Germany.OBJECTIVE: To investigate the benefit of contralateral residual hearing in a large group of cochlear implant recipients with different degrees of residual hearing.PATIENTS: One hundred and forty-one adult patients (age in years: mean 58.82, min 16.27, max 88.20) wearing a cochlear implant and a contralateral hearing aid, bimodal.INTERVENTION: Rehabilitative.MAIN OUTCOME MEASURES: All 141 patients underwent speech perception testing in quiet and noise with cochlear implant (CI) alone, and with CI and hearing aid (HA). Additionally, pure-tone air conduction threshold levels were measured in all subjects. The bimodal benefit was analyzed and correlations to the hearing threshold for different audiometric frequencies were calculated.RESULTS: Comparison between the scores for CI alone and CI + HA showed statistically significant advantages (p < 0.0001) in all four tests. The benefit for sentences in noise to each individual patient showed a negative correlation with the hearing threshold level of 125 Hz and 250 Hz, using a linear regression analysis applying the Spearman's rho correlation coefficient (r = -0.32, -0.232), and a significant difference at p = 0.006, p = 0.007. The correlations involving speech understanding in sentences in noise, and the hearing level of 500 Hz and above, are not significant for the benefit obtained with a contralateral hearing aid.CONCLUSION: The benefit of combined electric and acoustic hearing in bimodally fitted subjects depends mainly on residual hearing in the low-frequency range below 500 Hz. For bimodal fitting to yield significant benefits, hearing loss in the contralateral ear should not exceed 80 dB HL in the low frequencies.DOI: 10.1097/MAO.0000000000000529",pubmed,25058838,10.1097/MAO.0000000000000529
do hearing loss and cognitive function modulate benefit from different binaural noisereduction settings,"154. Ear Hear. 2014 May-Jun;35(3):e52-62. doi: 10.1097/AUD.0000000000000003.Do hearing loss and cognitive function modulate benefit from different binaural noise-reduction settings?Neher T(1), Grimm G, Hohmann V, Kollmeier B.Author information:(1)Cluster of Excellence ""Hearing4all,"" Medical Physics Section, Department of Medical Physics and Acoustics, Carl-von-Ossietzky University, Oldenburg, Germany.OBJECTIVES: Although previous research indicates that cognitive skills influence benefit from different types of hearing aid algorithms, comparatively little is known about the role of, and potential interaction with, hearing loss. This holds true especially for noise reduction (NR) processing. The purpose of the present study was thus to explore whether degree of hearing loss and cognitive function modulate benefit from different binaural NR settings based on measures of speech intelligibility, listening effort, and overall preference.DESIGN: Forty elderly listeners with symmetrical sensorineural hearing losses in the mild to severe range participated. They were stratified into four age-matched groups (with n = 10 per group) based on their pure-tone average hearing losses and their performance on a visual measure of working memory (WM) capacity. The algorithm under consideration was a binaural coherence-based NR scheme that suppressed reverberant signal components as well as diffuse background noise at mid to high frequencies. The strength of the applied processing was varied from inactive to strong, and testing was carried out across a range of fixed signal-to-noise ratios (SNRs). Potential benefit was assessed using a dual-task paradigm combining speech recognition with a visual reaction time (VRT) task indexing listening effort. Pairwise preference judgments were also collected. All measurements were made using headphone simulations of a frontal speech target in a busy cafeteria. Test-retest data were gathered for all outcome measures.RESULTS: Analysis of the test-retest data showed all data sets to be reliable. Analysis of the speech scores showed that, for all groups, speech recognition was unaffected by moderate NR processing, whereas strong NR processing reduced intelligibility by about 5%. Analysis of the VRT scores revealed a similar data pattern. That is, while moderate NR did not affect VRT performance, strong NR impaired the performance of all groups slightly. Analysis of the preference scores collapsed across SNR showed that all groups preferred some over no NR processing. Furthermore, the two groups with smaller WM capacity preferred strong over moderate NR processing; for the two groups with larger WM capacity, preference did not differ significantly between the moderate and strong settings.CONCLUSIONS: The present study demonstrates that, for the algorithm and the measures of speech recognition and listening effort used here, the effects of different NR settings interact with neither degree of hearing loss nor WM capacity. However, preferred NR strength was found to be associated with smaller WM capacity, suggesting that hearing aid users with poorer cognitive function may prefer greater noise attenuation even at the expense of poorer speech intelligibility. Further research is required to enable a more detailed (SNR-dependent) analysis of this effect and to test its wider applicability.DOI: 10.1097/AUD.0000000000000003",pubmed,24351610,10.1097/AUD.0000000000000003
crosssectional survey of hearing impairment and ear disease in uganda,"334. J Otolaryngol Head Neck Surg. 2008 Dec;37(6):753-8.Cross-sectional survey of hearing impairment and ear disease in Uganda.Westerberg BD(1), Lee PK, Lukwago L, Zaramba S, Bubikere S, Stewart I.Author information:(1)Division of Otolaryngology, University of British Columbia, Vancouver, British Columbia.OBJECTIVE: To determine the prevalence and causes of disabling hearing loss in adults and children in Uganda.STUDY DESIGN: Cross-sectional survey of ear disease and hearing impairment.SETTING: A random cluster sample design of the population from the Masindi district of Uganda following the World Health Organization (WHO) guidelines, using a modified version of the WHO Ear Disease Survey Protocol.MAIN OUTCOME MEASURE: The prevalence of disabling hearing impairment using the WHO definitions (excluding 0.5 kHz owing to high background noise levels).RESULTS: In the study, 6041 participants were enrolled and underwent audiometric evaluation and an ear examination. The prevalence of disabling hearing impairment was 11.7% in adults and 10.2% in children. A further 2.3% of children in whom thresholds could not be measured were deemed to have significant hearing loss based on screening questions and/or sound-field stimuli. Correctable causes such as dry perforations, cerumen impaction, and chronic suppurative otitis media resulted in disabling hearing loss in 17% of adult subjects and 41% of children. Preventable hearing loss, such as meningitis and noise-induced hearing loss, was present in a further significant percentage of subjects.CONCLUSIONS: Ear disease and hearing impairment were found to be important health problems in the Ugandan population. Preventable ear disease is a major cause of hearing loss in the population. It is hoped that the findings of this study will draw attention to the problem in Uganda and will lead to proper allocation of resources for the prevention and treatment of hearing loss and ear disease.",pubmed,19128699,
a comparison of selfreported hearing and pure tone threshold average in the iowa farm family health and hazard survey,"BACKGROUND: Self-reported hearing measures are useful for surveying hearing loss in a population because they are short, and easy to administer by either questionnaire or telephone. This study aims to assess the performance of several self-reported hearing measures to identify hearing loss in a group of Iowa farmers. METHODS: The study subjects were 98 male farmers who participated in the Iowa Farm Family Health and Hazard Survey. We tested three self-reported hearing measures; the hearing screening questions, the Rating Scale for Each Ear (RSEE), and the Health, Education and Welfare-Expanded Hearing Ability Scale (HEW-EHAS), which were originally developed and implemented in the National Health Interview Survey. The sensitivity and specificity of the self-reported hearing measures were assessed by comparing them with pure tone threshold averages. These sensitivity and specificity measures were compared between younger and older age groups. RESULTS: The sensitivities of the screening questions, RSEE, and HEW-EHAS were 73.0%, 66.7%, and 53.3%, respectively. The specificities of the self-reported hearing measures were similar, which ranged from 81.4% to 84.8%. The sensitivities of the self-reported hearing measures were higher in the younger age group while the specificities were higher in the older age group. CONCLUSIONS: The results of this study support the use of simple screening questions in identifying hearing loss among farmers.",cinahl,1059924X,10.1300/j096v10n03_04
multitransfer learning techniques for detecting auditory brainstem response,"The assessment of the well-being of the peripheral auditory nerve system in individuals experiencing hearing impairment is conducted through auditory brainstem response (ABR) testing. Audiologists assess and document the results of the ABR test. They interpret the findings and assign labels to them using reference-based markers like peak latency, waveform morphology, amplitude, and other relevant factors. Inaccurate assessment of ABR tests may lead to incorrect judgments regarding the integrity of the auditory nerve system; therefore, proper Hearing Loss (HL) diagnosis and analysis are essential. In order to identify and assess ABR automation while decreasing the possibility of human error, machine learning methods, notably deep learning, may be an appropriate option. To address these issues, this study proposed deep-learning models using the transfer-learning (TL) approach to extract features from ABR testing and diagnose HL using support vector machines (SVM). Pre-trained convolutional neural network (CNN) architectures like AlexNet, DenseNet, GoogleNet, InceptionResNetV2, InceptionV3, MobileNetV2, NASNetMobile, ResNet18, ResNet50, ResNet101, ShuffleNet, and SqueezeNet are used to extract features from the collected ABR reported images dataset in the proposed model. It has been decided to use six measures accuracy, precision, recall, geometric mean (GM), standard deviation (SD), and area under the ROC curve to measure the effectiveness of the proposed model. According to experimental findings, the ShuffleNet and ResNet50 models' TL is effective for ABR to diagnosis HL using an SVM classifier, with a high accuracy rate of 95% when using the 5-fold cross-validation method. © 2023 Elsevier Ltd",scopus,2-s2.0-85169794494,10.1016/j.apacoust.2023.109604
longitudinal predictors of aided speech audibility in infants and children,"130. Ear Hear. 2015 Nov-Dec;36 Suppl 1(0 1):24S-37S. doi: 10.1097/AUD.0000000000000211.Longitudinal Predictors of Aided Speech Audibility in Infants and Children.McCreery RW(1), Walker EA, Spratford M, Bentler R, Holte L, Roush P, Oleson J, Van Buren J, Moeller MP.Author information:(1)1Center for Audiology, Boys Town National Research Hospital, Omaha, Nebraska, USA; 2Department of Communication Sciences and Disorders, University of Iowa, Iowa City, Iowa, USA; 3Center for Childhood Deafness, Boys Town National Research Hospital, Omaha, Nebraska, USA; 4Department of Otolaryngology, University of North Carolina at Chapel Hill, Chapel Hill, North Carolina, USA; and 5Department of Biostatistics, University of Iowa, Iowa City, Iowa, USA.OBJECTIVES: Amplification is a core component of early intervention for children who are hard of hearing, but hearing aids (HAs) have unique effects that may be independent from other components of the early intervention process, such as caregiver training or speech and language intervention. The specific effects of amplification are rarely described in studies of developmental outcomes. The primary purpose of this article is to quantify aided speech audibility during the early childhood years and examine the factors that influence audibility with amplification for children in the Outcomes of Children with Hearing Loss study.DESIGN: Participants were 288 children with permanent hearing loss who were followed as part of the Outcomes of Children with Hearing Loss study. All of the children in this analysis had bilateral hearing loss and wore air-conduction behind-the-ear HAs. At every study visit, hearing thresholds were measured using developmentally appropriate behavioral methods. Data were obtained for a total of 1043 audiometric evaluations across all subjects for the first four study visits. In addition, the aided audibility of speech through the HA was assessed using probe microphone measures. Hearing thresholds and aided audibility were analyzed. Repeated-measures analyses of variance were conducted to determine whether patterns of thresholds and aided audibility were significantly different between ears (left versus right) or across the first four study visits. Furthermore, a cluster analysis was performed based on the aided audibility at entry into the study, aided audibility at the child's final visit, and change in aided audibility between these two intervals to determine whether there were different patterns of longitudinal aided audibility within the sample.RESULTS: Eighty-four percent of children in the study had stable audiometric thresholds during the study, defined as threshold changes <10 dB for any single study visit. There were no significant differences in hearing thresholds, aided audibility, or deviation of the HA fitting from prescriptive targets between ears or across test intervals for the first four visits. Approximately 35% of the children in the study had aided audibility that was below the average for the normative range for the Speech Intelligibility Index based on degree of hearing loss. The cluster analysis of longitudinal aided audibility revealed three distinct groups of children: a group with consistently high aided audibility throughout the study, a group with decreasing audibility during the study, and a group with consistently low aided audibility.CONCLUSIONS: The current results indicated that approximately 65% of children in the study had adequate aided audibility of speech and stable hearing during the study period. Limited audibility was associated with greater degrees of hearing loss and larger deviations from prescriptive targets. Studies of developmental outcomes will help to determine how aided audibility is necessary to affect developmental outcomes in children who are hard of hearing.DOI: 10.1097/AUD.0000000000000211PMCID: PMC4704126",pubmed,26731156,10.1097/AUD.0000000000000211
random forest classification to predict response to highdefinition transcranial direct current stimulation for tinnitus relief a preliminary feasibility study,"Objectives: Transcranial direct current stimulation (tDCS) of the right dorsolateral prefrontal cortex has been hypothesized to reduce tinnitus severity by modifying cortical activity in brain regions associated with the perception of tinnitus. However, individual response to tDCS has proven to be variable. We investigated the feasibility of using random forest classification to predict the response to high-definition (HD) tDCS for tinnitus relief.Design: A retrospective analysis was performed on a dataset consisting of 99 patients with subjective tinnitus receiving six consecutive sessions of HD-tDCS at the Antwerp University Hospital. A baseline assessment consisted of pure-tone audiometry and a set of questionnaires including the Tinnitus Functional Index (TFI), Hospital Anxiety and Depression Scale, and Edinburgh Handedness Inventory. Random forest classification was applied to predict, based on baseline questionnaire scores and hearing levels, whether each individual responded positively to the treatment (defined as a decrease of at least 13 points on the TFI). Further testing of the model was performed on an independent cohort of 32 patients obtained from the tinnitus center at the University of Regensburg.Results: Twenty-four participants responded positively to the HD-tDCS treatment. The random forest classifier predicted treatment response with an accuracy of 85.71% (100% sensitivity, 81.48% specificity), significantly outperforming a more traditional logistic regression approach. Performance of the classifier on an independent cohort was slightly but not significantly above chance level (71.88% accuracy, 66.67% sensitivity, 73.08% specificity). Feature importance analyses revealed that baseline tinnitus severity, co-occurrence of depressive symptoms and handedness were the most important predictors of treatment response. Baseline TFI scores were significantly higher in responders than in nonresponders.Conclusions: The proposed random forest classifier predicted treatment response with a high accuracy, significantly outperforming a more traditional statistical approach. Machine learning methods to predict treatment response might ultimately be used in a clinical setting to guide targeted treatment recommendations for individual tinnitus patients.",cinahl,1960202,10.1097/AUD.0000000000001246
segmentation of auditory brainstem response signals,"Auditory brainstem responses are used to detect hearing defects in audiology and otoneurology. The use of computer programs for the analysis of such recordings is increasing. To identify their detailed properties a pattern recognition algorithm implemented in an analysis program must be highly reliable, for the recognition process, some preprocessing phases after recording are necessary, such as filtering and often also segmentation. In the following, we will explore segmentation, which can be used in preprocessing of biomedical signals after filtering. We studied linear segmentation, where slopes of short signal segments are computed and divided into different classes according to their values. A segment length of 8 samples for a sampling frequency of 50 kHz employed was best according to our tests and error criteria. Using clustering, we found that less than 10 segment classes is suitable for pattern recognition.; Auditory brainstem responses are used to detect hearing defects in audiology and otoneurology. The use of computer programs for the analysis of such recordings is increasing. To identify their detailed properties a pattern recognition algorithm implemented in an analysis program must be highly reliable. For the recognition process, some preprocessing phases after recording are necessary, such as filtering and often also segmentation. In the following, we will explore segmentation, which can be used in preprocessing of biomedical signals after filtering. We studied linear segmentation, where slopes of short signal segments are computed and divided into different classes according to their values. A segment length of 8 samples for a sampling frequency of 50 kHz employed was best according to our tests and error criteria. Using clustering, we found that less than 10 segment classes is suitable for pattern recognition.",scopus,2-s2.0-0030482661,10.1016/S0020-7101(96)01212-3
the reception of environmental sounds through wearable tactual aids,"459. Ear Hear. 2003 Dec;24(6):528-38. doi: 10.1097/01.AUD.0000100207.97243.88.The reception of environmental sounds through wearable tactual Aids.Reed CM(1), Delhorne LA.Author information:(1)Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, USA. reed@zaphod.mit.eduOBJECTIVE: The objective of this study was to investigate the ability to identify environmental sounds through a wearable tactual aid.DESIGN: A test of the ability to identify environmental sounds was developed, employing closed sets of ten sounds in each of four different settings (General Home, Kitchen, Office, and Outdoors). The participants in the study included a group of three laboratory-trained subjects with normal hearing and a group of three subjects with profound deafness who were experienced users of a tactual device (the Tactaid 7). Identification testing was conducted in each of the four environmental-sound settings using a one-interval, ten-alternative, forced-choice procedure. The laboratory-trained subjects received training with trial-by-trial correct-answer feedback, followed by testing in the absence of feedback using the Tactaid 7 device. The experienced tactual-aid users were tested initially without feedback to establish baseline levels of performance derived from their prior field experience with the Tactaid 7. These subjects then received additional trials in the presence of correct-answer feedback to determine the effects of training on their performance. The data were summarized in terms of overall percent-correct identification scores and information transfer (IT) in bits. Confusion patterns were described using a hierarchical clustering analysis.RESULTS: Post-training results with the laboratory-trained subjects on the Tactaid 7 indicated that performance was similar for the four test environments, with percent-correct scores averaging 65% (and IT of 2.0 bits). For the experienced tactual-aid users, performance was similar across the four environments, averaging 36% correct (and IT of 1.4 bits) for initial testing without feedback. Scores were increased to 60% correct (and IT of 1.9 bits) in the presence of correct-answer feedback. Similar trends were observed in the hierarchical-clustering analysis across both groups of subjects. Within each stimulus set, certain items tended to cluster together, whereas other items tended to appear in single-item clusters. The highly identified stimuli tended to be characterized by unique temporal patterns and confused stimuli seemed to be most similar in terms of their spectral characteristics.CONCLUSIONS: Through the multi-channel spectral display of the Tactaid 7 device, subjects were able to identify roughly 2 bits of information in each of four 10-item sets of sounds representative of different environmental settings. Temporal cues appeared to play a larger role in identification of sounds than spectral or intensive cues.DOI: 10.1097/01.AUD.0000100207.97243.88",pubmed,14663352,10.1097/01.AUD.0000100207.97243.88
hearing recovery prediction and prognostic factors of idiopathic sudden sensorineural hearing loss a retrospective analysis with a deep neural network model,"Objective: Idiopathic Sudden Sensorineural Hearing Loss (ISSHL) is an otologic emergency, and an early prediction of prognosis may facilitate proper treatment. Therefore, we investigated the prognostic factors for predicting the recovery in patients with ISSHL treated with combined treatment method using machine learning models. Methods: We retrospectively reviewed the medical records of 298 patients with ISSHL at a tertiary medical institution between January 2015 and September 2020. Fifty-two variables were analyzed to predict hearing recovery. Recovery was defined using Siegel's criteria, and the patients were categorized into recovery and non-recovery groups. Recovery was predicted by various machine learning models. In addition, the prognostic factors were analyzed using the difference in the loss function. Results: There were significant differences in variables including age, hypertension, previous hearing loss, ear fullness, duration of hospital admission, initial hearing level of the affected and unaffected ears, and post-treatment hearing level between recovery and non-recovery groups. The deep neural network model showed the highest predictive performance (accuracy, 88.81%; area under the receiver operating characteristic curve, 0.9448). In addition, initial hearing level of affected and non-affected ear, post-treatment (2-weeks) hearing level of affected ear were significant factors for predicting the prognosis. Conclusion: The deep neural network model showed the highest predictive performance for recovery in patients with ISSHL. Some factors with prognostic value were identified. Further studies using a larger patient population are warranted. Level of evidence: Level 4. © 2023 Associação Brasileira de Otorrinolaringologia e Cirurgia Cérvico-Facial",scopus,2-s2.0-85161695588,10.1016/j.bjorl.2023.04.001
objective prediction of hearing aid benefit across listener groups using machine learning speech recognition performance with binaural noisereduction algorithms,"343. Trends Hear. 2018 Jan-Dec;22:2331216518768954. doi: 10.1177/2331216518768954.Objective Prediction of Hearing Aid Benefit Across Listener Groups Using Machine Learning: Speech Recognition Performance With Binaural Noise-Reduction Algorithms.Schädler MR(1), Warzybok A(1), Kollmeier B(1).Author information:(1)1 Medizinische Physik and Cluster of Excellence ""Hearing4all,"" Carl von Ossietzky Universität Oldenburg, Germany.The simulation framework for auditory discrimination experiments (FADE) was adopted and validated to predict the individual speech-in-noise recognition performance of listeners with normal and impaired hearing with and without a given hearing-aid algorithm. FADE uses a simple automatic speech recognizer (ASR) to estimate the lowest achievable speech reception thresholds (SRTs) from simulated speech recognition experiments in an objective way, independent from any empirical reference data. Empirical data from the literature were used to evaluate the model in terms of predicted SRTs and benefits in SRT with the German matrix sentence recognition test when using eight single- and multichannel binaural noise-reduction algorithms. To allow individual predictions of SRTs in binaural conditions, the model was extended with a simple better ear approach and individualized by taking audiograms into account. In a realistic binaural cafeteria condition, FADE explained about 90% of the variance of the empirical SRTs for a group of normal-hearing listeners and predicted the corresponding benefits with a root-mean-square prediction error of 0.6 dB. This highlights the potential of the approach for the objective assessment of benefits in SRT without prior knowledge about the empirical data. The predictions for the group of listeners with impaired hearing explained 75% of the empirical variance, while the individual predictions explained less than 25%. Possibly, additional individual factors should be considered for more accurate predictions with impaired hearing. A competing talker condition clearly showed one limitation of current ASR technology, as the empirical performance with SRTs lower than -20 dB could not be predicted.DOI: 10.1177/2331216518768954PMCID: PMC5949929",pubmed,29692200,10.1177/2331216518768954
application of data mining to a large hearingaid manufacturers dataset to identify possible benefits for clinicians manufacturers and users,"561. Trends Hear. 2018 Jan-Dec;22:2331216518773632. doi: 10.1177/2331216518773632.Application of Data Mining to a Large Hearing-Aid Manufacturer's Dataset to Identify Possible Benefits for Clinicians, Manufacturers, and Users.Mellor J(1), Stone MA(2)(3), Keane J(1)(4).Author information:(1)1 School of Computer Science, University of Manchester, Manchester, UK.(2)2 Manchester Centre for Audiology and Deafness, University of Manchester, Manchester, UK.(3)3 Manchester Academic Health Sciences Centre, University of Manchester, Manchester, UK.(4)4 Manchester Institute of Biotechnology, University of Manchester, Manchester, UK.Modern hearing instruments contain logging technology to record data, such as the acoustic environments in which the device is being used and how the signal processing is consequently operating. Combined with patient data, such as the audiogram, this information gives a more comprehensive picture of the user and their relationship with the aid. Here, a relatively large, anonymized dataset (>300,000 devices, >150,000 wearers) from a hearing-aid manufacturer was data mined for connections between subsets of the logged varieties of data. Apart from replicating links that have previously been reported in labor-intensive studies, a link between device style (in-the-ear/behind-the-ear) and the sound levels of encountered environments was demonstrated, suggesting that some device types are more successful from a lifestyle perspective. Furthermore, the data also suggested links between the audiogram and the sound environments in which the aid was operated. Modeling the expected link between the environment and the microphone directionality settings revealed patterns of either abnormal fitting or where the aid was not operating correctly-factors that may indicate a failed fitting. Given the necessarily redacted nature of the dataset, the reported findings represent a proof-of-concept of the use of relatively large-scale data mining to guide and assess hearing-aid fitting procedures for possible benefits to the clinician, manufacturer, and patient.DOI: 10.1177/2331216518773632PMCID: PMC6022813",pubmed,29848201,10.1177/2331216518773632
prevalence of cognitive impairment and its association with hearing loss among adults over 50 years of age results from a populationbased survey in santiago chile,"Purpose: The purpose of this study was to estimate the prevalence of cognitive impairment and explore its association with hearing loss and other socio-demographic and clinical risk factors, using an objective measurement of hearing levels, in adults over 50 years of age. Method: A population-based survey was completed in Santiago, Chile between December 2019 and March 2020. Participants were screened for cognitive impairment using the Short Chilean Mini-Mental State Examination and hearing levels were assessed with tonal audiometry (hearTest). Data on demographic, socioeconomic, and clinical characteristics were collected. Results: A total of 538 persons completed the assessment. The prevalence of cognitive impairment in the 50+ population was 9.3% (95% confidence interval [CI] [5.8, 14.7]). Cognitive impairment was significantly higher in individuals with any level of hearing loss (odds ratio [OR] = 2.19, 95% CI [1.00, 4.80], adjusted for age, sex, education, socioeconomic position [SEP], and head trauma). Subjects with hearing loss and who reported any use of hearing aids (16% of the sample) had a lower risk of cognitive impairment (OR of nonusers 3.64, 95% CI [1.00, 13.28], adjusted for age, sex, education, SEP, and head trauma). Conclusion: Strategies for addressing cognitive impairment should further explore the integration of early diagnosis of hearing loss and the regular use of hearing aids.",cinahl,10590889,10.1044/2022_AJA-22-00042
comparison of audiologic results and patient satisfaction for two osseointegrated bone conduction devices results of a prospective study,"422. Otol Neurotol. 2015 Jun;36(5):842-8. doi: 10.1097/MAO.0000000000000727.Comparison of audiologic results and patient satisfaction for two osseointegrated bone conduction devices: results of a prospective study.Busch S(1), Giere T, Lenarz T, Maier H.Author information:(1)*Department of Otolaryngology and †Cluster of Excellence Hearing4all, Medical University Hannover, Hannover, Germany.OBJECTIVE: Osseointegrated bone conduction (BC) devices are an important rehabilitation option for patients with mixed or conductive hearing loss or single-sided deafness. The development of new devices is ongoing and requires evaluation of the performance of new hearing aids. Here, we compared the audiologic outcome and subjective benefit of two different designs of osseointegrated implant systems from different manufacturers.STUDY DESIGN: Prospective, experimental, monocentric, crossover study performed at the Medical University Hannover, Germany.PATIENTS AND INTERVENTIONS: Eleven patients, already implanted with an adequate abutment, tested each device in daily life situations sequentially for a period of 3 weeks.MAIN OUTCOME MEASURES: Bone conduction, word recognition in quiet (Freiburg monosyllable test, L50%), and speech reception thresholds in noise (Oldenburg Sentence Test) were measured unaided and aided with the devices after each test period. The subjective benefit was assessed by the Abbreviated Profile of Hearing Aid Benefit; the Speech, Spatial and Qualities of Hearing Scale-Comparative questionnaire; and a self-developed handling questionnaire.RESULTS: Audiologic results indicate a slightly better performance of the BCB. However, subjective benefit and patient satisfaction and preference evaluated with questionnaires were higher with the BCP than with the BCB.CONCLUSION: Amplification-wise, both devices are suitable treatments for hearing-impaired patients. Nevertheless, audiometric tests do not reflect subjective benefit and patients' satisfaction, and both options should be tested to provide each patient with the best possible hearing solution. The study further elucidates the importance and necessity of questionnaires in the process of evaluating the hearing benefit of hearing devices.DOI: 10.1097/MAO.0000000000000727",pubmed,25730448,10.1097/MAO.0000000000000727
vestibular mapping assessment in idiopathic sudden sensorineural hearing loss,"Objective: The aim of this study was to investigate patterns of semicircular canal (SCC) and otolith organ dysfunction by vestibular mapping, and to determine the clinical implications of treatment outcomes in idiopathic sudden sensorineural hearing loss (ISSNHL). Methods: We retrospectively reviewed 135 consecutive patients diagnosed with ISSNHL from January 2016 to December 2020. Patients underwent video-head impulse tests (vHIT) for each SCC, cervical vestibular- evoked myogenic potential test for the saccules, ocular vestibular- evoked myogenic potential test for the utricles, and hearing tests. Hearing outcomes were evaluated according to the American Academy of Otolaryngology-Head and Neck Surgery criteria and factors associated with prognosis were assessed. We also conducted vestibular mapping assessments and hierarchical cluster analysis. Results: Overall, utricular impairment (76, 56.3%) was the most frequent diagnosis in the 135 ISSNHL patients, followed by saccular impairment (59, 43.7%) and posterior SCC impairment (30, 22.2%). The mean number of affected end organs was 1.37 ± 1.24, with higher numbers in the complete recovery group than in the partial/no recovery groups. In a multivariate analysis, higher initial hearing level and abnormal vHIT results in the posterior SCC were associated with poor prognosis in ISSNHL. In hierarchical cluster analysis, horizontal SCC and anterior SCC showed the highest similarity but were in different clusters than posterior SCC, and the saccule and utricle were in separate clusters from the three SCCs. Conclusions: The vestibular end organ showed various patterns of dysfunction in patients with ISSNHL. Of the five vestibular end organs, only abnormal posterior SCC was associated with poor prognosis for hearing recovery. © 2022 Lippincott Williams and Wilkins. All rights reserved.",scopus,2-s2.0-85122705465,10.1097/AUD.0000000000001129
auditory evoked potentials aeps response classification a fast fourier transform fft and support vector machine svm approach,"Hearing loss has become the world's most widespread sensory impairment. The applicability of a traditional hearing test is limited as it allows the subject to provide a direct response. The main aim of this study is to build an intelligent hearing level evaluation method using possible auditory evoked signals (AEPs). AEP responses are subjected to fixed acoustic stimulation strength for usual auditory and abnormal ear subjects to detect the hearing disorder. In this paper, the AEP responses have been captured from the sixteen subjects when the subject hears the auditory stimulus in the left or right ear. Then, the features have extracted with the help of Fast Fourier Transform (FFT), Power Spectral Density (PSD), Spectral Centroids, Standard Deviation algorithms. To classify the extracted features, the Support Vector Machine (SVM) approach using Radial Basis Kernel Function (RBF) has been used. Finally, the performance of the classifier in terms of accuracy, confusion matrix, true positive and false negative rate, precision, recall, and Cohen-Kappa-Score have been evaluated. The maximum classification accuracy of the developed SVM model with FFT feature was observed 95.29% (10 s time windows) which clearly indicates that the method provides a very encouraging performance for detecting the AEPs responses. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",scopus,2-s2.0-85116505518,10.1007/978-981-16-2406-3_41
tone enhancing model for disyllable words in chinese mandarin speech,"Tone recognition is the core function in Chinese speech perception. The tone perception ability of people with sensorineural hearing loss (SNHL) is often weaker than normal people. Automatically tone enhancement would be useful in helping them understand Chinese speech better. In this paper, we focus on the tone enhancing model for Chinese disyllable words. We first analyze the acoustic features related to tone perception. By agglomerative hierarchical clustering method, the first and second syllables of disyllable words are clustered into 6 clusters respectively. Discriminative features of these clusters are experimentally determined from a set of possible features related to tone perception, such as the pitch value, pitch range and position of minimum pitch, etc. We further propose a practicable tone enhancing model with these discriminative features: 1) an input pitch contour is classified by calculating the distance between it and the centroid of each cluster, and 2) selecting the smallest distance, then the unclassified pitch contour belongs to this cluster, 3) the pitch contour is modified for tone enhancement with model parameters corresponding to this cluster using TD-PSOLA. Both statistical and subjective experiments show that higher hit rate of tone recognition can be obtained after tone enhancement with the proposed model. Especially, the proposed enhancing model can also avoid traditional tone recognition, which is more convictive and less laborious. © 2014 NSP Natural Sciences Publishing Cor.",scopus,2-s2.0-84896867113,10.12785/amis/081L25
profiling hearing aid users through big data explainable artificial intelligence techniques,"727. Front Neurol. 2022 Aug 26;13:933940. doi: 10.3389/fneur.2022.933940. eCollection 2022.Profiling hearing aid users through big data explainable artificial intelligence techniques.Iliadou E(1), Su Q(2), Kikidis D(1), Bibas T(1), Kloukinas C(2).Author information:(1)1st Department of Otorhinolaryngology-Head and Neck Surgery, National and Kapodistrian University of Athens Medical School, Athens, Greece.(2)Department of Computer Science, University of London, London, United Kingdom.Debilitating hearing loss (HL) affects ~6% of the human population. Only 20% of the people in need of a hearing assistive device will eventually seek and acquire one. The number of people that are satisfied with their Hearing Aids (HAids) and continue using them in the long term is even lower. Understanding the personal, behavioral, environmental, or other factors that correlate with the optimal HAid fitting and with users' experience of HAids is a significant step in improving patient satisfaction and quality of life, while reducing societal and financial burden. In SMART BEAR we are addressing this need by making use of the capacity of modern HAids to provide dynamic logging of their operation and by combining this information with a big amount of information about the medical, environmental, and social context of each HAid user. We are studying hearing rehabilitation through a 12-month continuous monitoring of HL patients, collecting data, such as participants' demographics, audiometric and medical data, their cognitive and mental status, their habits, and preferences, through a set of medical devices and wearables, as well as through face-to-face and remote clinical assessments and fitting/fine-tuning sessions. Descriptive, AI-based analysis and assessment of the relationships between heterogeneous data and HL-related parameters will help clinical researchers to better understand the overall health profiles of HL patients, and to identify patterns or relations that may be proven essential for future clinical trials. In addition, the future state and behavioral (e.g., HAids Satisfiability and HAids usage) of the patients will be predicted with time-dependent machine learning models to assist the clinical researchers to decide on the nature of the interventions. Explainable Artificial Intelligence (XAI) techniques will be leveraged to better understand the factors that play a significant role in the success of a hearing rehabilitation program, constructing patient profiles. This paper is a conceptual one aiming to describe the upcoming data collection process and proposed framework for providing a comprehensive profile for patients with HL in the context of EU-funded SMART BEAR project. Such patient profiles can be invaluable in HL treatment as they can help to identify the characteristics making patients more prone to drop out and stop using their HAids, using their HAids sufficiently long during the day, and being more satisfied by their HAids experience. They can also help decrease the number of needed remote sessions with their Audiologist for counseling, and/or HAids fine tuning, or the number of manual changes of HAids program (as indication of poor sound quality and bad adaptation of HAids configuration to patients' real needs and daily challenges), leading to reduced healthcare cost.Copyright © 2022 Iliadou, Su, Kikidis, Bibas and Kloukinas.DOI: 10.3389/fneur.2022.933940PMCID: PMC9459083",pubmed,36090867,10.3389/fneur.2022.933940
multivariate models for decoding hearing impairment using eeg gammaband power spectral density,"Speech-in-noise (SIN) comprehension decreases with age, and these declines have been related to social isolation, depression, and dementia in the elderly. In this work, we build models to distinguish the normal hearing (NH) or mild hearing impairment (HI) using the different genres of machine learning. We compute band wise power spectral density (PSD) of source- derived EEGs as features in building models using support vector machine (SVM), k-nearest neighbors (KNN), and AdaBoost classifiers and compare their performance while listeners perceived clear or noise-degraded sounds. Combining all frequency bands features obtained from the whole-brain, the SVM registered the best performance. The group classification accuracy was found to be 94.90% [area under the curve (AUC) 94.75%; F1-score 95.00%] perceived the clear speech, and for noise- degraded speech perception, accuracy was found to be 92.52% (AUC 91.12%, and F1-score 93.00%). Remarkably, individual frequency band analysis on whole-brain data showed that γ frequency band segregated groups with a best accuracy of 96.78%, AUC 96.79% for clear speech data and noise-degraded speech data yielded slightly less accuracy of 93.62% with AUC 93.17% by using SVM. A separate analysis using the left hemisphere (LH) and right hemisphere (RH) data showed that the LH activity is a better predictor of groups compared to RH. These results are consistent with the dominance of LH in auditory-linguistic processing. Our results demonstrate that spectral features of the γ-band frequency could be used to differentiate NH and HI older adults in terms of their ability to process speech sounds. These findings would be useful to model attentional and listening assistive devices to amplify a more specific pitch than others. © 2020 IEEE.",scopus,2-s2.0-85093868427,10.1109/IJCNN48605.2020.9206731
improvement of speech perception for hearingimpaired listenersdp   2019,"Hearing impairment is becoming a prevalent health problem affecting 5% of world adult populations. Hearing aids and cochlear implant already play an essential role in helping patients over decades, but there are still several open problems that prevent them from providing the maximum benefits. Financial and discomfort reasons lead to only one of four patients choose to use hearing aids; Cochlear implant users always have trouble in understanding speech in a noisy environment. In this dissertation, we addressed the hearing aids limitations by proposing a new hearing aid signal processing system named Open-source Self-fitting Hearing Aids System (OS SF hearing aids). The proposed hearing aids system adopted the state-of-art digital signal processing technologies, combined with accurate hearing assessment and machine learning based self-fitting algorithm to further improve the speech perception and comfort for hearing aids users. Informal testing with hearing-impaired listeners showed that the testing results from the proposed system had less than 10 dB (by average) difference when compared with those results obtained from clinical audiometer. In addition, Sixteen-channel filter banks with adaptive differential microphone array provides up to six-dB SNR improvement in the noisy environment. Machine-learning based self-fitting algorithm provides more suitable hearing aids settings. To maximize cochlear implant users' speech understanding in noise, the sequential (S) and parallel (P) coding strategies were proposed by integrating high-rate desynchronized pulse trains (DPT) in the continuous interleaved sampling (CIS) strategy. Ten participants with severe hearing loss participated in the two rounds cochlear implants testing. The testing results showed CIS-DPT-S strategy significantly improved (11%) the speech perception in background noise, while the CIS-DPT-P strategy had a significant improvement in both quiet (7%) and noisy (9%) environment. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc18&AN=2019-23495-090
factors influencing classification of frequency following responses to speech and music stimuli references,"Successful mapping of meaningful labels to sound input requires accurate representation of that sound's acoustic variances in time and spectrum. For some individuals, such as children or those with hearing loss, having an objective measure of the integrity of this representation could be useful. Classification is a promising machine learning approach which can be used to objectively predict a stimulus label from the brain response. This approach has been previously used with auditory evoked potentials (AEP) such as the frequency following response (FFR), but a number of key issues remain unresolved before classification can be translated into clinical practice. Specifically, past efforts at FFR classification have used data from a given subject for both training and testing the classifier. It is also unclear which components of the FFR elicit optimal classification accuracy. To address these issues, we recorded FFRs from 13 adults with normal hearing in response to speech and music stimuli. We compared labeling accuracy of two cross-validation classification approaches using FFR data: (1) a more traditional method combining subject data in both the training and testing set, and (2) a ""leave-one-out"" approach, in which subject data is classified based on a model built exclusively from the data of other individuals. We also examined classification accuracy on decomposed and time-segmented FFRs. Our results indicate that the accuracy of leave-one-subject-out cross validation approaches that obtained in the more conventional cross-validation classifications while allowing a subject's results to be analysed with respect to normative data pooled from a separate population. In addition, we demonstrate that classification accuracy is highest when the entire FFR is used to train the classifier. Taken together, these efforts contribute key steps toward translation of classification-based machine learning approaches into clinical practice. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc19&DO=10.1016%2fj.heares.2020.108101
disrupted functional brain connectome in unilateral sudden sensorineural hearing loss references,"Sudden sensorineural hearing loss (SSNHL) is generally defined as sensorineural hearing loss of 30 dB or greater over at least three contiguous audiometric frequencies and within a three-day period. This hearing loss is usually unilateral and can be associated with tinnitus and vertigo. The pathogenesis of unilateral sudden sensorineural hearing loss is still unknown, and the alterations in the functional connectivity are suspected to involve one possible pathogenesis. Despite scarce findings with respect to alterations in brain functional networks in unilateral sudden sensorineural hearing loss, the alterations of the whole brain functional connectome and whether these alterations were already in existence in the acute period remains unknown. The aim of this study was to investigate the alterations of brain functional connectome in two large samples of unilateral sudden sensorineural hearing loss patients and to investigate the correlation between unilateral sudden sensorineural hearing loss characteristics and changes in the functional network properties. Pure tone audiometry was performed to assess hearing ability. Abnormal changes in the peripheral auditory system were examined using conventional magnetic resonance imaging. The graph theoretical network analysis method was used to detect brain connectome alterations in unilateral sudden sensorineural hearing loss. Compared with the control groups, both groups of unilateral SSNHL patients exhibited a significantly increased clustering coefficient, global efficiency, and local efficiency but a significantly decreased characteristic path length. In addition, the primary increased nodal strength (e.g., nodal betweenness, hubs) was observed in several regions primarily, including the limbic and paralimbic systems, and in the auditory network brain areas. These findings suggest that the alteration of network organization already exists in unilateral sudden sensorineural hearing loss patients within the acute period and that the functional connectome of unilateral SSNHL patients is characterized by a shift toward small-worldization. Additionally, we hope that these findings will help to elucidate unilateral SSNHL through a new research perspective and provide insight for the potential pathophysiology of unilateral SSNHL. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc15&DO=10.1016%2fj.heares.2016.02.016
a modelbased hearing compensation method using a selfsupervised framework,"Hearing aids can improve auditory perception for hearing-impaired (HI) listeners, but even state-of-art devices provide only limited benefits if not configured correctly for the listeners. The prescriptive fittings of hearing aids ignore the individual difference among HI listeners with identical hearing thresholds. This paper proposes a model-based hearing compensation method using a self-supervised framework with a given auditory model. The influence of outer/inner hair cells dysfunction was simulated in the auditory model. And then, a neural network was trained to compensate for the given hearing impairment. Both objective and subjective experiments were conducted to evaluate the present method, and the results showed that listeners are sensitive to the parameter controlling the contribution of outer hair cells dysfunction. Additionally, the result indicated that listeners significantly preferred the speech processed by the proposed method to the traditional perspective fitting. © 2023 IEEE.",scopus,2-s2.0-85177607364,10.1109/ICASSP49357.2023.10095767
random forest regression combined with mri brain morphometry predicts surgical outcome of cochlear implantation,"Long-term hearing loss in post-lingually deaf adults may lead to progressive structural changes in the cerebral cortices where auditory and language functions are processed. These alterations may affect the outcome of the cochlear implant (CI) surgery. In our study, we aim to predict the surgery outcome using imaging features that characterize such cortical structural changes. We used voxel-based morphometry approach to calculate the brain GM density. To reconstruct a smaller feature-set while collapsing the number of voxel-wise GM density values, we applied an anatomically hypothesized ROI-based method and a data-driven cluster-based method. We fed the reconstructed features to a Random-Forest Regression model combined with clinical features. We observed that the cluster-based method outperformed the ROI-based method and proved the competence of the image features in CI outcome prediction. Our data-driven approach found that the most accurate prediction was made with the clusters of GM density changes in the middle temporal cortex, a critical network node of language processing, and in the thalamus, a structure (dis-)engaging and (de-)coupling cortical language operations. © 2019 IEEE.",scopus,2-s2.0-85073902005,10.1109/ISBI.2019.8759541
relationship between binaural highfrequency mean hearing threshold and hypertension in female worker exposed to noise ,"Objective To explore the relationship between the binaural high-frequency mean hearing threshold and the hypertension of female workers exposed to noise, and to understand the application significance of the binaural high-frequency mean hearing threshold as an internal effect indicator of the risk of hypertension in female workers exposed to noise. Ｍｅｔｈｏｄｓ From January to December 2018, a total of 20882 female workers exposed to noise in Guangzhou were selected by cluster sampling. Pure tone audiometry, blood pressure, age and length of service were collected. Trend test was used to evaluate the effects of exposure to noise and binaural high-frequency mean hearing threshold on blood pressure. Binary logistic regression model was used to evaluate the risk of hypertension associated with exposure to noise and binaural high-frequency mean hearing threshold. Ｒｅｓｕｌｔｓ The detection rate of normal hearing threshold, mild hearing loss and severe hearing loss was 80.73% (16858/20882), 16.21% (3384/20882) and 3.06% (640/20882) respectively. The prevalence of hypertension was 6.04%(1018/16858) in normal hearing group, 10.28%(348/3384) in patients with high frequency mild hearing loss, and 11.25% (72/640) in patients with high frequency severe hearing loss. There was a linear relationship between the increase of working age and high-frequency mean hearing threshold and the increase of systolic and diastolic blood pressure (P< 0.05). Compared with those exposed to noise for less than 1 year, the risk of hypertension in female workers with 7-9 years and more than 9 years was decreased (OR= 0.79, 0.75, P<0.05). Compared with normal hearing group, the risk of hypertension in high frequency mild hearing loss group was increased (OR =1.31, P <0.05). Ｃｏｎｃｌｕｓｉｏｎ The increase in the binaural high-frequency mean hearing threshold of female workers exposed to noise can increase the blood pressure level and the risk of hypertension, and attention should be paid to female workers with high-frequency mild hearing loss. © 2021 Chinese Medical Journals Publishing House Co.Ltd. All Rights Reserved.",scopus,2-s2.0-85107456689,10.3760/cma.j.cn121094-20200413-00185
the effects of noiseinduced hair cell lesions on cochlear electromechanical responses a computational approach using a biophysical model,"234. Int J Numer Method Biomed Eng. 2022 May;38(5):e3582. doi: 10.1002/cnm.3582. Epub 2022 Feb 21.The effects of noise-induced hair cell lesions on cochlear electromechanical responses: A computational approach using a biophysical model.Saremi A(1), Stenfelt S(2).Author information:(1)Department of Applied Physics and Electronics, Umeå University, Umeå, Sweden.(2)Department of Biomedical and Clinical Sciences, Linköping University, Linköping, Sweden.A biophysically inspired signal processing model of the human cochlea is deployed to simulate the effects of specific noise-induced inner hair cell (IHC) and outer hair cell (OHC) lesions on hearing thresholds, cochlear compression, and the spectral and temporal features of the auditory nerve (AN) coding. The model predictions were evaluated by comparison with corresponding data from animal studies as well as human clinical observations. The hearing thresholds were simulated for specific OHC and IHC damages and the cochlear nonlinearity was assessed at 0.5 and 4 kHz. The tuning curves were estimated at 1 kHz and the contributions of the OHC and IHC pathologies to the tuning curve were distinguished by the model. Furthermore, the phase locking of AN spikes were simulated in quiet and in presence of noise. The model predicts that the phase locking drastically deteriorates in noise indicating the disturbing effect of background noise on the temporal coding in case of hearing impairment. Moreover, the paper presents an example wherein the model is inversely configured for diagnostic purposes using a machine learning optimization technique (Nelder-Mead method). Accordingly, the model finds a specific pattern of OHC lesions that gives the audiometric hearing loss measured in a group of noise-induced hearing impaired humans.© 2022 The Authors. International Journal for Numerical Methods in Biomedical Engineering published by John Wiley & Sons Ltd.DOI: 10.1002/cnm.3582PMCID: PMC9286811",pubmed,35150464,10.1002/cnm.3582
the crucial role of diverse animal models to investigate cochlear aging and hearing loss,"188. Hear Res. 2024 Apr;445:108989. doi: 10.1016/j.heares.2024.108989. Epub 2024 Mar 11.The crucial role of diverse animal models to investigate cochlear aging and hearing loss.Castaño-González K(1), Köppl C(2), Pyott SJ(3).Author information:(1)Department of Otorhinolaryngology, Head & Neck Surgery, University Medical Center Groningen; The Research School of Behavioural and Cognitive Neurosciences, University of Groningen, Groningen, The Netherlands.(2)Cluster of Excellence ""Hearing4All"", Department of Neuroscience, School of Medicine and Health Sciences, Carl von Ossietzky Universität; Research Center Neurosensory Science, Carl von Ossietzky Universität, Oldenburg, Germany.(3)Department of Otorhinolaryngology, Head & Neck Surgery, University Medical Center Groningen; The Research School of Behavioural and Cognitive Neurosciences, University of Groningen, Groningen, The Netherlands. Electronic address: s.pyott@umcg.nl.Age-related hearing loss affects a large and growing segment of the population, with profound impacts on quality of life. Age-related pathology of the cochlea-the mammalian hearing organ-underlies age-related hearing loss. Because investigating age-related changes in the cochlea in humans is challenging and often impossible, animal models are indispensable to investigate these mechanisms as well as the complex consequences of age-related hearing loss on the brain and behavior. In this review, we advocate for a comparative and interdisciplinary approach while also addressing the challenges of comparing age-related hearing loss across species with varying lifespans. We describe the experimental advantages and limitations as well as areas for future research in well-established models of age-related hearing loss, including mice, rats, gerbils, chinchillas, and birds. We also indicate the need to expand characterization of age-related hearing loss in other established animal models, especially guinea pigs, cats, and non-human primates, in which auditory function is well characterized but age-related cochlear pathology is understudied. Finally, we highlight the potential of emerging animal models for advancing our understanding of age-related hearing loss, including deer mice, with their notably extended lifespans and preserved hearing, naked mole rats, with their exceptional longevity and extensive vocal communications, as well as zebrafish, which offer genetic tractability and suitability for drug screening. Ultimately, a comparative and interdisciplinary approach in auditory research, combining insights from various animal models with human studies, is key to robust and reliable research outcomes that better advance our understanding and treatment of age-related hearing loss.Copyright © 2024 The Author(s). Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2024.108989",pubmed,38518394,10.1016/j.heares.2024.108989
prevalence risk factors and causes of hearing loss among adults 50 years and older in santiago chile results from a rapid assessment of hearing loss survey,"Among a representative sample of adults aged 50 years and older too (i) determine the prevalence of hearing loss, (ii) evaluate probable causes and risk factors of hearing loss, and (iii) assess the association between hearing loss measured by audiometry and self-report. A population-based survey of adults aged 50 and older in Santiago, Chile using the Rapid Assessment of Hearing Loss (RAHL) survey. 538 participants completed a questionnaire, which included questions on socio-demographic and health characteristics and self-reported hearing loss. Hearing and possible cause of hearing loss was assessed using pure tone audiometry (0.5–4.0 kHz), tympanometry, and otoscopy. The prevalence of any level of hearing loss in adults aged 50 years and older was 41% (95% CI 33.2, 49.2). In terms of aetiologies, 89.3% of ears with mild or worse hearing loss were classified as sensorineural. Otoscopy was abnormal in 10.7% of subjects with impacted earwax being the most common finding (4.4%) followed by chronic otitis media (3.5%). Hearing aid usage was 16.6%. Older age, lower socioeconomic position, lack of education, and solvent exposure were significantly associated with hearing loss. Hearing loss among individuals aged over 50 years was common in Santiago, Chile.",cinahl,14992027,10.1080/14992027.2021.1998675
stages of change in adults with acquired hearing impairment seeking help for the first time application of the transtheoretical model in audiologic rehabilitation,"Objectives: This study investigated the application of the transtheoreti-cal (stages-of-change) model in audiologic rehabilitation. More specifi-cally, it described the University of Rhode Island Change Assessment (URICA) scores of adults with acquired hearing impairment. It reported the psychometric properties (construct, concurrent, and predictive validity) of the stages-of-change model in this population. Design: At baseline, 153 adults with acquired hearing impairment seeking help for the first time completed the URICA as well as measures of degree of hearing impairment, self-reported hearing disability, and years since hearing impairment onset. Participants were subsequently offered intervention options: hearing aids, communication programs, and no intervention. Their intervention uptake and adherence were assessed 6 months later and their intervention outcomes were assessed 3 months after intervention completion. First, the stages-of-change construct validity was evaluated by investigating the URICA factor structure (principal component analysis), internal consistency, and correlations between stage scores. The URICA scores were reported in terms of the scores for each stage of change, composite scores, stages with highest scores, and stage clusters (cluster analysis). Second, the concurrent validity was assessed by examining associations between stages of change and degree of hearing impairment, self-reported hearing disability, and years since hearing impairment onset. Third, the predictive validity was evaluated by investigating associations between stages of change and intervention uptake, adherence, and outcomes. Results: First, in terms of construct validity, the principal component analysis identified four instead of three stages (precontemplation, contemplation, preparation, and action) for which the internal consistency was good. Most of the sample was in the action stage. Correlations between stage scores supported the model. Cluster analysis identified four stages-of-change clusters, which the authors named active change, initiation, disengagement, and ambivalence. In terms of concurrent validity, participants who reported a more advanced stage of change had a more severe hearing impairment, reported greater hearing disability, and had a hearing impairment for a longer period of time. In terms of predictive validity, participants who reported a more advanced stage of change were more likely to take up an intervention and to report successful intervention outcomes. However, stages of change did not predict intervention adherence. Conclusions: The majority of the sample was in the action stage. The construct, concurrent, and predictive validity of the stages-of-change model were good. The stages-of-change model has some validity in the rehabilitation of adults with hearing impairment. The data support that change might be better represented on a continuum rather than by movement from one step to the next. Of all the measures, the precon-templation stage score had the best concurrent and predictive validity. Therefore, further research should focus on addressing the precontem-plation stage with a measure suitable for clinical use. Copyright © 2012 by Lippincott Williams & Wilkins.",scopus,2-s2.0-84881542273,10.1097/AUD.0b013e3182772c49
automatic recognition of speechevoked brainstem responses to english vowels,"The objective of this study is to investigate automatic recognition of speech-evoked auditory brainstem responses (speech ABR) to the five English vowels (/a/, /ae/, /?/, /i/ and /u/). We used different automatic speech recognition methods to discriminate between the responses to the vowels. The best recognition result was obtained by applying principal component analysis (PCA) on the amplitudes of the first ten harmonic components of the envelope following response (based on spectral components at fundamental frequency and its harmonics) and of the frequency following response (based on spectral components in first formant region) and combining these two feature sets. With this combined feature set used as input to an artificial neural network, a recognition accuracy of 86.3% was achieved. This study could be extended to more complex stimuli to improve assessment of the auditory system for speech communication in hearing impaired individuals, and potentially help in the objective fitting of hearing aids.",scopus,2-s2.0-85015691826,10.2316/P.2015.827-019
on the use of machine learning for classifying auditory brainstem responses a scoping review,"Recent advances in machine learning have led to a surge of interest in classification of the auditory brainstem response. In this work, we conducted a search in the PubMed, Google Scholar, SpringerLink, ScienceDirect, and Scopus databases, and identified twelve studies that explored the use of machine learning to classify the auditory brainstem response as a complementary and objective method to (a) help clinicians better diagnose hearing impairment by discerning between healthy and pathological auditory brainstem response waveforms, (b) present a neural marker for potential applications in hearing aid tuning, and (c) provide a biometric marker for discriminating between subjects. A comparison between the studies presented in this review is not possible as they used different test subjects, group sizes, and stimuli, and evaluated auditory brainstem response differently. Instead, the result of these studies will be presented and their limitations as well as their potential applications will be discussed. Overall, the findings of these studies suggest that ABR classification using machine learning is a promising tool for assessing patients with hearing loss, optimizing technologies for tuning hearing aids, and discriminating between subjects.  © 2013 IEEE.",scopus,2-s2.0-85112635028,10.1109/ACCESS.2021.3102096
fmri of the central auditory system,"Challenges for auditory fMRI include the intense scanner acoustic sound and the effects of the magnetic field on sound delivery equipment and on the electronic hearing devices critical to the study of individuals with hearing impairment. Despite these difficulties, a body of neuroimaging studies in humans provides evidence for plasticity in the auditory system and is therefore informative in the clinical context. This chapter presents several clinical applications of fMRI that investigate the processing of nonlinguistic and linguistic sound features. These include studies of the functional reorganization of the auditory system as a consequence of adaptation to hearing loss and its remediation through amplification and the assessment of candidature for cochlear implantation. While the chapter illustrates opportunities for auditory fMRI to supplement the clinical decision-making process, it also highlights specific areas where there is a current lack of understanding and makes recommendations for future clinical research. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.",scopus,2-s2.0-85173321668,10.1007/978-3-031-10909-6_32
summating potential as marker of intracochlear position in bipolar electrocochleography,"165. Ear Hear. 2023 Jan-Feb 01;44(1):118-134. doi: 10.1097/AUD.0000000000001259. Epub 2022 Jul 27.Summating Potential as Marker of Intracochlear Position in Bipolar Electrocochleography.Baumhoff P(1), Rahbar Nikoukar L(1), de Andrade JSC(1)(2)(3), Lenarz T(1)(4), Kral A(1)(4)(5).Author information:(1)Department of Experimental Otology & Institute of AudioNeuroTechnology (VIANNA), ENT Clinics, Hannover Medical School, Hannover, Germany.(2)Department of Otorhinolaryngology and Head and Neck Surgery, Federal University of São Paulo (UNIFESP), São Paulo, Brazil.(3)Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES Foundation), Brasília, Brazil.(4)Cluster of Excellence ""Hearing4all"", Hannover, Germany.(5)Department of Biomedical Sciences, School of Medicine and Health Sciences, Macquarie University, Sydney, Australia.OBJECTIVES: Cochlear implantation criteria include subjects with residual low-frequency hearing. To minimize implantation trauma and to avoid unwanted interactions of electric- and acoustic stimuli, it is often recommended to stop cochlear implantation before the cochlear implant (CI) reaches the cochlear partition with residual hearing, as determined by an audiogram. For this purpose, the implant can be used to record acoustically evoked signals during implantation, including cochlear compound action potentials (CAP), cochlear microphonics (CMs), and summating potentials (SPs). The former two have previously been used to monitor residual hearing in clinical settings.DESIGN: In the present study we investigated the use of intracochlear, bipolar SP recordings to determine the exact cochlear position of the contacts of implanted CIs in guinea pig cochleae (n = 13). Polarity reversals of SPs were used as a functional marker of intracochlear position. Micro computed tomography (µCT) imaging and a modified Greenwood function were used to determine the cochleotopic positions of the contacts in the cochlea. These anatomical reconstructions were used to validate the SP-based position estimates.RESULTS: The precision of the SP-based position estimation was on average within ± 0.37 octaves and was not impaired by moderate hearing loss caused by noise exposure after implantation. It is important to note that acute hearing impairment did not reduce the precision of the method. The cochleotopic position of CI accounted for ~70% of the variability of SP polarity reversals. Outliers in the dataset were associated with lateral CI positions. Last, we propose a simplified method to avoid implantation in functioning parts of the cochlea by approaching a predefined frequency region using bipolar SP recordings through a CI.CONCLUSIONS: Bipolar SP recordings provide reliable information on electrode position in the cochlea. The position estimate remains reliable after moderate hearing loss. The technique presented here could be applied during CI surgery to monitor the CI approach to a predefined frequency region.Copyright © 2022 The Authors. Ear & Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health, Inc.DOI: 10.1097/AUD.0000000000001259",pubmed,35894668,10.1097/AUD.0000000000001259
auditory processing disorders diagnostic and therapeutic challenge,"BACKGROUND: The auditory processing disorders (APD) are characterized by normal peripheral hearing, but abnormal processing of auditory information within the central auditory nervous system and the neurobiological activity that underlies that processing and gives rise to the electrophysiological auditory potentials. Learning disorders (LD) are diagnosed when a subject's achievement on individually administered standardized tests in reading, mathematics or written expression is substantially below (defined as a discrepancy of more than two standard deviations from the mean) that expected for age, schooling and level of intelligence. Prevalence of APD in students diagnosed with LD is estimated to be as high but is still unclear the overlap between the APD and other developmental disorders. This lack of clarity is probably due to the use of multiple diagnostic criteria and different tests proposed that evaluate the same cognitive domains as memory, attention, speech production etc. METHODS: The aim of our study was to present a cluster analysis to determine the overall profile of students that are tested for dyslexia with the co-occurrence of poor performance on auditory skills. In absence of any audiometric hearing loss, they have been addressed for auditory processing assessment according to diagnostic criteria. We evaluated 70 patients (30 males and 40 females) aged between 17 and 55 years. The students were tested on cognitive, auditory, reading and language skills with an IQ assessment, dyslexia assessment, phonological awareness screening, instrumental evaluation for hearing threshold. Exclusion criteria: IQ below the norm (<70 points) and the presence of neurological and sensory deficits. RESULT S: Among 70 patients examined, 33% have poor SRT -PTA agreement because ITA Matrix test showed a SRT average of -3.8 dB SNR; of these 33%, 56% also showed a low score in repeating non-words with shielded mouth, 61% a speed less than 4th percentage in spelling and 39% less than the 5th percentage in the fusion test. Analyzing the profiles of the group with poor SRT-PTA agreement, we focused on four cases that are suspected for APD and we tested them. Only one subjects had a poor performance below two standard deviation on two tests according to diagnostic criteria, so we confirm an APD. CONCLUSIONS: The clinical presentation of APD has much in common especially with specific language impairment (SLI) and dyslexia and this occurrence suggests that may be a symptom of a more varied neurodevelopmental disorder. We conclude that all the patients with difficult on auditory skills with normal hearing threshold should be assess for an APD. The diagnosis of APD is still today a challenge that require a larger sample for further investigation. © 2021 EDIZIONI MINERVA MEDICA.",scopus,2-s2.0-85117306101,10.23736/S2724-6302.21.02387-2
determining user requirements for an audiology information system,"This paper examines user requirements for a new audiology information system for use in audiology departments by audiology staff. There is a lack of literature on requirements for an audiology information system. This ethnographic study describes the current audiology service and the equipment used in the diagnosis and treatment of hearing loss. Eliciting user requirements from all the users of a clinical information system provides the best possible outcomes for all user groups: clinical, administration and hearing aid technicians. The study includes a review of audiology internal policies and a survey of all potential end users of a new audiology information system in the Health Service Executive (HSE) community service. The findings identify the six most important user requirements in a new audiology information system. These priority requirements include the storage of all Noah files, multiple patient search options, have all test results stored in the patient's electronic record, be capable of generating IT-based reports and store the audiogram in the patient's electronic record (EHR). Other findings include the desire for a paperless system and for the integration of all audiology equipment and existing IT databases, with up-to-date? information system involving automated processes.",ieee,2372-9198,10.1109/CBMS.2018.00054
machine learning for vestibular schwannoma diagnosis using audiometrie data alone,"35. Otol Neurotol. 2022 Jun 1;43(5):e530-e534. doi: 10.1097/MAO.0000000000003539.Machine Learning for Vestibular Schwannoma Diagnosis Using Audiometrie Data Alone.Carey GE(1), Jacobson CE, Warburton AN, Biddle E, Mannarelli G, Wilson M, Stucken EZ.Author information:(1)Department of Otolaryngology, University of Michigan Medical School, Ann Arbor, Michigan.OBJECTIVE: The aim of this study is to compare machine learning algorithms and established rule-based evaluations in screening audiograms for the purpose of diagnosing vestibular schwannomas. A secondary aim is to assess the performance of rule-based evaluations for predicting vestibular schwannomas using the largest dataset in the literature.STUDY DESIGN: Retrospective case-control study.SETTING: Tertiary referral center.PATIENTS: Seven hundred sixty seven adult patients with confirmed vestibular schwannoma and a pretreatment audiogram on file and 2000 randomly selected adult controls with audiograms.INTERVENTIONS: Audiometric data were analyzed using machine learning algorithms and standard rule-based criteria for defining asymmetric hearing loss.MAIN OUTCOME MEASURES: The primary outcome is the ability to identify patients with vestibular schwannomas based on audiometric data alone, using machine learning algorithms and rule-based formulas. The secondary outcome is the application of conventional rule-based formulas to a larger dataset using advanced computational techniques.RESULTS: The machine learning algorithms had mildly improved specificity in some fields compared with rule-based evaluations and had similar sensitivity to previous rule-based evaluations in diagnosis of vestibular schwannomas.CONCLUSIONS: Machine learning algorithms perform similarly to rule-based evaluations in identifying patients with vestibular schwannomas based on audiometric data alone. Performance of established rule-based formulas was consistent with earlier performance metrics, when analyzed using a large dataset.Copyright © 2022, Otology & Neurotology, Inc.DOI: 10.1097/MAO.0000000000003539",pubmed,35617004,10.1097/MAO.0000000000003539
randomized trial of a hearing conservation intervention for rural students longterm outcomes references,"Objectives: We had the rare opportunity to conduct a cluster-randomized controlled trial to observe the long-term (16-year) effects of a well-designed hearing conservation intervention for rural high school students. This trial assessed whether the intervention resulted in (1) reduced prevalence of noise-induced hearing loss (NIHL) assessed clinically and/or (2) sustained use of hearing protection devices. Methods: In 1992-1996, 34 rural Wisconsin schools were recruited and 17 were assigned randomly to receive a comprehensive, 3-year, hearing conservation intervention. In 2009 -2010, extensive efforts were made to find and contact all students who completed the original trial. Participants in the 16-year follow-up study completed an exposure history questionnaire and a clinical audiometric examination. Rates of NIHL and use of hearing protection were compared. Results: We recruited 392 participants from the original trial, 200 (53%) from the intervention group and 192 (51%) from the control group. Among participants with exposure to agricultural noise, the intervention group reported significantly greater use of hearing protection compared with the control group (25.9% vs 19.6%; P = .015). The intervention group also reported significantly greater use of hearing protection for shooting guns (56.2% vs 41.6%; P = .029), but the groups reported similar uses of protection in other contexts. There was no significant difference between groups with respect to objective measures of NIHL. Conclusion: This novel trial provides objective evidence that a comprehensive educational intervention by itself may be of limited effectiveness in preventing NIHL in a young rural population. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc10&DO=10.1542%2fpeds.2011-0770
development of improved software intelligent system for audiological solutions,"527. J Med Syst. 2018 Jun 2;42(7):127. doi: 10.1007/s10916-018-0978-6.Development of Improved Software Intelligent System for Audiological Solutions.Rajkumar S(1), Muttan S(2), Sapthagirivasan V(3)(4), Jaya V(5), Vignesh SS(5).Author information:(1)Department of Biomedical Engineering, Rajalakshmi Engineering College, Chennai, Tamil Nadu, 602117, India. rajkumar.s@rajalakshmi.edu.in.(2)Department of Electronics and Communication Engineering, College of Engineering, Guindy, Anna University, Chennai, Tamil Nadu, 600025, India.(3)SRM University, Kancheepuram, Tamil Nadu, 603203, India.(4)Medical Solution's Group, Product and Engineering Services Department, IT Service Company, Bengaluru, Karnataka, 560066, India.(5)Department of Ear, Nose & Throat, Madras Medical College & Rajiv Gandhi Government Hospital, Chennai, Tamil Nadu, 600003, India.Of late, there has been an increase in hearing impairment cases and to provide the most advantageous solutions to them is an uphill task for audiologists. Significant difficulty faced by the audiologists is in effective programming of hearing aids to provide enhanced satisfaction to the users. The main aim of our study was to develop a software intelligent system (SIS): (i) to perform the required audiological investigations for finding the degree and type of hearing loss, and (ii) to suggest appropriate values of hearing aid parameters for enhancing the speech intelligibility and the satisfaction level among the hearing aid users. In this paper, we present a Neuro-Fuzzy based SIS to automatically predict and suggest the hearing-aid parameters such as gain values, compression ratio and threshold knee point, which are needed to be fixed for different octave frequencies of sound inputs during the hearing-aid trial. The test signals for audiological investigations are generated through the standard hardware present in a personal computer system and with the aid of a software algorithm. The proposed system was validated with 243 subjects' data collected at the Government General Hospital, Chennai, India. The calculated sensitivity, specificity and accuracy of the proposed audiometer incorporated in the SIS were 98.6%, 96.4 and 98.2%, respectively, by comparing its interpretations with those of the 'gold standard' audiometers. Furthermore, 91% (221 of 243) of the hearing impaired subjects attained satisfaction in the first hearing aid trials itself with the gain values as recommended by the improved SIS. The proposed system reduced around 75% of the 'trial and error' time spent by audiologists for enhancing satisfactory usage of the hearing aid. Hence, the proposed SIS could be used to find the degree and type of hearing loss and to recommend hearing aid parameters to provide optimal solutions to the hearing aid users.DOI: 10.1007/s10916-018-0978-6",pubmed,29860544,10.1007/s10916-018-0978-6
increases in spontaneous activity in the dorsal cochlear nucleus following exposure to high intensity sound a possible neural correlate of tinnitus,"785. Audit Neurosci. 1996;3(1):57-78.Increases in Spontaneous Activity in the Dorsal Cochlear Nucleus Following Exposure to High Intensity Sound: A Possible Neural Correlate of Tinnitus.Kaltenbach JA(1), McCaslin DL.Author information:(1)Department of Otolaryngology/Head and Neck Surgery, 5E-UHC Wayne State University School of Medicine, Detroit, Michigan 48201.The purpose of this study was to test the effects of intense tone exposure on the spontaneous activity of multiunit clusters in the mammalian dorsal cochlear nucleus (DCN). Adult hamsters (60-101 days of age) were exposed to a 10 kHz tone at levels between 125 and 130 dB SPL for a period of 4 hours. The effects of tone exposure were studied following a recovery period of 30-58 days and were quantified by measuring the spontaneous rates, response thresholds and frequency tuning properties of neural clusters at the surface of the DCN. Measures were performed at each of 10-15 sites along the tonotopic axis of the DCN. The effects of the tone exposure were examined by comparison with identical measures obtained from normal unexposed animals. Results indicate that tone exposure induced major chronic increases in the spontaneous activity of the DCN. Such increases were broadly distributed across the tonotopic range of the DCN and were generally found in tonotopic map areas characterized by tone-induced elevations of neural thresholds. Mean spontaneous rate reached its maximum value at or close to the tonotopic locus which normally represents the frequency of the exposure tone. The increased activity induced by tone exposure resembled the heightened activity in normal animals during presentation of a moderate level continuous tone. These changes in spontaneous activity indicate that central auditory neurons are in a state of elevated activity for extended periods following intense sound exposure and suggest that the affected neurons may signal the presence of acoustic stimulation even though such stimulation is not present. Possible mechanisms of these changes and their relation to the clinical problem of tinnitus are discussed.PMCID: PMC3826462",pubmed,24244077,
selfreported hearing handicap in adults aged 55 to 81 years is modulated by hearing abilities frailty mental health and willingness to use hearing aids,"253. Int J Audiol. 2021;60(sup2):71-79. doi: 10.1080/14992027.2020.1858237. Epub 2021 Jan 17.Self-reported hearing handicap in adults aged 55 to 81 years is modulated by hearing abilities, frailty, mental health, and willingness to use hearing aids.Nuesse T(1)(2), Schlueter A(1)(2), Lemke U(3), Holube I(1)(2).Author information:(1)Institute of Hearing Technology and Audiology, Jade University of Applied Sciences, Oldenburg, Germany.(2)Cluster of Excellence ""Hearing4All"", Oldenburg, Germany.(3)Sonova AG, Stäfa, Switzerland.OBJECTIVE: The aim of this study was to predict outcomes of the HHI questionnaire (Hearing Handicap Inventory) using individual variables beyond pure-tone hearing thresholds.DESIGN: An extensive health-related test battery was applied including a general anamnesis, questionnaires, audiological measures, examination of visual acuity, balance, and cognition, as well as tactile- and motor skills. Based on the self-assessment of health variables and different sensory and cognitive performance measures, a frailty index was calculated to describe the health status of the participants. A stepwise linear regression analysis was conducted to predict HHI scores.STUDY SAMPLE: A mixed sample (N = 212) of 55- to 81-year-old, participants with different hearing and aiding status completed the test battery.RESULTS: The regression analysis showed statistically significant contributions of pure-tone hearing thresholds, speech recognition in noise, age, frailty, mental health, and the willingness to use hearing aids on HHIE outcomes.CONCLUSIONS: Self-reported hearing handicap assessed with the HHI questionnaire reflects various individual variables additionally to pure-tone hearing loss and speech recognition in noise. It is necessary to be aware of the influences of age and health-related variables on HHI scores when using it in research as well as in clinical settings.DOI: 10.1080/14992027.2020.1858237",pubmed,33459099,10.1080/14992027.2020.1858237
auditory perception and language functional imaging of speech sensitive auditory cortex,"516. Rev Neurol (Paris). 2001 Sep;157(8-9 Pt 1):837-46.[Auditory perception and language: functional imaging of speech sensitive auditory cortex].[Article in French]Samson Y(1), Belin P, Thivard L, Boddaert N, Crozier S, Zilbovicius M.Author information:(1)CEA, DRM, Service Hospitalier Frédéric Joliot, Orsay, France. yves.samson@psl.ap-hop-paris.frSince the description of cortical deafness, it has been known that the superior temporal cortex is bilaterally involved in the initial stages of language auditory perception but the precise anatomical limits and the function of this area remain debated. Here we reviewed more than 40 recent papers of positron emission tomography and functional magnetic resonance imaging related to language auditory perception, and we performed a meta-analysis of the localization of the peaks of activation in the Talairach's space. We found 8 studies reporting word versus non-word listening contrasts with 54 activation peaks in the temporal lobes. These peaks clustered in a bilateral and well-limited area of the temporal superior cortex, which is here operationally defined as the speech sensitive auditory cortex. This area is more than 4cm long, located in the superior temporal gyrus and the superior temporal sulcus, both anterior and posterior to Heschl's gyrus. It do not include the primary auditory cortex nor the ascending part of the planum temporale. The speech sensitive auditory cortex is not activated by pure tones, environmental sounds, or attention directed toward elementary components of a sound such as intensity, pitch, or duration, and thus has some specificity for speech signals. The specificity is not perfect, since we found a number of non-speech auditory stimuli activating the speech sensitive auditory cortex. Yet the latter studies always involve auditory perception mechanisms which are also relevant to speech perception either at the level of primitive auditory scene analysis processes, or at the level of specific schema-based recognition processes. The dorsal part of the speech sensitive auditory cortex may be involved in primitive scene analysis processes, whereas distributed activation of this area may contribute to the emergence of a broad class of ""voice"" schemas and of more specific ""speech schemas/phonetic modules"" related to different languages. In addition, this area is activated by language-related lip movement, suggesting that a multimodal integration of the auditory and the visual information relevant in speech perception occurs at this level. Finally, there is a task-related top-down modulation of the pattern of activation of the speech sensitive auditory cortex which may reflect the fact that the different parts of this structure are connected to different down-stream cortical regions involved in the neural processing of different types of tasks.",pubmed,11677406,
soft classification and regression analysis of audiometric phenotypes of agerelated hearing loss,"133. Biometrics. 2024 Jan 29;80(1):ujae013. doi: 10.1093/biomtc/ujae013.Soft classification and regression analysis of audiometric phenotypes of age-related hearing loss.Yang C(1), Langworthy B(1), Curhan S(2)(3), Vaden KI Jr(4), Curhan G(1)(2)(3)(5), Dubno JR(4), Wang M(1)(2)(3)(6).Author information:(1)Department of Epidemiology, Harvard T.H. Chan School of Public Health, Boston, MA 02115, United States.(2)Harvard Medical School, Boston, MA 02115, United States.(3)Channing Division of Network Medicine, Department of Medicine, Brigham and Women's Hospital, Boston, MA 02115, United States.(4)Hearing Research Program, Department of Otolaryngology-Head and Neck Surgery, Medical University of South Carolina, Charleston, SC 29425, United States.(5)Renal Division, Department of Medicine, Brigham and Women's Hospital, Boston, MA 02115, United States.(6)Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA 02115, United States.Age-related hearing loss has a complex etiology. Researchers have made efforts to classify relevant audiometric phenotypes, aiming to enhance medical interventions and improve hearing health. We leveraged existing pattern analyses of age-related hearing loss and implemented the phenotype classification via quadratic discriminant analysis (QDA). We herein propose a method for analyzing the exposure effects on the soft classification probabilities of the phenotypes via estimating equations. Under reasonable assumptions, the estimating equations are unbiased and lead to consistent estimators. The resulting estimator had good finite sample performances in simulation studies. As an illustrative example, we applied our proposed methods to assess the association between a dietary intake pattern, assessed as adherence scores for the dietary approaches to stop hypertension diet calculated using validated food-frequency questionnaires, and audiometric phenotypes (older-normal, metabolic, sensory, and metabolic plus sensory), determined based on data obtained in the Nurses' Health Study II Conservation of Hearing Study, the Audiology Assessment Arm. Our findings suggested that participants with a more healthful dietary pattern were less likely to develop the metabolic plus sensory phenotype of age-related hearing loss.© The Author(s) 2024. Published by Oxford University Press on behalf of The International Biometric Society.DOI: 10.1093/biomtc/ujae013PMCID: PMC10941322",pubmed,38488465,10.1093/biomtc/ujae013
a comparison of supervised classification methods for auditory brainstem response determination,"391. Stud Health Technol Inform. 2007;129(Pt 2):1289-93.A comparison of supervised classification methods for auditory brainstem response determination.McCullagh P(1), Wang H, Zheng H, Lightbody G, McAllister G.Author information:(1)Department of Computing and Mathematics, University of Ulster, United Kingdom.The ABR is commonly used in the Audiology clinic to determine and quantify hearing loss. Its interpretation is subjective, dependent upon the expertise and experience of the clinical scientist. In this study we investigated the role of machine learning for pattern classification in this domain. We extracted features from the ABRs of 85 test subjects (550 waveforms) and compared four complimentary supervised classification methods: Naïve Bayes, Support Vector Machine Multi-Layer Perceptron and KStar. The Abr dataset comprised both high level and near threshold recordings, labeled as 'response' or 'no response' by the human expert. Features were extracted from single averaged recordings to make the classification process straightforward. A best classification accuracy of 83.4% was obtained using Naïve Bayes and five relevant features extracted from time and wavelet domains. Naïve Bayes also achieved the highest specificity (86.3%). The highest sensitivity (93.1%) was obtained with Support Vector Machine-based classification models. In terms of the overall classification accuracy, four classifiers have shown the consistent, relatively high performance, indicating the relevance of selected features and the feasibility of using machine learning and statistical classification models in the analysis of ABR.",pubmed,17911922,
combined effect of noise and handtransmitted vibration on noiseinduced hearing loss in the automobile manufacturing industry ,"ObjectiveTo investigate the combined effect of noise and hand-transmitted vibration on noise-induced hearing loss (NIHL) in the automobile manufacturing industry. MethodsFrom September 2018 to January 2019, cluster sampling was used to select 998 workers in an automobile factory as study subjects, among whom 352 workers exposed to noise alone were enrolled as noise group, 342 workers exposed to noise and hand-transmitted vibration were enrolled as combined effect group, and 304 workers without exposure to occupational hazardous factors were enrolled as control group. A questionnaire survey and pure tone audiometry were performed for all study subjects. An analysis of variance was used for comparison of continuous data between groups, and the chi-square test was used for comparison of categorical data between groups; a ordinal polytomous logistic regression analysis was used to investigate the influencing factors for NIHL (with 0.05 as the inclusion criteria and 0.10 as the exclusion criteria for independent variables) . ResultsThere was a significant difference in LAeq, 8 h between groups (P<0.05) ; the noise group and the combined effect group had a significantly higher LAeq, 8 h than the control group (P<0.05) , while there was no significant difference in LAeq, 8 h between the noise group and the combined effect group (P>0.05) . The control group had a significantly lower detection rate of hearing loss than the noise group and the combined effect group (P<0.0125) , and the combined effect group had a significantly higher detection rate of hearing loss than the noise group (P<0.0125) . The ordinal polytomous logistic regression analysis showed that after adjustment for confounding factors such as age, working years, sex, smoking, and drinking, both noise exposure and exposure to both noise and hand-transmitted vibration had an influence on workers' hearing (P<0.05) , and the workers exposed to both noise and hand-transmitted vibration had a higher risk of hearing loss than those exposed to noise alone. ConclusionThere may be a combined effect of noise and hand-transmitted vibration in the automobile manufacturing industry, which can increase the risk of NIHL in workers. © 2020 Chinese Medical Journals Publishing House Co.Ltd",scopus,2-s2.0-85087737955,10.3760/cma.j.cn121094-20191009-00470
machine learning improvements to compressive sensing for speech enhancement in hearing aid applications,"Purpose: Speech is the primary means of communication for humans. A proper functioning auditory system is needed for accurate cognition of speech. Compressed sensing (CS) is a method for simultaneous compression and sampling of a given signal. It is a novel method increasingly being used in many speech processing applications. The paper aims to use Compressive sensing algorithm for hearing aid applications to reduce surrounding noise. Design/methodology/approach: In this work, the authors propose a machine learning algorithm for improving the performance of compressive sensing using a neural network. Findings: The proposed solution is able to reduce the signal reconstruction time by about 21.62% and root mean square error of 43% compared to default L2 norm minimization used in CS reconstruction. This work proposes an adaptive neural network–based algorithm to enhance the compressive sensing so that it is able to reconstruct the signal in a comparatively lower time and with minimal distortion to the quality. Research limitations/implications: The use of compressive sensing for speech enhancement in a hearing aid is limited due to the delay in the reconstruction of the signal. Practical implications: In many digital applications, the acquired raw signals are compressed to achieve smaller size so that it becomes effective for storage and transmission. In this process, even unnecessary signals are acquired and compressed leading to inefficiency. Social implications: Hearing loss is the most common sensory deficit in humans today. Worldwide, it is the second leading cause for “Years lived with Disability” the first being depression. A recent study by World health organization estimates nearly 450 million people in the world had been disabled by hearing loss, and the prevalence of hearing impairment in India is around 6.3% (63 million people suffering from significant auditory loss). Originality/value: The objective is to reduce the time taken for CS reconstruction with minimal degradation to the reconstructed signal. Also, the solution must be adaptive to different characteristics of the signal and in presence of different types of noises. © 2021, Emerald Publishing Limited.",scopus,2-s2.0-85112031073,10.1108/WJE-06-2021-0324
prediction of hearing loss among the noiseexposed workers in a steel factory using artificial intelligence approach,"284. Int Arch Occup Environ Health. 2015 Aug;88(6):779-87. doi: 10.1007/s00420-014-1004-z. Epub 2014 Nov 29.Prediction of hearing loss among the noise-exposed workers in a steel factory using artificial intelligence approach.Aliabadi M(1), Farhadian M, Darvishi E.Author information:(1)Department of Occupational Health, School of Public Health, Hamadan University of Medical Science, Hamadan, Iran, mohsen.aliabadi@umsha.ac.ir.PURPOSE: Prediction of hearing loss in noisy workplaces is considered to be an important aspect of hearing conservation program. Artificial intelligence, as a new approach, can be used to predict the complex phenomenon such as hearing loss. Using artificial neural networks, this study aims to present an empirical model for the prediction of the hearing loss threshold among noise-exposed workers.METHODS: Two hundred and ten workers employed in a steel factory were chosen, and their occupational exposure histories were collected. To determine the hearing loss threshold, the audiometric test was carried out using a calibrated audiometer. The personal noise exposure was also measured using a noise dosimeter in the workstations of workers. Finally, data obtained five variables, which can influence the hearing loss, were used for the development of the prediction model. Multilayer feed-forward neural networks with different structures were developed using MATLAB software. Neural network structures had one hidden layer with the number of neurons being approximately between 5 and 15 neurons.RESULTS: The best developed neural networks with one hidden layer and ten neurons could accurately predict the hearing loss threshold with RMSE = 2.6 dB and R(2) = 0.89. The results also confirmed that neural networks could provide more accurate predictions than multiple regressions.CONCLUSIONS: Since occupational hearing loss is frequently non-curable, results of accurate prediction can be used by occupational health experts to modify and improve noise exposure conditions.DOI: 10.1007/s00420-014-1004-z",pubmed,25432298,10.1007/s00420-014-1004-z
prevalence of childhood hearing loss in rural alaska,"14. Ear Hear. 2023 Sep-Oct 01;44(5):1240-1250. doi: 10.1097/AUD.0000000000001368. Epub 2023 Jun 8.Prevalence of Childhood Hearing Loss in Rural Alaska.Emmett SD(1)(2)(3)(4)(5), Platt A(2)(6), Gallo JJ(7)(8), Labrique AB(9), Wang NY(8), Inglis-Jenson M(4), Jenson CD(10), Hofstetter P(11), Hicks KL(12), Ross AA(1)(3), Egger JR(2), Robler SK(4)(10).Author information:(1)Department of Head and Neck Surgery and Communication Science, Duke University School of Medicine, Durham, North Carolina, USA.(2)Duke Global Health Institute, Durham, North Carolina, USA.(3)Center for Health Policy and Inequalities Research, Duke University, Durham, North Carolina, USA.(4)Department of Otolaryngology-Head & Neck Surgery, University of Arkansas for Medical Sciences, Little Rock, Arkansas, USA.(5)Department of Epidemiology, Fay W. Boozman College of Public Health, University of Arkansas for Medical Sciences, Little Rock, Arkansas, USA.(6)Department of Biostatistics and Bioinformatics, Duke University, Durham, North Carolina, USA.(7)Department of Mental Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, USA.(8)Department of Medicine, Johns Hopkins University School of Medicine, Baltimore, Maryland, USA.(9)Department of International Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, USA.(10)Department of Audiology, Norton Sound Health Corporation, Nome, Alaska, USA.(11)Petersburg Medical Center, Petersburg, Alaska, USA.(12)Department of Otolaryngology-Head and Neck Surgery, University of North Carolina - Chapel Hill, Chapel Hill, North Carolina, USA.OBJECTIVES: Childhood hearing loss has well-known lifelong consequences. Certain rural populations are at higher risk for infection-related hearing loss. For Alaska Native children, historical data on hearing loss prevalence suggest a higher burden of infection-related hearing loss, but updated prevalence data are urgently needed in this high-risk population.DESIGN: Hearing data were collected as part of two school-based cluster-randomized trials in 15 communities in rural northwest Alaska over two academic years (2017-2019). All enrolled children from preschool to 12th grade were eligible. Pure-tone thresholds were obtained using standard audiometry and conditioned play when indicated. The analysis included the first available audiometric assessment for each child (n = 1634 participants, 3 to 21 years), except for the high-frequency analysis, which was limited to year 2 when higher frequencies were collected. Multiple imputation was used to quantify the prevalence of hearing loss in younger children, where missing data were more frequent due to the need for behavioral responses. Hearing loss in either ear was evaluated using both the former World Health Organization (WHO) definition (pure-tone average [PTA] > 25 dB) and the new WHO definition (PTA ≥ 20 dB), which was published after the study. Analyses with the new definition were limited to children 7 years and older due to incomplete data obtained on younger children at lower thresholds.RESULTS: The overall prevalence of hearing loss (PTA > 25 dB; 0.5, 1, 2, 4 kHz) was 10.5% (95% confidence interval [CI], 8.9 to 12.1). Hearing loss was predominately mild (PTA >25 to 40 dB; 8.9%, 95% CI, 7.4 to 10.5). The prevalence of unilateral hearing loss was 7.7% (95% CI, 6.3 to 9.0). Conductive hearing loss (air-bone gap of ≥ 10 dB) was the most common hearing loss type (9.1%, 95% CI, 7.6 to 10.7). Stratified by age, hearing loss (PTA >25 dB) was more common in children 3 to 6 years (14.9%, 95% CI, 11.4 to 18.5) compared to children 7 years and older (8.7%, 95% CI, 7.1 to 10.4). In children 7 years and older, the new WHO definition increased the prevalence of hearing loss to 23.4% (95% CI, 21.0 to 25.8) compared to the former definition (8.7%, 95% CI, 7.1 to 10.4). Middle ear disease prevalence was 17.6% (95% CI, 15.7 to 19.4) and was higher in younger children (23.6%, 95% CI, 19.7 to 27.6) compared to older children (15.2%, 95% CI, 13.2 to 17.3). High-frequency hearing loss (4, 6, 8kHz) was present in 20.5% (95% CI, 18.4 to 22.7 [PTA >25 dB]) of all children and 22.8% (95% CI, 20.3 to 25.3 [PTA >25 dB]) and 29.7% (95% CI, 27.0 to 32.4 [PTA ≥ 20 dB]) of children 7 years and older (limited to year 2).CONCLUSIONS: This analysis represents the first prevalence study on childhood hearing loss in Alaska in over 60 years and is the largest cohort with hearing data ever collected in rural Alaska. Our results highlight that hearing loss continues to be common in rural Alaska Native children, with middle ear disease more prevalent in younger children and high-frequency hearing loss more prevalent with increasing age. Prevention efforts may benefit from managing hearing loss type by age. Lastly, continued research is needed on the impact of the new WHO definition of hearing loss on field studies.Copyright © 2023 The Authors. Ear & Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health, Inc.DOI: 10.1097/AUD.0000000000001368PMCID: PMC10426776",pubmed,37287104,10.1097/AUD.0000000000001368
thoughts on the potential to compensate a hearing loss in noise,"43. F1000Res. 2021 Apr 22;10:311. doi: 10.12688/f1000research.51784.1. eCollection 2021.Thoughts on the potential to compensate a hearing loss in noise.Schädler MR(1).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4all, University of Oldenburg, Oldenburg, Germany.Background: The effect of hearing impairment on speech perception was described by Plomp (1978) as a sum of a loss of class A, due to signal attenuation, and a loss of class D, due to signal distortion. While a loss of class A can be compensated by linear amplification, a loss of class D, which severely limits the benefit of hearing aids in noisy listening conditions, cannot. The hearing loss of class D is assumed to be the main reason why not few users of hearing aids keep complaining about the limited benefit of their devices in noisy environments. Working compensation strategies against it are unknown. Methods: Recently, in an approach to model human speech recognition by means of a re-purposed automatic speech recognition (ASR) system, the loss of class D was explained by introducing a level uncertainty which reduces the individual accuracy of spectro-temporal signal levels. Based on this finding, an implementation of a patented dynamic range manipulation scheme (PLATT) is proposed which aims to mitigate the effect of increased level uncertainty on speech recognition in noise by expanding spectral modulation patterns in the range of 2 to 4 ERB. This compensation approach is objectively evaluated regarding the benefit in speech recognition thresholds in noise using the ASR-based speech recognition model. Recommendations for an evaluation with human listeners are derived. Results: The objective evaluation suggests that approximately half of the class D loss due to an increased level uncertainty might be compensable. To measure the effect with human listeners, an experiment needs to be carefully designed to prevent the confusion class A and D loss compensations. Conclusions: A working compensation strategy for the class D loss could provide previously unexploited potential for relief. Evidence has to be provided in experiments with human listeners.Copyright: © 2021 Schädler MR.DOI: 10.12688/f1000research.51784.1PMCID: PMC8524304",pubmed,34721841,10.12688/f1000research.51784.1
integrating cognitive and peripheral factors in predicting hearingaid processing effectiveness,"241. J Acoust Soc Am. 2013 Dec;134(6):4458. doi: 10.1121/1.4824700.Integrating cognitive and peripheral factors in predicting hearing-aid processing effectiveness.Kates JM(1), Arehart KH(1), Souza PE(2).Author information:(1)Department of Speech Language and Hearing Sciences, University of Colorado, Boulder, Colorado 80309.(2)Department of Communication Sciences and Disorders and Knowles Hearing Center, Northwestern University, Evanston, Illinois 60201.Individual factors beyond the audiogram, such as age and cognitive abilities, can influence speech intelligibility and speech quality judgments. This paper develops a neural network framework for combining multiple subject factors into a single model that predicts speech intelligibility and quality for a nonlinear hearing-aid processing strategy. The nonlinear processing approach used in the paper is frequency compression, which is intended to improve the audibility of high-frequency speech sounds by shifting them to lower frequency regions where listeners with high-frequency loss have better hearing thresholds. An ensemble averaging approach is used for the neural network to avoid the problems associated with overfitting. Models are developed for two subject groups, one having nearly normal hearing and the other mild-to-moderate sloping losses.DOI: 10.1121/1.4824700PMCID: PMC3874061",pubmed,25669257,10.1121/1.4824700
how to train your ears auditorymodel emulation for largedynamicrange inputs and mildtosevere hearing losses,"Advanced auditory models are useful in designing signal-processing algorithms for hearing-loss compensation or speech enhancement. Such auditory models provide rich and detailed descriptions of the auditory pathway, and might allow for individualization of signal-processing strategies, based on physiological measurements. However, these auditory models are often computationally demanding, requiring significant time to compute. To address this issue, previous studies have explored the use of deep neural networks to emulate auditory models and reduce inference time. While these deep neural networks offer impressive efficiency gains in terms of computational time, they may suffer from uneven emulation performance as a function of auditory-model frequency-channels and input sound pressure level, making them unsuitable for many tasks. In this study, we demonstrate that the conventional machine-learning optimization objective used in existing state-of-the-art methods is the primary source of this limitation. Specifically, the optimization objective fails to account for the frequency- and level-dependencies of the auditory model, caused by a large input dynamic range and different types of hearing losses emulated by the auditory model. To overcome this limitation, we propose a new optimization objective that explicitly embeds the frequency- and level-dependencies of the auditory model. Our results show that this new optimization objective significantly improves the emulation performance of deep neural networks across relevant input sound levels and auditory-model frequency channels, without increasing the computational load during inference. Addressing these limitations is essential for advancing the application of auditory models in signal-processing tasks, ensuring their efficacy in diverse scenarios.  © 2014 IEEE.",scopus,2-s2.0-85188662246,10.1109/TASLP.2024.3378099
functionality of hearing aids stateoftheart and future modelbased solutions,"19. Int J Audiol. 2018 Jun;57(sup3):S3-S28. doi: 10.1080/14992027.2016.1256504. Epub 2016 Dec 13.Functionality of hearing aids: state-of-the-art and future model-based solutions.Kollmeier B(1), Kiessling J(2).Author information:(1)a Medizinische Physik, Universität Oldenburg and Cluster of Excellence Hearing4all, Hörzentrum Oldenburg, HörTech gGmbH and Fraunhofer IDMT/HSA , Oldenburg , Germany and.(2)b Funktionsbereich Audiologie, Justus-Liebig-Universität Gießen , Giessen , Germany.A review about technical and perceptual factors in hearing aid technology, research and development is provided, covering current commercial solutions, underlying models of hearing loss for usage in hearing devices and emerging future technical solutions for hearing aid functionalities. A chain of techniques has provided incremental, but steady increases in user benefit, e.g. in the fields of hearing aid amplification, feedback suppression, dynamic compression, noise reduction and situation adaptation. The models describing the perceptual consequences of sensorineural hearing impairment describe the effects on the acoustical level, the neurosensory level and the cognitive level and provide the framework for compensatory (or even substitutional) functions of hearing aids in terms of the attenuation component, the distortion component and the neural component of the hearing loss. A major factor is the requirement of a strong individualisation of hearing aid solutions calling for an appropriate assessment of the different sensorineural components of a hearing loss, especially with respect to bilateral and binaural hearing aid solutions.DOI: 10.1080/14992027.2016.1256504",pubmed,27951738,10.1080/14992027.2016.1256504
a comparative study using vestibular mapping in sudden sensorineural hearing loss with and without vertigo,"90. Otolaryngol Head Neck Surg. 2023 Dec;169(6):1573-1581. doi: 10.1002/ohn.422. Epub 2023 Jul 7.A Comparative Study Using Vestibular Mapping in Sudden Sensorineural Hearing Loss With and Without Vertigo.Hong JP(1), Lee JY(1), Kim MB(1).Author information:(1)Department of Otorhinolaryngology-Head and Neck Surgery, Kangbuk Samsung Hospital, Sungkyunkwan University School of Medicine, Seoul, Korea.OBJECTIVE: To investigate the impairment patterns in peripheral vestibular organs in sudden sensorineural hearing loss (SSNHL) with and without vertigo.STUDY DESIGN: Retrospective study.SETTING: Single tertiary medical center.METHODS: Data from 165 SSNHL patients in a tertiary referral center from January 2017 to December 2022 were retrospectively analyzed. All patients underwent a video head impulse test, vestibular evoked myogenic potential test, and pure-tone audiometry. Hierarchical cluster analysis was performed to investigate vestibular impairment patterns. The prognosis of the hearing was determined using American Academy of Otolaryngology-Head and Neck Surgery recommendations.RESULTS: After excluding patients with vestibular schwannoma and Meniere's disease, 152 patients were included in this study. A total of 73 of 152 patients were categorized as SSNHL with vertigo (SSNHL_V) and showed an independent merge of the posterior semicircular canal (PSCC) in cluster analysis. A total of 79 of 152 patients were categorized as SSNHL without vertigo (SSNHL_N) and showed an independent merge of saccule in cluster analysis. The PSCC (56.2%) and saccule (20.3%) were the most frequently impaired vestibular organs in SSNHL_V and SSNHL_N, respectively. In terms of prognosis, 106 of 152 patients had partial/no recovery and showed an independent merge of the PSCC in cluster analysis. A total of 46 of 152 patients had a complete recovery and showed an independent merge of the saccule in cluster analysis.CONCLUSION: A tendency of isolated PSCC dysfunction was seen in SSNHL_V and partial/no recovery. A tendency of isolated saccular dysfunction was seen in SSNHL_N and complete recovery. Different treatments might be needed in SSNHL depending on the presence of vertigo.© 2023 American Academy of Otolaryngology-Head and Neck Surgery Foundation.DOI: 10.1002/ohn.422",pubmed,37418229,10.1002/ohn.422
variability in cochlear implantation outcomes in a large german cohort with a genetic etiology of hearing loss,"77. Ear Hear. 2023 Nov-Dec 01;44(6):1464-1484. doi: 10.1097/AUD.0000000000001386. Epub 2023 Jul 13.Variability in Cochlear Implantation Outcomes in a Large German Cohort With a Genetic Etiology of Hearing Loss.Tropitzsch A(1)(2)(3)(4), Schade-Mann T(1)(2), Gamerdinger P(1)(2), Dofek S(1), Schulte B(5), Schulze M(5), Fehr S(5), Biskup S(5), Haack TB(6), Stöbe P(6), Heyd A(1), Harre J(7)(8), Lesinski-Schiedat A(7)(8), Büchner A(7)(8), Lenarz T(7)(8), Warnecke A(7)(8), Müller M(1)(4), Vona B(1)(4), Dahlhoff E(1)(4), Löwenheim H(1)(4), Holderried M(1)(9).Author information:(1)Department of Otolaryngology-Head & Neck Surgery, University of Tübingen Medical Center, Tübingen, Germany.(2)Hearing Center, Department of Otolaryngology-Head & Neck Surgery, University of Tübingen Medical Center, Tübingen, Germany.(3)Center for Rare Hearing Disorders, Centre for Rare Diseases, University of Tübingen, Tübingen, Germany.(4)Neurosensory Center, Departments of Otolaryngology-Head & Neck Surgery and Ophthalmology, University of Tübingen Medical Center, Tübingen, Germany.(5)CeGaT GmbH und Praxis für Humangenetik Tübingen, Tübingen, Germany.(6)Institute of Medical Genetics and Applied Genomics, University of Tübingen, Tübingen, Germany.(7)Department of Otorhinolaryngology-Head & Neck Surgery, Hannover Medical School, Hannover, Germany.(8)Cluster of Excellence ""Hearing4all"" of the German Research Foundation, Hannover, Germany.(9)Department of Medical Development and Quality Management, University Hospital Tübingen, Tübingen, Germany.OBJECTIVES: The variability in outcomes of cochlear implantation is largely unexplained, and clinical factors are not sufficient for predicting performance. Genetic factors have been suggested to impact outcomes, but the clinical and genetic heterogeneity of hereditary hearing loss makes it difficult to determine and interpret postoperative performance. It is hypothesized that genetic mutations that affect the neuronal components of the cochlea and auditory pathway, targeted by the cochlear implant (CI), may lead to poor performance. A large cohort of CI recipients was studied to verify this hypothesis.DESIGN: This study included a large German cohort of CI recipients (n = 123 implanted ears; n = 76 probands) with a definitive genetic etiology of hearing loss according to the American College of Medical Genetics (ACMG)/Association for Molecular Pathology (AMP) guidelines and documented postoperative audiological outcomes. All patients underwent preoperative clinical and audiological examinations. Postoperative CI outcome measures were based on at least 1 year of postoperative audiological follow-up for patients with postlingual hearing loss onset (>6 years) and 5 years for children with congenital or pre/perilingual hearing loss onset (≤6 years). Genetic analysis was performed based on three different methods that included single-gene screening, custom-designed hearing loss gene panel sequencing, targeting known syndromic and nonsyndromic hearing loss genes, and whole-genome sequencing.RESULTS: The genetic diagnosis of the 76 probands in the genetic cohort involved 35 genes and 61 different clinically relevant (pathogenic, likely pathogenic) variants. With regard to implanted ears (n = 123), the six most frequently affected genes affecting nearly one-half of implanted ears were GJB2 (21%; n = 26), TMPRSS3 (7%; n = 9), MYO15A (7%; n = 8), SLC26A4 (5%; n = 6), and LOXHD1 and USH2A (each 4%; n = 5). CI recipients with pathogenic variants that influence the sensory nonneural structures performed at or above the median level of speech performance of all ears at 70% [monosyllable word recognition score in quiet at 65 decibels sound pressure level (SPL)]. When gene expression categories were compared to demographic and clinical categories (total number of compared categories: n = 30), mutations in genes expressed in the spiral ganglion emerged as a significant factor more negatively affecting cochlear implantation outcomes than all clinical parameters. An ANOVA of a reduced set of genetic and clinical categories (n = 10) identified five detrimental factors leading to poorer performance with highly significant effects ( p < 0.001), accounting for a total of 11.8% of the observed variance. The single strongest category was neural gene expression accounting for 3.1% of the variance.CONCLUSIONS: The analysis of the relationship between the molecular genetic diagnoses of a hereditary etiology of hearing loss and cochlear implantation outcomes in a large German cohort of CI recipients revealed significant variabilities. Poor performance was observed with genetic mutations that affected the neural components of the cochlea, supporting the ""spiral ganglion hypothesis.""Copyright © 2023 The Authors. Ear & Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health, Inc.DOI: 10.1097/AUD.0000000000001386PMCID: PMC10583923",pubmed,37438890,10.1097/AUD.0000000000001386
prevalence of hearing impairment in an adult population in southern taiwan references,"The objective of this study was to estimate the prevalence of hearing impairment in a representative adult population in southern Taiwan and compare the results to those of similar studies in other countries. A stratified systematic cluster sample of 1140 residents, aged >=20 years, of Tainan City was studied from 2001 to 2003. The test battery included otoscopy, pure-tone audiometry, and a questionnaire covering relevant personal, occupational, and family history. The hearing threshold level (HTL) was defined as the better ear pure-tone average (BPTA) (i.e. the average of hearing thresholds at frequencies 500, 1000, 2000, and 4000 Hz). The prevalence of hearing impairment was 21.4% (95% confidence interval: 19.3-23.7%) at BPTA >=25 dB HTL. Middle ear disease was a significant risk factor for hearing impairment in addition to age and gender. The overall prevalence of hearing impairment may be higher in Taiwan (17.1%) than in western populations (11.5%), but differences in the definition of hearing impairment severity and variation in sex distribution among studies may account for this higher prevalence. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc6&DO=10.1080%2f14992020701448986
toward automated cochlear implant fitting procedures based on eventrelated potentials,"601. Ear Hear. 2017 Mar/Apr;38(2):e118-e127. doi: 10.1097/AUD.0000000000000377.Toward Automated Cochlear Implant Fitting Procedures Based on Event-Related Potentials.Finke M(1), Billinger M, Büchner A.Author information:(1)Department of Otolaryngology, Hannover Medical School, Germany and Cluster of Excellence ""Hearing4all"", Hannover, Germany.OBJECTIVES: Cochlear implants (CIs) restore hearing to the profoundly deaf by direct electrical stimulation of the auditory nerve. To provide an optimal electrical stimulation pattern the CI must be individually fitted to each CI user. To date, CI fitting is primarily based on subjective feedback from the user. However, not all CI users are able to provide such feedback, for example, small children. This study explores the possibility of using the electroencephalogram (EEG) to objectively determine if CI users are able to hear differences in tones presented to them, which has potential applications in CI fitting or closed loop systems.DESIGN: Deviant and standard stimuli were presented to 12 CI users in an active auditory oddball paradigm. The EEG was recorded in two sessions and classification of the EEG data was performed with shrinkage linear discriminant analysis. Also, the impact of CI artifact removal on classification performance and the possibility to reuse a trained classifier in future sessions were evaluated.RESULTS: Overall, classification performance was above chance level for all participants although performance varied considerably between participants. Also, artifacts were successfully removed from the EEG without impairing classification performance. Finally, reuse of the classifier causes only a small loss in classification performance.CONCLUSIONS: Our data provide first evidence that EEG can be automatically classified on single-trial basis in CI users. Despite the slightly poorer classification performance over sessions, classifier and CI artifact correction appear stable over successive sessions. Thus, classifier and artifact correction weights can be reused without repeating the set-up procedure in every session, which makes the technique easier applicable. With our present data, we can show successful classification of event-related cortical potential patterns in CI users. In the future, this has the potential to objectify and automate parts of CI fitting procedures.DOI: 10.1097/AUD.0000000000000377",pubmed,27787394,10.1097/AUD.0000000000000377
tripolar configuration and pulse shape in cochlear implants reduce channel interactions in the temporal domain,"214. Hear Res. 2024 Mar 1;443:108953. doi: 10.1016/j.heares.2024.108953. Epub 2024 Jan 19.Tripolar configuration and pulse shape in cochlear implants reduce channel interactions in the temporal domain.Quass GL(1), Kral A(2).Author information:(1)Institute for AudioNeuroTechnology (VIANNA) & Department of Experimental Otology, Otolaryngology Clinics, Hannover Medical School, Hannover, Germany; Cluster of Excellence ""Hearing4All"" (EXC 2177), Germany. Electronic address: gquass@med.umich.edu.(2)Institute for AudioNeuroTechnology (VIANNA) & Department of Experimental Otology, Otolaryngology Clinics, Hannover Medical School, Hannover, Germany; Cluster of Excellence ""Hearing4All"" (EXC 2177), Germany; Australian Hearing Hub, School of Medicine and Health Sciences, Macquarie University, Sydney, Australia.The present study investigates effects of current focusing and pulse shape on threshold, dynamic range, spread of excitation and channel interaction in the time domain using cochlear implant stimulation. The study was performed on 20 adult guinea pigs using a 6-channel animal cochlear implant, recording was performed in the auditory midbrain using a multielectrode array. After determining the best frequencies for individual recording contacts with acoustic stimulation, the ear was deafened and a cochlear implant was inserted into the cochlea. The position of the implant was controlled by x-ray. Stimulation with biphasic, pseudomonophasic and monophasic stimuli was performed with monopolar, monopolar with common ground, bipolar and tripolar configuration in two sets of experiments, allowing comparison of the effects of the different stimulation strategies on threshold, dynamic range, spread of excitation and channel interaction. Channel interaction was studied in the temporal domain, where two electrodes were activated with pulse trains and phase locking to these pulse trains in the midbrain was quantified. The results documented multifactorial influences on the response properties, with significant interaction between factors. Thresholds increased with increasing current focusing, but decreased with pseudomonophasic and monophasic pulse shapes. The results documented that current focusing, particularly tripolar configuration, effectively reduces channel interaction, but that also pseudomonophasic and monophasic stimulation and phase duration intensity coding reduce channel interactions.Copyright © 2024 The Author(s). Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2024.108953",pubmed,38277881,10.1016/j.heares.2024.108953
individual hearing aid benefit in real life evaluated using ecological momentary assessment,"174. Trends Hear. 2021 Jan-Dec;25:2331216521990288. doi: 10.1177/2331216521990288.Individual Hearing Aid Benefit in Real Life Evaluated Using Ecological Momentary Assessment.von Gablenz P(1), Kowalk U(1), Bitzer J(1), Meis M(2), Holube I(1).Author information:(1)Institute of Hearing Technology and Audiology, Jade University of Applied Sciences and Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany.(2)Hörzentrum Oldenburg GmbH, Oldenburg, Germany.Ecological momentary assessment (EMA) was used in 24 adults with mild-to-moderate hearing loss who were seeking first hearing-aid (HA) fitting or HA renewal. At two stages in the aural rehabilitation process, just before HA fitting and after an average 3-month HA adjustment period, the participants used a smartphone-based EMA system for 3 to 4 days. A questionnaire app allowed for the description of the environmental context as well as assessments of various hearing-related dimensions and of well-being. In total, 2,042 surveys were collected. The main objectives of the analysis were threefold: First, describing the ""auditory reality"" of future and experienced HA users; second, examining the effects of HA fitting for individual participants, as well as for the subgroup of first-time HA-users; and third, reviewing whether the EMA data collected in the unaided condition predicted who ultimately decided for or against permanent HA use. The participants reported hearing-related disabilities across the full range of daily listening tasks, but communication events took the largest share. The effect of the HA intervention was small in experienced HA users. Generally, much larger changes and larger interindividual differences were observed in first-time compared with experienced HA users in all hearing-related dimensions. Changes were not correlated with hearing loss or with the duration of the HA adjustment period. EMA data collected in the unaided condition did not predict the cancelation of HA fitting. The study showed that EMA is feasible in a general population of HA candidates for establishing individual and multidimensional profiles of real-life hearing experiences.DOI: 10.1177/2331216521990288PMCID: PMC8020740",pubmed,33787404,10.1177/2331216521990288
learningbased referencefree speech quality measures for hearing aid applications,"Objective measures of speech quality are highly desirable in benchmarking and monitoring the performance of hearing aids (HAs). Existing HA speech quality indices such as the hearing aid speech quality index (HASQI) are intrusive in that they require a properly time-aligned and frequency-shaped reference signal to predict the quality of HA output. Two new reference-free HA speech quality indices are proposed in this paper, based on a model that amalgamates perceptual linear prediction (PLP), hearing loss (HL) modeling, and machine learning concepts. For the first index, HL-modified PLP coefficients and their statistics were used as the feature set, which was subsequently mapped to the predicted quality scores using support vector regression (SVR). For the second index, HL-impacted gammatone auditory filterbank energies and their second-order statistics constituted the feature set, which was again mapped using SVR. Two databases involving HA recordings were collected and utilized for the evaluation of the robustness and generalizability of the two indices. Experimental results showed that the index based on the gammatone filterbank energies not only correlated well with HA quality ratings by hearing impaired listeners, but also exhibited robust performance across different test conditions and was comparable to the full-reference HASQI performance. © 2018 IEEE.",scopus,2-s2.0-85050761034,10.1109/TASLP.2018.2860786
slightmild sensorineural hearing loss in children audiometric clinical and risk factor profiles,"238. Ear Hear. 2010 Apr;31(2):202-12. doi: 10.1097/AUD.0b013e3181c62263.Slight-mild sensorineural hearing loss in children: audiometric, clinical, and risk factor profiles.Cone BK(1), Wake M, Tobin S, Poulakis Z, Rickards FW.Author information:(1)Department of Speech, Language and Hearing Sciences, University of Arizona, Tucson, Arizona, USA. conewess@u.arizona.eduOBJECTIVES: Slight or mild hearing loss has been posited as a factor affecting speech, language, learning, and academic outcomes, but the risk factors for slight-mild sensorineural hearing loss (SNHL) have not been ascertained. The two specific aims for this research were (1) to describe the audiometric and clinical characteristics of children identified with slight-mild bilateral SNHL and (2) to compare children with slight-mild SNHL with those with normal hearing (NH) with respect to potential risk factors for congenital or acquired for hearing loss.DESIGN: A cross-sectional cluster sample survey of 6581 children enrolled in years 1 and 5 of Australian elementary school was completed. Children were screened for slight-mild SNHL, defined as a low- and/or high-frequency pure-tone average of 16 to 40 dB HL in the better ear, with air-bone gaps <10 dB. Children who did not pass the screen received air and bone conduction threshold and tympanometry tests to determine the type and degree of hearing loss. The parents of every child who participated in this study completed a questionnaire, before the hearing screening, to ascertain possible risk indicators. The questionnaire included items regarding the family's demographics, hearing status of family members, the presence of risk factors, and parental concern regarding the child's hearing.RESULTS: Fifty-five children with slight-mild SNHL and 5490 with NH were identified. Of the group with SNHL, 39 children had a slight loss (16 to 25 dB HL) and 16 had a mild loss (26 to 40 dB HL). The majority of the losses were bilateral and symmetrical, and the mean pure-tone average for the better ear for all 55 children was 22.4 dB HL (SD, 5.2). The most prevalent risk factor was ""neonatal intensive care unit/special care nursery admission,"" which was reported for 12.5% of the SNHL and 8.4% of the NH group. Reported use of personal stereos was a significant risk factor with an odds ratio of 1.7 (95% confidence interval = 1.0 to 3.0, p = 0.05). The questions relating to parental concern for their child's hearing had low sensitivity (<30%) and very low positive predictive values (<3%) for detecting slight-mild SNHL.CONCLUSIONS: Slight-mild SNHL had a prevalence of 0.88% among the school-aged population sampled, with the majority of these children exhibiting bilateral, symmetrical audiometric configurations. Conventional risk factors for hearing loss were not strongly predictive of slight-mild SNHL nor were parental concerns about the child's hearing ability. The association between slight-mild SNHL and the parent report of personal stereo use suggests that this type of noise exposure may be a risk factor for acquired hearing loss. This seems to be the first documentation of such an association in a large sample of young children.DOI: 10.1097/AUD.0b013e3181c62263",pubmed,20054279,10.1097/AUD.0b013e3181c62263
objective measurement of tinnitus using functional nearinfrared spectroscopy and machine learning,"595. PLoS One. 2020 Nov 18;15(11):e0241695. doi: 10.1371/journal.pone.0241695. eCollection 2020.Objective measurement of tinnitus using functional near-infrared spectroscopy and machine learning.Shoushtarian M(1)(2), Alizadehsani R(3), Khosravi A(3), Acevedo N(1), McKay CM(1)(2), Nahavandi S(3), Fallon JB(1)(2)(4).Author information:(1)The Bionics Institute, East Melbourne, Victoria, Australia.(2)Medical Bionics Department, The University of Melbourne, Melbourne, Australia.(3)Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Melbourne, Australia.(4)Department of Otolaryngology, The University of Melbourne, Melbourne, Australia.Chronic tinnitus is a debilitating condition which affects 10-20% of adults and can severely impact their quality of life. Currently there is no objective measure of tinnitus that can be used clinically. Clinical assessment of the condition uses subjective feedback from individuals which is not always reliable. We investigated the sensitivity of functional near-infrared spectroscopy (fNIRS) to differentiate individuals with and without tinnitus and to identify fNIRS features associated with subjective ratings of tinnitus severity. We recorded fNIRS signals in the resting state and in response to auditory or visual stimuli from 25 individuals with chronic tinnitus and 21 controls matched for age and hearing loss. Severity of tinnitus was rated using the Tinnitus Handicap Inventory and subjective ratings of tinnitus loudness and annoyance were measured on a visual analogue scale. Following statistical group comparisons, machine learning methods including feature extraction and classification were applied to the fNIRS features to classify patients with tinnitus and controls and differentiate tinnitus at different severity levels. Resting state measures of connectivity between temporal regions and frontal and occipital regions were significantly higher in patients with tinnitus compared to controls. In the tinnitus group, temporal-occipital connectivity showed a significant increase with subject ratings of loudness. Also in this group, both visual and auditory evoked responses were significantly reduced in the visual and auditory regions of interest respectively. Naïve Bayes classifiers were able to classify patients with tinnitus from controls with an accuracy of 78.3%. An accuracy of 87.32% was achieved using Neural Networks to differentiate patients with slight/ mild versus moderate/ severe tinnitus. Our findings show the feasibility of using fNIRS and machine learning to develop an objective measure of tinnitus. Such a measure would greatly benefit clinicians and patients by providing a tool to objectively assess new treatments and patients' treatment progress.DOI: 10.1371/journal.pone.0241695PMCID: PMC7673524",pubmed,33206675,10.1371/journal.pone.0241695
performance and potential of machine learning audiometry,,base,cad542bcda71321acdfbc54b789c5e67f51a582969693937b6a03462d47e6b47,
social inequalities in puretone hearing assessed using occupational stratification schemes,"58. Int J Audiol. 2017 Jul;56(7):443-452. doi: 10.1080/14992027.2017.1294767. Epub 2017 Mar 1.Social inequalities in pure-tone hearing assessed using occupational stratification schemes.von Gablenz P(1), Holube I(1).Author information:(1)a Institute of Hearing Technology and Audiology , Jade University of Applied Sciences and Cluster of Excellence ""Hearing4All"" , Oldenburg , Germany.OBJECTIVE: The objective of this study is to analyse the performance of two occupational stratification approaches and the impact of social position on adult hearing.DESIGN: The prevalence of hearing impairment, pure-tone averages (PTA) and prevalence ratios (PR) for relative hearing loss, which focuses on the position of one's PTA in the age- and gender-specific distribution, were compared in groups defined by ISCO Skill Level and the International Socio-Economic Index (ISEI).STUDY SAMPLE: About 1571 subjects aged 30-89, including 677 highly screened adults, from the cross-sectional study HÖRSTAT.RESULTS: ISCO Skill Level and ISEI yielded qualitatively the same results. The prevalence difference between the socially least and most advantaged group ranges between 10 and 16%, varying with the scheme applied. Low- and high-frequency PTA and PR for relative hearing loss confirm the gradient. Screening reduced, but did not negate the social differences. The prevalence difference dropped to 6-7% in the otologically normal subsample.CONCLUSIONS: Social groups defined by hierarchical, occupational measures differ in their pure-tone hearing, even if the main risk factors are controlled for. This underlines the need for population-based sampling, the relevance of reporting the study group's social composition and the importance of advancing the discussion on appropriate social measures in hearing research.DOI: 10.1080/14992027.2017.1294767",pubmed,28635505,10.1080/14992027.2017.1294767
environmental factors for hearing loss and middle ear disease in alaska native children and adolescents a crosssectional analysis from a cluster randomized trial,"520. Ear Hear. 2023 Jan-Feb 01;44(1):2-9. doi: 10.1097/AUD.0000000000001265. Epub 2022 Aug 23.Environmental Factors for Hearing Loss and Middle Ear Disease in Alaska Native Children and Adolescents: A Cross-Sectional Analysis from a Cluster Randomized Trial.Hicks KL(1), Robler SK(2)(3), Platt A(4)(5), Morton SN(4)(5), Egger JR(5), Emmett SD(5)(6)(7).Author information:(1)Department of Otolaryngology/Head and Neck Surgery, University of North Carolina - Chapel Hill, Chapel Hill, North Carolina, USA.(2)Department of Audiology, Norton Sound Health Corporation, Nome, Alaska, USA.(3)Department of Otolaryngology, Head and Neck Surgery, University of Arkansas for Medical Sciences, Little Rock, Arkansas, USA.(4)Department of Biostatistics and Bioinformatics, Duke University, Durham, North Carolina, USA.(5)Duke Global Health Institute, Durham, North Carolina, USA.(6)Department of Head and Neck Surgery and Communication Sciences, Duke University School of Medicine, Durham, North Carolina, USA.(7)Center for Health Policy and Inequalities Research, Duke University, Durham, North Carolina, USA.OBJECTIVES: Infection-related childhood hearing loss is one of the few preventable chronic health conditions that can affect a child's lifelong trajectory. This study sought to quantify relationships between infection-mediated hearing loss and middle ear disease and environmental factors, such as exposure to wood smoke, cigarette smoke, household crowding, and lack of access to plumbed (running) water, in a northwest region of rural Alaska.DESIGN: This study is a cross-sectional analysis to estimate environmental factors of infection-related hearing loss in children aged 3 to 21 years. School hearing screenings were performed as part of two cluster randomized trials in rural Alaska over two academic years (2017-2018 and 2018-2019). The first available screening for each child was used for this analysis. Sociodemographic questionnaires were completed by parents/guardians upon entry into the study. Multivariable regression was performed to estimate prevalence differences and prevalence ratios (PR). A priori knowledge about the prevalence of middle ear disease and the difficulty inherent in obtaining objective hearing loss data in younger children led to analysis of children by age (3 to 6 years versus 7 years and older) and a separate multiple imputation sensitivity analysis for pure-tone average (PTA)-based infection-related hearing loss measures.RESULTS: A total of 1634 children participated. Hearing loss was present in 11.1% of children sampled based on otoacoustic emission as the primary indicator of hearing loss and was not associated with exposure to cigarette smoke (PR = 1.07; 95% confidence interval [CI], 0.48 to 2.38), use of a wood-burning stove (PR = 0.85; 95% CI, 0.55 to 1.32), number of persons living in the household (PR = 1.06; 95% CI, 0.97 to 1.16), or lack of access to running water (PR = 1.38; 95% CI, 0.80 to 2.39). Using PTA as a secondary indicator of hearing loss also showed no association with environmental factors. Middle ear disease was present in 17.4% of children. There was a higher prevalence of middle ear disease in homes without running water versus those with access to running water (PR = 1.53; 95% CI, 1.03 to 2.27). There was little evidence to support any cumulative effects of environmental factors. Heterogeneity of effect models by age found sample prevalence of hearing loss higher for children aged 3 to 6 years (12.2%; 95% CI, 9.3 to 15.7) compared to children 7 years and older (10.6%; 95% CI, 8.9 to 2.6), as well as for sample prevalence of middle ear disease (22.7%; 95% CI, 18.9 to 26.9 and 15.3%; 95% CI, 13.3 to 17.5, respectively).CONCLUSIONS: Lack of access to running water in the home was associated with increased prevalence of middle ear disease in this rural, Alaska Native population, particularly among younger children (aged 3 to 6 years). There was little evidence in this study that cigarette smoke, wood-burning stoves, and greater numbers of persons in the household were associated with infection-mediated hearing loss or middle ear disease. Future research with larger sample sizes and more sensitive measures of environmental exposure is necessary to further evaluate these relationships. Children who live in homes without access to running water may benefit from earlier and more frequent hearing health visits.Copyright © 2022 The Authors. Ear & Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health.DOI: 10.1097/AUD.0000000000001265PMCID: PMC9780156",pubmed,35998103,10.1097/AUD.0000000000001265
objective diagnosis of tinnitus using restingstate eec big databased support vector machine learning1st world tinnitus congress and xii international tinnitus seminar 2224 may 2017 warsaw,"Although tinnitus is a subjective symptom, it would be highly desirable to diagnose the presence of tinnitus in an objective way. Recently, scientists have developed support vector machine learning (SVML) techniques that can learn to recognize patterns by classifying seen data. By using SVML, based on the known properties learned from the trained data, these algorithms may predict the presence or absence of tinnitus. We therefore combined resting- state quantitative electroencephalography (rs-qEEG) with SVML to develop a brain-based electrophysiological signature for the presence of tinnitus. One hundred and twenty-nine tinnitus patients and 233 healthy controls underwent rs-qEEG measurements for 5 minutes. Using these data as training sets, feature extraction was performed both by principal component analysis (PCA) and by NMF, and a classification model was developed by the linear-kernel SVML with 10-fold cross-validation. The accuracy of classification and the area under curve (AUC) were calculated for each method. As compared with PCA-SVM approach (accuracy 0.85, AUC 0.94), NMF-SVM approach yielded better classification performance (accuracy 0.92, AUC 0.96). When compared to upper-10% baseline activity of the control group, that of the tinnitus group showed relatively higher activity at the left auditory cortex, inferior frontal cortex, dorsal anterior cingulate cortex, and parahippocampus. The classifier developed in the current study distinguished tinnitus or no-tinnitus with high fidelity. Also, the main areas that classified tinnitus or no-.innitus replicated previously reported areas that are involved in the generation of tinnitus or tinnitus-related distress. NMF-SVM-based diagnosis of the presence of tinnitus has yielded an accuracy of 92%. Also, relatively increased baseline activities in the tinnitus group as compared with the control group in areas that were repeatedly found in previous tinnitus research may reconfirm the validity of the current approach. Future study using bigger database may raise the classification accuracy and eventually enable us to develop an objective diagnostic tool of tinnitus.",cinahl,2083389X,
brain network interactions in auditory visual and linguistic processing,"600. Brain Lang. 2004 May;89(2):377-84. doi: 10.1016/S0093-934X(03)00349-3.Brain network interactions in auditory, visual and linguistic processing.Horwitz B(1), Braun AR.Author information:(1)Voice, Speech, Language Branch, National Institute on Deafness and Other Communications Disorders, National Institutes of Health, 9000 Rockville Pike, Bldg. 10, Rm. 6C420, MSC 1591, Bethesda, MD 20892, USA. horwitz@helix.nih.govIn the paper, we discuss the importance of network interactions between brain regions in mediating performance of sensorimotor and cognitive tasks, including those associated with language processing. Functional neuroimaging, especially PET and fMRI, provide data that are obtained essentially simultaneously from much of the brain, and thus are ideal for enabling one to assess interregional functional interactions. Two ways to use these types of data to assess network interactions are presented. First, using PET, we demonstrate that anterior and posterior perisylvian language areas have stronger functional connectivity during spontaneous narrative production than during other less linguistically demanding production tasks. Second, we show how one can use large-scale neural network modeling to relate neural activity to the hemodynamically-based data generated by fMRI and PET. We review two versions of a model of object processing - one for visual and one for auditory objects. The regions comprising the models include primary and secondary sensory cortex, association cortex in the temporal lobe, and prefrontal cortex. Each model incorporates specific assumptions about how neurons in each of these areas function, and how neurons in the different areas are interconnected with each other. Each model is able to perform a delayed match-to-sample task for simple objects (simple shapes for the visual model; tonal contours for the auditory model). We find that the simulated electrical activities in each region are similar to those observed in nonhuman primates performing analogous tasks, and the absolute values of the simulated integrated synaptic activity in each brain region match human fMRI/PET data. Thus, this type of modeling provides a way to understand the neural bases for the sensorimotor and cognitive tasks of interest.DOI: 10.1016/S0093-934X(03)00349-3",pubmed,15068921,10.1016/S0093-934X(03)00349-3
apparent change of masking functions with compressiontype digital hearing aid,"Signal perception ability under conditions of a narrow band masker in subjects with hearing aids was examined using a theoretical model of the auditory nerve fibre (ANF) with a deteriorated tuning curve in addition to measurements of actual masking function in subjects wearing hearing aids. The results obtained indicate that the apparent masking function could be affected by the frequency-gain character as well as by the degree of compression. Usually, the compression-type of amplification with flat and/or high-frequency weighted characteristics improves not only the apparent thresholds but also the apparent masked thresholds under conditions of lower frequency masking. On the other hand, a low-frequency masker amplified by a higher gain with low-frequency weighted amplification could cause larger upward-masking effects on the signal perception of the higher frequency signal in some conditions. The present study may contribute to our understanding of the underlying mechanisms of the effects of different amplification by the aid.",cinahl,1050397,10.1080/010503900750042725
characteristics of realworld signal to noise ratios and speech listening situations of older adults with mild to moderate hearing loss,"324. Ear Hear. 2018 Mar/Apr;39(2):293-304. doi: 10.1097/AUD.0000000000000486.Characteristics of Real-World Signal to Noise Ratios and Speech Listening Situations of Older Adults With Mild to Moderate Hearing Loss.Wu YH(1), Stangl E(1), Chipara O(1), Hasan SS(1), Welhaven A(1), Oleson J(1).Author information:(1)Department of Communication Sciences and Disorders, The University of Iowa, Iowa City, Iowa, USA.OBJECTIVES: The first objective was to determine the relationship between speech level, noise level, and signal to noise ratio (SNR), as well as the distribution of SNR, in real-world situations wherein older adults with hearing loss are listening to speech. The second objective was to develop a set of prototype listening situations (PLSs) that describe the speech level, noise level, SNR, availability of visual cues, and locations of speech and noise sources of typical speech listening situations experienced by these individuals.DESIGN: Twenty older adults with mild to moderate hearing loss carried digital recorders for 5 to 6 weeks to record sounds for 10 hours per day. They also repeatedly completed in situ surveys on smartphones several times per day to report the characteristics of their current environments, including the locations of the primary talker (if they were listening to speech) and noise source (if it was noisy) and the availability of visual cues. For surveys where speech listening was indicated, the corresponding audio recording was examined. Speech-plus-noise and noise-only segments were extracted, and the SNR was estimated using a power subtraction technique. SNRs and the associated survey data were subjected to cluster analysis to develop PLSs.RESULTS: The speech level, noise level, and SNR of 894 listening situations were analyzed to address the first objective. Results suggested that as noise levels increased from 40 to 74 dBA, speech levels systematically increased from 60 to 74 dBA, and SNR decreased from 20 to 0 dB. Most SNRs (62.9%) of the collected recordings were between 2 and 14 dB. Very noisy situations that had SNRs below 0 dB comprised 7.5% of the listening situations. To address the second objective, recordings and survey data from 718 observations were analyzed. Cluster analysis suggested that the participants' daily listening situations could be grouped into 12 clusters (i.e., 12 PLSs). The most frequently occurring PLSs were characterized as having the talker in front of the listener with visual cues available, either in quiet or in diffuse noise. The mean speech level of the PLSs that described quiet situations was 62.8 dBA, and the mean SNR of the PLSs that represented noisy environments was 7.4 dB (speech = 67.9 dBA). A subset of observations (n = 280), which was obtained by excluding the data collected from quiet environments, was further used to develop PLSs that represent noisier situations. From this subset, two PLSs were identified. These two PLSs had lower SNRs (mean = 4.2 dB), but the most frequent situations still involved speech from in front of the listener in diffuse noise with visual cues available.CONCLUSIONS: The present study indicated that visual cues and diffuse noise were exceedingly common in real-world speech listening situations, while environments with negative SNRs were relatively rare. The characteristics of speech level, noise level, and SNR, together with the PLS information reported by the present study, can be useful for researchers aiming to design ecologically valid assessment procedures to estimate real-world speech communicative functions for older adults with hearing loss.DOI: 10.1097/AUD.0000000000000486PMCID: PMC5824438",pubmed,29466265,10.1097/AUD.0000000000000486
a method for enhancing speech and warning signals based on parallel convolutional neural networks in a noisy environment,"1077/1 “Hearing4all.” All authors received travel support from Cochlear Ltd. for meetings. B. Waldmann is an employee of Cochlear Ltd., which provided the declared support.267. Technol Health Care. 2021;29(S1):141-152. doi: 10.3233/THC-218015.A method for enhancing speech and warning signals based on parallel convolutional neural networks in a noisy environment.Kang HL(1)(1), Na SD(2)(1), Kim MN(3).Author information:(1)Department of Medical & Biological Engineering, Graduate School, Kyungpook National University, Daegu 700-422, Korea.(2)Department of Biomedical Engineering, Kyungpook National University Hospital, Daegu 700-422, Korea.(3)Department of Biomedical Engineering, School of Medicine, Kyungpook National University, Daegu 700-422, Korea.BACKGROUND: Digital hearing aids are based on technology that amplifies sound and removes noise according to the frequency of hearing loss in hearing loss patients. However, within the noise removed is a warning sound that alert the listener; the listener may be exposed to danger because the warning sound is not recognized.OBJECTIVE: In this paper, a deep learning model was used to improve these limits and propose a method to distinguish the warning sound in speech signals mixed with noise. In addition, the improved speech and warning sound were derived by removing noise present in the classification sound signals.METHODS: To classify the sound dataset, an adaptive convolution filter that changes according to two signals is proposed. The proposed convolution filter is applied to the PCNNs model to analyze the characteristics of the time and frequency domains of the dataset and classify the presence or absence of warning sound. In addition, the CEDN model was used to improve the intelligibility of the warning and the speech in the signal based on the warning sound classification from the proposed PCNNs model.RESULTS: Experimental results show that the PCNNs model using the proposed multiplicative filters is efficient for analyzing sound signals with complex frequencies. In addition, the CEDN model was used to improve the intelligibility of the warning and the speech in the signal based on the warning sound classification from the proposed PCNNs model.CONVLUSION: We confirmed that the PCNN model with the proposed filter showed the highest training rate, lowest error rate, and the most stable results. In addition, the CEDN model confirmed that speech and warning sounds were recognized, but it was confirmed that there was a limitation in clearly recognizing speech as the noise ratio increased.DOI: 10.3233/THC-218015PMCID: PMC8150607",pubmed,33682754,10.3233/THC-218015
designing softhardware complex for gesture language recognition using neural network methods,"In recent years, the problem of social adaptation of people with disabilities has been actively solved using developments including artificial intelligence methods. The article describes elaboration of a software and hardware complex (SHWC) for recognizing the sign language of disabled people with hearing and speech impairments. The analysis of analogue products for automatic sign language translation is carried out, technical and design requirements for the soft-hardware complex are formulated, the architecture of the SHWC, the functionality of the server and user application subsystems are described. The design and implementation of a subsystem for the collection and processing of photo and video materials with elements of sign language, including static and dynamic fingerprints, gestures, simple phrases, was carried out. The structure of the file data storage and metadata base has been developed. Scenarios and algorithms for sequencing and transforming video data are described. The sequence of data preprocessing when forming a training set using the augmentation method is described. A model for detecting hands in an image is described.",ieee,,10.1109/EMCTECH49634.2020.9261564
clarity2021 challenges machine learning challenges for advancing hearing aid processing,"In recent years, rapid advances in speech technology have been made possible by machine learning challenges such as CHiME, REVERB, Blizzard, and Hurricane. In the Clarity project, the machine learning approach is applied to the problem of hearing aid processing of speech-in-noise, where current technology in enhancing the speech signal for the hearing aid wearer is often ineffective. The scenario is a (simulated) cuboid-shaped living room in which there is a single listener, a single target speaker and a single interferer, which is either a competing talker or domestic noise. All sources are static, the target is always within ±30° azimuth of the listener and at the same elevation, and the interferer is an omnidirectional point source at the same elevation. The target speech comes from an open source 40-speaker British English speech database collected for this purpose. This paper provides a baseline description of the round one Clarity challenges for both enhancement (CEC1) and prediction (CPC1). To the authors' knowledge, these are the first machine learning challenges to consider the problem of hearing aid speech signal processing.  Copyright © 2021 ISCA.",scopus,2-s2.0-85119182231,10.21437/Interspeech.2021-1574
histopathologic observations of the aging gerbil cochlea,"461. Hear Res. 1997 Feb;104(1-2):101-11. doi: 10.1016/s0378-5955(96)00184-0.Histopathologic observations of the aging gerbil cochlea.Adams JC(1), Schulte BA.Author information:(1)Department of Otolaryngology, Massachusetts Eye and Ear Infirmary, Boston 02114, USA.Age-related histopathologic changes were examined in cochleas from 17 gerbils born and kept in a quiet environment until near the end of their life expectancy. Hearing loss varied greatly as did the loss of outer hair cells (OHC). Inner hair cells (IHC) were seldom missing even in cochleas with severe hearing losses. Flask- and spherical-shaped OHCs were frequently seen in the apical turn. Stereocilia were usually present and orderly on OHCs, but the tallest row of stereocilia on IHCs was often disarrayed and sometimes missing. Alterations in supporting cells were sometimes present in regions of extensive OHC loss. Although pillar cells were seldom missing, the nuclei of outer pillar cells were commonly displaced from their normal basal position. The density of radial fibers appeared similar to that in young gerbils except in the apical turn of one old ear where a marked loss of radial fibers occurred without an attendant loss of IHCs. All of the quiet-aged cochleas showed a characteristic clustering of epithelial cells lining the scala media surface of Reissner's membrane. This structural rearrangement was not accompanied by a significant decrease in the total number of cells forming Reissner's membrane and did not appear to be associated with hearing loss. The findings confirm and extend earlier work showing that several different types of cells are susceptible to histopathologic changes in old ears. The extent of histopathologic changes varied widely as did the degree of hearing loss in animals with a restricted genetic background and maintained under carefully controlled environmental conditions. It was not possible, based on these initial findings, to relate specific structural to specific functional changes in the aging cochlea. Further light and electron microscopic analysis of other regions from these aged cochleas may provide more conclusive data.DOI: 10.1016/s0378-5955(96)00184-0",pubmed,9119754,10.1016/s0378-5955(96)00184-0
toward better ear disease diagnosis a multimodal multifusion model using endoscopic images of the tympanic membrane and puretone audiometry,"Chronic otitis media is characterized by recurrent infections, leading to serious complications, such as meningitis, facial palsy, and skull base osteomyelitis. Therefore, active treatment based on early diagnosis is essential. This study developed a multi-modal multi-fusion (MMMF) model that automatically diagnoses ear diseases by applying endoscopic images of the tympanic membrane (TM) and pure-tone audiometry (PTA) data to a deep learning model. The primary aim of the proposed MMMF model is adding 'normal with hearing loss' as a category, and improving the diagnostic accuracy of the conventional four ear diseases: normal, TM perforation, retraction, and cholesteatoma. To this end, the MMMF model was trained on 1,480 endoscopic images of the TM and PTA data to distinguish five ear disease states: normal, TM perforation, retraction, cholesteatoma, and normal (hearing loss). It employs a feature fusion strategy of cross-attention, concatenation, and gated multi-modal units in a multi-modal architecture encompassing a convolutional neural network (CNN) and multi-layer perceptron. We expanded the classification capability to include an additional category, normal (hearing loss), thereby enhancing the diagnostic performance of extant ear disease classification. The MMMF model demonstrated superior performance when implemented with EfficientNet-B7, achieving 92.9% accuracy and 90.9% recall, thereby outpacing the existing feature fusion methods. In addition, five-fold cross-validation experiments were conducted, in which the model consistently demonstrated robust performance when endoscopic images of the TM and PTA data were applied to the deep learning model across all datasets. The proposed MMMF model is the first to include a category of normal ear disease state with hearing loss. The developed model demonstrated superior performance compared to existing CNN models and feature fusion methods. Consequently, this study substantiates the utility of simultaneously applying PTA data and endoscopic images of the TM for the automated diagnosis of ear diseases in clinical settings and validates the usefulness of the multi-fusion method. © 2013 IEEE.",scopus,2-s2.0-85174827245,10.1109/ACCESS.2023.3325346
development of an automatic classifier for the prediction of hearing impairment from industrial noise exposure,"402. J Acoust Soc Am. 2019 Apr;145(4):2388. doi: 10.1121/1.5096643.Development of an automatic classifier for the prediction of hearing impairment from industrial noise exposure.Zhao Y(1), Tian Y(1), Zhang M(2), Li J(1), Qiu W(3).Author information:(1)Key Laboratory for Biomedical Engineering of Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China.(2)Institute of Environmental and Occupational Health, Zhejiang Provincial Center for Disease Control and Prevention, Hangzhou, China.(3)Auditory Research Laboratory, State University of New York at Plattsburgh, Plattsburgh, New York 12901, USA.The ISO-1999 [(2013). International Organization for Standardization, Geneva, Switzerland] standard is the most commonly used approach for estimating noise-induced hearing trauma. However, its insensitivity to noise characteristics limits its practical application. In this study, an automatic classification method using the support vector machine (SVM) was developed to predict hearing impairment in workers exposed to both Gaussian (G) and non-Gaussian (non-G) industrial noises. A recently collected human database (N = 2,110) from industrial workers in China was used in the present study. A statistical metric, kurtosis, was used to characterize the industrial noise. In addition to using all the data as one group, the data were also broken down into the following four subgroups based on the level of kurtosis: G/quasi-G, low-kurtosis, middle-kurtosis, and high-kurtosis groups. The performance of the ISO-1999 and the SVM models was compared over these five groups. The results showed that: (1) The performance of the SVM model significantly outperformed the ISO-1999 model in all five groups. (2) The ISO-1999 model could not properly predict hearing impairment for the high-kurtosis group. Moreover, the ISO-1999 model is likely to underestimate hearing impairment caused by both G and non-G noise exposures. (3) The SVM model is a potential tool to predict hearing impairment caused by diverse noise exposures.DOI: 10.1121/1.5096643",pubmed,31046337,10.1121/1.5096643
hearing aid technology to improve speech intelligibility in noise,"675. Semin Hear. 2021 Aug;42(3):175-185. doi: 10.1055/s-0041-1735174. Epub 2021 Sep 24.Hearing Aid Technology to Improve Speech Intelligibility in Noise.Alexander JM(1).Author information:(1)Department of Speech, Language, and Hearing Sciences, Purdue University, West Lafayette, Indiana.Understanding speech in noise is difficult for individuals with normal hearing and is even more so for individuals with hearing loss. Difficulty understanding speech in noise is one of the primary reasons people seek hearing assistance. Despite amplification, many hearing aid users still struggle to understand speech in noise. In response to this persistent problem, hearing aid manufacturers have invested significantly in developing new solutions. Any solution is not without its tradeoffs, and decisions must be made when optimizing and implementing them. Much of this happens behind the scenes, and casual observers fail to appreciate the nuances of developing new hearing aid technologies. The difficulty of communicating this information to clinicians may hinder the use or the fine-tuning of the various technologies available today. The purpose of this issue of Seminars in Hearing is to educate professionals and students in audiology, hearing science, and engineering about different approaches to combat problems related to environmental and wind noise using technologies that include classification, directional microphones, binaural signal processing, beamformers, motion sensors, and machine learning. To accomplish this purpose, some of the top researchers and engineers from the world's largest hearing aid manufacturers agreed to share their unique insights.The Author(s). This is an open access article published by Thieme under the terms of the Creative Commons Attribution-NonDerivative-NonCommercial License, permitting copying and reproduction so long as the original work is given appropriate credit. Contents may not be used for commercial purposes, or adapted, remixed, transformed or built upon. ( https://creativecommons.org/licenses/by-nc-nd/4.0/ ).DOI: 10.1055/s-0041-1735174PMCID: PMC8463122",pubmed,34594083,10.1055/s-0041-1735174
extension and evaluation of a nearend listening enhancement algorithm for listeners with normal and impaired hearing,"283. J Acoust Soc Am. 2017 Apr;141(4):2526. doi: 10.1121/1.4979591.Extension and evaluation of a near-end listening enhancement algorithm for listeners with normal and impaired hearing.Rennies J(1), Drefs J(1), Hülsmeier D(1), Schepker H(2), Doclo S(2).Author information:(1)Project Group Hearing, Speech and Audio Technology, Fraunhofer Institute for Digital Media Technology IDMT and Cluster of Excellence Hearing4All, D-26129 Oldenburg, Germany.(2)Signal Processing Group, Department of Medical Physics and Acoustics and Cluster of Excellence Hearing4All, University of Oldenburg, D-26111 Oldenburg, Germany.In many applications in which speech is played back via a sound reinforcement system such as public address systems and mobile phones, speech intelligibility is degraded by additive environmental noise. A possible solution to maintain high intelligibility in noise is to pre-process the speech signal based on the estimated noise power at the position of the listener. The previously proposed AdaptDRC algorithm [Schepker, Rennies, and Doclo (2015). J. Acoust. Soc. Am. 138, 2692-2706] applies both frequency shaping and dynamic range compression under an equal-power constraint, where the processing is adaptively controlled by short-term estimates of the speech intelligibility index. Previous evaluations of the algorithm have focused on normal-hearing listeners. In this study, the algorithm was extended with an adaptive gain stage under an equal-peak-power constraint, and evaluated with eleven normal-hearing and ten mildly to moderately hearing-impaired listeners. For normal-hearing listeners, average improvements in speech reception thresholds of about 4 and 8 dB compared to the unprocessed reference condition were measured for the original algorithm and its extension, respectively. For hearing-impaired listeners, the average improvements were about 2 and 6 dB, indicating that the relative improvement due to the proposed adaptive gain stage was larger for these listeners than the benefit of the original processing stages.DOI: 10.1121/1.4979591",pubmed,28464693,10.1121/1.4979591
auditory cortical images of cochlearimplant stimuli coding of stimulus channel and current level,"440. J Neurophysiol. 2002 Jan;87(1):493-507. doi: 10.1152/jn.00211.2001.Auditory cortical images of cochlear-implant stimuli: coding of stimulus channel and current level.Middlebrooks JC(1), Bierer JA.Author information:(1)Kresge Hearing Research Institute (Department of Otorhinolaryngology) and Neuroscience Program, University of Michigan, Ann Arbor, Michigan 48109-0506, USA. jmidd@umich.eduThis study quantified the accuracy with which populations of neurons in the auditory cortex can represent aspects of electrical cochlear stimuli presented through a cochlear implant. We tested the accuracy of coding of the place of stimulation (i.e., identification of the active stimulation channel) and of the stimulus current level. Physiological data came from the companion study, which recorded spike activity of neurons simultaneously from 16 sites along the tonotopic axis of the guinea pig's auditory cortex. In that study, cochlear electrical stimuli were presented to acutely deafened animals through a 6-electrode animal version of the 22-electrode Nucleus banded electrode array (Cochlear). Cochlear electrode configurations consisted of monopolar (MP), bipolar (BP + N) with N inactive electrodes between the active and return electrodes (0 < or = N < or = 3), tripolar (TP) with one active electrode and two flanking return electrodes, and common ground (CG) with one active electrode and as many as five return electrodes. In the present analysis, an artificial neural network was trained to recognize spatiotemporal patterns of cortical activity in response to single presentations of particular stimuli and, thereby, to identify those stimuli. The accuracy of pair-wise discrimination of stimulation channels or of current levels was represented by the discrimination index, d', where d' = 1 was taken as threshold. In many cases, the threshold for discrimination of place of cochlear stimulation was < 0.75 mm, and the threshold for discrimination of current levels was < 1 dB. Cochlear electrode configurations varied in the accuracy with which they signaled to the auditory cortex the place of cochlear stimulation. The BP + N and TP configurations provided considerably greater sensitivity to place of stimulation than did the MP configuration. The TP configuration maintained accurate signaling of place of stimulation up to the highest current levels, whereas sensitivity was degraded at high current levels in BP + N configurations. Electrode configurations also varied in the dynamic range over which they signaled stimulus current level. Dynamic ranges were widest for the BP + 0 configuration and narrowest for the TP configuration. That is, the configuration that showed the most accurate signaling of cochlear place of stimulation (TP) showed the most restricted dynamic range for signaling of current level. These results suggest that the choice of the optimal electrode configuration for use by human cochlear-prosthesis users would depend on the particular demands of the speech-processing strategy that is to be employed.DOI: 10.1152/jn.00211.2001",pubmed,11784765,10.1152/jn.00211.2001
brief review of recent researches in speech enhancement from filters to neural networks,"According to the World Health Organization, more and more people will suffer from hearing loss in the future. Therefore, there will be greater demand for the output and technology of hearing aids. Under the help of artificial intelligence, the technology of smart hearing aids will also become more intelligent, so that the wearer can get a better experience. This paper mainly studies the related problems of speech signal processing in intelligent digital hearing aids. This article focuses on the speech enhancement in digital hearing aids. First, this work studies the application of filter in speech enhancement technology, mainly introduces the Wiener filter algorithm using the minimum mean square error criterion; the Kalman filter algorithm that can solve discrete signals; spectral subtraction based on multi-window spectrum estimation. Secondly, this paper studies some specific methods of deep learning technology in speech enhancement, mainly introduces DNN-based speech enhancement method, deep learning-based auditory cepstrum coefficient speech enhancement algorithm, and AE-CGAN-based speech enhancement algorithm. Finally, this paper studies the related problems of acoustic scene classification, divides the listening environment into Gaussian white noise, impact noise, and music noise, and explains the solutions for each listening environment. © 2020 IEEE",scopus,2-s2.0-85098843429,10.1109/CDS49703.2020.00059
real time conversion of american sign language to text with emotion using machine learning,"Auditory impairment or hearing loss is a major problem in today's human population. Sign language helps hearing-impaired people to live their social life without much difficulty. The latest technology used in detecting sign language connects them to the rest of the world. Sign language recognition and conversion to the text based on the movement of hands and the shape formed by the fingers is a complex system. The solutions using machine learning give significant success for this complex system. This paper mainly focuses on developing a system for recognizing the different hand signs in American Sign Language and their emotions simultaneously in real-time and converting them into text. The system resolves the need for a translator to bridge the gap between a sign language based user and a non-sign language-based user. Most state-of-the-art technology involves CNN models with image pixels by identifying specific key point coordinates on the face/hand obtained. Using the latest technologies like MediaPipe, an improved CNN model is developed based on the distances between each unique identified vital point. The customized model able to achieve 80 percentage of accuracy for the live image.  © 2022 IEEE.",scopus,2-s2.0-85146433127,10.1109/I-SMAC55078.2022.9987362
a semisupervised support vector machine model for predicting the language outcomes following cochlear implantation based on preimplant brain fmri imaging,"Introduction: We developed a machine learning model to predict whether or not a cochlear implant (CI) candidate will develop effective language skills within 2 years after the CI surgery by using the pre-implant brain fMRI data from the candidate. Methods: The language performance was measured 2 years after the CI surgery by the Clinical Evaluation of Language Fundamentals-Preschool, Second Edition (CELF-P2). Based on the CELF-P2 scores, the CI recipients were designated as either effective or ineffective CI users. For feature extraction from the fMRI data, we constructed contrast maps using the general linear model, and then utilized the Bag-of-Words (BoW) approach that we previously published to convert the contrast maps into feature vectors. We trained both supervised models and semi-supervised models to classify CI users as effective or ineffective. Results: Compared with the conventional feature extraction approach, which used each single voxel as a feature, our BoW approach gave rise to much better performance for the classification of effective versus ineffective CI users. The semi-supervised model with the feature set extracted by the BoW approach from the contrast of speech versus silence achieved a leave-one-out cross-validation AUC as high as 0.97. Recursive feature elimination unexpectedly revealed that two features were sufficient to provide highly accurate classification of effective versus ineffective CI users based on our current dataset. Conclusion: We have validated the hypothesis that pre-implant cortical activation patterns revealed by fMRI during infancy correlate with language performance 2 years after cochlear implantation. The two brain regions highlighted by our classifier are potential biomarkers for the prediction of CI outcomes. Our study also demonstrated the superiority of the semi-supervised model over the supervised model. It is always worthwhile to try a semi-supervised model when unlabeled data are available. © 2015 Published by Wiley Periodicals, Inc.",scopus,2-s2.0-84954364514,10.1002/brb3.391
changing population of neurons and glia in the human cochlear nucleus with progressive age  a stereological study,"Introduction: The human cochlear nucleus (CN) is populated by morphologically diverse types of neurons that contribute specifically in the formation of the complex functional networks in the auditory pathway. On the basis of cytoarchitecture and topography different types of neurons can be identified in the CN. The present study was undertaken to investigate the morphological parameters of neurons and glia of the human CN with aging. Methods: Forty-one brainstems (Birth-90 years) from cadaveric donors were collected from the mortuary of the All India Institute of Medical Sciences (AIIMS), New Delhi, with ethical committee permission. They were grouped into nine decades and processed for light microscopy and morphometry. Hierarchical clustering was done to classify the neuron population according to their neuronal and nuclear area into different clusters. Results: There was a gradual increase in the mean neuronal and neuronal nucleus volume from decade 1 to 3. Decade 1 had minimum and 3 had maximum nuclear volume and neuron number respectively. An increase in total glial population was observed in decade 9. Eight neuron clusters were identified which were present in the decades 2 and 3 whereas decades 1 and 4 had seven, decades 5-8 had six and decade 9 had four clusters respectively. Discussion: Major changes were observed within the clusters from middle to old age, especially after decade 5. This may be useful in explaining the vulnerability of some specific subpopulation of neurons more than others and understanding the pathophysiology of altered hearing loss with age. © 2014 Anatomical Society of India.",scopus,2-s2.0-84920617424,10.1016/j.jasi.2014.11.002
neural networks applied to retrocochlear diagnosis,"Methodologies have been developed, based on insights from signal detection theory, to evaluate quantitatively the diagnostic performance of tests. Several studies have demonstrated that, in fact, performance of a test battery can be inferior to the best of the tests it includes. These studies have been quite persuasive in damping enthusiasm for the test battery approach. Because the results of all tests in a battery were weighted equally in these studies, it is not surprising that an individual test with good sensitivity and specificity is more effective diagnostically than a combination of tests with poorer sensitivity and specificity. The authors of many of these studies were well aware of the limitations of this approach. In the present study, neural networks were applied to evaluate audiological tests used to predict retrocochlear pathology by differentially weighting the results of the tests in the battery. This technique avoids some of the limitations of previous approaches. Of the audiological tests evaluated in the present analysis, the superiority of the auditory brainstem evoked response (ABR) in predicting retrocochlear disease was again demonstrated. However, the results also demonstrated that identification accuracy could be improved by combining the ABR with other tests (in this case contralateral acoustic reflex at 2000 Hz, ipsilateral acoustic reflex at 2000 Hz, tone decay, and word recognition score). Further, it was demonstrated that performance could be improved over that obtained using dichotomous test measures (i.e., positive or negative presence of pathology) by using raw test measures in conjunction with ABR.",cinahl,10924388,10.1044/jslhr.4202.287
disrupted topological organization of restingstate functional brain networks in agerelated hearing loss references,"Purpose: Age-related hearing loss (ARHL), associated with the function of speech perception decreases characterized by bilateral sensorineural hearing loss at high frequencies, has become an increasingly critical public health problem. This study aimed to investigate the topological features of the brain functional network and structural dysfunction of the central nervous system in ARHL using graph theory. Methods: Forty-six patients with ARHL and forty-five age, sex, and education-matched healthy controls were recruited to undergo a resting-state functional magnetic resonance imaging (fMRI) scan in this study. Graph theory was applied to analyze the topological properties of the functional connectomes by studying the local and global organization of neural networks. Results: Compared with healthy controls, the patient group showed increased local efficiency (Eloc) and clustering coefficient (Cp) of the small-world network. Besides, the degree centrality (Dc) and nodal efficiency (Ne) values of the left inferior occipital gyrus (IOG) in the patient group showed a decrease in contrast with the healthy control group. In addition, the intra-modular interaction of the occipital lobe module and the inter-modular interaction of the parietal occipital module decreased in the patient group, which was positively correlated with Dc and Ne. The intra-modular interaction of the occipital lobe module decreased in the patient group, which was negatively correlated with the Eloc. Conclusion: Based on fMRI and graph theory, we indicate the aberrant small-world network topology in ARHL and dysfunctional interaction of the occipital lobe and parietal lobe, emphasizing the importance of dysfunctional left IOG. These results suggest that early diagnosis and treatment of patients with ARHL is necessary, which can avoid the transformation of brain topology and decreased brain function. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc21&DO=10.3389%2ffnagi.2022.907070
development and evaluation of a novel method for adult hearing screening towards a dedicated smartphone app,"Towards implementation of adult hearing screening tests that can be delivered via a mobile app, we have recently designed a novel speech-in-noise test based on the following requirements: user-operated, fast, reliable, accurate, viable for use by listeners of unknown native language and viable for testing at a distance. This study addresses specific models to (i) investigate the ability of the test to identify ears with mild hearing loss using machine learning; and (ii) address the range of the output levels generated using different transducers. Our results demonstrate that the test classification performance using decision tree models is in line with the performance of validated, language-dependent speech-in-noise tests. We observed, on average, 0.75 accuracy, 0.64 sensitivity and 0.81 specificity. Regarding the analysis of output levels, we demonstrated substantial variability of transducers’ characteristics and dynamic range, with headphones yielding higher output levels compared to earphones. These findings confirm the importance of a self-adjusted volume option. These results also suggest that earphones may not be suitable for test execution as the output levels may be relatively low, particularly for subjects with hearing loss or for those who skip the volume adjustment step. Further research is needed to fully address test performance, e.g. testing a larger sample of subjects, addressing different classification approaches, and characterizing test reliability in varying conditions using different devices and transducers. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.",scopus,2-s2.0-85102772915,10.1007/978-3-030-69963-5_1
audiometric configurations of hearing impaired children in hong kong implications for amplification,"457. Disabil Rehabil. 2002 Nov 20;24(17):904-13. doi: 10.1080/09638280210148602.Audiometric configurations of hearing impaired children in Hong Kong: implications for amplification.Yuen KC(1), McPherson B.Author information:(1)Department of Speech and Hearing Sciences, University of Hong Kong, Hong Kong, China.PURPOSE: Children with hearing loss who require special school placement may have a wide range of audiometric configurations. Since such children will vary in auditory status their amplification requirements may also be diverse. This study examined the audiological records of 231 children attending four schools for hearing impaired children in Hong Kong to gain an understanding of common audiometric patterns found in the school children and their auditory rehabilitation needs.METHOD: Data on the children's aetiology of hearing loss, hearing status, tympanometric findings and the electroacoustic characteristics of their hearing aids were obtained. For 424 children's ears considered having essentially sensorineural hearing loss, k-means cluster analysis methods were used to categorize audiometric configuration groups.RESULTS: Cluster analysis that indicated that five distinct audiometric configurations could be found among the school children. Different clusters contained children who had differing amplification needs. The study analysed a number of parameters to check fitting outcomes, including average prescribed gain, frequency-specific measured versus prescribed gain, prescribed frequency response, measured versus prescribed frequency response and the predicted aided thresholds for the children.CONCLUSION: The amplification needs associated with these five configurations, including recommended prescription gain, maximum power output and possible signal processing strategies, were considered. The clustering algorithm approach proved useful as a means of grouping distinctive audiometric profiles.DOI: 10.1080/09638280210148602",pubmed,12519486,10.1080/09638280210148602
preschool vitamin a supplementation middle ear infection and young adult hearing loss in nepaldp   2009,"Background. Vitamin A deficiency is associated with risk of chronic otitis media, the leading cause of preventable childhood hearing loss in the world. Despite the co-existence of undernutrition and middle ear infections in many disadvantaged populations, little is known about their association with hearing loss and no studies have assessed the efficacy of vitamin A supplementation on attenuating the risk of hearing loss. Methods. From 2006 to 2008, we assessed 3,628 young adults 16-23 years of age who, as preschool children (1989-1991), participated in a cluster-randomized, double-masked, placebo-controlled vitamin A supplementation trial in southern Nepal. In the original trial, anthropometric measures (height or length, weight, mid-upper arm circumference [MUAC]) and one-week parental morbidity reports, including ear discharge (external ear otorrhea), were collected at the time of dosing every four months over a 16-month period. Household socioeconomic status (SES) variables were also recorded. In the follow-up health survey 17 years later, audiometric, ear health and tympanometry evaluations were performed in addition to nutritional, other health and socioeconomic assessments. Hearing loss was defined as both failing to hear a 30dB screening tone at 0.5, 1, 2, 4 and 8 kHz and exhibiting an average air conduction threshold value greater than 30 dB in the worse ear across the frequencies 0.5, 1, 2 and 4 kHz. Results. The prevalence of hearing loss in this population of older adolescents and young adults was 6.2% (95% confidence interval [CI]: 5.6-6.7%). This prevalence figure suggests that there are approximately 30,840 young people 16 to 23 years of age in rural Nepal with test-based hearing impairment. Those with hearing loss were 6- to 8-fold more likely than their normal hearing peers to report problems in communicating with others. Ear discharge in the preschool years was associated, in a dose-response manner, with hearing loss later in life. The odds ratio (OR) for being hearing impaired as young adults ranged from 1.9 to 19.6 among subjects reporting 1 to 7 weekly episodes of ear discharge in their preschool years compared to individuals reporting no episodes. The OR for hearing loss was 5.14 (95% CI: 3.54-7.46) for any preschool episode of preschool ear discharge compared to being free of apparent ear infection; an increase in risk that was concentrated in subjects with abnormal tympanometry in young adulthood. Preschool supplementation with 200,000 IU of vitamin A every four months was associated with a non-significant 18% reduction in hearing loss relative to controls (OR = 0.82; 95% CI: 0.61-1.09). However, among subjects who had reported any preschool episode of ear discharge, vitamin A supplementation reduced the risk of hearing loss by 37% (OR = 0.63; 95% CI: 0.40-0.98), an effect that persisted after adjusting for age, sex, and childhood SES. Both stunting and wasting, based on indicators less than -2 Z scores, in the preschool years were associated with increased risk of hearing loss. Children who were underweight or wasted, but not stunted, in the preschool years were more likely to have middle ear dysfunction, as measured by tympanometry, in young adulthood. Gender, age and socioeconomic status in early childhood were not associated with young adult hearing loss. Conclusion. Hearing loss is a public health problem among young adults living in rural, southern Nepal. The condition is associated with the frequency of ear infection and the severity of undernutrition in the preschool years. Periodically supplementing children with high-potency vitamin A in the preschool years reduced the risk of permanent hearing loss by early adulthood, particularly among subjects with prospectively documented suppurative ear infections in early childhood. These findings suggest that periodic vitamin A supplementati... (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc8&AN=2009-99200-531
predictive sensitivity and concordance of machinelearning tools for diagnosing dfna9 in a large series of ppro51ser variant carriers in the cochgene,"310. Otol Neurotol. 2021 Jun 1;42(5):671-677. doi: 10.1097/MAO.0000000000003028.Predictive Sensitivity and Concordance of Machine-learning Tools for Diagnosing DFNA9 in a Large Series of p.Pro51Ser Variant Carriers in the COCH-gene.Salah M(1)(2), de Varebeke SJ(1)(3), Fransen E(4)(5), Topsakal V(2)(3), Van Camp G(5), Van Rompaey V(2)(3).Author information:(1)Department Otorhinolaryngology and Head and Neck Surgery, Jessa Hospital, Hasselt.(2)Department of Otorhinolaryngology, University Hospital Antwerp.(3)Department of Translational Neurosciences, Faculty of Medicine and Health Sciences.(4)StatUa Center for Statistics.(5)Center for medical genetics, University of Antwerp, Edegem, Belgium.OBJECTIVE: In this study we aimed to evaluate the predictive cross-sectional sensitivity and longitudinal concordance of a machine-learning algorithm in a series of genetically confirmed p.(Pro51Ser) variant carriers (DFNA9).STUDY DESIGN: Cross-sectional study.SETTING: Tertiary and secondary referral center.PATIENTS: Audiograms of 111 subjects with the p.(Pro51Ser) mutation in the COCH-gene were analyzed cross-sectionally. A subset of 17 subjects with repeated audiograms were used for longitudinal analysis.INTERVENTIONS: All audiological thresholds were run through the web-based AudioGene v4.0 software.MAIN OUTCOME MEASURES: Sensitivity for accurate prediction of DFNA9 for cross-sectional data and concordance of correct prediction for longitudinal auditory data.RESULTS: DFNA9 was predicted with a sensitivity of 93.7% in a series of 222 cross-sectionally collected audiological thresholds (76.1% as first gene locus). When using the hearing thresholds of the best ear, the sensitivity was 94.6%. The sensitivity was significantly higher in DFNA9 patients aged younger than 40 and aged 60 years or older, compared to the age group of 40 to 59 years, with resp. 97.6% (p < 0.0001) and 98.8% (p < 0.0001) accurate predictions. An average concordance of 91.6% was found to show the same response in all successive longitudinal audiometric data per patient.CONCLUSIONS: Audioprofiling software can accurately predict DFNA9 in an area with a high prevalence of confirmed carriers of the p.(Pro51Ser) variant in the COCH-gene. This algorithm yields high promises for helping clinicians in directing genetic testing in case of a strong family history of progressive hearing loss, especially for very young and old carriers.Copyright © 2021, Otology & Neurotology, Inc.DOI: 10.1097/MAO.0000000000003028",pubmed,33492061,10.1097/MAO.0000000000003028
estimating hearing aid fitting presets with machine learningbased clustering strategies,"796. JASA Express Lett. 2021 Nov;1(11):115204. doi: 10.1121/10.0007149. Epub 2021 Nov 19.Estimating hearing aid fitting presets with machine learning-based clustering strategies.Belitz C(1), Ali H(1), Hansen JHL(1).Author information:(1)Center for Robust Speech Systems, The University of Texas at Dallas, Richardson, Texas, 75075 USA.Although there exist nearly 35 × 106 hearing impaired people in the U.S., only an estimated 25% use hearing aids (HA), while others elect not to use prescribed HAs. Lack of HA acceptance can be attributed to several factors including (i) performance variability in diverse environments, (ii) time-to-convergence for best HA operating configuration, (iii) unrealistic expectations, and (iv) cost/insurance. This study examines a nationwide dataset of pure-tone audiograms and HA fitting configurations. An overview of data characteristics is presented, followed by use of machine learning clustering to suggest ways of obtaining effective starting configurations, thereby reducing time-to-convergence to improve HA retention.DOI: 10.1121/10.0007149PMCID: PMC9245508",pubmed,35784455,10.1121/10.0007149
desktop based speech recognition for hearing and visually impaired using machine learning,"Using Speech acknowledgement with several speakers, a PC-based application is proposed in this study. An informal ID is shown on the PC screen next to a speech-to-message change text yield, indicating that the recognised speaker has been identified. Three features are included in the framework that has been proposed: A client-server model and multi-stringing notion are used to transmit and collect the modified speech over to a message in a client-server model and a client-server model. The system consists of PCs that can transmit and acquire data, a PC programme that connects various devices through WiFi, and a receiver that receives spoken input from the customers. Every one of the functionalities is executed as a PC application with an easy to understand graphical client interface (GUI). There are two modes of operation for the application, one near and one far. Other than in a case when there are speakers within earshot, the customer chooses a location that is either near or remote. Individuals who are deaf or hard of hearing are the primary focus of the proposed structure. Despite the fact that portable hearing aid technology has been much improved, it relies on an obstructed individual's internal, exterior, and center ear, as well as their hearing nerve, to aid in proper hearing. The portable amplifier isn't functional if the nerve of hearing is damaged as well. To assist the meeting impaired, the suggested framework provides Speech as text and recognises the speakers in question. It accordingly forestalls weakness brought about by listening exertion and stress. This prompts expanded use of the debilitated people. The proposed assistive framework likewise causes it feasible for typical individuals who don't know to communicate through signing to address hearing debilitation. This makes the existence of hearing disabled individuals more satisfied which works on their satisfaction.",ieee,,10.1109/ICAAIC53929.2022.9793245
handheld device based personal auditory training system to hearing loss,"The assistive hearing devices are the only aids to help subjects with hearing loss to use their residual hearing. However, the performance of those devices is closely dependent on auditory training. To develop handheld devices based personal auditory training system with perceptional discrimination analysis and automatic test item generation is very helpful for subjects with hearing loss. Besides, it would ease the burden of speech-language pathologists in developing a personal auditory training. In this study, the mel-frequency cepstrum coefficients and automatic speech recognition are applied to objectively estimate the phonemic confusions. For reducing computational complex, multidimensional scaling is then used to transfer the phonemic confusions into a Euclidean space. Thus, a suitable training material could be automatically generated by simple random process. Finally, the Android based mobile phones are selected as a platform for auditory training. It is convenient for subjects to use the auditory training system. The experimental results show that the average score of mean opinion score is 3.73, which means that the system is very useful. © 2013 IEEE.",scopus,2-s2.0-84886703017,10.1109/CIRAT.2013.6613818
enhancing the sensitivity of the envelopefollowing response for cochlear synaptopathy screening in humans the role of stimulus envelope,"360. Hear Res. 2021 Feb;400:108132. doi: 10.1016/j.heares.2020.108132. Epub 2020 Dec 1.Enhancing the sensitivity of the envelope-following response for cochlear synaptopathy screening in humans: The role of stimulus envelope.Vasilkov V(1), Garrett M(2), Mauermann M(2), Verhulst S(3).Author information:(1)Hearing Technology @ WAVES, Department of Information Technology, Ghent University, Technologiepark 126, Zwijnaarde 9052, Belgium.(2)Medizinische Physik and Cluster of Excellence ""Hearing4all"", Department of Medical Physics and Acoustics, Carl von Ossietzky University of Oldenburg, Carl-von-Ossietzky-Straße 9-11, Oldenburg, 26129, Germany.(3)Hearing Technology @ WAVES, Department of Information Technology, Ghent University, Technologiepark 126, Zwijnaarde 9052, Belgium. Electronic address: s.verhulst@ugent.be.Auditory de-afferentation, a permanent reduction in the number of inner-hair-cells and auditory-nerve synapses due to cochlear damage or synaptopathy, can reliably be quantified using temporal bone histology and immunostaining. However, there is an urgent need for non-invasive markers of synaptopathy to study its perceptual consequences in live humans and to develop effective therapeutic interventions. While animal studies have identified candidate auditory-evoked-potential (AEP) markers for synaptopathy, their interpretation in humans has suffered from translational issues related to neural generator differences, unknown hearing-damage histopathologies or lack of measurement sensitivity. To render AEP-based markers of synaptopathy more sensitive and differential to the synaptopathy aspect of sensorineural hearing loss, we followed a combined computational and experimental approach. Starting from the known characteristics of auditory-nerve physiology, we optimized the stimulus envelope to stimulate the available auditory-nerve population optimally and synchronously to generate strong envelope-following-responses (EFRs). We further used model simulations to explore which stimuli evoked a response that was sensitive to synaptopathy, while being maximally insensitive to possible co-existing outer-hair-cell pathologies. We compared the model-predicted trends to AEPs recorded in younger and older listeners (N=44, 24f) who had normal or impaired audiograms with suspected age-related synaptopathy in the older cohort. We conclude that optimal stimulation paradigms for EFR-based quantification of synaptopathy should have sharply rising envelope shapes, a minimal plateau duration of 1.7-2.1 ms for a 120-Hz modulation rate, and inter-peak intervals which contain near-zero amplitudes. From our recordings, the optimal EFR-evoking stimulus had a rectangular envelope shape with a 25% duty cycle and a 95% modulation depth. Older listeners with normal or impaired audiometric thresholds showed significantly reduced EFRs, which were consistent with how (age-induced) synaptopathy affected these responses in the model.Copyright © 2020. Published by Elsevier B.V.DOI: 10.1016/j.heares.2020.108132",pubmed,33333426,10.1016/j.heares.2020.108132
a hybrid deep learning approach to identify preventable childhood hearing loss,"42. Ear Hear. 2023 Sep-Oct 01;44(5):1262-1270. doi: 10.1097/AUD.0000000000001380. Epub 2023 Jun 15.A Hybrid Deep Learning Approach to Identify Preventable Childhood Hearing Loss.Jin FQ(1)(2), Huang O(1)(2), Kleindienst Robler S(3)(4), Morton S(5), Platt A(5)(6), Egger JR(5), Emmett SD(5)(7), Palmeri ML(1).Author information:(1)Department of Biomedical Engineering, Duke University, Durham, North Carolina, USA.(2)These Authors contributed equally to this work.(3)Department of Audiology, Norton Sound Health Corporation, Nome, Alaska, USA.(4)Department of Otolaryngology-Head and Neck Surgery, University of Arkansas for Medical Sciences, Little Rock, Arkansas, USA.(5)Duke Global Health Institute, Durham, North Carolina, USA.(6)Department of Biostatistics and Bioinformatics, Duke University School of Medicine, Durham, North Carolina, USA.(7)Department of Head and Neck Surgery and Communication Sciences, Duke University School of Medicine, Durham, North Carolina, USA.OBJECTIVE: Childhood hearing loss has well-known, lifelong consequences. Infection-related hearing loss disproportionately affects underserved communities yet can be prevented with early identification and treatment. This study evaluates the utility of machine learning in automating tympanogram classifications of the middle ear to facilitate layperson-guided tympanometry in resource-constrained communities.DESIGN: Diagnostic performance of a hybrid deep learning model for classifying narrow-band tympanometry tracings was evaluated. Using 10-fold cross-validation, a machine learning model was trained and evaluated on 4810 pairs of tympanometry tracings acquired by an audiologist and layperson. The model was trained to classify tracings into types A (normal), B (effusion or perforation), and C (retraction), with the audiologist interpretation serving as reference standard. Tympanometry data were collected from 1635 children from October 10, 2017, to March 28, 2019, from two previous cluster-randomized hearing screening trials (NCT03309553, NCT03662256). Participants were school-aged children from an underserved population in rural Alaska with a high prevalence of infection-related hearing loss. Two-level classification performance statistics were calculated by treating type A as pass and types B and C as refer.RESULTS: For layperson-acquired data, the machine-learning model achieved a sensitivity of 95.2% (93.3, 97.1), specificity of 92.3% (91.5, 93.1), and area under curve of 0.968 (0.955, 0.978). The model's sensitivity was greater than that of the tympanometer's built-in classifier [79.2% (75.5, 82.8)] and a decision tree based on clinically recommended normative values [56.9% (52.4, 61.3)]. For audiologist-acquired data, the model achieved a higher AUC of 0.987 (0.980, 0.993), had an equivalent sensitivity of 95.2 (93.3, 97.1), and a higher specificity of 97.7 (97.3, 98.2).CONCLUSIONS: Machine learning can detect middle ear disease with comparable performance to an audiologist using tympanograms acquired either by an audiologist or a layperson. Automated classification enables the use of layperson-guided tympanometry in hearing screening programs in rural and underserved communities, where early detection of treatable pathology in children is crucial to prevent the lifelong adverse effects of childhood hearing loss.Copyright © 2023 The Authors. Ear & Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health, Inc.DOI: 10.1097/AUD.0000000000001380PMCID: PMC10426782",pubmed,37318215,10.1097/AUD.0000000000001380
experimentalneuromodeling framework for understanding auditory object processing integrating data across multiple scales,"213. J Physiol Paris. 2006 Jul-Sep;100(1-3):133-41. doi: 10.1016/j.jphysparis.2006.09.006. Epub 2006 Oct 31.Experimental-neuromodeling framework for understanding auditory object processing: integrating data across multiple scales.Husain FT(1), Horwitz B.Author information:(1)Brain Imaging and Modeling Section, National Institute on Deafness and Other Communication Disorders, National Institutes of Health, Building 10, Rm 8S235-D, 9000 Rockville Pike, Bethesda, MD 20892, USA. husainf@mail.nih.govIn this article, we review a combined experimental-neuromodeling framework for understanding brain function with a specific application to auditory object processing. Within this framework, a model is constructed using the best available experimental data and is used to make predictions. The predictions are verified by conducting specific or directed experiments and the resulting data are matched with the simulated data. The model is refined or tested on new data and generates new predictions. The predictions in turn lead to better-focused experiments. The auditory object processing model was constructed using available neurophysiological and neuroanatomical data from mammalian studies of auditory object processing in the cortex. Auditory objects are brief sounds such as syllables, words, melodic fragments, etc. The model can simultaneously simulate neuronal activity at a columnar level and neuroimaging activity at a systems level while processing frequency-modulated tones in a delayed-match-to-sample task. The simulated neuroimaging activity was quantitatively matched with neuroimaging data obtained from experiments; both the simulations and the experiments used similar tasks, sounds, and other experimental parameters. We then used the model to investigate the neural bases of the auditory continuity illusion, a type of perceptual grouping phenomenon, without changing any of its parameters. Perceptual grouping enables the auditory system to integrate brief, disparate sounds into cohesive perceptual units. The neural mechanisms underlying auditory continuity illusion have not been studied extensively with conventional neuroimaging or electrophysiological techniques. Our modeling results agree with behavioral studies in humans and an electrophysiological study in cats. The results predict a particular set of bottom-up cortical processing mechanisms that implement perceptual grouping, and also attest to the robustness of our model.DOI: 10.1016/j.jphysparis.2006.09.006PMCID: PMC1941673",pubmed,17079121,10.1016/j.jphysparis.2006.09.006
phenomenology demography and diagnosis in late paraphrenia,"852. Psychol Med. 1994 May;24(2):397-410. doi: 10.1017/s0033291700027379.Phenomenology, demography and diagnosis in late paraphrenia.Howard R(1), Almeida O, Levy R.Author information:(1)Section of Old Age Psychiatry, Institute of Psychiatry, London.One hundred and one patients with late paraphrenia were examined using the Present State Examination. The established high prevalence rates of female gender, the unmarried state and sensory impairment were confirmed. All of the symptoms of schizophrenia, with the exception of formal thought disorder, were found in the subjects with approximately the same prevalence as reported in schizophrenics with a symptom onset in younger life. The presence of visual hallucinosis was significantly associated with visual impairment, but the same association was not found between auditory hallucinations and deafness. Mean age at onset of symptoms was high at 74.1 years. Using ICD-10 diagnostic criteria the patients were categorized as schizophrenia (61.4%), delusional disorder (30.7%) and schizoaffective disorder (7.9%). Patients in these diagnostic categories differed in their pre-morbid IQ estimations, current cognitive state measured by the Mini-Mental State Examination and in the number of scored positive psychotic PSE symptoms and their systematization of and preoccupation with delusions and hallucinations. There were no significant differences between the patients in the ICD-10 schizophrenia and delusional disorder groups in terms of age at symptom onset, sex ratio, response to treatment, being unmarried, the presence of insight or sensory impairment. The high degree of clinical similarity between patients with late paraphrenia combined with the inability of ICD-10 to define diagnostic subgroups that correspond to patient clusters derived from clinical symptoms or which are meaningfully different from each other in terms of demographic and prognostic factors provide a strong argument for the retention of late paraphrenia as the most appropriate diagnosis for such patients.DOI: 10.1017/s0033291700027379",pubmed,8084935,10.1017/s0033291700027379
contributions and limitations of using machine learning to predict noiseinduced hearing loss,"10. Int Arch Occup Environ Health. 2021 Jul;94(5):1097-1111. doi: 10.1007/s00420-020-01648-w. Epub 2021 Jan 25.Contributions and limitations of using machine learning to predict noise-induced hearing loss.Chen F(#)(1), Cao Z(#)(2), Grais EM(1), Zhao F(3)(4).Author information:(1)Centre for Speech and Language Therapy and Hearing Science, Cardiff School of Sport and Health Sciences, Cardiff Metropolitan University, Cardiff, UK.(2)Center for Rehabilitative Auditory Research, Guizhou Provincial People's Hospital, Guiyang, China.(3)Centre for Speech and Language Therapy and Hearing Science, Cardiff School of Sport and Health Sciences, Cardiff Metropolitan University, Cardiff, UK. fzhao@cardiffmet.ac.uk.(4)Department of Hearing and Speech Science, Xinhua College, Sun Yat-Sen University, Guangzhou, China. fzhao@cardiffmet.ac.uk.(#)Contributed equallyPURPOSE: Noise-induced hearing loss (NIHL) is a global issue that impacts people's life and health. The current review aims to clarify the contributions and limitations of applying machine learning (ML) to predict NIHL by analyzing the performance of different ML techniques and the procedure of model construction.METHODS: The authors searched PubMed, EMBASE and Scopus on November 26, 2020.RESULTS: Eight studies were recruited in the current review following defined inclusion and exclusion criteria. Sample size in the selected studies ranged between 150 and 10,567. The most popular models were artificial neural networks (n = 4), random forests (n = 3) and support vector machines (n = 3). Features mostly correlated with NIHL and used in the models were: age (n = 6), duration of noise exposure (n = 5) and noise exposure level (n = 4). Five included studies used either split-sample validation (n = 3) or ten-fold cross-validation (n = 2). Assessment of accuracy ranged in value from 75.3% to 99% with a low prediction error/root-mean-square error in 3 studies. Only 2 studies measured discrimination risk using the receiver operating characteristic (ROC) curve and/or the area under ROC curve.CONCLUSION: In spite of high accuracy and low prediction error of machine learning models, some improvement can be expected from larger sample sizes, multiple algorithm use, completed reports of model construction and the sufficient evaluation of calibration and discrimination risk.DOI: 10.1007/s00420-020-01648-wPMCID: PMC8238747",pubmed,33491101,10.1007/s00420-020-01648-w
acoustic and perceptual impact of face masks on speech a scoping review,"During the COVID-19 pandemic, personal protective equipment such as facial masks and coverings were mandated all over the globe to protect against the virus. Although the primary aim of wearing face masks is to protect against viral transmission, they pose a potential burden on communication. The purpose of this scoping review was to identify the state of the evidence of the effect of facial coverings on acoustic and perceptual speech outcomes. The scoping review followed the framework created by Arksey & O'Malley (2005) and the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews guidelines (PRISMA-ScR; Tricco et al., 2018). The search was completed in May 2021 across the following databases: PubMed, EMBASE, PsycINFO, Web of Science, and Google Scholar. A total of 3,846 records were retrieved from the database search. Following the removal of duplicates, 3,479 remained for the title/abstract screen and 149 were selected for the full-text review. Of these, 52 were included in the final review and relevant data were extracted. The 52 articles included in the final review consisted of; 11 studied perceptual outcomes only, 16 studied acoustic outcomes only, and 14 studied both perceptual and acoustic outcomes. 13 of these investigated acoustic features that could be used for mask classification. Although the findings varied from article to article, many trends stood out. Many articles revealed that face masks act as a low pass filter, dampening sounds at higher frequencies; however, the frequency range and the degree of attenuation varied based on face mask type. All but five articles that reported on perceptual outcomes showed a common trend that wearing a face mask was associated with poorer speech intelligibility. The findings of the scoping review provided evidence that facial coverings negatively impacted speech intelligibility, which is likely due to a combination of auditory and visual cue degradation. Due to the continued prevalence of mask use, how facial coverings affect a wider variety of speaker populations, such as those with communication impairments, and strategies for overcoming communication challenges should be explored.  © 2023 Badh, Knowles. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",scopus,2-s2.0-85168740649,10.1371/journal.pone.0285009
identification model for hearing loss symptoms using machine learning techniques,,base,9efb3b8e25ec5877aed1e2dc9dea53db002466550b2b4341f125a2ea19b87bb4,
stages of change in adults who have failed an online hearing screening,"368. Ear Hear. 2015 Jan;36(1):92-101. doi: 10.1097/AUD.0000000000000085.Stages of change in adults who have failed an online hearing screening.Laplante-Lévesque A(1), Brännström KJ, Ingo E, Andersson G, Lunner T.Author information:(1)1Department of Behavioural Sciences and Learning, Swedish Institute for Disability Research, Linköping University, Linköping, Sweden; 2Eriksholm Research Centre, Oticon A/S, Snekkersten, Denmark; 3Department of Logopedics, Phoniatrics and Audiology, Lund University, Lund, Sweden; and 4Department of Clinical Neuroscience, Karolinska Institute, Stockholm, Sweden.OBJECTIVES: Hearing screening has been proposed to promote help-seeking and rehabilitation in adults with hearing impairment. However, some longitudinal studies point to low help-seeking and subsequent rehabilitation after a failed hearing screening (positive screening result). Some barriers to help-seeking and rehabilitation could be intrinsic to the profiles and needs of people who have failed a hearing screening. Theories of health behavior change could help to understand this population. One of these theories is the transtheoretical (stages-of-change) model of health behavior change, which describes profiles and needs of people facing behavior changes such as seeking help and taking up rehabilitation. According to this model, people go through distinct stages toward health behavior change: precontemplation, contemplation, action, and finally, maintenance. The present study describes the psychometric properties (construct validity) of the stages of change in adults who have failed an online hearing screening. Stages of change were measured with the University of Rhode Island Change Assessment (URICA). Principal component analysis is presented, along with cluster analysis. Internal consistency was investigated. Finally, relationships between URICA scores and speech-in-noise recognition threshold, self-reported hearing disability, and self-reported duration of hearing disability are presented.DESIGN: In total, 224 adults who had failed a Swedish online hearing screening test (measure of speech-in-noise recognition) completed further questionnaires online, including the URICA.RESULTS: A principal component analysis identified the stages of precontemplation, contemplation, and action, plus an additional stage, termed preparation (between contemplation and action). According to the URICA, half (50%) of the participants were in the preparation stage of change. The contemplation stage was represented by 38% of participants, while 9% were in the precontemplation stage. Finally, the action stage was represented by approximately 3% of the participants. Cluster analysis identified four stages-of-change clusters: they were named decision making (44% of sample), participation (28% of sample), indecision (16% of sample), and reluctance (12% of sample). The construct validity of the model was good. Participants who reported a more advanced stage of change had significantly greater self-reported hearing disability. However, participants who reported a more advanced stage of change did not have a significantly worse speech-in-noise recognition threshold or reported a significantly longer duration of hearing impairment.CONCLUSIONS: The additional stage this study uncovered, and which other studies have also uncovered, preparation, highlights the need for adequate guidance for adults who are yet to seek help for their hearing. The fact that very few people were in the action stage (approximately 3% of the sample) signals that screening alone is unlikely to be enough to improve help-seeking and rehabilitation rates. As expected, people in the later stages of change reported significantly greater hearing disability. The lack of significant relationships between stages-of-change measures and speech-in-noise recognition threshold and self-reported duration of hearing disability highlights the complex interplay between impairment, disability, and behaviors in adults who have failed an online hearing screening and who are yet to seek help.DOI: 10.1097/AUD.0000000000000085",pubmed,25158981,10.1097/AUD.0000000000000085
an analysis of current source density profiles activated by local stimulation in the mouse auditory cortex in vitrodp   mar 15 2017,"To examine local network properties of the mouse auditory cortex in vitro, we recorded extracellular spatiotemporal laminar profiles driven by short electric local stimulation on a planar multielectrode array substrate. The recorded local field potentials were subsequently evaluated using current source density (CSD) analysis to identify sources and sinks. Current sinks are thought to be an indicator of net synaptic current in the small volume of cortex surrounding the recording site. Thus, CSD analysis combined with multielectrode arrays enabled us to compare mean synaptic activity in response to small current stimuli on a layer-by-layer basis. We also used senescence-accelerated mice (SAM), some strains of which show earlier onset of age-related hearing loss, to examine the characteristic spatiotemporal CSD profiles stimulated by electrodes in specific cortical layers. Thus, the CSD patterns were classified into several clusters based on stimulation sites in the cortical layers. We also found some differences in CSD patterns between the two SAM strains in terms of aging according to principle component analysis with dimension reduction. For simultaneous two-site stimulation, we modeled the obtained CSD profiles as a linear superposition of the CSD profiles to individual single-site stimulation. The model analysis indicated the nonlinearity of spatiotemporal integration over stimulus-driven activity in a layer-specific manner. Finally, on the basis of these results, we discuss the auditory cortex local network properties and the effects of aging on these mouse strains. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc16&DO=10.1016%2fj.brainres.2017.01.021
changing the paradigm for school hearing screening globally evaluation of screening protocols from two randomized trials in rural alaska,"413. Ear Hear. 2023 Jul-Aug 01;44(4):877-893. doi: 10.1097/AUD.0000000000001336. Epub 2023 Mar 13.Changing the Paradigm for School Hearing Screening Globally: Evaluation of Screening Protocols From Two Randomized Trials in Rural Alaska.Robler SK(1)(2), Platt A(3)(4), Jenson CD(2), Meade Inglis S(1)(3)(5), Hofstetter P(6), Ross AA(5)(7), Wang NY(8)(9), Labrique A(10), Gallo JJ(11), Egger JR(3), Emmett SD(5)(7)(12).Author information:(1)Department of Otolaryngology, Head & Neck Surgery, University of Arkansas for Medical Sciences, Little Rock, Arkansas, USA.(2)Department of Audiology, Norton Sound Health Corporation, Nome, Alaska, USA.(3)Duke Global Health Institute, Durham, North Carolina, USA.(4)Department of Biostatistics & Bioinformatics, Duke School of Medicine, Durham, North Carolina, USA.(5)Center for Health Policy and Inequalities Research, Duke University, Durham, North Carolina, USA.(6)Petersburg Medical Center, Petersburg, Alaska, USA.(7)Department of Head and Neck Surgery and Communication Sciences, Duke University School of Medicine, Durham, North Carolina, USA.(8)Department of Medicine, Johns Hopkins University School of Medicine, Baltimore, Maryland, USA.(9)Departments of Biostatistics and Epidemiology, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, USA.(10)Department of International Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, USA.(11)Department of Mental Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, USA.(12)Department of Epidemiology, Fay W. Boozman College of Public Health, University of Arkansas for Medical Sciences, Little Rock, Arkansas, USA.OBJECTIVES: Diagnostic accuracy was evaluated for various screening tools, including mobile health (mHealth) pure-tone screening, tympanometry, distortion product otoacoustic emissions (DPOAE), and inclusion of high frequencies to determine the most accurate screening protocol for identifying children with hearing loss in rural Alaska where the prevalence of middle ear disease is high.DESIGN: Hearing screening data were collected as part of two cluster randomized trials conducted in 15 communities in rural northwest Alaska. All children enrolled in school from preschool to 12th grade were eligible. Analysis was limited to data collected 2018 to 2019 (n = 1449), when both trials were running and measurement of high frequencies were included in the protocols. Analyses included estimates of diagnostic accuracy for each screening tool, as well as exploring performance by age and grade. Multiple imputation was used to assess diagnostic accuracy in younger children, where missing data were more prevalent due to requirements for conditioned responses. The audiometric reference standard included otoscopy, tympanometry, and high frequencies to ensure detection of infection-related and noise-induced hearing loss.RESULTS: Both the mHealth pure-tone screen and DPOAE screen performed better when tympanometry was added to the protocol (increase in sensitivity of 19.9%, 95% Confidence Interval (CI): 15.9 to 24.1 for mHealth screen, 17.9%, 95% CI: 14.0 to 21.8 for high-frequency mHealth screen, and 10.4%, 95% CI: 7.5 to 13.9 for DPOAE). The addition of 6 kHz to the mHealth pure-tone screen provided an 8.7 percentage point improvement in sensitivity (95% CI: 6.5 to 11.3). Completeness of data for both the reference standard and the mHealth screening tool differed substantially by age, due to difficulty with behavioral testing in young children. By age 7, children were able to complete behavioral testing, and data indicated that high-frequency mHealth pure-tone screen with tympanometry was the superior tool for children 7 years and older. For children 3 to 6 years of age, DPOAE plus tympanometry performed the best, both for complete data and multiply imputed data, which better approximates accuracy for children with missing data.CONCLUSIONS: This study directly evaluated pure-tone, DPOAE, and tympanometry tools as part of school hearing screening in rural Alaskan children (3 to 18+ years). Results from this study indicate that tympanometry is a key component in the hearing screening protocol, particularly in environments with higher prevalence of infection-related hearing loss. DPOAE is the preferred hearing screening tool when evaluating children younger than 7 years of age (below 2nd grade in the United States) due to the frequency of missing data with behavioral testing in this age group. For children 7 years and older, the addition of high frequencies to pure-tone screening increased the accuracy of screening, likely due to improved identification of hearing loss from noise exposure. The lack of a consistent reference standard in the literature makes comparing across studies challenging. In our study with a reference standard inclusive of otoscopy, tympanometry, and high frequencies, less than ideal sensitivities were found even for the most sensitive screening protocols, suggesting more investigation is necessary to ensure screening programs are appropriately identifying noise- and infection-related hearing loss in rural, low-resource settings.Copyright © 2023 The Authors. Ear & Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health, Inc.DOI: 10.1097/AUD.0000000000001336PMCID: PMC10262989",pubmed,36907833,10.1097/AUD.0000000000001336
modeling intelligibility of hearingaid compression circuits,"The active filtering effect in the inner ear is disrupted with sensorineural hearing impairment. This causes a loss of frequency selectivity and dynamic range. Compression is often used in hearing-aids in an attempt to re-establish the normal dynamic range of the cochlear response. While some studies show increased speech intelligibility with artificial noise sources for compressive hearing-aids, most show little (< 1 dB versus linear aids) or no advantage in competing speech. In this paper we explore a quantitative model to explain the empirical performance of compressive hearing-aids in competing speech. By combining an accurate cochlear model with a model of higher auditory feature analysis based on spectral-temporal clustering of onsets, we provide an explanation for the failure of hearing-aid compression algorithms to increase intelligibility. Our proposed spectral-temporal intelligibility model suggests that increasing intelligibility for a hearing impaired person in competing speech requires both spectral and temporal suppression.",scopus,2-s2.0-4143149413,
deephear a multimodal subtitle positioning system dedicated to deaf and hearingimpaired people,"In this paper, we introduce the DEEP-HEAR framework, a multimodal dynamic subtitle positioning system designed to increase the accessibility of deaf and hearing impaired people (HIP) to multimedia documents. The proposed system exploits both computer vision algorithms and deep convolutional neural networks specifically designed and tuned in order to detect and recognize the identity of the active speaker. The main contributions of the paper concern: a novel method dedicated to recognizing various characters existent in the video stream. A video temporal segmentation algorithm that divides the video sequence into semantic units, based on face tracks and visual consistency. Finally, the core of our approach concerns a novel active speaker recognition method relying on the multimodal information fusion from the text, audio, and video streams. The experimental results carried out on a large scale dataset of more than 30 videos, validate the proposed methodology with average accuracy and recognition rates superior to 90%. Moreover, the method shows robustness to important object/camera motion and face pose variation, yielding gains of more than 8% in precision and recall rates when compared with state-of-the-art techniques. The subjective evaluation of the proposed dynamic subtitle positioning system demonstrates the effectiveness of our approach.",ieee,2169-3536,10.1109/ACCESS.2019.2925806
a systematic review on systemsbased sensory gloves for sign language pattern recognition an update from 2017 to 2022,"Sign language is the predominant mode of communication for the Hearing impaired community. For the millions of people who suffer from hearing loss around the world, interaction with people who have the ability to hear and do not suffer from hearing impairment or loss is considered as complicated. In line with this issue, technology is perceived as a crucial factor in being an enabler of providing solutions to enhance the quality of life of the hearing impairment by increasing accessibility. This research aims to review and analyze articles related to sign language recognition based on the sensor- based glove system, in order to identify academic motivations, challenges, and recommendations related to this field. The search for the relevant review materials and articles was performed on four major databases ranging from 2017 to 2022: Science Direct, Web of Science, IEEE Xplore, and Scopus. The articles were chosen based on our inclusion and exclusion criteria. The literature findings of this paper indicate the dataset size to be open issues and challenges for hand gesture recognition. Furthermore, the majority of research on sign language recognition based on data glove was performed on static, single hand, and isolated gestures. Moreover, recognition accuracy typically achieved results higher than 90%. However, most experiments were carried out with a limited number of gestures. Overall, it is hoped that this study will serve as a roadmap for future research and raise awareness among researchers in the field of sign language recognition.",ieee,2169-3536,10.1109/ACCESS.2022.3219430
functional mri study of auditory cortical responses in normal subjects and unilateral sensorineural hearing loss subjects,"515. Lin Chuang Er Bi Yan Hou Tou Jing Wai Ke Za Zhi. 2010 Nov;24(22):1018-22.[Functional MRI study of auditory cortical responses in normal subjects and unilateral sensorineural hearing loss subjects].[Article in Chinese]Ji H(1), Huang Z, Yang M, Feng X, Meng L.Author information:(1)Department of Otorhinolaryngology, Zhongda Hospital of Southeast University, Nanjing, 210009, China.OBJECTIVE: Amplitude modulation of auditory cortical responses was evaluated with functional MRI (fMRI) in subjects of unilateral sensorineural hearing loss (USNHL) and those of normal hearing (NH).METHOD: Twenty-one subjects with USNHL and 11 with normal hearing were examined with fMRI in response to amplitude modulation tones of 500 Hz with the modulation frequency at 8 Hz. An event related design was combined with a sparse clustered volume acquisitioning paradigm in data collection in order to reduce the influence of acoustic scanner noise. SPM2 software was used for offline data analyzing.RESULT: Significant activation, including volume and intensity, were found in the temporal lobe of control subjects, and significant differences in the volume and intensity were noted between the contralateral and ipsilateral activated auditory cortexes in them, exhibiting clearly contralateral predominance. When the normal ear with unilateral sensorineural hearing loss received signals, while significant activations in bilateral auditory cortexes, greater activation in the contralateral auditory cortexes was found in the normal ear.CONCLUSION: The difference in the lateralization between the two groups suggests the plasticity of auditory cortex with unilateral sensorineural hearing loss.",pubmed,21322926,
reading vocabulary in children with and without hearing loss the roles of task and word type,"Purpose: To address the problem of low reading comprehension scores among children with hearing impairment, it is necessary to have a better understanding of their reading vocabulary. In this study, the authors investigated whether task and word type differentiate the reading vocabulary knowledge of children with and without severe hearing loss. Method: Seventy-two children with hearing loss and 72 children with normal hearing performed a lexical and a use decision task. Both tasks contained the same 180 words divided over 7 clusters, each cluster containing words with a similar pattern of scores on 8 word properties (word class, frequency, morphological family size, length, age of acquisition, mode of acquisition, imageability, and familiarity). Results: Whereas the children with normal hearing scored better on the 2 tasks than the children with hearing loss, the size of the difference varied depending on the type of task and word. Conclusions: Performance differences between the 2 groups increased as words and tasks became more complex. Despite delays, children with hearing loss showed a similar pattern of vocabulary acquisition as their peers with normal hearing. For the most precise assessment of reading vocabulary possible, a range of tasks and word types should be used. © American Speech-Language-Hearing Association.",scopus,2-s2.0-84878090286,10.1044/1092-4388(2012/11-0138)
singlesided deafnessoutcomes of three interventions for profound unilateral sensorineural hearing loss a randomized clinical trial,"222. Otol Neurotol. 2020 Jul;41(6):736-744. doi: 10.1097/MAO.0000000000002633.Single-Sided Deafness-Outcomes of Three Interventions for Profound Unilateral Sensorineural Hearing Loss: A Randomized Clinical Trial.Fogels J(1)(2)(3), Jönsson R(4), Sadeghi A(1)(2)(3), Flynn M(5), Flynn T(6).Author information:(1)Division of Audiology, Department of Health and Rehabilitation, Institute of Neuroscience and Physiology, Sahlgrenska Academy, University of Gothenburg, Sweden.(2)Region Västra Götaland, Habilitation & Health, Hearing Organisation, Gothenburg, Sweden.(3)Division of Audiology, Department of Health and Rehabilitation, Institute of Neuroscience and Physiology, Sahlgrenska Academy, University of Gothenburg.(4)Department of Otorhinolaryngology, Institute of Clinical Science, Sahlgrenska Academy, University of Gothenburg, Gothenburg, Sweden.(5)Better Health, Healthcare and Treatment Global Impact Cluster, Research and Innovation, University of Newcastle, Callaghan, Australia.(6)School of Humanities and Social Science, Faculty of Education and Arts, University of New Castle, Australia.OBJECTIVE: A comparison of three interventions for profound unilateral sensorineural hearing loss.STUDY DESIGN: Prospective, crossover randomized clinical trial.PARTICIPANTS: Fifteen participants with profound unilateral sensorineural hearing loss.INTERVENTIONS: Three potential technical interventions were compared: Bone Conduction Device on softband, Contralateral Routing of Signal (CROS), and Remote Microphone . Each intervention was randomly trialed for a period of 3 weeks, separated by a 1 week washout period.OUTCOME MEASURES: Speech in noise recognition test performed under four conditions (lateral noise poorer ear, lateral noise better ear, speech poorer ear, speech better ear). Standardized questionnaires (Abbreviated Profile of Hearing Aid Benefit, Bern Benefit in Single Sided Deafness Questionnaire, and Speech, Spatial, and Other Qualities 12) were used to evaluate amplification benefit at baseline and following each intervention.RESULTS: The use of remote microphone provided the best results in the speech recognition in noise test. A benefit in some signal-to-noise ratios was presented of the CROS over bone conduction device on softband in the Speech Poor Ear condition. On questionnaires of benefit, participants did not rate a particular intervention as significantly better than any other. Following the study, CROS was the intervention preferred by the 8 of 15 participants (53%). The majority of participants (80%) chose to continue with an intervention rather than no treatment.CONCLUSION: The use of all interventions resulted in increased performance in speech recognition in noise and rated higher on subjective benefits in comparison with baseline. People with SSD are a heterogeneous population when considering perceived difficulties. Future research should focus on segmenting the population of SSD depending on factors such as etiology, high frequency loss in the better ear, and age of acquired loss for the poorer ear. This stratification may possibly increase the benefit for the patient in terms of more individual-based clinical routines.DOI: 10.1097/MAO.0000000000002633",pubmed,32574478,10.1097/MAO.0000000000002633
deep learning models for predicting hearing thresholds based on swepttone stimulusfrequency otoacoustic emissions,"190. Ear Hear. 2024 Mar-Apr 01;45(2):465-475. doi: 10.1097/AUD.0000000000001443. Epub 2023 Nov 22.Deep Learning Models for Predicting Hearing Thresholds Based on Swept-Tone Stimulus-Frequency Otoacoustic Emissions.Liu Y(1), Gong Q(1)(2).Author information:(1)Department of Biomedical Engineering, School of Medicine, Tsinghua University, Beijing, China.(2)School of Medicine, Shanghai University, Shanghai, China.OBJECTIVES: This study aims to develop deep learning (DL) models for the quantitative prediction of hearing thresholds based on stimulus-frequency otoacoustic emissions (SFOAEs) evoked by swept tones.DESIGN: A total of 174 ears with normal hearing and 388 ears with sensorineural hearing loss were studied. SFOAEs in the 0.3 to 4.3 kHz frequency range were recorded using linearly swept tones at a rate of 2 Hz/msec, with stimulus level changing from 40 to 60 dB SPL in 10 dB steps. Four DL models were used to predict hearing thresholds at octave frequencies from 0.5 to 4 kHz. The models-a conventional convolutional neural network (CNN), a hybrid CNN-k-nearest neighbor (KNN), a hybrid CNN-support vector machine (SVM), and a hybrid CNN-random forest (RF)-were individually built for each frequency. The input to the DL models was the measured raw SFOAE amplitude spectra and their corresponding signal to noise ratio spectra. All DL models shared a CNN-based feature self-extractor. They differed in that the conventional CNN utilized a fully connected layer to make the final regression decision, whereas the hybrid CNN-KNN, CNN-SVM, and CNN-RF models were designed by replacing the last fully connected layer of CNN model with a traditional machine learning (ML) regressor, that is, KNN, SVM, and RF, respectively. The model performance was evaluated using mean absolute error and SE averaged over 20 repetitions of 5 × 5 fold nested cross-validation. The performance of the proposed DL models was compared with two types of traditional ML models.RESULTS: The proposed SFOAE-based DL models resulted in an optimal mean absolute error of 5.98, 5.22, 5.51, and 6.06 dB at 0.5, 1, 2, and 4 kHz, respectively, superior to that obtained by the traditional ML models. The produced SEs were 8.55, 7.27, 7.58, and 7.95 dB at 0.5, 1, 2, and 4 kHz, respectively. All the DL models outperformed any of the traditional ML models.CONCLUSIONS: The proposed swept-tone SFOAE-based DL models were capable of quantitatively predicting hearing thresholds with satisfactory performance. With DL techniques, the underlying relationship between SFOAEs and hearing thresholds at disparate frequencies was explored and captured, potentially improving the diagnostic value of SFOAEs.Copyright © 2023 Wolters Kluwer Health, Inc. All rights reserved.DOI: 10.1097/AUD.0000000000001443",pubmed,37990395,10.1097/AUD.0000000000001443
overview of the 2023 icassp sp clarity challenge speech enhancement for hearing aids,"This paper reports on the design and outcomes of the ICASSP SP Clarity Challenge: Speech Enhancement for Hearing Aids. The scenario was a listener attending to a target speaker in a noisy, domestic environment. There were multiple interferers and head rotation by the listener. The challenge extended the second Clarity Enhancement Challenge (CEC2) by fixing the amplification stage of the hearing aid; evaluating with a combined metric for speech intelligibility and quality; and providing two evaluation sets, one based on simulation and the other on real-room measurements. Five teams improved on the baseline system for the simulated evaluation set, but the performance on the measured evaluation set was much poorer. Investigations are on-going to determine the exact cause of the mismatch between the simulated and measured data sets. The presence of transducer noise in the measurements, lower order Ambisonics harming the ability for systems to exploit binaural cues and the differences between real and simulated room impulse responses are suggested causes. © 2023 IEEE.",scopus,2-s2.0-85185224716,10.1109/ICASSP49357.2023.10433922
development of auditory processing in 6 to 11yrold children,"Objectives: The aim of this study is to provide developmental standards on a variety of temporal, spectral, and binaural psychoacoustic (auditory processing [AP]) tests in typically developing children, including immediate and delayed retest reliability, and comparisons between single listener performance on different tests. This study also informs choices on the selection of tests for clinical evaluation of hearing and listening (e.g., for auditory processing disorder). Design: This is a laboratory-based study of AP threshold and variability of 75 children, aged 6 to 11 yrs, and 21 young adults with normal audiometry recruited from local schools and colleges. Data were gathered in clinic-like conditions, without training and across two sessions. Eleven individual (e.g., simultaneous masking and backward masking [BM], amplitude modulation [AM], and frequency modulation [FM] detection) and three derived (temporal integration, frequency resolution, masking level difference) measure tests were embedded within a suite of computer games, each employing a three-interval, three-alternative (odd-one-out) forced choice response paradigm and a staircase adaptive method. Results: AP measures generally showed lower thresholds and reduced variance with increasing age. At 6 to 7 yrs, performance was markedly poorer than in the older groups; 35% of the children could not do the test of frequency discrimination (FD). However, on all the tasks, some children in the same group performed at near-adult levels. The distribution of performance between individuals varied widely across tasks, with clustered performance on tests of tone detection (with or without a simultaneous masker) and AM detection, and scattered performance on BM, FM detection, and FD. Threshold maturity was achieved at different rates across tests and by 10 to 11 yrs of age on all tests except FD. Masking level difference (MLD) performance did not change with age. Retest reliability was mostly high within test sessions but, again, was poorer for some of the younger children. Between test sessions separated by one to several weeks, reliability varied from poor (for FM detection) to high (for long tone detection in quiet, BM, and FD). Correlations between thresholds on different tests were generally low. Conclusions: Data suggest that the perception of different auditory stimuli occurs and develops using rather independent mechanisms, even for tasks that are closely related in procedure. While individual children can perform reliably on several distinct tasks, differences between individuals on the same tasks can be large. Because some of the youngest children perform reliably across time, at or near adult levels, immaturity between 6 and 11 yrs of age, as reflected in group statistics, reflects poor performance of some individual children rather than obligate, age-related deficits in AP. While several of the tests used were found to have potential clinical applicability, because of their reliability and ability to distinguish between individuals, it is currently unclear how performance on such tests relates to everyday listening skills. © 2011 Lippincott Williams & Wilkins, Inc.",scopus,2-s2.0-79955534726,10.1097/AUD.0b013e318201c468
relating neuronal dynamics for auditory object processing to neuroimaging activity a computational modeling and an fmri study,"473. Neuroimage. 2004 Apr;21(4):1701-20. doi: 10.1016/j.neuroimage.2003.11.012.Relating neuronal dynamics for auditory object processing to neuroimaging activity: a computational modeling and an fMRI study.Husain FT(1), Tagamets MA, Fromm SJ, Braun AR, Horwitz B.Author information:(1)Brain Imaging and Modeling Section, National Institute on Deafness and Other Communication Disorders, National Institutes of Health, Bethesda, MD 20892, USA. husainf@nidcd.nih.govWe investigated the neural basis of auditory object processing in the cerebral cortex by combining neural modeling and functional neuroimaging. We developed a large-scale, neurobiologically realistic network model of auditory pattern recognition that relates the neuronal dynamics of cortical auditory processing of frequency modulated (FM) sweeps to functional neuroimaging data of the type obtained using PET and fMRI. Areas included in the model extend from primary auditory to prefrontal cortex. The electrical activities of the neuronal units of the model were constrained to agree with data from the neurophysiological literature regarding the perception of FM sweeps. We also conducted an fMRI experiment using stimuli and tasks similar to those used in our simulations. The integrated synaptic activity of the neuronal units in each region of the model, convolved with a hemodynamic response function, was used as a correlate of the simulated fMRI activity, and generally agreed with the experimentally observed fMRI data in the brain areas corresponding to the regions of the model. Our results demonstrate that the model is capable of exhibiting the salient features of both electrophysiological neuronal activities and fMRI values that are in agreement with empirically observed data. These findings provide support for our hypotheses concerning how auditory objects are processed by primate neocortex.DOI: 10.1016/j.neuroimage.2003.11.012",pubmed,15050592,10.1016/j.neuroimage.2003.11.012
use of selforganizing neural networks kohonen maps for classification of voice acoustic signals exemplified by the infant voice with and without timedelayed auditory feedback,"474. HNO. 1996 Apr;44(4):201-6.[Use of self-organizing neural networks (Kohonen maps) for classification of voice acoustic signals exemplified by the infant voice with and without time-delayed auditory feedback].[Article in German]Schönweiler R(1), Kaese S, Möller S, Rinscheid A, Ptok M.Author information:(1)Klinik für Phoniatrie und Pädaudiologie, Medizinische Hochschule Hannover.Subjective and auditory assessment of the voice is now more commonly being replaced by objective voice analysis. Because of the amount of data available from computer-aided voice analysis, subjective selection and interpretation of single data sets remain a matter of experience of the individual investigator. Since neuronal networks are widely used in telecommunication and speech recognition, we applied self-organizing Kohonen networks to classify voice patterns. In the phase of ""learning,"" the Kohonen map is adapted to patterns of the primary signals obtained. If, in the phase of using the map, the input signal hits the field of the primary signals, it will resemble them closely. In this study, we recorded newborn and young infant cries using a DAT recorder and a high-quality microphone. The cries were elicited by wearing uncomfortable headphones (""cries of discomfort""). Spectrographic characteristics of the cries were classified by 20-step bark spectra and then applied to the neuronal networks. It was possible to recognize similarities of different cries of the same children and interindividual differences, as well as cries of children with profound hearing loss. In addition, delayed auditory feedback at 80 dB SL was presented to 27 children via headphone using a three-headed tape-recorder as a model for induced individual cry changes. However, it was not possible to classify short-term changes as in a delayed feedback procedure. Nevertheless, neuronal networks may be helpful as an additional tool in spectrographic voice analysis.",pubmed,8655351,
cochlear transcriptome analysis of an outbred mouse population cfw,"672. bioRxiv [Preprint]. 2023 Jun 10:2023.02.15.528661. doi: 10.1101/2023.02.15.528661.Cochlear transcriptome analysis of an outbred mouse population (CFW).Boussaty EC(1), Tedeschi N(2), Novotny M(2), Ninoyu Y(1), Du E(1), Draf C(1), Zhang Y(2), Manor U(3), Scheuermann RH(2)(4), Friedman R(1).Author information:(1)Department of Otolaryngology, University of California, San Diego, CA.(2)J. Craig Venter Institute, La Jolla, CA.(3)Salk Institute for Biological Studies, Waitt Advanced Biophotonics Center, La Jolla, CA, United States.(4)Department of Pathology, University of California, San Diego, CA.Update in    Front Cell Neurosci. 2023 Nov 29;17:1256619.Age-related hearing loss (ARHL) is the most common cause of hearing loss and one of the most prevalent conditions affecting the elderly worldwide. Despite evidence from our lab and others about its polygenic nature, little is known about the specific genes, cell types and pathways involved in ARHL, impeding the development of therapeutic interventions. In this manuscript, we describe, for the first time, the complete cell-type specific transcriptome of the aging mouse cochlea using snRNA-seq in an outbred mouse model in relation to auditory threshold variation. Cochlear cell types were identified using unsupervised clustering and annotated via a three-tiered approach - first by linking to expression of known marker genes, then using the NS-Forest algorithm to select minimum cluster-specific marker genes and reduce dimensional feature space for statistical comparison of our clusters with existing publicly-available data sets on the gEAR website (https://umgear.org/), and finally, by validating and refining the annotations using Multiplexed Error Robust Fluorescence In Situ Hybridization (MERFISH) and the cluster-specific marker genes as probes. We report on 60 unique cell-types expanding the number of defined cochlear cell types by more than two times. Importantly, we show significant specific cell type increases and decreases associated with loss of hearing acuity implicating specific subsets of hair cell subtypes, ganglion cell subtypes, and cell subtypes withing the stria vascularis in this model of ARHL. These results provide a view into the cellular and molecular mechanisms responsible for age-related hearing loss and pathways for therapeutic targeting.DOI: 10.1101/2023.02.15.528661PMCID: PMC9948975",pubmed,36824745,10.1101/2023.02.15.528661
simulations with fade of the effect of impaired hearing on speech recognition performance cast doubt on the role of spectral resolution,"326. Hear Res. 2020 Sep 15;395:107995. doi: 10.1016/j.heares.2020.107995. Epub 2020 Jul 8.Simulations with FADE of the effect of impaired hearing on speech recognition performance cast doubt on the role of spectral resolution.Hülsmeier D(1), Warzybok A(2), Kollmeier B(2), Schädler MR(2).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4all, CvO Universität Oldenburg, 26129, Oldenburg, Germany. Electronic address: david.huelsmeier@uni-oldenburg.de.(2)Medizinische Physik and Cluster of Excellence Hearing4all, CvO Universität Oldenburg, 26129, Oldenburg, Germany.Listeners with hearing impairment show sub-optimal processing of acoustic signals which affects their ability to recognize speech. In this contribution, three effective signal processing deficits are proposed to simulate sensorineural hearing impairment and their effect on simulated speech recognition performance is studied. Psychoacoustic and speech recognition experiments were simulated with the framework for auditory discrimination experiments (FADE). Loss in absolute hearing threshold was modeled as lower level limit, supra-threshold loss in envelope amplitude resolution as multiplicative noise, and reduced spectral resolution was simulated with an increase of the analysis bandwidth. Their effects on the speech recognition performance with the German matrix test in quiet and noise, the audiogram, and tone in (notched) noise detection experiments were systematically examined. The simulations indicate that each psychoacoustic experiment relates to at least one signal processing deficit. This indicates the possibility to determine individual model parameters from the outcome of psychoacoustic experiments. Moreover, absolute hearing thresholds yield the highest effects on simulated speech recognition thresholds, followed by supra-threshold loss in envelope amplitude resolution, and-to a much smaller degree-spectral resolution. A reduced spectral resolution only affected recognition performance in fluctuating masker for normal hearing thresholds, indicating its potential relevance for more complex listening conditions. In contrast to popular interpretations in the literature, the simulations reveal that reduced spectral resolution plays a minor role compared to a reduced envelope amplitude resolution in characterizing supra-threshold hearing loss at least in stationary noise.Copyright © 2020 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2020.107995",pubmed,32702612,10.1016/j.heares.2020.107995
cortical thickness of left heschls gyrus correlates with hearing acuity in adultsa surfacebased morphometry study references,"To date, research examining the relationship between brain structure and hearing acuity is sparse, especially given the context of a broad age range. To investigate this relationship, we applied an automated surface-based morphometry (SBM) approach (FreeSurfer) in this study to re-examine a sample of normal-hearing (n = 17) and hearing-impaired (n = 17) age- and education-matched adults, aged between 20 and 63 years (Alfandari et al., 2018). The SBM approach allows the disentanglement of cortical surface area (CSA) from cortical thickness (CT), the 2 independent constituents of cortical volume (CV). We extend the findings of Alfandari and colleagues by showing several clusters in auditory-related areas as well as in the left and right angular gyrus that showed reduced CT, CSA and CV in hearing-impaired compared to normal-hearing listeners. Nevertheless, none of the clusters found correlated significantly with hearing acuity, measured by pure-tone thresholds, in the 2 groups. An additional vertex-wise correlation analysis between hearing acuity and morphometric parameters over all participants revealed a single significant cluster encompassing the left Heschl's gyrus. Higher hearing thresholds were associated with a thinner cortex within this cluster. Our results imply that hearing impairment is associated with reduced thickness in primary and secondary auditory cortex regions, those regions especially involved in perceiving and processing relevant speech cues. This decrease was observed not only in older but also in younger and middle-aged adults, independent of age-related decline in the cognitive domain and age-dependent whole-brain atrophy. Further, the results show the value added when considering CV, CT and CSA separately, relative to previous studies which have solely relied on voxel-based morphometry to investigate brain structure and hearing acuity across the lifespan. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc18&DO=10.1016%2fj.heares.2019.107823
audiogene predicting hearing loss genotypes from phenotypes to guide genetic screening,"327. Hum Mutat. 2013 Apr;34(4):539-45. doi: 10.1002/humu.22268. Epub 2013 Feb 19.AudioGene: predicting hearing loss genotypes from phenotypes to guide genetic screening.Taylor KR(1), Deluca AP, Shearer AE, Hildebrand MS, Black-Ziegelbein EA, Anand VN, Sloan CM, Eppsteiner RW, Scheetz TE, Huygen PL, Smith RJ, Braun TA, Casavant TL.Author information:(1)Department of Electrical and Computer Engineering, University of Iowa, Iowa City, IA, USA.Autosomal dominant nonsyndromic hearing loss (ADNSHL) is a common and often progressive sensory deficit. ADNSHL displays a high degree of genetic heterogeneity and varying rates of progression. Accurate, comprehensive, and cost-effective genetic testing facilitates genetic counseling and provides valuable prognostic information to affected individuals. In this article, we describe the algorithm underlying AudioGene, a software system employing machine-learning techniques that utilizes phenotypic information derived from audiograms to predict the genetic cause of hearing loss in persons segregating ADNSHL. Our data show that AudioGene has an accuracy of 68% in predicting the causative gene within its top three predictions, as compared with 44% for a majority classifier. We also show that AudioGene remains effective for audiograms with high levels of clinical measurement noise. We identify audiometric outliers for each genetic locus and hypothesize that outliers may reflect modifying genetic effects. As personalized genomic medicine becomes more common, AudioGene will be increasingly useful as a phenotypic filter to assess pathogenicity of variants identified by massively parallel sequencing.© 2012 Wiley Periodicals, Inc.DOI: 10.1002/humu.22268PMCID: PMC3753227",pubmed,23280582,10.1002/humu.22268
technology and social interaction in the multimodal multispace setting of audiometric testing,"A frequent motivation for integrating the technological and the social sciences lies in understanding the users of technologies in order to innovate [1-3]. This paper argues that a shift is necessary from 'the user' to 'the interaction in the participation framework' because it is here where the interactants display to each other their relevancies. Using Conversation Analysis [5-8], this point is exemplified by the examination of interaction in an audiological consultation where the interface of sociality and technology is relevant as a barrier. The analysis focuses on what aspects the participants in their talk and nonverbal conduct orient to as problematic given the task and the technology in this multimodal, multispace environment. The analytical results are discussed for innovation within the framework of User-Centered Design. © 2012 Springer-Verlag.",scopus,2-s2.0-84865960926,10.1007/978-3-642-32090-3_22
suprathreshold auditory processing deficits in noise effects of hearing loss and age,"109. Hear Res. 2016 Jan;331:27-40. doi: 10.1016/j.heares.2015.10.004. Epub 2015 Oct 22.Suprathreshold auditory processing deficits in noise: Effects of hearing loss and age.Kortlang S(1), Mauermann M(2), Ewert SD(2).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, D-26111 Oldenburg, Germany. Electronic address: steffen.kortlang@uni-oldenburg.de.(2)Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, D-26111 Oldenburg, Germany.People with sensorineural hearing loss generally suffer from a reduced ability to understand speech in complex acoustic listening situations, particularly when background noise is present. In addition to the loss of audibility, a mixture of suprathreshold processing deficits is possibly involved, like altered basilar membrane compression and related changes, as well as a reduced ability of temporal coding. A series of 6 monaural psychoacoustic experiments at 0.5, 2, and 6 kHz was conducted with 18 subjects, divided equally into groups of young normal-hearing, older normal-hearing and older hearing-impaired listeners, aiming at disentangling the effects of age and hearing loss on psychoacoustic performance in noise. Random frequency modulation detection thresholds (RFMDTs) with a low-rate modulator in wide-band noise, and discrimination of a phase-jittered Schroeder-phase from a random-phase harmonic tone complex are suggested to characterize the individual ability of temporal processing. The outcome was compared to thresholds of pure tones and narrow-band noise, loudness growth functions, auditory filter bandwidths, and tone-in-noise detection thresholds. At 500 Hz, results suggest a contribution of temporal fine structure (TFS) to pure-tone detection thresholds. Significant correlation with auditory thresholds and filter bandwidths indicated an impact of frequency selectivity on TFS usability in wide-band noise. When controlling for the effect of threshold sensitivity, the listener's age significantly correlated with tone-in-noise detection and RFMDTs in noise at 500 Hz, showing that older listeners were particularly affected by background noise at low carrier frequencies.Copyright © 2015 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2015.10.004",pubmed,26471199,10.1016/j.heares.2015.10.004
predicting depression from hearing loss using machine learning,"17. Ear Hear. 2021 July/Aug;42(4):982-989. doi: 10.1097/AUD.0000000000000993.Predicting Depression From Hearing Loss Using Machine Learning.Crowson MG(1)(2), Franck KH(1)(2), Rosella LC(3), Chan TCY(4).Author information:(1)Department of Otolaryngology-Head and Neck Surgery, Massachusetts Eye and Ear, Boston, Massachusetts, USA.(2)Department of Otolaryngology-Head and Neck Surgery, Harvard Medical School, Boston, Massachusetts, USA.(3)Dalla Lana School of Public Health, University of Toronto, Ontario, Canada.(4)Department of Mechanical & Industrial Engineering, University of Toronto, Ontario, Canada.OBJECTIVES: Hearing loss is the most common sensory loss in humans and carries an enhanced risk of depression. No prior studies have attempted a contemporary machine learning approach to predict depression using subjective and objective hearing loss predictors. The objective was to deploy supervised machine learning to predict scores on a validated depression scale using subjective and objective audiometric variables and other health determinant predictors.DESIGN: A large predictor set of health determinants from the National Health and Nutrition Examination Survey 2015-2016 database was used to predict adults' scores on a validated instrument to screen for the presence and severity of depression (Patient Health Questionnaire-9 [PHQ-9]). After model training, the relative influence of individual predictors on depression scores was stratified and analyzed. Model prediction performance was determined by prediction error metrics.RESULTS: The test set mean absolute error was 3.03 (95% confidence interval: 2.91 to 3.14) and 2.55 (95% confidence interval: 2.48 to 2.62) on datasets with audiology-only predictors and all predictors, respectively, on the PHQ-9's 27-point scale. Participants' self-reported frustration when talking to members of family or friends due to hearing loss was the fifth-most influential of all predictors. Of the top 10 most influential audiometric predictors, five were related to social contexts, two for significant noise exposure, two objective audiometric parameters, and one presence of bothersome tinnitus.CONCLUSIONS: Machine learning algorithms can accurately predict PHQ-9 depression scale scores from National Health and Nutrition Examination Survey data. The most influential audiometric predictors of higher scores on a validated depression scale were social dynamics of hearing loss and not objective audiometric testing. Such models could be useful in predicting depression scale scores at the point-of-care in conjunction with a standard audiologic assessment.Copyright © 2021 Wolters Kluwer Health, Inc. All rights reserved.DOI: 10.1097/AUD.0000000000000993",pubmed,33577219,10.1097/AUD.0000000000000993
the future of hearing aid technology can technology turn us into superheroes die zukunft der hrgertetechnologie kann die technologie uns in superhelden verwandeln,"Background: Hearing aid technology has proven to be successful in the rehabilitation of hearing loss, but its performance is still limited in difficult everyday conditions characterized by noise and reverberation. Objective: Introduction to the current state of hearing aid technology and presentation of the current state of research and future developments. Methods: The current literature was analyzed and several specific new developments are presented. Results: Both objective and subjective data from empirical studies show the limitations of the current technology. Examples of current research show the potential of machine learning-based algorithms and multimodal signal processing for improving speech processing and perception, of using virtual reality for improving hearing device fitting and of mobile health technology for improving hearing health services. Conclusion: Hearing device technology will remain a key factor in the rehabilitation of hearing impairments. New technology, such as machine learning and multimodal signal processing, virtual reality and mobile health technology, will improve speech enhancement, individual fitting and communication training, thus providing better support for all hearing-impaired patients, including older patients with disabilities or declining cognitive skills. © 2023, The Author(s), under exclusive licence to Springer Medizin Verlag GmbH, ein Teil von Springer Nature.",scopus,2-s2.0-85153971746,10.1007/s00391-023-02179-y
algorithmbased hearing and speech therapy rehabilitation after cochlear implantation,"Introduction: Due to the changes in the indication range for cochlear implants and the demographic development towards an aging society, more and more people are in receipt of cochlear implants. An implantation requires a close-meshed audiological and logopedic aftercare. Hearing therapy rehabilitation currently requires great personnel effort and is time consuming. Hearing and speech therapy rehabilitation can be supported by digital hearing training programs. However, the apps currently on the market are to a limited degree personalized and structured. Increasing digital-ization makes it possible, especially in times of pandemics, to decouple hearing therapy treatment from everyday clinical practice. Material and Methods: For this purpose, an app is in development that provides hearing therapy tailored to the patient. The individual factors that influence hearing outcome are considered. Using intelligent algorithms, the app determines the selection of exercises, the level of difficulty and the speed at which the difficulty is increased. Results: The app works autonomously without being connected to local speech therapists. In addition, the app is able to analyze patient difficulties within the exercises and provides conclusions about the need for technical adjustments. Conclusions: The presented newly developed app represents a possibility to support, replace, expand and improve the classic outpatient hearing and speech therapy after CI implantation. The way the application works allows it to reach more people and provide a time-and cost-saving alternative to traditional therapy. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",scopus,2-s2.0-85129992444,10.3390/brainsci12050580
hearing impairment among young chinese in a rural area,"492. Public Health. 1996 Sep;110(5):293-7. doi: 10.1016/s0033-3506(96)80092-8.Hearing impairment among young Chinese in a rural area.Morioka I(1), Luo WZ, Miyashita K, Takeda S, Wang YX, Li SC.Author information:(1)Department of Hygiene, School of Medicine, Wakayama Medical University, Japan.To evaluate hearing levels in Chinese young people, audiometry was carried out at a rural village in Shandong Prefecture. The subjects were 282 healthy school children and students ranging in age from 7-17 y. All subjects were asked to complete a brief questionnaire on otological symptoms, personal histories and use of noisy playthings. Audiometric threshold testing was performed at the audiometric frequencies of 0.5, 1, 2, 4 and 8 kHz. Cluster analysis was used to estimate the associations between questions in the questionnaire and hearing impairment. Fifty-six subjects (20% subjects) were excluded from the normal groups. Twenty-two ears of the excluded subjects showed 4 kHz-dip and 38 ears showed high frequency hearing loss. An increased prevalence of hearing impairment was found when compared with young Japanese (1% from the nationwide school health survey) and with young Chinese in Shandong Prefecture (0.5%). In the questionnaire, 4 questions on dizziness, head trauma, aminoglycoside administration, and suspicion of Meniere's syndrome, were included in the cluster of hearing impairment. The cause of this hearing impairment was proposed to be the potentiating effects of aminoglycoside antibiotics and exposure to noise.DOI: 10.1016/s0033-3506(96)80092-8",pubmed,8885666,10.1016/s0033-3506(96)80092-8
forwardmasked frequency selectivity improvements in simulated and actual cochlear implant users using a preprocessing algorithm,"553. Trends Hear. 2016 Sep 7;20:2331216516659632. doi: 10.1177/2331216516659632.Forward-Masked Frequency Selectivity Improvements in Simulated and Actual Cochlear Implant Users Using a Preprocessing Algorithm.Langner F(1), Jürgens T(2).Author information:(1)Medizinische Physik, Cluster of Excellence ""Hearing4all,"" Carl von Ossietzky University, Oldenburg, Germany Forschungszentrum Neurosensorik, Carl von Ossietzky University, Oldenburg, Germany Department of Otolaryngology, Medical University Hannover, Hannover, Germany langner.florian@mh-hannover.de.(2)Medizinische Physik, Cluster of Excellence ""Hearing4all,"" Carl von Ossietzky University, Oldenburg, Germany Forschungszentrum Neurosensorik, Carl von Ossietzky University, Oldenburg, Germany.Frequency selectivity can be quantified using masking paradigms, such as psychophysical tuning curves (PTCs). Normal-hearing (NH) listeners show sharp PTCs that are level- and frequency-dependent, whereas frequency selectivity is strongly reduced in cochlear implant (CI) users. This study aims at (a) assessing individual shapes of PTCs in CI users, (b) comparing these shapes to those of simulated CI listeners (NH listeners hearing through a CI simulation), and (c) increasing the sharpness of PTCs using a biologically inspired dynamic compression algorithm, BioAid, which has been shown to sharpen the PTC shape in hearing-impaired listeners. A three-alternative-forced-choice forward-masking technique was used to assess PTCs in 8 CI users (with their own speech processor) and 11 NH listeners (with and without listening through a vocoder to simulate electric hearing). CI users showed flat PTCs with large interindividual variability in shape, whereas simulated CI listeners had PTCs of the same average flatness, but more homogeneous shapes across listeners. The algorithm BioAid was used to process the stimuli before entering the CI users' speech processor or the vocoder simulation. This algorithm was able to partially restore frequency selectivity in both groups, particularly in seven out of eight CI users, meaning significantly sharper PTCs than in the unprocessed condition. The results indicate that algorithms can improve the large-scale sharpness of frequency selectivity in some CI users. This finding may be useful for the design of sound coding strategies particularly for situations in which high frequency selectivity is desired, such as for music perception.© The Author(s) 2016.DOI: 10.1177/2331216516659632PMCID: PMC5017570",pubmed,27604785,10.1177/2331216516659632
brain morphological modifications in congenital and acquired auditory deprivation a systematic review and coordinatebased metaanalysis,"732. Front Neurosci. 2022 Mar 28;16:850245. doi: 10.3389/fnins.2022.850245. eCollection 2022.Brain Morphological Modifications in Congenital and Acquired Auditory Deprivation: A Systematic Review and Coordinate-Based Meta-Analysis.Grégoire A(1)(2), Deggouj N(1)(2), Dricot L(2), Decat M(1)(2), Kupers R(2)(3)(4).Author information:(1)Department of ENT, Cliniques Universitaires Saint-Luc, Brussels, Belgium.(2)Institute of NeuroScience (IoNS), UCLouvain, Brussels, Belgium.(3)Department of Neuroscience, Panum Institute, University of Copenhagen, Copenhagen, Denmark.(4)Ecole d'Optométrie, Université de Montréal, Montréal, QC, Canada.Erratum in    Front Neurosci. 2024 Jan 22;18:1354571.Neuroplasticity following deafness has been widely demonstrated in both humans and animals, but the anatomical substrate of these changes is not yet clear in human brain. However, it is of high importance since hearing loss is a growing problem due to aging population. Moreover, knowing these brain changes could help to understand some disappointing results with cochlear implant, and therefore could improve hearing rehabilitation. A systematic review and a coordinate-based meta-analysis were realized about the morphological brain changes highlighted by MRI in severe to profound hearing loss, congenital and acquired before or after language onset. 25 papers were included in our review, concerning more than 400 deaf subjects, most of them presenting prelingual deafness. The most consistent finding is a volumetric decrease in gray matter around bilateral auditory cortex. This change was confirmed by the coordinate-based meta-analysis which shows three converging clusters in this region. The visual areas of deaf children is also significantly impacted, with a decrease of the volume of both gray and white matters. Finally, deafness is responsible of a gray matter increase within the cerebellum, especially at the right side. These results are largely discussed and compared with those from deaf animal models and blind humans, which demonstrate for example a much more consistent gray matter decrease along their respective primary sensory pathway. In human deafness, a lot of other factors than deafness could interact on the brain plasticity. One of the most important is the use of sign language and its age of acquisition, which induce among others changes within the hand motor region and the visual cortex. But other confounding factors exist which have been too little considered in the current literature, such as the etiology of the hearing impairment, the speech-reading ability, the hearing aid use, the frequent associated vestibular dysfunction or neurocognitive impairment. Another important weakness highlighted by this review concern the lack of papers about postlingual deafness, whereas it represents most of the deaf population. Further studies are needed to better understand these issues, and finally try to improve deafness rehabilitation.Copyright © 2022 Grégoire, Deggouj, Dricot, Decat and Kupers.DOI: 10.3389/fnins.2022.850245PMCID: PMC8995770",pubmed,35418829,10.3389/fnins.2022.850245
a software system for diagnosis and classification of deafness,"This work attempts to model the expert reasoning processes of an Ear, Nose and Throat surgeon (otorhinolaryngologist) in his everyday work of diagnosing the level of deafness in his patients, via the use of the Pure Tone Audiometry (PTA) test. The coding is done using the Visual Prolog language. Data obtained from five patients is used to test the software. © EuroJournals Publishing, Inc. 2009.",scopus,2-s2.0-65449146749,
diagnosis of hearing impairment based on wavelet transformation and machine learning approach,"Hearing impairment has become the most widespread sensory disorder in the world, obstructing human-to-human communication and comprehension. The EEG-based brain-computer interface (BCI) technology may be an important solution to rehabilitating their hearing capacity for people who are unable to sustain verbal contact and behavioral response by sound stimulation. Auditory evoked potentials (AEPs) are a kind of EEG signal produced by an acoustic stimulus from the brain scalp. This study aims to develop an intelligent hearing level assessment technique using AEP signals to address these concerns. First, we convert the raw AEP signals into the time–frequency image using the continuous wavelet transform (CWT). Then, the Support vector machine (SVM) approach is used for classifying the time–frequency images. This study uses the reputed publicly available dataset to check the validation of the proposed approach. This approach achieves a maximum of 95.21% classification accuracy, which clearly indicates that the approach provides a very encouraging performance for detecting the AEPs responses in determining human auditory level. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",scopus,2-s2.0-85126937271,10.1007/978-981-16-8690-0_62
using machine learning to understand the relationships between audiometric data speech perception temporal processing and cognition,"Aging and hearing loss cause communication difficulties, particularly for speech perception in demanding situations, which have been associated with factors including cognitive processing and extended high-frequency (>8 kHz) hearing. Quantifying such associations and finding other (possibly unintuitive) associations is well suited to machine learning. We constructed ensemble models for 443 participants who varied in age and hearing loss. Audiometric, perceptual, electrophysiological, and cognitive data were used to predict speech perception in noise, reverberation, and with time compression. Speech perception was best predicted by variables associated with audiometric thresholds (including new across-frequency composite variables) between 1-4 kHz, followed by basic temporal processing ability. Cognitive factors and extended high-frequency thresholds had little to no predictive ability of speech perception. Future associations or lack thereof will inform the field as we attempt to better understand the intertwined effects of speech perception, aging, hearing loss, and cognition. © 2023 IEEE.",scopus,2-s2.0-85177565730,10.1109/ICASSP49357.2023.10095325
acquisition of subcortical auditory potentials with aroundtheear ceegrid technology in normal and hearing impaired listeners,"559. Front Neurosci. 2019 Jul 16;13:730. doi: 10.3389/fnins.2019.00730. eCollection 2019.Acquisition of Subcortical Auditory Potentials With Around-the-Ear cEEGrid Technology in Normal and Hearing Impaired Listeners.Garrett M(1)(2), Debener S(2)(3), Verhulst S(4).Author information:(1)Department of Medical Physics and Acoustics, University of Oldenburg, Oldenburg, Germany.(2)Cluster of Excellence ""Hearing4all"", Oldenburg, Germany.(3)Neuropsychology Laboratory, Department of Psychology, University of Oldenburg, Oldenburg, Germany.(4)Department of Information Technology, Hearing Technology @ WAVES, Ghent University, Ghent, Belgium.Even though the principles of recording brain electrical activity remain unchanged since their discovery, their acquisition has seen major improvements. The cEEGrid, a recently developed flex-printed multi-channel sensory array, can be placed around the ear and successfully record well-known cortical electrophysiological potentials such as late auditory evoked potentials (AEPs) or the P300. Due to its fast and easy application as well as its long-lasting signal recording window, the cEEGrid technology offers great potential as a flexible and 'wearable' solution for the acquisition of neural correlates of hearing. Early potentials of auditory processing such as the auditory brainstem response (ABR) are already used in clinical assessment of sensorineural hearing disorders and envelope following responses (EFR) have shown promising results in the diagnosis of suprathreshold hearing deficits. This study evaluates the suitability of the cEEGrid electrode configuration to capture these AEPs. cEEGrid potentials were recorded and compared to cap-EEG potentials for young normal-hearing listeners and older listeners with high-frequency sloping audiograms to assess whether the recordings are adequately sensitive for hearing diagnostics. ABRs were elicited by presenting clicks (70 and 100-dB peSPL) and stimulation for the EFRs consisted of 120 Hz amplitude-modulated white noise carriers presented at 70-dB SPL. Data from nine bipolar cEEGrid channels and one classical cap-EEG montage (earlobes to vertex) were analysed and outcome measures were compared. Results show that the cEEGrid is able to record ABRs and EFRs with comparable shape to those recorded using a conventional cap-EEG recording montage and the same amplifier. Signal strength is lower but can still produce responses above the individual neural electrophysiological noise floor. This study shows that the application of the cEEGrid can be extended to the acquisition of early auditory evoked potentials.DOI: 10.3389/fnins.2019.00730PMCID: PMC6646709",pubmed,31379484,10.3389/fnins.2019.00730
functional brain connections identify sensorineural hearing loss and predict the outcome of cochlear implantation,"667. Front Comput Neurosci. 2022 Mar 30;16:825160. doi: 10.3389/fncom.2022.825160. eCollection 2022.Functional Brain Connections Identify Sensorineural Hearing Loss and Predict the Outcome of Cochlear Implantation.Song Q(1), Qi S(1)(2), Jin C(1), Yang L(1), Qian W(3), Yin Y(4), Zhao H(5), Yu H(6).Author information:(1)College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China.(2)Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, China.(3)Department of Electrical and Computer Engineering, University of Texas at El Paso, El Paso, TX, United States.(4)Department of Radiology, The Affiliated Hospital of Guizhou Medical University, Guiyang, China.(5)Department of Otolaryngology, The Affiliated Hospital of Guizhou Medical University, Guiyang, China.(6)Department of Radiology, The Seventh Affiliated Hospital, Southern Medical University, Foshan, China.Identification of congenital sensorineural hearing loss (SNHL) and early intervention, especially by cochlear implantation (CI), are crucial for restoring hearing in patients. However, high accuracy diagnostics of SNHL and prognostic prediction of CI are lacking to date. To diagnose SNHL and predict the outcome of CI, we propose a method combining functional connections (FCs) measured by functional magnetic resonance imaging (fMRI) and machine learning. A total of 68 children with SNHL and 34 healthy controls (HC) of matched age and gender were recruited to construct classification models for SNHL and HC. A total of 52 children with SNHL that underwent CI were selected to establish a predictive model of the outcome measured by the category of auditory performance (CAP), and their resting-state fMRI images were acquired. After the dimensional reduction of FCs by kernel principal component analysis, three machine learning methods including the support vector machine, logistic regression, and k-nearest neighbor and their voting were used as the classifiers. A multiple logistic regression method was performed to predict the CAP of CI. The classification model of voting achieves an area under the curve of 0.84, which is higher than that of three single classifiers. The multiple logistic regression model predicts CAP after CI in SNHL with an average accuracy of 82.7%. These models may improve the identification of SNHL through fMRI images and prognosis prediction of CI in SNHL.Copyright © 2022 Song, Qi, Jin, Yang, Qian, Yin, Zhao and Yu.DOI: 10.3389/fncom.2022.825160PMCID: PMC9005839",pubmed,35431849,10.3389/fncom.2022.825160
hearing aids for the profoundly deaf based on neural net speech processing,"A new speech processing concept for Cochlear Implant (CI) - systems has been developed. It is based on robust feature extraction and a neural net classifier: Feature coefficients, extracted either by relative spectral perceptual linear predictive technique or regular CI-filtering, are classified into 'auditory related units'. The classifier is based on an adapted self-organizing Kohonen algorithm which finds representative clusters in the input feature vector space. These clusters are closely related to the statistical distribution of the feature coefficients and represent phonetic units. Firing neural net output nodes control the synthesis of a limited 'stimulus pattern alphabet'. Each 'letter' represents a subphoneme and is linked to a highly distinguishable complex stimulus pattern. The concept has been implemented with CINSTIM V2.0. First experimental results confirm the new CI speech processing strategy.",scopus,2-s2.0-0028996650,
association of metabolic syndrome with sensorineural hearing loss,"The prevalence of sensorineural hearing loss has increased along with increases in life expectancy and exposure to noisy environments. Metabolic syndrome (MetS) is a cluster of co-occurring conditions that increase the risk of heart disease, stroke and type 2 diabetes, along with other conditions that affect the blood vessels. Components of MetS include insulin resistance, body weight, lipid concentration, blood pressure, and blood glucose concentration, as well as other features of insulin resistance such as microalbuminuria. MetS has become a major public health problem affecting 20–30% of the global population. This study utilized health examination to investigate whether metabolic syndrome was related to hearing loss. Methods: A total of 94,223 people who underwent health check-ups, including hearing tests, from January 2010 to December 2020 were evaluated. Subjects were divided into two groups, with and without metabolic syndrome. In addition, Scopus, Embase, PubMed, and Cochrane libraries were systematically searched, using keywords such as “hearing loss” and “metabolic syndrome”, for studies that evaluated the relationship between the two. Results: Of the 94,223 subjects, 11,414 (12.1%) had metabolic syndrome and 82,809 did not. The mean ages of subjects in the two groups were 46.1 and 43.9 years, respectively. A comparison of hearing thresholds by age in subjects with and without metabolic syndrome showed that the average pure tone hearing thresholds were significantly higher in subjects with metabolic syndrome than in subjects without it in all age groups. (p < 0.001) Rates of hearing loss in subjects with 0, 1, 2, 3, 4, and 5 of the components of metabolic syndrome were 7.9%, 12.1%, 13.8%, 13.8%, 15.5% and 16.3%, respectively, indicating a significant association between the number of components of metabolic syndrome and the rate of hearing loss (p < 0.0001). The odds ratio of hearing loss was significantly higher in subjects with four components of metabolic syndrome: waist circumference, blood pressure, and triglyceride and fasting blood sugar concentrations (p < 0.0001). (4) Conclusions: The number of components of the metabolic syndrome is positively correlated with the rate of sensorineural hearing loss. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",scopus,2-s2.0-85117464041,10.3390/jcm10214866
perceptions of public primary school teachers regarding noiseinduced hearing loss in south africa,"204. S Afr J Commun Disord. 2017 Mar 28;64(1):e1-e12. doi: 10.4102/sajcd.v64i1.185.Perceptions of public primary school teachers regarding noise-induced hearing loss in South Africa.Ehlert K(1).Author information:(1)Department Speech-Language Pathology and Audiology, Sefako Makgatho Health Sciences University. katerina.ehlert@smu.ac.za.BACKGROUND: Noise-induced hearing loss (NIHL) is an increasingly growing problem in young children. This is attributed to recreational noise being the most common cause of this problem. In young children, hearing problems can delay language development and reduce academic achievements. South Africa, in particular, has limited information and protective measures regarding the conservation of hearing in school-aged children.OBJECTIVES: The main aim of the study was to determine the perception of primary school teachers regarding NIHL. The study also aimed to determine if any hearing conservation programmes are being implemented in schools and the need for training of primary school teachers regarding NIHL.METHOD: A survey was conducted. In order to cover the population of interest, the sampled schools in Pretoria were clustered into urban, semi-urban and rural areas.RESULTS: The majority of the teachers included in this study are aware of NIHL and its effects. They, however, lack the necessary resources and knowledge to effectively use this information. Most (67.5%) of the teachers indicated that they have never been exposed to children with NIHL in a school setting. It was also found that the majority (84%) of the schools included in the study do not implement hearing screening and conservation programmes.CONCLUSION: Although the sample size was limited, the results correlate with other research in this field indicating a need for planning and implementation of hearing conservation programmes in schools, including training of teachers in order for these programmes to be effective.DOI: 10.4102/sajcd.v64i1.185PMCID: PMC5843150",pubmed,28397520,10.4102/sajcd.v64i1.185
when hearing does not mean understanding on the neural processing of syntactically complex sentences by listeners with hearing loss,"201. J Speech Lang Hear Res. 2021 Jan 14;64(1):250-262. doi: 10.1044/2020_JSLHR-20-00262. Epub 2021 Jan 5.When Hearing Does Not Mean Understanding: On the Neural Processing of Syntactically Complex Sentences by Listeners With Hearing Loss.Vogelzang M(1)(2), Thiel CM(2)(3), Rosemann S(2)(3), Rieger JW(2)(4), Ruigendijk E(1)(2).Author information:(1)Institute of Dutch Studies, Carl von Ossietzky University of Oldenburg, Germany.(2)Cluster of Excellence ""Hearing4all"", Carl von Ossietzky University of Oldenburg, Germany.(3)Biological Psychology Lab, Department of Psychology, Faculty of Medicine and Health Sciences, Carl von Ossietzky University of Oldenburg, Germany.(4)Applied Neurocognitive Psychology Lab, Department of Psychology, Carl von Ossietzky University of Oldenburg, Germany.Purpose Adults with mild-to-moderate age-related hearing loss typically exhibit issues with speech understanding, but their processing of syntactically complex sentences is not well understood. We test the hypothesis that listeners with hearing loss' difficulties with comprehension and processing of syntactically complex sentences are due to the processing of degraded input interfering with the successful processing of complex sentences. Method We performed a neuroimaging study with a sentence comprehension task, varying sentence complexity (through subject-object order and verb-arguments order) and cognitive demands (presence or absence of a secondary task) within subjects. Groups of older subjects with hearing loss (n = 20) and age-matched normal-hearing controls (n = 20) were tested. Results The comprehension data show effects of syntactic complexity and hearing ability, with normal-hearing controls outperforming listeners with hearing loss, seemingly more so on syntactically complex sentences. The secondary task did not influence off-line comprehension. The imaging data show effects of group, sentence complexity, and task, with listeners with hearing loss showing decreased activation in typical speech processing areas, such as the inferior frontal gyrus and superior temporal gyrus. No interactions between group, sentence complexity, and task were found in the neuroimaging data. Conclusions The results suggest that listeners with hearing loss process speech differently from their normal-hearing peers, possibly due to the increased demands of processing degraded auditory input. Increased cognitive demands by means of a secondary visual shape processing task influence neural sentence processing, but no evidence was found that it does so in a different way for listeners with hearing loss and normal-hearing listeners.DOI: 10.1044/2020_JSLHR-20-00262",pubmed,33400550,10.1044/2020_JSLHR-20-00262
an innovative method for transimpedance matrix interpretation in hearing pathologies discrimination,"Trans-impedance measurement is a novel methodology for assessing the positioning of a cochlear implant (CI). This study proposes an innovative use of trans-impedance measurements to characterize specific hearing pathologies by means of the trans-impedance matrix (TIM) quantitative analysis. Three indices are used: Shannon Entropy, the Exponential Decay constant and Spatial Correlation. These indices were computed on the TIMs of two groups of patients, clustered in terms of hearing pathology: (i) congenital hearing loss (CONG) and (ii) otosclerosis (OTO). The study aimed to demonstrate the sensitivity of the above synthetic indices in relation to the considered hearing pathologies. Furthermore, the first two indices were employed to explore the influence of the positioning of the electrode, either over (i) the basal or (ii) the apical regions, on the TIMs patterns. The results suggest that the indices were statistically different for the patient groups and the positioning impacted solely on OTO patients. In particular: (i) CONG patients displayed significantly higher Shannon Entropy (p = 0.0002) and (ii) a lower Exponential Decay constant than OTO patients (p = 0.001); (iii) the OTO patients exhibited a lower Shannon Entropy and a higher Exponential Decay constant over the basal regions than the apical regions (p < 0.008); (iv) Spatial Correlation demonstrated that TIMs had specific patterns according to the hearing pathology (p < 0.008). © 2022 IPEM",scopus,2-s2.0-85124691412,10.1016/j.medengphy.2022.103771
machine learning models for predicting hearing prognosis in unilateral idiopathic sudden sensorineural hearing loss,,base,b790b3454f62fd6dcf0f064b80c111ee4b834caff76a2f1da923873d9dfe116d,
prediction of hearing prognosis after intact canal wall mastoidectomy with tympanoplasty using artificial intelligence,"Objective: To evaluate the performance of a machine learning model and the effects of major prognostic factors on hearing outcomes following intact canal wall (ICW) mastoidectomy with tympanoplasty. Study Design: Retrospective cross-sectional study. Setting: Tertiary hospital. Methods: A total of 484 patients with chronic otitis media who underwent ICW tympanomastoidectomy between January 2007 and December 2020 were included in this study. Successful hearing outcomes were defined by a postoperative air-bone gap (ABG) of ≤20 dB and preoperative air conduction (AC)-postoperative AC value of ≥15 dB according to the Korean Otological Society guidelines for outcome reporting after chronic otitis media surgery. The light gradient boosting machine (LightGBM) and multilayer perceptron (MLP) models were tested as artificial intelligence models and compared using logistic regression. The main outcome assessed was the successful hearing outcome after surgery, measured using the area under the receiver operating characteristic curve (AUROC). Results: In the analysis using the postoperative ABG criterion, the LightGBM exhibited a significantly higher AUROC compared to those of the baseline model (mean, 0.811). According to the difference between preoperative and postoperative AC, the MLP showed a significantly higher AUROC than those of the baseline model (mean, 0.795). Conclusion: This study analyzed multiple factors that could affect the hearing outcome using different artificial intelligence models and found that preoperative hearing status was the most important factor. Our findings provide additional information regarding postoperative hearing for clinicians. © 2023 American Academy of Otolaryngology–Head and Neck Surgery Foundation.",scopus,2-s2.0-85166776786,10.1002/ohn.472
contralateral suppression of human hearing sensitivity in singlesided deaf cochlear implant users,"72. Hear Res. 2019 Mar 1;373:121-129. doi: 10.1016/j.heares.2018.06.001. Epub 2018 Jun 15.Contralateral suppression of human hearing sensitivity in single-sided deaf cochlear implant users.Nogueira W(1), Krüger B(2), Büchner A(2), Lopez-Poveda E(3).Author information:(1)Medical University Hannover, Cluster of Excellence ""Hearing4all"", Hannover, Germany. Electronic address: nogueiravazquez.waldo@mh-hannover.de.(2)Medical University Hannover, Cluster of Excellence ""Hearing4all"", Hannover, Germany.(3)University of Salamanca, Salamanca, Spain.Cochlear implants (CIs) are being implanted in people with unilateral hearing loss because they can improve speech intelligibility and sound source localization. Though designed to restore the afferent auditory stimulation, the CI possibly restores some efferent effects. The present study aimed at investigating this possibility. Five single-sided deaf CI users with less than 30 dB hearing loss up to 4 kHz in their acoustic ear participated in the study. Absolute thresholds for their acoustic ears were measured for pure tones of 500 and 4000 Hz with durations of 10 and 200 ms in the presence and in the absence of contralateral broadband electrical stimulation (CBES) delivered with the CI. The electrical stimulus consisted of pulse trains (symmetric biphasic pulses with phase duration 36 μs) on all 16 electrodes sequentially stimulated at a rate of 843 Hz. Its intensity was set to sound as loud as broadband noise at 50 or 60 dB SPL in the acoustic ear. Thresholds were measured using a three-interval, three-alternative, forced-choice procedure with a two-down, one-up adaptive rule to estimate the level for 71% correct in the psychometric function. Thresholds measured without the CBES were lower for the longer than for the shorter tones, and the difference was larger at 500 than at 4000 Hz. CBES equivalent to 50 or 60 dB SPL caused significant threshold elevation only for short (10 ms) and low frequency (500 Hz) acoustic tones of 1.2 and 2.2 dB. These increases appear smaller than previously reported for normal hearing listeners in related experiments. These results support the notion that for single-sided deaf CI users, the CI modulates hearing in the acoustic ear. The possible mechanisms that may be contributing this effect are discussed.Copyright © 2018 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2018.06.001",pubmed,29941311,10.1016/j.heares.2018.06.001
the physiological basis and clinical use of the binaural interaction component of the auditory brainstem response,"97. Ear Hear. 2016 Sep-Oct;37(5):e276-e290. doi: 10.1097/AUD.0000000000000301.The Physiological Basis and Clinical Use of the Binaural Interaction Component of the Auditory Brainstem Response.Laumen G(#)(1), Ferber AT(#)(2), Klump GM(1), Tollin DJ(2).Author information:(1)Cluster of Excellence Hearing4all, Animal Physiology and Behavior Group, Department for Neuroscience, School of Medicine and Health Sciences, Oldenburg University, 26111 Oldenburg, Germany.(2)Department of Physiology and Biophysics, School of Medicine, University of Colorado, Aurora, Colorado 80045, USA.(#)Contributed equallyThe auditory brainstem response (ABR) is a sound-evoked noninvasively measured electrical potential representing the sum of neuronal activity in the auditory brainstem and midbrain. ABR peak amplitudes and latencies are widely used in human and animal auditory research and for clinical screening. The binaural interaction component (BIC) of the ABR stands for the difference between the sum of the monaural ABRs and the ABR obtained with binaural stimulation. The BIC comprises a series of distinct waves, the largest of which (DN1) has been used for evaluating binaural hearing in both normal hearing and hearing-impaired listeners. Based on data from animal and human studies, the authors discuss the possible anatomical and physiological bases of the BIC (DN1 in particular). The effects of electrode placement and stimulus characteristics on the binaurally evoked ABR are evaluated. The authors review how interaural time and intensity differences affect the BIC and, analyzing these dependencies, draw conclusion about the mechanism underlying the generation of the BIC. Finally, the utility of the BIC for clinical diagnoses are summarized.DOI: 10.1097/AUD.0000000000000301PMCID: PMC4996694",pubmed,27232077,10.1097/AUD.0000000000000301
applications of multivariate statistical and data mining analyses to the search for biomarkers of sensorineural hearing loss tinnitus and vestibular dysfunction,"700. Front Neurol. 2021 Mar 3;12:627294. doi: 10.3389/fneur.2021.627294. eCollection 2021.Applications of Multivariate Statistical and Data Mining Analyses to the Search for Biomarkers of Sensorineural Hearing Loss, Tinnitus, and Vestibular Dysfunction.Smith PF(1)(2)(3), Zheng Y(1)(2)(3).Author information:(1)Department of Pharmacology and Toxicology, Brain Health Research Centre, School of Biomedical Sciences, University of Otago, Dunedin, New Zealand.(2)Brain Research New Zealand Centre of Research Excellence, University of Auckland, Auckland, New Zealand.(3)The Eisdell Moore Centre for Hearing and Balance Research, University of Auckland, Auckland, New Zealand.Disorders of sensory systems, as with most disorders of the nervous system, usually involve the interaction of multiple variables to cause some change, and yet often basic sensory neuroscience data are analyzed using univariate statistical analyses only. The exclusive use of univariate statistical procedures, analyzing one variable at a time, may limit the potential of studies to determine how interactions between variables may, as a network, determine a particular result. The use of multivariate statistical and data mining methods provides the opportunity to analyse many variables together, in order to appreciate how they may function as a system of interacting variables, and how this system or network may change as a result of sensory disorders such as sensorineural hearing loss, tinnitus or different types of vestibular dysfunction. Here we provide an overview of the potential applications of multivariate statistical and data mining techniques, such as principal component and factor analysis, cluster analysis, multiple linear regression, random forest regression, linear discriminant analysis, support vector machines, random forest classification, Bayesian classification, and orthogonal partial least squares discriminant analysis, to the study of auditory and vestibular dysfunction, with an emphasis on classification analytic methods that may be used in the search for biomarkers of disease.Copyright © 2021 Smith and Zheng.DOI: 10.3389/fneur.2021.627294PMCID: PMC7966509",pubmed,33746881,10.3389/fneur.2021.627294
neuroanatomical alterations in tinnitus assessed with magnetic resonance imaging,"748. Front Aging Neurosci. 2016 Sep 21;8:221. doi: 10.3389/fnagi.2016.00221. eCollection 2016.Neuroanatomical Alterations in Tinnitus Assessed with Magnetic Resonance Imaging.Allan TW(1), Besle J(1), Langers DR(2), Davies J(2), Hall DA(2), Palmer AR(1), Adjamian P(1).Author information:(1)Medical Research Council Institute of Hearing Research, The University of Nottingham Nottingham, UK.(2)Nottingham Hearing Biomedical Research Unit, National Institute for Health Research (NIHR)Nottingham, UK; Otology and Hearing Group, Division of Clinical Neuroscience, School of Medicine, The University of NottinghamNottingham, UK.Previous studies of anatomical changes associated with tinnitus have provided inconsistent results, with some showing significant cortical and subcortical changes, while others have found effects due to hearing loss, but not tinnitus. In this study, we examined changes in brain anatomy associated with tinnitus using anatomical scans from 128 participants with tinnitus and hearing loss, tinnitus with clinically normal hearing, and non-tinnitus controls with clinically normal hearing. The groups were matched for hearing loss, age and gender. We employed voxel- and surface-based morphometry (SBM) to investigate gray and white matter volume and thickness within regions-of-interest (ROI) that were based on the results of previous studies. The largest overall effects were found for age, gender, and hearing loss. With regard to tinnitus, analysis of ROI revealed numerous small increases and decreases in gray matter and thickness between tinnitus and non-tinnitus controls, in both cortical and subcortical structures. For whole brain analysis, the main tinnitus-related significant clusters were found outside sensory auditory structures. These include a decrease in cortical thickness for the tinnitus group compared to controls in the left superior frontal gyrus (SFG), and a decrease in cortical volume with hearing loss in left Heschl's gyrus (HG). For masked analysis, we found a decrease in gray matter volume in the right Heschle's gyrus for the tinnitus group compared to the controls. We found no changes in the subcallosal region as reported in some previous studies. Overall, while some of the morphological differences observed in this study are similar to previously published findings, others are entirely different or even contradict previous results. We highlight other discrepancies among previous results and the increasing need for a more precise subtyping of the condition.DOI: 10.3389/fnagi.2016.00221PMCID: PMC5030287",pubmed,27708577,10.3389/fnagi.2016.00221
a new active osseointegrated implant system in patients with singlesided deafness,"142. Audiol Neurootol. 2022;27(1):83-92. doi: 10.1159/000515489. Epub 2021 Apr 26.A New Active Osseointegrated Implant System in Patients with Single-Sided Deafness.Willenborg K(1), Avallone E(1), Maier H(1)(2), Lenarz T(1)(2), Busch S(1)(2).Author information:(1)Department of Otorhinolaryngology, Medical University Hannover, Hannover, Germany.(2)Cluster of Excellence Hearing4all, Medical University Hannover, Hannover, Germany.OBJECTIVE: The Cochlear™ Osia® System (Osia) is an active transcutaneous bone conduction implant system intended for patients with conductive and mixed hearing loss but can also be used in cases of single-sided deafness (SSD) for the contralateral routing of signal (CROS). The Osia implant is placed subcutaneously under the intact skin behind the ear with the piezoelectric actuator connected to an osseointegrated BI300 implant - a titanium screw used for a 2-stage Baha surgery - on the mastoid. The external processor is magnetically attached to the subcutaneous implant receiver coil. As the Osia has recently been CE certified and is new on the market, with limited patient outcome data for SSD available, the objective of this study was the evaluation of surgical procedure, audiological results, and patient satisfaction for the Osia in SSD patients.STUDY DESIGN: In a prospective, monocentric clinical observation study, 6 patients (18 years of age or older) with SSD and bone conduction thresholds pure tone average 0.5, 1, 2, and 4 kHz ≤25 dB HL on the contralateral side were implanted with an Osia. Analysis of clinical outcome data with respect to surgical technique, adverse events, audiological measurement, and subjective benefit for SSD patients was conducted. Audiological measurements performed included hearing thresholds, sound field thresholds, word recognition scores (WRS; in %) in quiet, and speech recognition thresholds in noise (in dB SNR). All tests were performed unaided and aided with the Osia. The subjective benefit with the Osia was determined by using 2 questionnaires; the Abbreviated Profile of Hearing Aid Benefit (APHAB) and the Bern Benefit in Single-Sided Deafness (BBSSD).RESULTS: Preliminary results indicate a straightforward surgical procedure with a low rate of complications and an improvement in speech perception in quiet, listening performance in everyday situations and patient satisfaction. However, in one of 6 subjects, a revision surgery had to be performed.CONCLUSION: Provided that SSD patients are open for CROS hearing, they can benefit from the Osia by reduced head shadow effects and better speech recognition. Special caution should be given to the skin at the site of implantation to avoid complications.© 2021 The Author(s) Published by S. Karger AG, Basel.DOI: 10.1159/000515489",pubmed,33902037,10.1159/000515489
toxicodynamics and toxicokinetics of amikacin in the guinea pig cochlea,"404. Hear Res. 1995 Mar;83(1-2):62-79. doi: 10.1016/0378-5955(94)00192-s.Toxicodynamics and toxicokinetics of amikacin in the guinea pig cochlea.Beaubien AR(1), Karpinski K, Ormsby E.Author information:(1)Biopharmaceutics and Pharmacodynamics Division, Ottawa, Ontario, Canada.An extensive overview of the relationship between cochlear toxicity and amikacin blood concentrations in the guinea pig is provided which should assist in the clinical application of this class of antibiotic. A data set previously used to relate the incidence of amikacin ototoxicity to dosing rates and blood concentrations was re-examined to assess the toxicodynamics of amikacin in terms of decibels of hearing loss across dosing rate, hearing frequency and time following drug exposure. Animals in this data set had received continuously i.v. infused amikacin over an 8-fold range of dosing rates. Preliminary analysis indicated that the data were consistent with a sigmoid relationship between hearing loss (decibels) and area under the amikacin plasma concentration vs time curve cumulated over the entire course of drug administration (cAUC). The sigmoid model was therefore used as the backbone of a far more comprehensive toxicodynamic model which described all the data with a single equation. Testing with this model showed that the cAUC required to produce half-maximum hearing loss (cAUC-1/2) was related to dosing rate (P < 0.01), to hearing frequency (P < 0.00001), and to post-drug interval (P < 0.00001). Maximum hearing loss (difference between upper and lower sigmoid asymptotes) was less than total and was significantly related to frequency (P < 0.00001). No effects could be detected on the sigmoid slope. Further modelling of the significant effects detected by the comprehensive toxicodynamic model was done to determine if they could be described by simple relationships or by biologically relevant sub-models. Modelling of maximum hearing loss (postulated to represent loss of mainly outer hair cell function) indicated that this parameter was constant at about 61 decibels for 2-12 kHz and linearly decreased with log frequency for frequencies > 12 kHz. Modelling of cAUC-1/2 on frequency indicated that there was a strong inverse linear relationship to log frequency. Modelling of cAUC-1/2 on post-drug interval indicated that delayed ototoxicity continued at progressively slower rates for at least 56 days after drug administration had ceased. Modelling of cAUC-1/2 on dosing rate showed an increased requirement for drug as the dosing rate decreased. However, cAUC-1/2 changed no more than 20% across the range of dosing rates compared to the 8-fold difference in mean steady-state plasma concentrations, suggesting that plasma concentration is not a primary determinant of ototoxicity. A toxicokinetic model was developed which explained the dosing rate effect on cAUC-1/2 very successfully.(ABSTRACT TRUNCATED AT 400 WORDS)DOI: 10.1016/0378-5955(94)00192-s",pubmed,7607992,10.1016/0378-5955(94)00192-s
comparison of alternative coupling methods of the vibrant soundbridge floating mass transducer,"340. Audiol Neurootol. 2016;21(6):347-355. doi: 10.1159/000453354. Epub 2017 Jan 10.Comparison of Alternative Coupling Methods of the Vibrant Soundbridge Floating Mass Transducer.Busch S(1), Lenarz T, Maier H.Author information:(1)Department of Otorhinolaryngology and Cluster of Excellence ""Hearing4all,"" Hannover Medical School, Hannover, Germany.The active middle ear implant Vibrant Soundbridge© provides a variety of coupling modalities of the floating mass transducer (FMT) to various structures of the ossicular chain and the round window. A retrospective analysis was performed on 125 subjects (n = 137 ears) (1) to compare the efficacy of the different FMT coupling modalities with increasing degree of hearing loss, (2) to compare the performance in speech outcome and the effective gain between the coupling types, and (3) to evaluate the risk of additional hearing loss of each coupling procedure. The patients were grouped according to their type of FMT coupling into incus vibroplasty (incus group, n = 59), round window vibroplasty with coupler (RWC group, n = 23), round window vibroplasty without coupler (RW group, n = 22), and oval window vibroplasty with coupler (OWC group, n = 33). For each coupling group, pre- and postoperative thresholds, the results of the Freiburg monosyllable test at 65 dB SPL, and the effective gain across frequencies (0.5-6 kHz) were evaluated. A logistic regression function was used to describe the relationship between word recognition scores (WRS, in % correct) and the mean bone conduction (BC) hearing loss. The surgical procedure had no clinically relevant effect on BC thresholds of patients in each coupling group. The BC pure tone average (PTA4) for 50% WRS predicted by the model function was similar for the incus (48.2 dB nHL), RW (47.8 dB nHL), and OWC (49.0 dB nHL) groups, but higher for the RWC group (67.9 dB nHL). However, the median WRS was 80% or better with no significant differences in speech perception between coupling types (Kruskal-Wallis test, p = 0.229). The effective gain shows an advantage for the incus coupling between 0.5 and 2 kHz over the other coupling types. The performance of the FMT coupling modalities is equally good for patients with a mild-to-moderate hearing loss, but the efficacy of coupling types differs for patients with greater hearing loss (>48 dB BC HL).© 2017 S. Karger AG, Basel.DOI: 10.1159/000453354",pubmed,28068651,10.1159/000453354
sensory audio focusing detection using braincomputer interface archetype,"Everyday people are placed in environments where countless conversations simultaneously take place within earshot. Speech intelligibility in the presence of multiple speakers, commonly known as the 'Cocktail Party Phenomenon', is significantly reduced for most hearing-impaired listeners who use hearing assistive devices [1]. Prior research addressing this issue include noise filtering based on trajectories of multiple moving speakers and locations of talking targets based on face detection [2][3]. This study focuses on the practicality of audio filtering through measuring electroencephalogram (EEG) signals using a Brain-Computer Interfaces (BCI) system. The study explores the use of machine learning algorithms to classify which speaker the listener is focusing on. In this study, training data is obtained of a listener focusing on one auditory stimulus (audiobook) while other auditory stimuli are presented at the same time. A g.Nautilus BCI headset was used to obtain EEG data. After collecting trial data for each audio source, a machine learning algorithm trains a classifier to distinguish one audiobook between another. Data was collected from five subjects in each trial. Results yielded an accuracy of above 90% from all three experiments. © 2019 IEEE.",scopus,2-s2.0-85081274441,10.1109/CogMI48466.2019.00022
digital approaches to automated and machine learning assessments of hearing scoping review references,"Background: Hearing loss affects 1 in 5 people worldwide and is estimated to affect 1 in 4 by 2050. Treatment relies on the accurate diagnosis of hearing loss; however, this first step is out of reach for > 80% of those affected. Increasingly automated approaches are being developed for self-administered digital hearing assessments without the direct involvement of professionals. Objective: This study aims to provide an overview of digital approaches in automated and machine learning assessments of hearing using pure-tone audiometry and to focus on the aspects related to accuracy, reliability, and time efficiency. This review is an extension of a 2013 systematic review. Methods: A search across the electronic databases of PubMed, IEEE, and Web of Science was conducted to identify relevant reports from the peer-reviewed literature. Key information about each report's scope and details was collected to assess the commonalities among the approaches. Results: A total of 56 reports from 2012 to June 2021 were included. From this selection, 27 unique automated approaches were identified. Machine learning approaches require fewer trials than conventional threshold-seeking approaches, and personal digital devices make assessments more affordable and accessible. Validity can be enhanced using digital technologies for quality surveillance, including noise monitoring and detecting inconclusive results. Conclusions: In the past 10 years, an increasing number of automated approaches have reported similar accuracy, reliability, and time efficiency as manual hearing assessments. New developments, including machine learning approaches, offer features, versatility, and cost-effectiveness beyond manual audiometry. Used within identified limitations, automated assessments using digital devices can support task-shifting, self-care, telehealth, and clinical care pathways. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc22&DO=10.2196%2f32581
how do hearing aid owners acquire hearing aid management skills,"274. J Am Acad Audiol. 2019 Jun;30(6):516-532. doi: 10.3766/jaaa.17129. Epub 2019 Mar 7.How do Hearing Aid Owners Acquire Hearing Aid Management Skills?Bennett RJ(1)(2), Meyer CJ(3), Eikelboom RH(1)(2)(4).Author information:(1)Ear Science Institute Australia, Subiaco, Australia.(2)Ear Sciences Centre, The University of Western Australia, Nedlands, Australia.(3)School of Health and Rehabilitation Sciences, University of Queensland, St. Lucia, Australia.(4)Department of Speech-Language Pathology and Audiology, University of Pretoria, Pretoria, South Africa.BACKGROUND: Clinical studies have found up to 90% of hearing aid owners demonstrate difficulty with basic hearing aid management tasks and almost 50% of hearing aid owners self-report not receiving enough practical help from their clinician regarding how to use their hearing aid. Although studies have highlighted the overwhelming amount of information and training required to learn how to use a hearing aid appropriately, a gap remains in the literature regarding the range of methods by which hearing aid owners acquire the knowledge and skills for hearing aid use, and whether these approaches are considered beneficial.PURPOSE: To gain insight into how both hearing aid owners and hearing health clinicians view the acquisition of hearing aid management skills and the efficacy of currently used methods of hearing aid training.RESEARCH DESIGN: Concept mapping techniques were used to identify key themes, wherein participants generated, sorted, and rated the importance of statements in response to the question ""How do hearing aid owners learn the skills required to use, handle, manage, maintain, and care for their hearing aids?""STUDY SAMPLE: Twenty-four hearing aid owners (aged 56-91 years; 54.2% male) and 22 clinicians (aged 32-69 years; 9.1% male).DATA COLLECTION AND ANALYSIS: Participant perspectives were collected via group concept mapping sessions in October 2015. Hierarchical cluster analysis was used to identify themes and develop a framework for understanding how skill acquisition occurs. Participants rated each method of hearing aid skill acquisition as to how beneficial it was and how often it was used.RESULTS: Participants identified 75 unique items describing how hearing aid management skills are acquired within six concepts: (1) Relationship with the clinician, (2) clinician as a source of knowledge and support, (3) hands-on experience, (4) seeking additional information, (5) asking support people for help, and (6) external resources.CONCLUSIONS: The results of this study highlight the diverse methods and sources by which hearing aid owners learn the skills necessary to use, manage, and maintain their hearing aids. Significant emphasis was placed on the role of the hearing health clinician to provide training, support, and an ongoing professional relationship, with lesser roles played by family, friends, and other health professionals.American Academy of Audiology.DOI: 10.3766/jaaa.17129",pubmed,30969909,10.3766/jaaa.17129
patterns of hearing aid usage predict hearing aid use amount data logged and selfreported and overreport,"507. J Am Acad Audiol. 2014 Feb;25(2):187-98. doi: 10.3766/jaaa.25.2.7.Patterns of hearing aid usage predict hearing aid use amount (data logged and self-reported) and overreport.Laplante-Lévesque A(1), Nielsen C(2), Jensen LD(2), Naylor G(2).Author information:(1)Eriksholm Research Centre, Oticon A/S, Denmark; Department of Behavioural Sciences and Learning, Linköping University, Sweden.(2)Eriksholm Research Centre, Oticon A/S, Denmark.BACKGROUND: Previous studies found that, on average, users overreport their daily amount of hearing aid use compared to objective measures such as data logging. However, the reasons for this are unclear.PURPOSE: This study assessed data-logged and self-reported amount of hearing aid use in a clinical sample of hearing aid users. It identified predictors of data-logged hearing aid use, self-reported hearing aid use, and hearing aid use overreport.RESEARCH DESIGN: This observational study recruited adult hearing aid users from 22 private dispensers in the Netherlands and in Denmark.STUDY SAMPLE: The sample consisted of 228 hearing aid users. Typical participants were over the age of 65 and retired, were fitted binaurally, and had financially contributed to the cost of their hearing aids. Participants had on average a mild-to-severe sloping bilateral hearing impairment.DATA COLLECTION AND ANALYSIS: Participants completed a purposefully designed questionnaire regarding hearing aid usage and the International Outcome Inventory-Hearing Aids. Dispensers collected audiometric results and data logging. Multiple linear regression identified predictors of data-logged hearing aid use, self-reported hearing aid use, and hearing aid use overreport when controlling for covariates.RESULTS: Data logging showed on average 10.5 hr of hearing aid use (n = 184), while participants reported on average 11.8 hr of daily hearing aid use (n = 206). In participants for which both data-logged and self-reported hearing aid use data were available (n = 166), the average absolute overreport of daily hearing aid use was 1.2 (1 hr and 11 min). Relative overreport was expressed as a rate of absolute overreport divided by data-logged hearing aid use. A positive rate denotes hearing aid use overreport: the average overreport rate was .38. Cluster analysis identified two data-logged patterns: ""Regular,"" where hearing aids are typically switched on for between 12 and 20 hr before their user powers them off (57% of the sample), and ""On-off,"" where hearing aids are typically switched on for shorter periods of time before being powered off (43% of the sample). In terms of self-report, 77% of the sample described their hearing aid use to be the same every day, while 23% of the sample described their hearing aid use to be different from day to day. Participants for whom data logging showed an On-off pattern or who reported their hearing aid use to be different from day to day had significantly fewer data-logged and self-reported hours of hearing aid use. Having an On-off data-logging pattern or describing hearing aid use as the same every day was associated with a significantly greater hearing aid use overreport.CONCLUSIONS: Data-logged and self-reported usage patterns significantly predicted data-logged hearing aid use, self-reported hearing aid use, and overreport when controlling for covariates. The results point to patterns of hearing aid usage as being at least as important a concept as amount of hearing aid use. Dispensers should discuss not only the ""how much"", but also the ""how"" of hearing aid usage with their clients.American Academy of Audiology.DOI: 10.3766/jaaa.25.2.7",pubmed,24828219,10.3766/jaaa.25.2.7
does hearing aid use affect audiovisual integration in mild hearing impairment,"525. Exp Brain Res. 2018 Apr;236(4):1161-1179. doi: 10.1007/s00221-018-5206-6. Epub 2018 Feb 16.Does hearing aid use affect audiovisual integration in mild hearing impairment?Gieseler A(1)(2), Tahden MAS(3)(4), Thiel CM(3)(5), Colonius H(3)(4).Author information:(1)Cluster of Excellence 'Hearing4all', University of Oldenburg, Oldenburg, Germany. anja.gieseler@uni-oldenburg.de.(2)Cognitive Psychology Lab, Department of Psychology, University of Oldenburg, Oldenburg, Germany. anja.gieseler@uni-oldenburg.de.(3)Cluster of Excellence 'Hearing4all', University of Oldenburg, Oldenburg, Germany.(4)Cognitive Psychology Lab, Department of Psychology, University of Oldenburg, Oldenburg, Germany.(5)Biological Psychology Lab, Department of Psychology, University of Oldenburg, Oldenburg, Germany.There is converging evidence for altered audiovisual integration abilities in hearing-impaired individuals and those with profound hearing loss who are provided with cochlear implants, compared to normal-hearing adults. Still, little is known on the effects of hearing aid use on audiovisual integration in mild hearing loss, although this constitutes one of the most prevalent conditions in the elderly and, yet, often remains untreated in its early stages. This study investigated differences in the strength of audiovisual integration between elderly hearing aid users and those with the same degree of mild hearing loss who were not using hearing aids, the non-users, by measuring their susceptibility to the sound-induced flash illusion. We also explored the corresponding window of integration by varying the stimulus onset asynchronies. To examine general group differences that are not attributable to specific hearing aid settings but rather reflect overall changes associated with habitual hearing aid use, the group of hearing aid users was tested unaided while individually controlling for audibility. We found greater audiovisual integration together with a wider window of integration in hearing aid users compared to their age-matched untreated peers. Signal detection analyses indicate that a change in perceptual sensitivity as well as in bias may underlie the observed effects. Our results and comparisons with other studies in normal-hearing older adults suggest that both mild hearing impairment and hearing aid use seem to affect audiovisual integration, possibly in the sense that hearing aid use may reverse the effects of hearing loss on audiovisual integration. We suggest that these findings may be particularly important for auditory rehabilitation and call for a longitudinal study.DOI: 10.1007/s00221-018-5206-6",pubmed,29453491,10.1007/s00221-018-5206-6
effects of directional sound processing and listeners motivation on eeg responses to continuous noisy speech do normalhearing and aided hearingimpaired listeners differ,"309. Hear Res. 2019 Jun;377:260-270. doi: 10.1016/j.heares.2019.04.005. Epub 2019 Apr 11.Effects of directional sound processing and listener's motivation on EEG responses to continuous noisy speech: Do normal-hearing and aided hearing-impaired listeners differ?Mirkovic B(1), Debener S(2), Schmidt J(3), Jaeger M(4), Neher T(5).Author information:(1)Department of Psychology, University of Oldenburg, Ammerländer Heerstraße 114, 26129, Oldenburg, Germany; Cluster of Excellence ""Hearing4all"", Oldenburg, Germany. Electronic address: bojana.mirkovic@uni-oldenburg.de.(2)Department of Psychology, University of Oldenburg, Ammerländer Heerstraße 114, 26129, Oldenburg, Germany; Cluster of Excellence ""Hearing4all"", Oldenburg, Germany. Electronic address: stefan.debener@uni-oldenburg.de.(3)Department of Psychology, University of Oldenburg, Ammerländer Heerstraße 114, 26129, Oldenburg, Germany. Electronic address: julia.schmidt@uni-oldenburg.de.(4)Department of Psychology, University of Oldenburg, Ammerländer Heerstraße 114, 26129, Oldenburg, Germany. Electronic address: manuela.jaeger@uni-oldenburg.de.(5)Institute of Clinical Research, University of Southern Denmark, Campusvej 55, 5230, Odense M, Denmark. Electronic address: tneher@health.sdu.dk.OBJECTIVE: It has been suggested that the next major advancement in hearing aid (HA) technology needs to include cognitive feedback from the user to control HA functionality. In order to enable automatic brainwave-steered HA adjustments, attentional processes underlying speech-in-noise perception in aided hearing-impaired individuals need to be better understood. Here, we addressed the influence of two important factors for the listening performance of HA users - hearing aid processing and motivation - by analysing ongoing neural responses during long-term listening to continuous noisy speech.METHODS: Sixteen normal-hearing (NH) and 15 linearly aided hearing-impaired (aHI) participants listened to an audiobook recording embedded in realistic speech babble noise at individually adjusted signal-to-noise ratios (SNRs). A HA simulator was used for simulating a directional microphone setting as well as for providing individual amplification. To assess listening performance behaviourally, participants answered questions about the contents of the audiobook. We manipulated (1) the participants' motivation by offering a monetary reward for good listening performance in one half of the measurements and (2) the SNR by engaging/disengaging the directional microphone setting. During the speech-in-noise task, electroencephalography (EEG) signals were recorded using wireless, mobile hardware. EEG correlates of listening performance were investigated using EEG impulse responses, as estimated using the cross-correlation between the recorded EEG signal and the temporal envelope of the audiobook at the output of the HA simulator.RESULTS: At the behavioural level, we observed better performance for the NH listeners than for the aHI listeners. Furthermore, the directional microphone setting led to better performance for both participant groups, and when the directional microphone setting was disengaged motivation also improved the performance of the aHI participants. Analysis of the EEG impulse responses showed faster N1P2 responses for both groups and larger N2 peak amplitudes for the aHI group when the directional microphone setting was activated, but no physiological correlates of motivation.SIGNIFICANCE: The results of this study indicate that motivation plays an important role for speech understanding in noise. In terms of neuro-steered HAs, our results suggest that the latency of attentional processes is influenced by HA-induced stimulus changes, which can potentially be used for inferring benefit from noise suppression processing automatically. Further research is necessary to identify the neural correlates of motivation as an exclusive top-down process and to combine such features with HA-driven ones for online HA adjustments.Copyright © 2019 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2019.04.005",pubmed,31003037,10.1016/j.heares.2019.04.005
the prevalence of hearing impairment within the cape town metropolitan area,"392. S Afr J Commun Disord. 2016 Apr 8;63(1):105. doi: 10.4102/sajcd.v63i1.105.The prevalence of hearing impairment within the Cape Town Metropolitan area.Ramma L(1), Sebothoma B.Author information:(1)Division of Communication Sciences & Disorders, University of Cape Town. Lebogang.Ramma@uct.ac.za.BACKGROUND: There is a lack of data on the prevalence of hearing impairment in South Africa. Current data is unreliable as it is based on national census information which tends to underestimate the prevalence of hearing impairment.AIM: The aim of this study was to estimate the prevalence of hearing impairment in the Cape Town Metropolitan area and to determine factors associated with hearing impairment.METHOD: A cross-sectional household survey involving 2494 partcipants from 718 households was conducted between the months of February and October 2013. Random cluster sampling was used to select four health sub-districts from eight health sub-districts in the Cape Town Metropolitan area using a method of probability proportional to size (PPS). The survey was conducted according to the World Health Organization (WHO) Ear and Hearing Disorders Survey Protocol and the classifcation of hearing impairment matched the WHO's criteria for the grading of hearing impairment.RESULTS: The overall prevalence of hearing impairment in the population of this study was 12.35% (95%CI: 11.06% - 13.64%) and prevalence of disabling hearing impairment was 4.57% (95% CI: 3.75% - 5.39%) amongst individuals ≥ 4 years old. The following factors were found to be associated with hearing impairment; male gender, age, hypertension, a history of head and neck trauma and a family history of hearing impairment.CONCLUSION: Based on the data from communities surveyed during this study, hearing impairment is more prevalent than previously estimated based on national population census information. Interventions for the prevention of hearing impairment in these communities should focus on individuals with associated risk factors.DOI: 10.4102/sajcd.v63i1.105PMCID: PMC5843235",pubmed,27247255,10.4102/sajcd.v63i1.105
design and integration of alert signal detector and separator for hearing aid applications,"Alert signals like sirens and home alarms are important as they warn people of precarious situations. This work presents the detection and separation of these acoustically important alert signals, not to be attenuated as noise, to assist the hearing impaired listeners. The proposed method is based on convolutional neural network (CNN) and convolutional-recurrent neural network (CRNN). The developed method consists of two blocks, the detector block, and the separator block. The entire setup is integrated with speech enhancement (SE) algorithms, and before the compression stage, used in a hearing aid device (HAD) signal processing pipeline. The detector recognizes the presence of alert signal in various noisy environments. The separator block separates the alert signal from the mixture of noisy signals before passing it through SE to ensure minimal or no attenuation of the alert signal. It is implemented on a smartphone as an application that seamlessly works with HADs in real-time. This smartphone assistive setup allows the hearing aid users to know the presence of the alert sounds even when these are out of sight. The algorithm is computationally efficient with a low processing delay. The key contribution of this paper includes the development and integration of alert signal separator block with SE and the realization of the entire setup on a smartphone in real-time. The proposed method is compared with several state-of-the-art techniques through objective measures in various noisy conditions. The experimental analysis demonstrates the effectiveness and practical usefulness of the developed setup in real-world noisy scenarios.",ieee,2169-3536,10.1109/ACCESS.2020.2999546
functional mri in normal subjects and sudden unilateral sensorineural hearing loss patients,"550. Zhonghua Yi Xue Za Zhi. 2009 Sep 8;89(33):2329-32.[Functional MRI in normal subjects and sudden unilateral sensorineural hearing loss patients].[Article in Chinese]Yang M(1), Liu B, Teng GJ, Huang ZC, Gao WW, Ji H, Wu M, Feng X, Zhang HY, Wang J.Author information:(1)Department of Radiology, Zhongda Hospital of Southeast University, Nanjing 210009, China.OBJECTIVE: Brain activities in responses to amplitude modulation (AM) was evaluated using functional MRI (fMRI) in subjects with sudden unilateral sensorineural hearing loss (SSNHL) and those with normal hearing (NH).METHODS: Totally 25 subjects with normal hearing and 30 with SSNHL were examined with fMRI in response to AM tones of 500, 2000 and 4000 Hz respectively with the modulation frequency at 8 Hz. The fMRI was examined within 12 days after the onset of SSNHL. The AM signals were presented at 96 dB SPL binaurally. An event-related design was combined with a sparse clustered volume acquisitioning paradigm in data collection in the attempt to reduce the influence of acoustic scanner noise. SPM2 software was used for offline data analyzing.RESULTS: Brain activation in fMRI image was found mainly in the primary auditory cortex (PAC) in both subjects with NH and SSNHL NH subjects showed a clear lateralization to left cerebral hemisphere(11/16) and SSNHL patients showed a lateralization ipsilateral to the impaired ear(16/22). The activation voxel and intensity shown in BOLD were found to be decreased with increasing signal frequency in both groups.CONCLUSION: The difference in the lateralization between the two groups suggests that an adaptive process occurs shortly after the onset of SSNHL",pubmed,20095354,
neuronal networks and selforganizing maps new computer techniques in the acoustic evaluation of the infant cry,"365. Int J Pediatr Otorhinolaryngol. 1996 Dec 5;38(1):1-11. doi: 10.1016/s0165-5876(96)01389-4.Neuronal networks and self-organizing maps: new computer techniques in the acoustic evaluation of the infant cry.Schönweiler R(1), Kaese S, Möller S, Rinscheid A, Ptok M.Author information:(1)Department of Phoniatrics and Pedaudiology, Hannover Medical School, Germany.Neuronal networks are computer-based techniques for the evaluation and control of complex information systems and processes. So far, they have been used in engineering, telecommunications, artificial speech and speech recognition. A new approach in neuronal network is the self-organizing map (Kohonen map). In the phase of 'learning', the map adapts to the patterns of the primary signals. If, the phase of 'using the map', the input signal hits the field of the primary signals, it resembles them and is called a 'winner'. In our study, we recorded the cries of newborns and young infants using digital audio tape (DAT) and a high quality microphone. The cries were elicited by tactile stimuli wearing headphones. In 27 cases, delayed auditory feedback was presented to the children using a headphone and an additional three-head tape-recorder. Spectrographic characteristics of the cries were classified by 20-step bark spectra and then applied to the neuronal networks. It was possible to recognize similarities of different cries of the same children as well as interindividual differences, which are also audible to experienced listeners. Differences were obvious in profound hearing loss. We know much about the cries of both healthy and sick infants, but a reliable investigation regimen, which can be used for clinical routine purposes, has yet not been developed. If, in the future, it becomes possible to classify spectrographic characteristics automatically, even if they are not audible, neuronal networks may be helpful in the early diagnosis of infant diseases.DOI: 10.1016/s0165-5876(96)01389-4",pubmed,9119588,10.1016/s0165-5876(96)01389-4
preliminary investigation of the diagnosis and gene function of deep learning ptpn11 gene mutation syndrome deafness,"Syndromic deafness caused by PTPN11 gene mutation has gradually come into the public’s view. In the past, many people did not understand its application mechanism and role and only focused on non-syndromic deafness, so the research on syndromic deafness is not in-depth and there is a large degree of lack of research in this area. In order to let the public know more about the diagnosis and gene function of deafness caused by PTPN11 gene mutation syndrome, this paper used deep learning technology to study the diagnosis and gene function of deafness caused by syndrome with the concept of intelligent medical treatment, and finally drew a feasible conclusion. This paper provided a theoretical and practical basis for the diagnosis of deafness caused by PTPN11 gene mutation syndrome and the study of gene function. This paper made a retrospective analysis of the clinical data of 85 deaf children who visited Hunan Children’s Hospital,P.R. China from January 2020 to December 2021. The conclusion were as follows: Children aged 1–6 years old had multiple syndrome deafness, while children under 1 year old and children aged 6–12 years old had relatively low probability of complex deafness; girls were not easy to have comprehensive deafness, but there was no specific basis to prove that the occurrence of comprehensive deafness was necessarily related to gender; the hearing loss of patients with Noonan Syndrome was mainly characterized by moderate and severe damage and abnormal inner ear and auditory nerve; most of the mutation genes in children were located in Exon1 and Exon3, with a total probability of 57.65%. In the course of the experiment, it was found that deep learning was effective in the diagnosis of deafness with PTPN11 gene mutation syndrome. This technology could be applied to medical diagnosis to facilitate the diagnosis and treatment of more patients with deafness with syndrome. Intelligent medical treatment was also becoming a hot topic nowadays. By using this concept to analyze and study the pathological characteristics of deafness caused by PTPN11 gene mutation syndrome, it not only promoted patients to find diseases in time, but also helped doctors to diagnose and treat such diseases, which was of great significance to patients and doctors. The study of PTPN11 gene mutation syndrome deafness was also of great significance in genetics. The analysis of its genes not only enriched the gene pool, but also provided reference for future research. Copyright © 2023 Wu, Huang, Huang, Zhao, Xie, Liu and Chang.",scopus,2-s2.0-85147668351,10.3389/fgene.2023.1113095
predicting and classifying hearing loss in sailors working on speed vessels using neural networks a field study,"94. Med Lav. 2022 Jun 28;113(3):e2022023. doi: 10.23749/mdl.v113i3.12734.Predicting and classifying hearing loss in sailors working on speed vessels using neural networks: a field study.Esmaeili R(1), Zare S(2), Ghasemian F(3), Pourtaghi F(4), Saeidnia H(5), Pourtaghi G(6).Author information:(1)Marine Medicine Research Center, Baqiyatallah University of Medical Sciences, Tehran, Iran. rezaesmaeili794@yahoo.com.(2)Department of Occupational Health Engineering and Safety at Work, Faculty of Public Health, Kerman University of Medical Sciences, Kerman, Iran. ss_zare87@yahoo.com.(3)Department of Computer Engineering, Faculty of Engineering, Shahid Bahonar University of Kerman, Kerman, Iran. Ghasemianfahime@uk.ac.ir.(4)School of Medicine, Shahid Beheshti University of Medical Sciences, Tehran, Iran. dr.f.pourtaghi@gmail.com.(5)Marine Medicine Research Center, Baqiyatallah University of Medical Sciences, Tehran, Iran. hamidsaednia@gmail.com.(6)Health Research Center, Lifestyle institute, Baqiyatallah University of Medical Sciences, Tehran, Iran. pourtaghi@bmsu.ac.ir. Noise-induced hearing loss (NIHL) is one of the main risk factors affecting people's health and wellbeing in the workplace. Analysing NIHL and consequently controlling the causing factors can significantly affect the improvement of working environments. Methods: One hundred and twelve male sailors participated in this study. They were classified into three groups depending on occupational noise exposure: (A) none, i.e., sound pressure level (SPL) lower than 70dBA, (B) exposed to SPL in the range of 70-85dBA, and (C) exposed to SPL exceeding 80dBA. In a first phase, hearing loss shaping risk factors were identified and analysed, including hearing loss in different frequencies, age, work experience, sound pressure level (SPL), marital status, and systolic and diastolic blood pressure. Then, neural networks were trained to predict the hearing loss changes of personnel and used to determine the weight of hearing loss factors. Finally, the accuracy of predicting models was calculated relying on Bayesian statistics. Results and conclusion: In the present study using neural networks, five models were developed. Their accuracy ranged from 92% to 100%. The frequencies of 4000Hz and 2000Hz showed the strongest association with the hearing loss of the sailors. Also, including systolic and diastolic blood pressure did not have any impact on predicted hearing loss, indicating that SPL was poorly correlated with extra-auditory effects.DOI: 10.23749/mdl.v113i3.12734PMCID: PMC9437656",pubmed,35766647,10.23749/mdl.v113i3.12734
antioxidants and vasodilators for the treatment of noiseinduced hearing loss are they really effective,"680. Front Cell Neurosci. 2020 Jul 22;14:226. doi: 10.3389/fncel.2020.00226. eCollection 2020.Antioxidants and Vasodilators for the Treatment of Noise-Induced Hearing Loss: Are They Really Effective?Alvarado JC(1), Fuentes-Santamaría V(1), Juiz JM(1)(2).Author information:(1)Facultad de Medicina, Instituto de Investigación en Discapacidades, Neurológicas (IDINE), Universidad de Castilla-La Mancha, Albacete, Spain.(2)Department of Otolaryngology, Hannover Medical School, NIFE-VIANNA, Cluster of Excellence Hearing4all-German Research Foundation, Hannover, Germany.We live in a world continuously immersed in noise, an environmental, recreational, and occupational factor present in almost every daily human activity. Exposure to high-level noise could affect the auditory function of individuals at any age, resulting in a condition called noise-induced hearing loss (NIHL). Given that by 2018, more than 400 million people worldwide were suffering from disabling hearing loss and that about one-third involved noise over-exposure, which represents more than 100 million people, this hearing impairment represents a serious health problem. As of today, there are no therapeutic measures available to treat NIHL. Conventional preventive measures, including public awareness and education and physical barriers to noise, do not seem to suffice, as the population is still being affected by damaging noise levels. Therefore, it is necessary to develop or test pharmacological agents that may prevent and/or diminish the impact of noise on hearing. Data availability about the pathophysiological processes involved in triggering NIHL has allowed researchers to use compounds, that could act as effective therapies, by targeting specific mechanisms such as the excess generation of free radicals and blood flow restriction to the cochlea. In this review, we summarize the advantages/disadvantages of these therapeutic agents, providing a critical view of whether they could be effective in the human clinic.Copyright © 2020 Alvarado, Fuentes-Santamaría and Juiz.DOI: 10.3389/fncel.2020.00226PMCID: PMC7387569",pubmed,32792910,10.3389/fncel.2020.00226
in situ 3dimaging of the inner ear synapses with a cochlear implant,"722. Life (Basel). 2021 Apr 1;11(4):301. doi: 10.3390/life11040301.In Situ 3D-Imaging of the Inner Ear Synapses with a Cochlear Implant.Malfeld K(1)(2), Armbrecht N(1), Volk HA(2), Lenarz T(1)(3), Scheper V(1)(3).Author information:(1)Department of Otolaryngology, Hannover Medical School, Carl-Neuberg-Str. 1, 30625 Hannover, Germany.(2)Department of Small Animal Medicine and Surgery, University of Veterinary Medicine Hannover, 30559 Hannover, Germany.(3)Cluster of Excellence ""Hearing4all"", German Research Foundation, DFG (Deutsche Forschungsgemeinschaft), Hannover Medical School, Carl-Neuberg-Str. 1, 30625 Hannover, Germany.In recent years sensorineural hearing loss was found to affect not exclusively, nor at first, the sensory cells of the inner ear. The sensory cells' synapses and subsequent neurites are initially damaged. Auditory synaptopathies also play an important role in cochlear implant (CI) care, as they can lead to a loss of physiological hearing in patients with residual hearing. These auditory synaptopathies and in general the cascades of hearing pathologies have been in the focus of research in recent years with the aim to develop more targeted and individually tailored therapeutics. In the current study, a method to examine implanted inner ears of guinea pigs was developed to examine the synapse level. For this purpose, the cochlea is made transparent and scanned with the implant in situ using confocal laser scanning microscopy. Three different preparation methods were compared to enable both an overview image of the cochlea for assessing the CI position and images of the synapses on the same specimen. The best results were achieved by dissection of the bony capsule of the cochlea.DOI: 10.3390/life11040301PMCID: PMC8066088",pubmed,33915846,10.3390/life11040301
hearing impairment and socioeconomic factors a populationbased survey of an urban locality in southern brazil,"314. Rev Panam Salud Publica. 2007 Jun;21(6):381-7. doi: 10.1590/s1020-49892007000500006.Hearing impairment and socioeconomic factors: a population-based survey of an urban locality in southern Brazil.Béria JU(1), Raymann BC, Gigante LP, Figueiredo AC, Jotz G, Roithman R, Selaimen da Costa S, Garcez V, Scherer C, Smith A.Author information:(1)Graduate School of Public Health, Medical School, Lutheran University of Brazil, Canoas, Rio Grande do Sul, Brazil.OBJECTIVE: To provide the first population-based data on deafness and hearing impairment in Brazil.METHODS: In 2003, a cross-sectional household survey was conducted of 2,427 persons 4 years old and over. The study population was composed of 1,040 systematically chosen households in 40 randomly selected census tracts (dwelling clusters) in the city of Canoas, which is in the state of Rio Grande do Sul, in southern Brazil. Hearing function was evaluated in all subjects by both pure-tone audiometry and physical examination, using the World Health Organization Ear and Hearing Disorders Survey Protocol and definitions of hearing levels. The socioeconomic data that were gathered included the amount of schooling of all individuals tested and the income of the head of the household.RESULTS: It was found that 26.1% of the population studied showed some level of hearing impairment, and 6.8% (95% confidence interval (CI) = 5.5%-8.1%) were classified in the disabling hearing impairment group. The prevalence of moderate hearing loss was 5.4% (95% CI = 4.4%-6.4%); for severe hearing loss, 1.2% (95% CI = 0.7%-1.7%); and for profound hearing loss, 0.2% (95% CI = 0.03%-0.33%). The groups at higher risk for hearing loss were men (odds ratio (OR) = 1.54; 95% CI = 1.06-2.23); participants 60 years of age and over (OR = 12.55; 95% CI = 8.38-18.79); those with fewer years of formal schooling (OR = 3.92; 95% CI = 2.14-7.16); and those with lower income (OR = 1.56; 95% CI = 1.06-2.27).CONCLUSIONS: These results support advocacy by health policy planners and care providers for the prevention of deafness and hearing impairment. The findings could help build awareness in the community, in universities, and in government agencies of the health care needs that hearing problems create.DOI: 10.1590/s1020-49892007000500006",pubmed,17761050,10.1590/s1020-49892007000500006
effects of unilateral and bilateral cochlear implantation on cortical activity measured by an eeg neuroimaging method in childrendp   2014,"Bilateral implantation of a cochlear implant (CI) after a >2 year period of unilateral hearing with a second implant has been shown to result in altered latencies in brainstem responses in children with congenital deafness. In this thesis, a neural source localization method was developed to investigate the effects of unilateral CI use on cortical development after the implantation of a 2nd CI. The electroencephalography (EEG) source localization method is based on the linearly constrained minimum variance (LCMV) vector beamformer and utilizes null constraints to minimize the electrical artifact produced by the CI. The accuracy of the method was assessed and optimized through simulations and comparisons to beamforming with magnetoencephalography (MEG) data. After using cluster analyses to ensure that sources compared across subjects originate from the same neural generators, a study was done to examine the effects of unilateral CI hearing on hemispheric lateralization to monaural responses. It was found that a >2 year period of unilateral hearing results in expanded projections from the 1st implanted ear to the contralateral auditory area that is not reversed by implantation of a 2nd CI. A subsequent study was performed to examine the effects of unilateral CI hearing on the contributions of the 1st and 2nd implanted ears to the binaural response. It was found that in children with > 2 years of unilateral hearing, the binaural response is dominated by the 1st implanted ear. Together, these results suggest that the delay between the 1st and 2 nd CI should be minimized in bilateral implantation to avoid dominance of auditory pathways from the 1st implanted ear. This dominance limits developmental competition from the 2nd CI and potentially contributes to poorer performance in speech detection in noise tasks. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc13&AN=2014-99040-380
prediction of hearing recovery in unilateral sudden sensorineural hearing loss using artificial intelligence,"53. Sci Rep. 2022 Mar 10;12(1):3977. doi: 10.1038/s41598-022-07881-2.Prediction of hearing recovery in unilateral sudden sensorineural hearing loss using artificial intelligence.Lee MK(#)(1), Jeon ET(#)(2)(3), Baek N(2)(3), Kim JH(1), Rah YC(1), Choi J(4).Author information:(1)Department of Otorhinolaryngology-Head and Neck Surgery, Korea University Ansan Hospital, College of Medicine, Korea University, 123, Jeokgeum-ro (Gojan-dong), Danwon-gu, Ansan-si, Gyeonggi-do, 15355, Republic of Korea.(2)Department of Neurology, Korea University Ansan Hospital, College of Medicine, Korea University, Ansan, Republic of Korea.(3)Medical Science Research Center, Korea University Ansan Hospital, Korea University College of Medicine, Ansan, Republic of Korea.(4)Department of Otorhinolaryngology-Head and Neck Surgery, Korea University Ansan Hospital, College of Medicine, Korea University, 123, Jeokgeum-ro (Gojan-dong), Danwon-gu, Ansan-si, Gyeonggi-do, 15355, Republic of Korea. mednlaw@korea.ac.kr.(#)Contributed equallyDespite the significance of predicting the prognosis of idiopathic sudden sensorineural hearing loss (ISSNHL), no predictive models have been established. This study used artificial intelligence to develop prognosis models to predict recovery from ISSNHL. We retrospectively reviewed the medical data of 453 patients with ISSNHL (men, 220; women, 233; mean age, 50.3 years) who underwent treatment at a tertiary hospital between January 2021 and December 2019 and were followed up after 1 month. According to Siegel's criteria, 203 patients recovered in 1 month. Demographic characteristics, clinical and laboratory data, and pure-tone audiometry were analyzed. Logistic regression (baseline), a support vector machine, extreme gradient boosting, a light gradient boosting machine, and multilayer perceptron were used. The outcomes were the area under the receiver operating characteristic curve (AUROC) primarily, area under the precision-recall curve, Brier score, balanced accuracy, and F1 score. The light gradient boosting machine model had the best AUROC and balanced accuracy. Together with multilayer perceptron, it was also significantly superior to logistic regression in terms of AUROC. Using the SHapley Additive exPlanation method, we found that the initial audiogram shape is the most important prognostic factor. Machine/deep learning methods were successfully established to predict the prognosis of ISSNHL.© 2022. The Author(s).DOI: 10.1038/s41598-022-07881-2PMCID: PMC8913667",pubmed,35273267,10.1038/s41598-022-07881-2
the prevalence and causes of hearing impairment in oman a communitybased crosssectional study,"A community-based nationwide survey for hearing loss was conducted in Oman in 1996?97. Audiometric tests and ear examinations were conducted for 12400 persons in phase I. For children aged less than 4 years, subjective screening tests were used. In phase II, otologists examined the hearing-impaired subjects to determine the cause. The prevalence of bilateral hearing impairment was 55/1000 (95% CI 51.08-59.47). Gender difference was not significant. The rates were 325/1000 and 17/1000, respectively, in the 60-year and < 10-year age groups. Presbyacusis and middle ear diseases, respectively, were the causes of 33% and 20% of bilateral hearing impairment. In 30% of the bilateral hearing-impaired subjects, the cause could not be determined. The prevalence of bilateral disabling hearing loss was 21/1000 (95% CI 18.07-23.29). Noise-induced trauma was responsible for only 1.4% of cases of disabling hearing loss. Establishing primary ear care, introducing hearing screening for neonates and schoolchildren, promoting safe preventive practices for ear care, strengthening secondary-level ear care services and introducing comprehensive rehabilitative initiatives for the hearing-disabled are recommended to reduce the hearing loss rates.",cinahl,14992027,10.1080/14992020400050062
the use of a robot to insert an electrode array of cochlear implants in the cochlea a feasibility study and preliminary results,"352. Audiol Neurootol. 2021;26(5):361-367. doi: 10.1159/000513509. Epub 2021 Apr 26.The Use of a Robot to Insert an Electrode Array of Cochlear Implants in the Cochlea: A Feasibility Study and Preliminary Results.Barriat S(1), Peigneux N(1), Duran U(2), Camby S(1), Lefebvre PP(1).Author information:(1)Department of Otorhinolaryngology, Liège University, CHU de Liège, Liège, Belgium.(2)Department of Radiology, Liège University, CHU de Liège, Liège, Belgium.INTRODUCTION: Cochlear implants (CIs) are commonly used for the rehabilitation of profound bilateral hearing loss. However, patients with substantial residual acoustic hearing are potential CI candidates. Because of both improvements in technology and advancements in surgical techniques, it may be possible to preserve hearing to some extent. For more than a decade, it has been suggested that robots are used to perform middle ear surgery. We evaluated the use of the RobOtol® otologic robot specifically to insert CI electrodes into the inner ear.METHODS: CI surgery with the conventional approach was performed under general anesthesia. The MED-El Flex 24-electrode array was inserted using RobOtol®. Video recordings were used to calculate the speed of insertion. The positions of the electrodes were evaluated using a cone beam CT. All subjects underwent pure-tone audiometry tests before and after surgery, and the pure-tone average (PTA) was calculated from 250 to 4,000 Hz.RESULTS: The robot inserted implants in 5 patients, and complete insertion of the electrode array was achieved. The speed of insertion of the electrode array was 0.88 ± 0.12 mm/s. The mean loss of the PTA for 5 frequencies (250, 500, 1,000, 2,000, and 4,000 Hz) was 13.60 ± 7.70 dB. Only 1 patient showed a loss of the PTA by >20 dB. For these 5 patients, the cone beam CT findings showed that all the electrode arrays were in the tympanic ramp and had a grade of 0. The results were compared with those obtained from a cohort of 17 patients who underwent manual implantation of a MED-El Flex 24-electrode array.CONCLUSION: To minimize disturbance to the cochlea while atraumatic electrode arrays are inserted, electrodes can be inserted at a constant, slow speed in the inner ear with the assistance of the RobOtol® robot in a normal clinical surgical setting.© 2021 S. Karger AG, Basel.DOI: 10.1159/000513509",pubmed,33902040,10.1159/000513509
are experienced hearing aid users faster at grasping the meaning of a sentence than inexperienced users an eyetracking study,"431. Trends Hear. 2016 Sep 5;20:2331216516660966. doi: 10.1177/2331216516660966.Are Experienced Hearing Aid Users Faster at Grasping the Meaning of a Sentence Than Inexperienced Users? An Eye-Tracking Study.Habicht J(1), Kollmeier B(2), Neher T(2).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4all, Oldenburg University, Germany julia.habicht@uni-oldenburg.de.(2)Medizinische Physik and Cluster of Excellence Hearing4all, Oldenburg University, Germany.This study assessed the effects of hearing aid (HA) experience on how quickly a participant can grasp the meaning of an acoustic sentence-in-noise stimulus presented together with two similar pictures that either correctly (target) or incorrectly (competitor) depict the meaning conveyed by the sentence. Using an eye tracker, the time taken by the participant to start fixating the target (the processing time) was measured for two levels of linguistic complexity (low vs. high) and three HA conditions: clinical linear amplification (National Acoustic Laboratories-Revised), single-microphone noise reduction with National Acoustic Laboratories-Revised, and linear amplification ensuring a sensation level of ≥ 15 dB up to at least 4 kHz for the speech material used here. Timed button presses to the target stimuli after the end of the sentences (offline reaction times) were also collected. Groups of experienced (eHA) and inexperienced (iHA) HA users matched in terms of age, hearing loss, and working memory capacity took part (N = 15 each). For the offline reaction times, no effects were found. In contrast, processing times increased with linguistic complexity. Furthermore, for all HA conditions, processing times were longer (poorer) for the iHA group than for the eHA group, despite comparable speech recognition performance. Taken together, these results indicate that processing times are more sensitive to speech processing-related factors than offline reaction times. Furthermore, they support the idea that HA experience positively impacts the ability to process noisy speech quickly, irrespective of the precise gain characteristics.© The Author(s) 2016.DOI: 10.1177/2331216516660966PMCID: PMC5014089",pubmed,27595793,10.1177/2331216516660966
optimising hearing aid fittings for speech in noise with a differentiable hearing loss model,"Current hearing aids normally provide amplification based on a general prescriptive fitting, and the benefits provided by the hearing aids vary among different listening environments despite the inclusion of noise suppression feature. Motivated by this fact, this paper proposes a data-driven machine learning technique to develop hearing aid fittings that are customised to speech in different noisy environments. A differentiable hearing loss model is proposed and used to optimise fittings with back-propagation. The customisation is reflected on the data of speech in different noise with also the consideration of noise suppression. The objective evaluation shows the advantages of optimised custom fittings over general prescriptive fittings.  Copyright © 2021 ISCA.",scopus,2-s2.0-85119175694,10.21437/Interspeech.2021-1613
prediction of individual speech recognition performance in complex listening conditions,"47. J Acoust Soc Am. 2020 Mar;147(3):1379. doi: 10.1121/10.0000759.Prediction of individual speech recognition performance in complex listening conditions.Kubiak AM(1), Rennies J(1), Ewert SD(2), Kollmeier B(3).Author information:(1)Fraunhofer IDMT, Project Group Hearing, Speech and Audio Technology, Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany.(2)Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, 26111 Oldenburg, Germany.(3)Fraunhofer IDMT, Project Group Hearing, Speech and Audio Technology, Cluster of Excellence ""Hearing4all,"" Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, 26111 Oldenburg, Germany.This study examined how well individual speech recognition thresholds in complex listening scenarios could be predicted by a current binaural speech intelligibility model. Model predictions were compared with experimental data measured for seven normal-hearing and 23 hearing-impaired listeners who differed widely in their degree of hearing loss, age, as well as performance in clinical speech tests. The experimental conditions included two masker types (multi-talker or two-talker maskers), and two spatial conditions (maskers co-located with the frontal target or symmetrically separated from the target). The results showed that interindividual variability could not be well predicted by a model including only individual audiograms. Predictions improved when an additional individual ""proficiency factor"" was derived from one of the experimental conditions or a standard speech test. Overall, the current model can predict individual performance relatively well (except in conditions high in informational masking), but the inclusion of age-related factors may lead to even further improvements.DOI: 10.1121/10.0000759",pubmed,32237817,10.1121/10.0000759
a review on enhancing accessibility through image and video processing solutions for differently abled individuals,"This paper presents innovative methodologies in image and video processing aimed at augmenting accessibility for differently abled individuals. Central to this research is the development of advanced algorithms that enable enhanced interpretation and interaction with multimedia content, thereby empowering users with sensory impairments. The study introduces a multi-layered framework that integrates adaptive filtering, object recognition, and augmented reality, tailored to the needs of users with visual and auditory challenges. Semantic scene analysis is leveraged to provide descriptive audio annotations for the visually impaired, facilitating a comprehensive understanding of visual data. For individuals with hearing impairments, the system incorporates real-time sign language interpretation within videos, utilizing deep learning techniques. The efficacy of these solutions is measured against conventional accessibility tools, demonstrating significant improvements in user engagement and comprehension. A novel contribution of this research is the application of machine learning to calibrate the system according to individual user profiles, ensuring a personalized and intuitive user experience. The scalability of the proposed system is validated through its implementation across various platforms and content formats. The findings suggest that such technological advancements have the potential to significantly reduce the barriers faced by differently abled individuals in accessing multimedia information. © The Authors, published by EDP Sciences.",scopus,2-s2.0-85190614011,10.1051/e3sconf/202450503007
towards a mechanisticdriven precision medicine approach for tinnitus,"602. J Assoc Res Otolaryngol. 2019 Apr;20(2):115-131. doi: 10.1007/s10162-018-00709-9. Epub 2019 Mar 1.Towards a Mechanistic-Driven Precision Medicine Approach for Tinnitus.Tzounopoulos T(1), Balaban C(2), Zitelli L(2)(3), Palmer C(2)(3).Author information:(1)Pittsburgh Hearing Research Center and Department of Otolaryngology, University of Pittsburgh, Pittsburgh, PA, 15261, USA. thanos@pitt.edu.(2)Pittsburgh Hearing Research Center and Department of Otolaryngology, University of Pittsburgh, Pittsburgh, PA, 15261, USA.(3)Department of Communication Science and Disorders, University of Pittsburgh, Pittsburgh, PA, 15213, USA.In this position review, we propose to establish a path for replacing the empirical classification of tinnitus with a taxonomy from precision medicine. The goal of a classification system is to understand the inherent heterogeneity of individuals experiencing and suffering from tinnitus and to identify what differentiates potential subgroups. Identification of different patient subgroups with distinct audiological, psychophysical, and neurophysiological characteristics will facilitate the management of patients with tinnitus as well as the design and execution of drug development and clinical trials, which, for the most part, have not yielded conclusive results. An alternative outcome of a precision medicine approach in tinnitus would be that additional mechanistic phenotyping might not lead to the identification of distinct drivers in each individual, but instead, it might reveal that each individual may display a quantitative blend of causal factors. Therefore, a precision medicine approach towards identifying these causal factors might not lead to subtyping these patients but may instead highlight causal pathways that can be manipulated for therapeutic gain. These two outcomes are not mutually exclusive, and no matter what the final outcome is, a mechanistic-driven precision medicine approach is a win-win approach for advancing tinnitus research and treatment. Although there are several controversies and inconsistencies in the tinnitus field, which will not be discussed here, we will give a few examples, as to how the field can move forward by exploring the major neurophysiological tinnitus models, mostly by taking advantage of the common features supported by all of the models. Our position stems from the central concept that, as a field, we can and must do more to bring studies of mechanisms into the realm of neuroscience.DOI: 10.1007/s10162-018-00709-9PMCID: PMC6453992",pubmed,30825037,10.1007/s10162-018-00709-9
sensitivity and specificity of automatic audiological classification using expertlabelled audiological data and common audiological functional parameters,"As a step towards the development of an audiological diagnostic supporting tool employing machine learning methods, this article aims at evaluating the classification performance of different audiological measures as well as Common Audiological Functional Parameters (CAFPAs). CAFPAs are designed to integrate different clinical databases and provide abstract representations of measures. Classification and evaluation of classification performance in terms of sensitivity and specificity are performed on a data set from a previous study, where statistical models of diagnostic cases were estimated from expert-labelled data. The data set contains 287 cases. The classification performance in clinically relevant comparison sets of two competing categories was analysed for audiological measures and CAFPAs. It was found that for different audiological diagnostic questions a combination of measures using different weights of the parameters is useful. A set of four to six measures was already sufficient to achieve maximum classification performance which indicates that the measures contain redundant information. The current set of CAFPAs was confirmed to yield in most cases approximately the same classification performance as the respective optimum set of audiological measures. Overall, the concept of CAFPAs as compact, abstract representation of auditory deficiencies is confirmed.",cinahl,14992027,10.1080/14992027.2020.1817581
somatic memory and gain increase as preconditions for tinnitus insights from congenital deafness,"521. Hear Res. 2016 Mar;333:37-48. doi: 10.1016/j.heares.2015.12.018. Epub 2015 Dec 21.Somatic memory and gain increase as preconditions for tinnitus: Insights from congenital deafness.Eggermont JJ(1), Kral A(2).Author information:(1)Departments of Psychology, Physiology and Pharmacology, University of Calgary, Calgary, Alberta, Canada. Electronic address: eggermon@ucalgary.ca.(2)Cluster of Excellence Hearing4all, Institute of AudioNeuroTechnology and Dept. of Experimental Otology of the ENT Clinics, Hannover Medical School, Germany.Tinnitus is the conscious perception of sound heard in the absence of physical sound sources internal or external to the body. The characterization of tinnitus by its spectrum reflects the missing frequencies originally represented in the hearing loss, i.e., partially or completely deafferented, region. The tinnitus percept, despite a total hearing loss, may thus be dependent on the persisting existence of a somatic memory for the ""lost"" frequencies. Somatic memory in this context is the reference for phantom sensations attributed to missing sensory surfaces or parts thereof. This raises the question whether tinnitus can exist in congenital deafness, were somatic representations have not been formed. We review the development of tonotopic maps in altricial and precocial animals evidence for a lack of tinnitus in congenital deafness and the effects of cochlear implants on the formation of tonotopic maps in the congenitally deaf. The latter relates to the emergence of tinnitus in these subjects. The reviewed material is consistent with the hypothesis that tinnitus requires an established and actively used somatotopic map that leads to a corresponding somatic memory. The absence of such experience explains the absence of tinnitus in congenital bilateral and unilateral deafness.Copyright © 2015 The Authors. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2015.12.018",pubmed,26719143,10.1016/j.heares.2015.12.018
determinants of communication skills development in children with hearing impairment,"346. J Soc Bras Fonoaudiol. 2012;24(4):335-41.Determinants of communication skills development in children with hearing impairment.[Article in English, Portuguese]Novaes BC(1), Versolatto-Cavanaugh MC, Figueiredo Rde S, Mendes Bde C.Author information:(1)Graduate Program in Speech-Language Pathology and Audiology, Pontifícia Universidade Católica de São Paulo, São Paulo, SP, Brazil. beatriznovaes@pucsp.brPURPOSE: To establish relationships between age at onset of individual hearing aid use, functional hearing, communication skills, family involvement and family expectations regarding language development of children diagnosed with hearing loss during the first three years of life.METHODS: Thirty-five babies diagnosed with moderate to severe hearing loss who were receiving treatment at the Children's Hearing Center/Derdic (CeAC) were evaluated during a period of 24 months. Assessments were carried out every six months and included: VRA--Visual reinforcement audiometry (with and without amplification); IT-MAIS; MUSS; and satisfaction of family regarding child development.RESULTS: Cluster analysis was performed among the subjects. Consistent use of hearing aids was the only variable that exhibited a strong relationship with hearing and language skills. Children whose parents were not satisfied exhibited severe hearing loss and limited auditory capacity even with the use of hearing aid, and, consequently, poor auditory skills and speech production.CONCLUSION: Datalogging monitoring can guide the knowledge of speech-language pathologists and audiologists and it can also be used on strategic planning. Family involvement, quality of parental participation in the intervention program as well as expectations about the future are also important aspects to consider as these can aid therapists and researchers on the assessment of deaf babies intervention effectiveness.",pubmed,23306683,
conformities and gaps of clinical audiological data with the international classification of functioning disability and health core sets for hearing loss,"539. Int J Audiol. 2023 Jun;62(6):552-561. doi: 10.1080/14992027.2022.2078433. Epub 2022 Jun 19.Conformities and gaps of clinical audiological data with the international classification of functioning disability and health core sets for hearing loss.Afghah T(1)(2), Schütze J(3), Meis M(1)(2), Kollmeier B(1)(2)(3), Wagener KC(1)(2).Author information:(1)Hörzentrum Oldenburg gGmbH, Oldenburg, Germany.(2)Cluster of Excellence Hearing4all, Oldenburg, Germany.(3)Carl von Ossietzky, Universität Oldenburg, Oldenburg, Germany.OBJECTIVE: The International Classification of Functioning Disability and Health (ICF) is a classification of health and health-related domains created by the World Health Organization and can be used as a standard to evaluate the health and disability of individuals. The ICF Core Set for Hearing Loss (CSHL) refers to the ICF categories found to be relative to Hearing Loss (HL) and the consequences of it on daily life. This study aimed to adapt the content of a database gathered in Hörzentrum Oldenburg gGmbH that included HL medical assessments and audiological data to the ICF.DESIGN: ICF linking rules were applied to these assessment methods including medical interviews, ear examinations, pure-tone audiometry, Adaptive Categorical Loudness Scaling, and speech intelligibility test.STUDY SAMPLE: 1316 subjects.RESULTS: In total, 44% of the brief and 18% of the comprehensive CSHL categories were addressed. The hearing functions were broadly evaluated. ""Activities and Participation"" and ""Environmental Factors"" were poorly examined (17% and 12% of the comprehensive CSHL categories, respectively).CONCLUSIONS: The HL correlation with day-to-day activities limitation, performance restriction, and environmental conditions were poorly addressed. This study showed the essence of incorporating these methodologies with approaches that assess the daily-life challenges caused by HL in rehabilitation.DOI: 10.1080/14992027.2022.2078433",pubmed,35722856,10.1080/14992027.2022.2078433
analysis of correlation between window duration for kurtosis computation and accuracy of noiseinduced hearing loss prediction,"177. J Acoust Soc Am. 2021 Apr;149(4):2367. doi: 10.1121/10.0003954.Analysis of correlation between window duration for kurtosis computation and accuracy of noise-induced hearing loss prediction.Tian Y(1), Ding W(1), Zhang M(2), Zhou T(1), Li J(1), Qiu W(3).Author information:(1)Engineering Research Center of EMR and Intelligent Expert System, Ministry of Education, College of Biomedical Engineering and Instrument Science, Zhejiang University, Hangzhou, China.(2)Institute of Environmental and Occupational Health, Zhejiang Provincial Center for Disease Control and Prevention, Hangzhou, China.(3)Auditory Research Laboratory, State University of New York at Plattsburgh, Plattsburgh, New York 12901, USA.Kurtosis is considered an important metric for evaluating noise-induced hearing loss (NIHL). However, how to select window duration to calculate kurtosis remains unsolved. In this study, two algorithms were designed to investigate the correlation between window duration for kurtosis computation and the accuracy of NIHL prediction using a Chinese industrial database. Pure-tone hearing threshold levels (HTLs) and full-shift noise were recorded from each subject. In the statistical comparison, subjects were divided into high- and low-kurtosis groups based on kurtosis values computed over different window durations. Mann-Whitney U test was used to compare the difference in group HTLs to find the optimal window duration to best distinguish these two groups. In the support vector machine NIHL prediction model, kurtosis obtained from different window durations was used as a feature of the model for NIHL evaluation. The area under the curve was used to evaluate the performances of models. Fourteen window durations were tested for each algorithm. Results showed that 60 s was an optimal window duration that allows for both efficient computation and high accuracy for NIHL evaluation at test frequencies of 3, 4 and 6 kHz, and the geometric mean of kurtosis sequence was the best metric in NIHL evaluation.DOI: 10.1121/10.0003954",pubmed,33940921,10.1121/10.0003954
interactive visuallearning based tool for hearing impaired children to improve language and cognitive skills,"Hearing impairment is a common condition that affects millions of people worldwide, with approximately 1 in 1000 newborns experiencing some degree of hearing loss. This condition can significantly impact a person's quality of life, causing communication difficulties and social isolation. Early childhood hearing impairment can present significant challenges to a child's cognitive development, making it difficult for them to learn in a traditional educational setting. Consequently, there is an urge for effective learning tools that can assist hearing-impaired children in learning their first language. This paper introduces visual-based and interactive learning tools as a promising approach to enhancing the learning experience and engagement of hearing-impaired children. The proposed system utilizes machine learning to assess a child's initial status and subsequently generates adaptive content while continuously monitoring progress. Moreover, the system incorporates an object exploration feature that enables children to learn from their surroundings. Additionally, it employs natural language learning processes to present contextually similar content, supported by audio and lip movement analysis features that guide correct pronunciation. In response to the ML models utilized, object detection was accomplished through YOLO5, while lip movements were analyzed using LipNet. Regression models were deployed for assessing a child's initial status and subsequent progress. The data collection process involved comprehensive sources that covered a wide array of visual, auditory, and linguistic elements, with the dataset being rigorously tested and verified. Testing implementations were conducted in various real-world settings, encompassing diverse environmental factors and learning conditions. The YOLO model demonstrated an accuracy of 90.3%, LipNet achieved 92.2% accuracy in lip movement analysis, and the multivariate regression models showed a prediction accuracy of 92.8% in evaluating child progress.  © 2023 IEEE.",scopus,2-s2.0-85171805246,10.1109/ICIT58056.2023.10225863
objective detection of the central auditory processing disorder a new machine learning approach,"The objective detection of binaural interaction is of diagnostic interest for the evaluation of the central auditory processing disorder (CAPD). The β-wave of the binaural interaction component in auditory brainstem responses has been suggested as an objective measure of binaural interaction and has been shown to be of diagnostic value in the CAPD diagnosis. However, a reliable and automated detection of the β-wave capable of clinical use still remains a challenge. We propose a new machine learning approach to the detection of the CAPD that is based on adapted tight frame decompositions which are tailored for support vector machines with radial kernels. Using shift-invariant scale and morphological features of the binaurally evoked brainstem potentials, our approach provides at least comparable results to the β-wave detection in view of the discrimination of subjects being at risk for CAPD and subjects being not at risk for CAPD. Furthermore, as no information from the monaurally potentials is necessary, the measurement cost is reduced by two-thirds compared to the computation of the binaural interaction component. We conclude that a machine learning approach in the form of a hybrid tight frame-support vector classification is effective in the objective detection of the CAPD.",scopus,2-s2.0-2942729638,10.1109/TBME.2004.827948
recovery from sudden sensorineural hearing loss may be linked to chronic stress levels and steroid treatment resistance references,"Purpose: This article investigates the possible connections between the level of chronic stress and success of steroid therapy in patients with sudden sensorineural hearing loss (SSNHL). Method: A single-center, retrospective, longitudinal cohort study on 55 patients in a tertiary referral otology center was examined. Patients diagnosed with SSNHL between 2014 and 2017 were asked to complete a Measure of Perceived Stress (Brajac, Tkalcic, Dragojevic, & Gruber, 2003) questionnaire. Inclusion criteria were patients > 18 years of age, SSNHL diagnosed within 4 previous weeks, completed steroid treatment, and complete documentation. Results: There were 30 patients (55%) that showed significant improvement in their pure-tone audiogram (PTA) hearing threshold average (>= 15 dB) after steroid treatment. Two-step cluster analysis identified 3 clusters based on average PTA hearing threshold recovery and average Measure of Perceived Stress scores. The difference between pretreatment and posttreatment hearing levels was significantly higher in the cluster with moderate stress compared to clusters with mild and high stress levels (Kruskal-Wallis test, Friedman test, p < .001). There were no significant differences in average PTA hearing threshold recovery after steroid therapy between groups of patients with mild and severe stress. Conclusion: Patients with moderate stress levels show significantly better results after steroid treatment for SSNHL than patients with low or high stress levels. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc18&DO=10.1044%2f2019_AJA-18-0127
expert system for aiding diagnosis in hearing tests,"490. Biomed Tech (Berl). 2000 Sep;45(9):248-54. doi: 10.1515/bmte.2000.45.9.248.[Expert system for aiding diagnosis in hearing tests].[Article in German]Buller G(1), Hoth S, Suchandt S.Author information:(1)Hochschule Wismar, Fachbereich Elektrotechnik und Informatik. g.buller@et.hs-wismar.deFor expert systems intended to aid diagnosis, a structure with five levels is proposed. These levels are the original area, the parameter and a reduced parameter layer, the classification and the final-decision layer. On the basis of this structures, an expert system was developed specifically for neonatal hearing screening with transitory evoked otoacoustic emissions (TEOAE). In a second step, this system was investigated for its suitability to classify emissions, regardless of patient age. For the comparison measurements in 252 mainly adult patients, some with an acquired hearing impairment, were used. To adapt the pass/fail decision to the extended evaluation criteria, the false classifications from a first run with the new data were used for training. Thereafter, the expert system, working with a wider data basis, classified the new data with a sensitivity that was increased by 4.8% to 97.2%, and a 2.0% improvement in specificity to 95.5% when classifying new data, These results, together with those of 97.3% and 94.3% achieved with exclusively neonatal TEOAE classification, clearly show the advantage of the expert system structures chosen, and document evidence of the practical applicability of the method.DOI: 10.1515/bmte.2000.45.9.248",pubmed,11030095,10.1515/bmte.2000.45.9.248
maximising the ability of stimulusfrequency otoacoustic emissions to predict hearing status and thresholds using machinelearning models references,"Objective: This study aimed to maximise the ability of stimulus-frequency otoacoustic emissions (SFOAEs) to predict hearing status and thresholds based on machine-learning models. Design: SFOAE data and audiometric thresholds were collected at octave frequencies from 0.5 to 8 kHz. Support vector machine, k-nearest neighbour, back propagation neural network, decision tree, and random forest algorithms were used to build classification models for status identification and to develop regression models for threshold prediction. Study sample: About 230 ears with normal hearing and 737 ears with sensorineural hearing loss. Results: All classification models yielded areas under the receiver operating characteristic curve of 0.926-0.994 at 0.5-8 kHz, superior to the previous SFOAE study. The regression models produced lower standard errors (8.1-12.2 dB, mean absolute errors: 5.53-8.97 dB) as compared to those for distortion-product and transient-evoked otoacoustic emissions previously reported (8.6-19.2 dB). Conclusions: SFOAEs using machine-learning approaches offer promising tools for the prediction of hearing capabilities, at least at 0.5-4 kHz. Future research may focus on further improvements in accuracy and reductions in test time to improve clinical utility. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc19&DO=10.1080%2f14992027.2020.1821252
characteristics of six otologic diseases involving vertigo,"661. Am J Otol. 1996 Nov;17(6):883-92.Characteristics of six otologic diseases involving vertigo.Kentala E(1).Author information:(1)Department of Otolaryngology, University Hospital of Helsinki, Finland.Comment in    Am J Otol. 1997 Mar;18(2):267.To characterize otologic causes for vertigo, data on 564 patients with the six most common diseases involving vertigo were retrieved from the database of a computer-aided diagnostic system for neurotologic diseases. The diseases were Meniere's disease, vestibular schwannoma, benign paroxysmal positional vertigo, vestibular neuritis, sudden deafness, and traumatic vertigo. The prevalence of tinnitus in the study population was 76%. The most severe forms of vertigo and nausea were found in vestibular neuritis, whereas the most severe case of tinnitus appeared in Meniere's disease. Of the patients with vestibular schwannoma, 49% had had vertigo. A linear discrimination analysis using case history classified 90% of the patients into correct groups. The key questions discriminating between the diseases concerned the frequency and duration of vertigo attacks, the duration of hearing loss and vertigo, and the occurrence of head injury. Making a correct diagnosis during the first office visit can be difficult, especially for sudden deafness, vestibular schwannoma, and Meniere's disease. Neurotologic and audiometric information was of minor value in distinguishing between these six diseases. Vestibular schwannoma had significantly greater asymmetry in electronystagmography and smaller gains in smooth pursuit in comparison with the other disease. Factorial analysis did not aid the clustering of these diseases.",pubmed,8915417,
diagnosis of sensorineural hearing loss with neural networks versus logistic regression modeling of distortion product otoacoustic emissions,"377. Audiol Neurootol. 2004 Mar-Apr;9(2):81-7. doi: 10.1159/000075999.Diagnosis of sensorineural hearing loss with neural networks versus logistic regression modeling of distortion product otoacoustic emissions.Ziavra N(1), Kastanioudakis I, Trikalinos TA, Skevas A, Ioannidis JP.Author information:(1)ENT Department, University Hospital of Ioannina, Ioannina, Greece.We investigated whether modeling with artificial neural networks or logistic regression of distortion product otoacoustic emissions (DPOAE), across diverse frequencies, may achieve an accurate diagnosis of sensorineural hearing loss (SNHL) of cochlear origin. 256 ears (90 with SNHL and 166 with normal hearing) were evaluated with pure-tone audiometry, impedance audiometry, speech audiometry and DPOAE. Ears were split into training (n = 176) and validation (n = 80) sets. Input variables included gender, age, examination time, DPOAE intensity at F(2) frequencies 593, 937, 1906, 3812 and 6031 Hz, and respective values corrected for noise levels. In the validation data set, an average network had an area under the receiver operating characteristic curve (AUC) of 0.86 (accuracy 84%). Logistic regressions including all these variables or those selected by backward elimination had AUC values of 0.91 and 0.92, respectively (accuracy 85% both). Eleven of 12 trained networks had better specificity than the backward elimination logistic regression, and the backward elimination logistic regression had a better sensitivity than 11 of the 12 networks. Both modeling approaches correctly identified all ears with sudden hearing loss, congenital hearing loss, head trauma, nuclear jaundice and ototoxicity, and 2-3 of 5 ears with acoustic trauma, but missed 1-3 of 3 ears with Ménière's disease and 4-6 of 8 ears with abnormal pure-tone thresholds on audiometry which had no accompanying findings. For SNHL exceeding 45 dB HL on a pure-tone threshold, sensitivity was 83% (15/18) by neural networks and 84 or 94% (16/18 or 17/18) by logistic regression. Both neural-network-based analysis and logistic regression modeling of the DPOAE pattern across a range of frequencies offer promising approaches for the objective diagnosis of moderate and severe SNHL.Copyright 2004 S. Karger AG, BaselDOI: 10.1159/000075999",pubmed,14981356,10.1159/000075999
hand gesture recognition using cnn  publication of worlds largest asl database,"Sign language is used throughout the world by the hearing impaired to communicate. Recent advancements in Computer Vision and Deep Learning has given rise to many machine learning based translators. In this research paper, a solution to recognize the English alphabet presented as static signs in the American Sign Language (ASL) is proposed. The classifications are achieved by a four layer CNN. The model is trained and tested on a dataset created for this paper. This dataset will be published as a contribution to the community and is currently the world's largest ASL database consisting of 624,000 images. Split into two sections, the database contains images in both the IR and RGB spectrum. Classifications on both sets of data achieve state-of-the-art results when compared to similar research. An accuracy of 99.89% and 99.91 % are achieved when classifying the IR and RGB datasets respectively.",ieee,2642-7389,10.1109/ISCC53001.2021.9631255
cochlear implantation in children with charge syndrome therapeutic decisions and outcomes,"513. Laryngoscope. 2007 Jul;117(7):1260-6. doi: 10.1097/MLG.0b013e31806009c9.Cochlear implantation in Children with CHARGE syndrome: therapeutic decisions and outcomes.Lanson BG(1), Green JE, Roland JT Jr, Lalwani AK, Waltzman SB.Author information:(1)Department of Otolaryngology, NYU School of Medicine, New York, NY 10016, USA.OBJECTIVES: Ear anomalies and deafness are associated with CHARGE syndrome, which also presents with a cluster of features including coloboma of the eye, heart defects, atresia of the choanae, developmental retardation, and genitourinary abnormalities. The aim of this study is to explore the viability of cochlear implantation in children with CHARGE syndrome and to assess the outcome.STUDY DESIGN: Retrospective chart review.METHODS: Eleven children presenting with severe to profound sensorineural hearing loss associated with CHARGE syndrome were the subjects of this study. Routine audiometric measurements and the Infant Toddler Meaningful Auditory Integration Scale (IT-MAIS) were performed pre- and postoperatively. In addition, the degree of the subjects' cochlear deformity were measured and correlated to outcome.RESULTS: All patients had varying degrees of ear anomalies, seven patients suffered from coloboma of the eyes, two had heart defects, five exhibited choanal atresia, eleven showed developmental retardation, and six had genitourinary abnormalities. Ten of the children underwent cochlear implantation with complete insertion of the electrode array without complication and were followed over a 3-month to a 7-year period. The eleventh child was not implanted because of severe retardation. All of the implanted children showed varying, but limited degrees, of auditory benefit as measured by routine audiometry and the IT-MAIS.CONCLUSIONS: Careful treatment planning for children with sensorineural hearing loss and CHARGE syndrome can lead to varying, but limited degrees, of auditory benefit with no increase in surgical complications. Although the implant enhanced the children's 'connectivity' to the environment, it did not promote the development of oral language skills in this population.DOI: 10.1097/MLG.0b013e31806009c9",pubmed,17507827,10.1097/MLG.0b013e31806009c9
acemgmediated hearing preservation in cochlear implant patients receiving different electrode lengths prohearing study protocol for a randomized controlled trial,"425. Trials. 2016 Aug 8;17:394. doi: 10.1186/s13063-016-1526-7.ACEMg-mediated hearing preservation in cochlear implant patients receiving different electrode lengths (PROHEARING): study protocol for a randomized controlled trial.Scheper V(1)(2), Leifholz M(3), von der Leyen H(4), Keller M(4), Denkena U(4), Koch A(5), Karch A(5), Miller J(6), Lenarz T(3)(7).Author information:(1)Department of Otolaryngology, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany. scheper.verena@mh-hannover.de.(2)Cluster of Excellence Hearing4all, Hannover and Oldenburg, Germany. scheper.verena@mh-hannover.de.(3)Department of Otolaryngology, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany.(4)Hannover Clinical Trial Center, Carl-Neuberg-Str. 1, 30625, Hannover, Germany.(5)Institute for Biostatistics, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany.(6)Kresge Hearing Research Institute, University of Michigan, 4605 Medical Science Unit II, Ann Arbor, MI, 48109-5616, USA.(7)Cluster of Excellence Hearing4all, Hannover and Oldenburg, Germany.BACKGROUND: The indications for a cochlear implant (CI) have been extended to include patients with some residual hearing. Shorter and thinner atraumatic electrodes have been designed to preserve the residual hearing in the implanted ear. However, the insertion of the electrode array into the cochlea, with potential mechanical trauma and the presence of this foreign body inside the cochlea, may lead to free radical formation and reduced blood perfusion of the cochlea which can result in the loss of residual hearing.METHODS/DESIGN: In this single-center, randomized, placebo-controlled, double-blind phase II clinical trial the effect of free radical scavengers and a vasodilator on the residual hearing of 140 CI patients will be evaluated. The formulation is composed of β-carotene (vitamin A), ascorbic acid (vitamin C), dl-α-tocopherol acetate (vitamin E) and the vasodilator magnesium (Mg), or ACEMg. Medication is administered twice daily per os for approximately 3 months. The primary measure is based upon the reduction in postoperative low-frequency air-conducted pure-tone thresholds compared to preoperative thresholds in ACEMg-treated patients compared to those of a placebo group. Additionally, the effect of different electrode lengths (20, 24 and 28 mm) is analyzed. Study visits are scheduled 2 days before surgery, at first fitting, which is the adjustment and start of stimulation via CI 4 weeks after surgery and 3, 6, 9 and 12 months after first fitting. The primary endpoint is the air-conduction hearing loss at 500 Hz 3 months after first fitting. Additionally, speech recognition tests, hearing aid benefit in the implanted ear and electrophysiological measurements of implant function are assessed. Since this is a blinded clinical trial and recruitment is still ongoing, data continue to accrue and we cannot yet analyze the outcome of the ACEMg treatment.DISCUSSION: There is an unfulfilled need for new strategies to preserve acoustic hearing in CI patients. This study will provide first-in-man data on ACEMg-mediated protection of residual hearing in CI patients. Performing all surgeries and patient follow-up at one study site improves consistency in diagnosis and therapy and less variability in surgery, audiological test techniques and fitting. This approach will allow investigation of the influence of ACEMg on residual hearing in CI patients.TRIAL REGISTRATION: The German Bundesinstitut für Arzneimittel und Medizinprodukte (BfArM) application number 4039192, was registered on 6 December 2013 with protocol amendment version 3.0 from 19 August 2014. EudraCT number: 2012-005002-22 .DOI: 10.1186/s13063-016-1526-7PMCID: PMC4977680",pubmed,27502589,10.1186/s13063-016-1526-7
eye problems in children with hearing impairment,"Purpose: To compare the prevalence of refractive errors, amblyopia, and strabismus between hearing-impaired and normal children (7-22 years old) in Mashhad. Methods: In this cross-sectional study, cases were selected from hearing-impaired children in Mashhad. The control group consisted of children with no hearing problem. The sampling was done utilizing the cluster sampling method. All of the samples underwent refraction, cover test, and visual examinations. Results: 254 children in the hearing-impaired group (case) and 506 children in the control group were assessed. The mean spherical equivalent was 1.7 ± 1.9 D in the case group, which was significantly different from the control group (0.2 ± 1.5) (P < 0.001). The prevalence of hyperopia was 57.15% and 21.5% in deaf and normal children, respectively, but myopia was mostly seen in the control group (5.5% versus 11.9%, P = 0.007). The mean cylinder was 0.65 ± 1.3 D and 0.43 ± 0.62 D in deaf and normal subjects, respectively (P = 0.002). 12.2% of deaf subjects and 1.2% of normal subjects were amblyopic (P < 0.001), and the prevalence of strabismus was 3.1% in the case group and 2.6% in the control group (P = 0.645). Conclusion: In a comparison of children of the same ages, hearing-impaired children have significantly more eye problems; therefore, a possible relation between deafness and eye problems must exist. Paying attention to eye health assessment in hearing-impaired children may help prevent adding eye problems to hearing difficulties. © 2015 Iranian Society of Ophthalmology.",scopus,2-s2.0-84983167710,10.1016/j.joco.2015.10.001
the application of artificial neural network on the assessment of lexical tone production of pediatric cochlear implant users,"423. Zhonghua Er Bi Yan Hou Tou Jing Wai Ke Za Zhi. 2017 Aug 7;52(8):573-579. doi: 10.3760/cma.j.issn.1673-0860.2017.08.003.[The application of artificial neural network on the assessment of lexical tone production of pediatric cochlear implant users].[Article in Chinese; Abstract available in Chinese from the publisher]Mao YT(1), Chen ZM(2), Xu L(3).Author information:(1)Department of Radiology, Xiangya Hospital, Central South University, Changsha 410008, China; School of Rehabilitation and Communication Sciences, Ohio University, Athens, OH 45701, USA.(2)Department of Rehabilitation Medicine, Language Disorder Center, the First Affiliated Hospital of Jinan University, Guangzhou 510630, China.(3)School of Rehabilitation and Communication Sciences, Ohio University, Athens, OH 45701, USA.Objective: The present study was carried out to explore the tone production ability of the Mandarin-speaking children with cochlear implants (CI) by using an artificial neural network model and to examine the potential contributing factors underlining their tone production performance. The results of this study might provide useful guidelines for post-operative rehabilitation processes of pediatric CI users. Methods: Two hundred and seventy-eight prelingually deafened children who received unilateral CI participated in this study. As controls, 170 similarly-aged children with normal hearing (NH) were recruited. A total of 36 Chinese monosyllabic words were selected as the tone production targets. Vocal production samples were recorded and the fundamental frequency (F0) contour of each syllable was extracted using an auto-correlation algorithm followed by manual correction. An artificial neural network was created in MATLAB to classify the tone production. The relationships between tone production and several demographic factors were evaluated. Results: Pediatric CI users produced Mandarin tones much less accurately than did the NH children (58.8% vs. 91.5% correct). Tremendous variability in tone production performance existed among the CI children. Tones 2 and 3 were produced less accurately than tones 1 and 4 for both groups. For the CI group, all tones when in error tended to be judged as tone 1. The tone production accuracy was negatively correlated with age at implantation and positively correlated with CI use duration with correlation coefficients (r) of -0.215 (P=0.003) and 0.203 (P=0.005), respectively. Age was one of the determinants of tonal ability for NH children. Conclusions: For children with severe to profound hearing loss, early implantation and persistent use of CI are beneficial to their tone production development. Artificial neural network is a convenient and reliable assessment tool for the development of tonal ability of hearing-impaired children who are in the rehabilitation processes that focus on speech and language expression.Publisher: 目的： 了解以普通话为母语的语前聋人工耳蜗植入儿童在普通话声调发声上存在的问题并探索影响其声调发声能力的潜在因素，对基于计算机技术进行声调发声的特征提取和自动化判别的可行性进行实验论证，以期对此类儿童的术后声调发声康复提供指导。 方法： 278例语前聋单侧人工耳蜗植入儿童和170名年龄相仿的听力正常儿童参加了本项研究。对受试儿童的声调发声样本进行录音，在MATLAB平台上对每个受试儿童的每个发声音节进行基频提取并手动纠偏。构建人工神经网络，加以训练后对受试儿童的基频数据进行判读并分析结果。利用MATLAB统计工具对所得结果进行统计分析。 结果： 人工耳蜗植入儿童的声调发声准确率(58.8%)明显低于同龄正常儿童(91.5%)，且个体差异较大。二声和三声的发声准确率较一声和四声低，人工耳蜗植入儿童的声调发声出错时最易被判读成一声。人工耳蜗植入儿童声调发声准确率与植入年龄呈负相关(r＝－0.215, P＝0.003)，与设备使用时长呈正相关(r＝0.203, P＝0.005)。年龄是影响正常听力儿童声调发声能力的决定因素之一。 结论： 对于确诊为重度至极重度聋的儿童，早期植入人工耳蜗并长期坚持使用有利于其声调能力的发展。人工神经网络可作为监测聋儿声调发声能力发展及康复效果的方便且可靠的评估手段。.DOI: 10.3760/cma.j.issn.1673-0860.2017.08.003",pubmed,28822408,10.3760/cma.j.issn.1673-0860.2017.08.003
using visible speech to train perception and production of speech for individuals with hearing loss references,"The main goal of this study was to implement a computer-animated talking head, Baldi, as a language tutor for speech perception and production for individuals with hearing loss. Baldi can speak slowly; illustrate articulation by making the skin transparent to reveal the tongue, teeth, and palate; and show supplementary articulatory features, such as vibration of the neck to show voicing and turbulent airflow to show frication. Seven students with hearing loss between the ages of 8 and 13 were trained for 6 hours across 21 weeks on 8 categories of segments (4 voiced vs. voiceless distinctions, 3 consonant cluster distinctions, and 1 fricative vs. affricate distinction). Training included practice at the segment and the word level. Perception and production improved for each of the 7 children. Speech production also generalized to new words not included in the training lessons. Finally, speech production deteriorated somewhat after 6 weeks without training, indicating that the training method rather than some other experience was responsible for the improvement that was found. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc5&DO=10.1044%2f1092-4388%25282004%2f025%2529
plasticity in the tonotopic organization of the medial geniculate body in adult cats following restricted unilateral cochlear lesions,"To investigate subcortical contributions to cortical reorganization, the frequency organization of the ventral nucleus of the medial geniculate body (MGv) in six normal adult cats and in eight cats with restricted unilateral cochlear lesions was investigated using multiunit electrophysiological recording techniques. The tonotopic organization of MGv in the lesioned animals, with severe mid-to-high frequency hearing losses, was investigated 40-186 days following the lesioning procedure. Frequency maps were generated from neural responses to pure tone bursts presented separately to each ear under barbiturate anesthesia. Consideration of the frequency organization in normal animals, and of the apparently normal representation of the ipsilateral (unlesioned) cochlea in lesioned animals, allowed for a detailed specification of the extent of changes observed in MGv. In the lesioned animals it was found that, in the region of MGv in which mid-to-high frequencies are normally represented, there was an ""expanded representation"" of lesion-edge frequencies. Neuron clusters within these regions of enlarged representation that had ""new"" characteristic frequencies displayed response properties (latency, bandwidth) very similar to those in normal animals. Thresholds of these neurons were not consistent with the argument that the changes merely reflect the residue of prelesion responses, suggesting a dynamic process of reorganization. The tonotopic reorganization observed in MGv is similar to that seen in the primary auditory cortex and is more extensive than the reorganization found in the auditory midbrain, suggesting that the auditory thalamus plays an important role in cortical plasticity. © 2003 Wiley-Liss, Inc.",scopus,2-s2.0-0037433776,10.1002/cne.10586
an algorithm to increase speech intelligibility for hearingimpaired listeners in novel segments of the same noise type,"329. J Acoust Soc Am. 2015 Sep;138(3):1660-9. doi: 10.1121/1.4929493.An algorithm to increase speech intelligibility for hearing-impaired listeners in novel segments of the same noise type.Healy EW(1), Yoho SE(1), Chen J(2), Wang Y(2), Wang D(3).Author information:(1)Department of Speech and Hearing Science, Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, Ohio 43210, USA.(2)Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio 43210, USA.(3)Department of Computer Science and Engineering, Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, Ohio 43210, USA.Machine learning algorithms to segregate speech from background noise hold considerable promise for alleviating limitations associated with hearing impairment. One of the most important considerations for implementing these algorithms into devices such as hearing aids and cochlear implants involves their ability to generalize to conditions not employed during the training stage. A major challenge involves the generalization to novel noise segments. In the current study, sentences were segregated from multi-talker babble and from cafeteria noise using an algorithm that employs deep neural networks to estimate the ideal ratio mask. Importantly, the algorithm was trained on segments of noise and tested using entirely novel segments of the same nonstationary noise type. Substantial sentence-intelligibility benefit was observed for hearing-impaired listeners in both noise types, despite the use of unseen noise segments during the test stage. Interestingly, normal-hearing listeners displayed benefit in babble but not in cafeteria noise. This result highlights the importance of evaluating these algorithms not only in human subjects, but in members of the actual target population.DOI: 10.1121/1.4929493PMCID: PMC4592427",pubmed,26428803,10.1121/1.4929493
a review for different sign language recognition systems,"The fundamental aspects of communication that take place between human beings are exemplified by human language. For people who are deaf or hard of hearing, sign language is the primary mode of communication because the spoken language is inaccessible to those who are hard of hearing. As a result, many people are disabled because of hearing loss. The understanding of sign languages is a particular area of research interest. This study provides an overview and review of hand signals, gestures, and the most important methods utilised to recognise sign languages. The methods for understanding Sign Language are shown and explained. For each method, the accuracy is given. Many researchers have presented their research based on the main categories of these techniques. There are advantages and downsides, or limits associated with each technique. This study should be used as a guide to choose the best model to implement and as a road map for future research. This will help us improve the accuracy of future models and give the sign language community a better way to study how to make a fully video-based translator.",ieee,,10.1109/AIST55798.2022.10065037
musical hallucinosis in acquired deafness phenomenology and brain substrate,"623. Brain. 2000 Oct;123 ( Pt 10):2065-76. doi: 10.1093/brain/123.10.2065.Musical hallucinosis in acquired deafness. Phenomenology and brain substrate.Griffiths TD(1).Author information:(1)Department of Neurology, Newcastle University,Newcastle-upon-Tyne, UK. t.d.griffiths@ncl.ac.ukSix subjects with musical hallucinations following acquired deafness are described. The subjects all experienced the condition in the absence of any other features to suggest epilepsy or psychosis. I propose a neuropsychological model for the condition consistent with detailed observation of the subjects' phenomenology. The model is based on spontaneous activity within a cognitive module for the analysis of temporal pattern in segmented sound. Functional imaging was carried out to test the hypothesis that musical hallucinosis is due to activity within such a module, for which the neural substrate is a distributed network distinct from the primary auditory cortex. PET was carried out on the six subjects to identify areas where brain activity increased as a function of the severity of the hallucination. In a group analysis, no effect was demonstrated in the primary auditory cortices. Clusters of correlated activity were demonstrated in the posterior temporal lobes, the right basal ganglia, the cerebellum and the inferior frontal cortices. This network is similar to that previously demonstrated during the normal perception and imagery of patterned-segmented sound, and is consistent with the proposed neuropsychological and neural mechanism.DOI: 10.1093/brain/123.10.2065",pubmed,11004124,10.1093/brain/123.10.2065
improving hearing aid performance in noise challenges and strategies,[No abstract available],scopus,2-s2.0-0036528739,10.1097/01.HJ.0000293357.41334.07
methods for evaluating and improving audiometric examinations  orl system fowler,"The purpose of this work is to design the system, which eliminates redundant and repetitive tasks in the certain types of hearing function examinations. Proposed system reduces time needed to examine patient and also provides the full digitization of the results. The obtained database can recently become the base for subsequent processing in artificial intelligence expert systems. Such systems can increase diagnostic potentials. In cooperation with physicians, the above mentioned system began to be developed. Beside the steps automatizing examination there will be also described expert system based on the obtained data. The system is focused on pointing the examination which can reveal otosclerosis diagnosis.",ieee,,10.1109/CarpathianCC.2012.6228626
exploring the link between cognitive abilities and speech recognition in the elderly under different listening conditions,"694. Front Psychol. 2018 May 11;9:678. doi: 10.3389/fpsyg.2018.00678. eCollection 2018.Exploring the Link Between Cognitive Abilities and Speech Recognition in the Elderly Under Different Listening Conditions.Nuesse T(1)(2), Steenken R(1)(2), Neher T(2)(3)(4), Holube I(1)(2).Author information:(1)Institute of Hearing Technology and Audiology, Jade University of Applied Sciences, Oldenburg, Germany.(2)Cluster of Excellence ""Hearing4All"", Oldenburg, Germany.(3)Medizinische Physik, Oldenburg University, Oldenburg, Germany.(4)Faculty of Health Sciences, Institute of Clinical Research, University of Southern Denmark, Odense, Denmark.Elderly listeners are known to differ considerably in their ability to understand speech in noise. Several studies have addressed the underlying factors that contribute to these differences. These factors include audibility, and age-related changes in supra-threshold auditory processing abilities, and it has been suggested that differences in cognitive abilities may also be important. The objective of this study was to investigate associations between performance in cognitive tasks and speech recognition under different listening conditions in older adults with either age appropriate hearing or hearing-impairment. To that end, speech recognition threshold (SRT) measurements were performed under several masking conditions that varied along the perceptual dimensions of dip listening, spatial separation, and informational masking. In addition, a neuropsychological test battery was administered, which included measures of verbal working and short-term memory, executive functioning, selective and divided attention, and lexical and semantic abilities. Age-matched groups of older adults with either age-appropriate hearing (ENH, n = 20) or aided hearing impairment (EHI, n = 21) participated. In repeated linear regression analyses, composite scores of cognitive test outcomes (evaluated using PCA) were included to predict SRTs. These associations were different for the two groups. When hearing thresholds were controlled for, composed cognitive factors were significantly associated with the SRTs for the ENH listeners. Whereas better lexical and semantic abilities were associated with lower (better) SRTs in this group, there was a negative association between attentional abilities and speech recognition in the presence of spatially separated speech-like maskers. For the EHI group, the pure-tone thresholds (averaged across 0.5, 1, 2, and 4 kHz) were significantly associated with the SRTs, despite the fact that all signals were amplified and therefore in principle audible.DOI: 10.3389/fpsyg.2018.00678PMCID: PMC5968383",pubmed,29867654,10.3389/fpsyg.2018.00678
automatic identification of tinnitus malingering based on overt and covert behavioral responses during psychoacoustic testing,"**Abstract**

Tinnitus, or ringing in the ears, is a prevalent condition that imposes a substantial health and financial burden on the patient and to society. The diagnosis of tinnitus, like pain, relies on patient self-report, which can complicate the distinction between actual and fraudulent claims. Here, we combined tablet-based self-directed hearing assessments with neural network classifiers to automatically differentiate participants with tinnitus (*N* = 24) from a malingering cohort, who were instructed to feign an imagined tinnitus percept (*N* = 28). We identified clear differences between the groups, both in their overt reporting of tinnitus features, but also covert differences in their fingertip movement trajectories on the tablet surface as they performed the reporting assay. Using only 10 min of data, we achieved 81% accuracy classifying patients and malingerers (ROC AUC = 0.88) with leave-one-out cross validation. Quantitative, automated measurements of tinnitus salience could improve clinical outcome assays and more accurately determine tinnitus incidence.",cinahl,23986352,10.1038/s41746-022-00675-w
health hazards and hearing loss risk assessment of workers exposed to noise in an automobile manufacturing enterprise ,"Objective To investigate the current situation of occupational exposure to noise among noise workers in an automobile manufacturing enterprise in Tianjin, understand the impact of noise on workers' nervous system and hearing, and assess the risk of hearing loss among noise workers. Methods In May 2021, 3516 workers in an automobile manufacturing enterprise were investigated by using a self-made questionnaire ""Noise Workers Questionnaire"" and cluster sampling method. The occupational noise hygiene survey and occupational hazards detection were carried out in their workplaces. They were divided into noise exposure group and non-noise exposure group according to whether they were exposed to noise or not. The general characteristics, hearing and nervous system symptoms of the two groups of workers were compared, and the risk of hearing loss was assessed. Results There were 758 workers in the noise exposure group, aged (26±5) years old, with a working age of 3.0(2.0, 6.0) years exposed to noise. 2758 workers in the non-noise exposure group, aged (25 ±6) years old, with a working age of 2.0 (1.0, 4.0) years. There were statistically significant differences in the distribution of workers'education level, working age and memory loss between the two groups (χ2 =37.98, 38.70, 5.20, P <0.05). The workers in the noise exposure group showed a decreasing trend of insomnia, dreaminess, sweating and fatigue with the increase of working age (χ2trend=6.16, 7.99, P<0.05). The risk classification of binaural high-frequency hearing loss for workers in all noise positions until the age of 50 and 60 was negligible, the risk of occupational noise deafness was low for workers in stamping and welding noise positions until the age of 60. Conclusion The occupational noise exposed to automobile manufacturing workers may cause certain harm to their nervous and auditory systems. Noise protection measures should be taken to reduce the risk of hearing loss and occupational noise deafness. © 2022 Chinese Medical Journals Publishing House Co.Ltd. All Rights Reserved.",scopus,2-s2.0-85133283169,10.3760/cma.j.cn121094-20210615-00286
physiological assessment of speech and voice production of adults with hearing loss,"348. J Speech Hear Res. 1994 Jun;37(3):510-21. doi: 10.1044/jshr.3703.510.Physiological assessment of speech and voice production of adults with hearing loss.Higgins MB(1), Carney AE, Schulte L.Author information:(1)Boys Town National Research Hospital, Omaha, NE.The purpose of this investigation was to study the impact of hearing loss on phonatory, velopharyngeal, and articulatory functioning using a comprehensive physiological approach. Electroglottograph (EGG), nasal/oral air flow, and intraoral air pressure signals were recorded simultaneously from adults with impaired and normal hearing as they produced syllables and words of varying physiological difficulty. The individuals with moderate-to-profound hearing loss had good to excellent oral communication skills. Intraoral pressure, nasal air flow, durations of lip, velum, and vocal fold articulations, estimated subglottal pressure, mean phonatory air flow, fundamental frequency, and EGG abduction quotient were compared between the two subject groups. Data from the subjects with hearing loss also were compared across aided and unaided conditions to investigate the influence of auditory feedback on speech motor control. The speakers with hearing loss had significantly higher intraoral pressures, subglottal pressures, laryngeal resistances, and fundamental frequencies than those with normal hearing. There was notable between-subject variability. All of the individuals with profound hearing loss had at least one speech/voice physiology measure that fell outside of the normal range, and most of the subjects demonstrated unique clusters of abnormal behaviors. Abnormal behaviors were more evident in the phonatory than articulatory or velopharyngeal systems and were generally consistent with vocal fold hyperconstriction. There was evidence from individual data that vocal fold posturing influenced articulatory timing. The results did not support the idea that the speech production skills of adults with moderate-to-profound hearing loss who are good oral communicators deteriorate when there are increased motoric demands on the velopharyngeal and phonatory mechanism. Although no significant differences were found between the aided and unaided conditions, 7 of 10 subjects showed the same direction of change for subglottal pressure, intraoral pressure, nasal air flow, and the duration of lip and vocal fold articulations. We conclude that physiological assessments provide important information about the speech/voice production abilities of individuals with moderate-to-profound hearing loss and are a valuable addition to standard assessment batteries.DOI: 10.1044/jshr.3703.510",pubmed,8084183,10.1044/jshr.3703.510
artificial intelligence with deep learning based automated ear infection detection,"Artificial intelligence (AI) related to intelligent control in healthcare denotes using AI techniques to enhance the management and control of healthcare processes and systems. Damage to the inner and middle ear caused by accidents and diseases even causes hearing impairment in the ear that has been harmed or injured. Traditional otoscopy devices were utilized to check the tympanic membrane (TM) to identify OM in medical practice, and a conclusion is drawn depending on the outcomes of the examination. While developing a computer-aided method to support the OM diagnosis, it is possible to focus on methods like feature extraction, image pre-processing, classification, and image segmentation. The existing methodology of detecting the ear infection experiences a reduction of accuracy due to the influence of the noise in the input ear image. This presence of noise affects the feature extraction process, directly influences the accuracy in detection process. To overcome this issue, in this manuscript, a Deep learning (DL) is utilized to find biomedical ear infections by examining images of the eardrum and ear canal. The process includes training a DL method with a large dataset of ear images, where the images were labeled as either not infected or infected. With this motivation, this article emphasizes the design of Bayesian optimization with a deep learning-based automated ear infection detection and classification (BODL-AEIDC) model. The BODL-AEIDC technique exploits the DL model with a metaheuristic optimization algorithm for the ear infection classification process. The BODL-AEIDC technique employs a Wiener filtering (WF) based noise removal process to eliminate the noise data. In addition, the BODL-AEIDC technique exploits W-Net-based segmentation and the EfficientNet model for feature extraction purposes. Moreover, the BODL-AEIDC technique employs a fuzzy Restricted Boltzmann machine (FRBM) model for ear infection detection. Furthermore, the BO algorithm is utilized to adjust the FRBM technique's hyperparameter values effectively. The BODL-AEIDC technique's experimental outcomes occur using the medical dataset. The comprehensive comparative study stated the enhanced performance of the BODL-AEIDC approach over other existing methods.  © 2013 IEEE.",scopus,2-s2.0-85189622111,10.1109/ACCESS.2024.3383835
notch polymorphisms associated with sensitivity of noise induced hearing loss among chinese textile factory workers,"212. BMC Med Genet. 2018 Sep 14;19(1):168. doi: 10.1186/s12881-018-0676-8.Notch polymorphisms associated with sensitivity of noise induced hearing loss among Chinese textile factory workers.Ding E(1), Liu J(2), Shen H(3), Gong W(1), Zhang H(1), Song H(2), Zhu B(4).Author information:(1)Institute of Occupational Disease Prevention, Jiangsu Provincial Center for Disease Prevention and Control, No.172 Jiangsu Road, Nanjing, Jiangsu Province, 210009, People's Republic of China.(2)Nanjing Prevention and Treatment Center for Occupational Disease, Nanjing, Jiangsu Province, China.(3)Kunshan Centers for Disease Prevention and Control, Kunshan, Jiangsu Province, China.(4)Institute of Occupational Disease Prevention, Jiangsu Provincial Center for Disease Prevention and Control, No.172 Jiangsu Road, Nanjing, Jiangsu Province, 210009, People's Republic of China. zhubljscdc@126.com.BACKGROUND: Noise induced hearing loss (NIHL) is a polygenic disease involving both genetic and environmental factors, and is one of the most important occupational health hazards worldwide. To date, the influence of Notch1 variants on the risk to develop NIHL has not been illuminated. This study was conducted to explore the effects of Notch1 polymorphisms on individual susceptibility to NIHL.METHODS: A total of 2689 industrial workers from one textile factory in east China were recruited to participate in the current study. Venous blood was collected, basic clinical data was obtained by questionnaires and pure-tone audiometry (PTA) tests were conducted by specialist physicians. Next we performed genotyping of three selected SNPs (rs3124594, rs3124599 and rs3124603) in the Notch1 gene in 535 NIHL patients and 535 controls. Subsequently, the main effects of the genotypes and their interactions were evaluated.RESULTS: Our results revealed that individuals with a GG of rs3124594, TT of rs3124603 (OR = 4.70 and 1.59 respectively) and the haplotype AAC (rs3124594-rs3124599-rs3124603) (OR = 14.95) were associated with an increased risk of NIHL in our study cohort. Stratified analysis showed that an increased NIHL risk was found in individuals exposed to work related noise for ≤16 years that also had the rs3124594 GG or rs3124603 CT/TT genotype with an OR of 4.20 and 1.73 respectively. Multifactor dimensionality reduction analysis indicated that rs3124594, rs3124599 and rs3124603 interacted with each other and were related to an increased risk to develop NIHL (OR = 3.60).CONCLUSIONS: The genetic polymorphisms rs3124594 and rs3124603 within the Notch1 gene are associated with an increased risk of NIHL in a Chinese population and could potentially be used as biomarkers for NIHL in noise exposed workers.DOI: 10.1186/s12881-018-0676-8PMCID: PMC6137875",pubmed,30217173,10.1186/s12881-018-0676-8
epidemiologic study on hearing impairment and ear diseases in old people,"433. Zhonghua Er Bi Yan Hou Tou Jing Wai Ke Za Zhi. 2006 Sep;41(9):661-4.[Epidemiologic study on hearing impairment and ear diseases in old people].[Article in Chinese]Liu C(1), Bu XK, Xing GQ, Zhou L, Xu X, Wang DY, Chen ZB, Zhou H, Tian HQ, Li XL, Lu L, Zhao XN, Li FL, Tan CQ.Author information:(1)Department of Otorhinolaryngology, First Affiliated Hospital of Nanjing Medical University, Nanjing 210029, China.OBJECTIVE: To investigate the prevalence of hearing impairment and ear diseases in old people and provide scientific data for drawing up the prevention and treatment strategies.METHODS: Using the probability proportion to size (PPS) method, 1261 people over 60 years were investigated in 40 clusters in Jiangsu Province with the WHO protocol.RESULTS: The prevalence of hearing impairment was 58.1% (the standardized rate: 59.5% in the whole country, 60.9% in Jiangsu province). Degrees of hearing impairment were mild (33.1%), moderate (17.8%), severe (5.9%) and profound (1.3%). The prevalence of hearing disability was 25.0% (the standardized rate: 26.6% in the whole country, 28.1% in Jiangsu province). There were significant difference of the prevalence between male and female, as well as urban and rural, and different ages. The prevalence of the ear diseases was auricle malformation (0.2%), wax (1.7%), otitis externa (0.1%), fungi (0.5%), serous otitis media (1.2%), chronic suppurative otitis media (1.6%), dry perforation of tympanic membrance (2.3%). The causes of hearing impairment were ear diseases (2.9%), non-infectious condition (92.6%), genetic condition (0.3%) and undetermined causes (4.2%). Of which, 31.1% of persons needed hearing aids while 2.3% of persons needed medicine treatment, but 0.9% of persons needed non-urgent surgery and 1.0% of persons needed other treatment.CONCLUSIONS: The prevalence of hearing impairment and disability in the old rised obviously than the last investigation in 1987. It was a heavy burden for social development in China. The government and the whole society should take more concern about the problem. The scientific strategies of prevention and treatment were urgently needed and implemented.",pubmed,17111805,
association between metabolic syndrome and hearing impairment a study on 200 subjects,"719. Indian J Otolaryngol Head Neck Surg. 2024 Feb;76(1):262-267. doi: 10.1007/s12070-023-04138-w. Epub 2023 Aug 22.Association Between Metabolic Syndrome and Hearing Impairment: a Study on 200 Subjects.Sahni D(1), Bhagat S(1), Bhatia L(1), Singh P(1), Chawla S(1), Kaur A(1).Author information:(1)Government Medical College and Rajindra Hospital, Patiala, Punjab 147001 India.The metabolic syndrome (MS) is a cluster of conditions that occur. togehther, increase risk of heart disease, storke, type 2 diabetes mellitus and hypertension as a possible outcome. The previous research has shown a link between hearing loss and being overweight, diabetic, or suffering from heart disease. However, research on the possible link between hearing loss and metabolic syndrome is limited. Hearing loss due to metabolic syndrome was evaluated in the present investigation. Two hundred individuals with metabolic syndrome were included. All the patients were evaluated on three types of audiometry (pure tone, impedence, and DPOAE).Anthropometric data, blood pressure, blood sugar, and lipid profiles, were all collected from each patient. We also asked about their smoking and drinking habits in the past. SPSS v. 22.0 was used to conduct the statistical analysis. Overall, SNHL affected 58.5% of patients. Patients having moderate hearing loss were the largest demographic group (40%), followed by those with mild hearing loss (15% ). Severe hearing loss only occurred in 3.5% of patients. Hearing loss was shown to be more prevalent in patients with more than three components of metabolic syndrome. Significant associations were found between hearing impairment and metabolic risk factors as waist circumference, fasting blood sugar, serum high-density lipoprotein, serum triglycerides, and systolic and diastolic blood pressure. Hearing loss was only marginally connected to smoking and excessive drinking.© Association of Otolaryngologists of India 2023. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.DOI: 10.1007/s12070-023-04138-wPMCID: PMC10909006",pubmed,38440660,10.1007/s12070-023-04138-w
comparing binaural preprocessing strategies iii speech intelligibility of normalhearing and hearingimpaired listeners,"218. Trends Hear. 2015 Dec 30;19:2331216515618609. doi: 10.1177/2331216515618609.Comparing Binaural Pre-processing Strategies III: Speech Intelligibility of Normal-Hearing and Hearing-Impaired Listeners.Völker C(1), Warzybok A(2), Ernst SM(2).Author information:(1)Abteilung Medizinische Physik, Carl von Ossietzky Universität Oldenburg, Germany Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany christoph.voelker@uni-oldenburg.de.(2)Abteilung Medizinische Physik, Carl von Ossietzky Universität Oldenburg, Germany Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany.A comprehensive evaluation of eight signal pre-processing strategies, including directional microphones, coherence filters, single-channel noise reduction, binaural beamformers, and their combinations, was undertaken with normal-hearing (NH) and hearing-impaired (HI) listeners. Speech reception thresholds (SRTs) were measured in three noise scenarios (multitalker babble, cafeteria noise, and single competing talker). Predictions of three common instrumental measures were compared with the general perceptual benefit caused by the algorithms. The individual SRTs measured without pre-processing and individual benefits were objectively estimated using the binaural speech intelligibility model. Ten listeners with NH and 12 HI listeners participated. The participants varied in age and pure-tone threshold levels. Although HI listeners required a better signal-to-noise ratio to obtain 50% intelligibility than listeners with NH, no differences in SRT benefit from the different algorithms were found between the two groups. With the exception of single-channel noise reduction, all algorithms showed an improvement in SRT of between 2.1 dB (in cafeteria noise) and 4.8 dB (in single competing talker condition). Model predictions with binaural speech intelligibility model explained 83% of the measured variance of the individual SRTs in the no pre-processing condition. Regarding the benefit from the algorithms, the instrumental measures were not able to predict the perceptual data in all tested noise conditions. The comparable benefit observed for both groups suggests a possible application of noise reduction schemes for listeners with different hearing status. Although the model can predict the individual SRTs without pre-processing, further development is necessary to predict the benefits obtained from the algorithms at an individual level.© The Author(s) 2015.DOI: 10.1177/2331216515618609PMCID: PMC4771033",pubmed,26721922,10.1177/2331216515618609
wholeexome sequencing for screening noiseinduced hearing loss susceptibility genes,"28. Acta Otolaryngol. 2023 May;143(5):408-415. doi: 10.1080/00016489.2023.2201287. Epub 2023 May 2.Whole-exome sequencing for screening noise-induced hearing loss susceptibility genes.Fan B(1)(2)(3), Wang G(2)(3), Liu G(2)(3), Zhang X(2)(3), Wu W(1)(2)(3).Author information:(1)Department of Otorhinolaryngology Head and Neck Surgery, The 306th Hospital of PLA-Peking University Teaching Hospital, Beijing, China.(2)Department of Otorhinolaryngology Head and Neck Surgery, PLA Strategic Support Force Characteristic Medical Center, Beijing, China.(3)Hearing Impairment Laboratory, State Environmental Protection Key Laboratory of Environmental Sense Organ Stress and Health, Beijing, China.BACKGROUND: High-throughput sequencing of genes indicating susceptibility to noise-induced hearing loss has not previously been reported.AIMS/OBJECTIVES: To identify and analyze genes associated with susceptibility to noise-induced hearing loss (NIHL) and characterize differences in susceptibility to hearing loss by genotype.MATERIAL AND METHODS: Pure tone audiometry tests were performed on 113 workers exposed to high-intensity noise. Whole-exome sequencing (WES) was conducted and NIHL susceptibility genes screened for training unsupervised and supervised machine learning models. Immunofluorescence staining of mouse cochlea was used to observe patterns of NIHL susceptibility gene expression.RESULTS: Participants were divided into a NIHL and a control group, according to the results of audiometry tests. Seventy-three possible NIHL susceptibility genes were input into the machine learning model. Two subgroups of NIHL could be distinguished by unsupervised machine learning and the classification was evaluated by the supervised machine learning algorithm. The VWF gene had the highest mutation frequency in the NIHL group and was expressed mainly in the spiral ligament.CONCLUSIONS AND SIGNIFICANCE: NIHL susceptibility genes were screened and NIHL subgroups could be distinguished. VWF may be a novel NIHL susceptibility gene.DOI: 10.1080/00016489.2023.2201287",pubmed,37129226,10.1080/00016489.2023.2201287
brain stem responses evoked by stimulation of the mature cochlear nucleus with an auditory brain stem implant,"Objectives: The Nucleus auditory brain stem implant (ABI) has been used in the hearing rehabilitation of totally deaf individuals for whom a cochlear implant is not an option such as in the case of neurofibromatosis type 2 (NF2). Intraoperative electrically evoked auditory brain stem responses (EABRs) are recorded to assist in the placement of the electrode array over the dorsal and ventral cochlear nuclei in the lateral recess of the IVth ventricle of the brain stem. This study had four objectives: (1) to characterize EABRs evoked by stimulation with an ABI in adolescents and adults with NF2, (2) to evaluate how the EABR morphology relates to auditory sensations elicited from stimulation by an ABI, (3) to establish whether there is evidence of morphology changes in the EABR with site of stimulation by the ABI, and (4) to investigate how the threshold of the EABR relates to behavioral threshold and comfortably loud sensations measured at initial device activation. Design: Intraoperative EABRs were recorded from 34 subjects with ABIs: 19 male and 15 female, mean age 27 yrs (range 12 to 52 yrs). ABI stimulation was applied at seven different sites using either wide bipolar stimulation across the array or in subsections of the array from medial to lateral and inferior to superior. The EABRs were analyzed with respect to morphology, peak latency, and changes in these characteristics with the site of stimulation. In a subset of eight subjects, additional narrow bipolar sites were stimulated to compare the intraoperative EABR threshold levels with the behavioral threshold (T) and comfortably loud (C) levels of stimulation required at initial device activation. Results: EABRs were elicited from 91% of subjects. Morphology varied from one to four vertex-positive peaks with mean latencies of 0.76, 1.53, 2.51, and 3.64 msecs, respectively. The presence of an EABR from stimulation by electrodes across the whole array had a high predictive value for the presence of auditory electrodes at initial device activation. When examining subsections of the array, the absence of an EABR was a poor predictor for the absence of auditory electrodes. The morphology of the EABRs varied with site of stimulation in 16 cases, but there was no consistent pattern of change with stimulation site. There was a trend for more auditory electrodes to be present in stimulation sites that evoked EABRs with a higher number of peaks in the waveform. The EABR threshold was closer to the behavioral C level than the T level, but there was no overall correlation between the intraoperative EABR threshold level and the behavioral T and C levels. Conclusions: The presence of an intraoperative EABR corresponded well to the presence of auditory electrodes. The absence of an EABR from stimulating subsections of the array was not; however, a good indicator for the absence of auditory electrodes and the EABR from such stimulation would not be of assistance in identifying the nonauditory sections of the array to exclude in behavioral fitting of the device. The morphology of the EABR did not relate to site of stimulation. More peaks in the EABR was associated with a greater number of electrodes with auditory sensations, suggesting that correct positioning of the ABI activated more auditory subsystems within the cochlear nucleus. The intraoperative EABR thresholds did not correlate with the behavioral T and C levels and could not be used to assist in device fitting.",cinahl,1960202,10.1097/AUD.0b013e3181fc9d72
different patterns of hearing loss among tinnitus patients a latent class analysis of a large sample,"808. Front Neurol. 2017 Feb 20;8:46. doi: 10.3389/fneur.2017.00046. eCollection 2017.Different Patterns of Hearing Loss among Tinnitus Patients: A Latent Class Analysis of a Large Sample.Langguth B(1), Landgrebe M(2), Schlee W(1), Schecklmann M(1), Vielsmeier V(3), Steffens T(3), Staudinger S(4), Frick H(5), Frick U(6).Author information:(1)Department of Psychiatry and Psychotherapy, University of Regensburg, Regensburg, Germany; Interdisciplinary Tinnitus Center of the University of Regensburg, Regensburg, Germany.(2)Department of Psychiatry and Psychotherapy, University of Regensburg, Regensburg, Germany; kbo Lech-Mangfall-Klinik Agatharied, Hausham, Germany.(3)Interdisciplinary Tinnitus Center of the University of Regensburg, Regensburg, Germany; Department of Otorhinolaryngology, University of Regensburg, Regensburg, Germany.(4)Interdisciplinary Tinnitus Center of the University of Regensburg , Regensburg , Germany.(5)Department of Statistical Science, University College London , London , UK.(6)Department of Psychiatry and Psychotherapy, University of Regensburg, Regensburg, Germany; HSD University of Applied Sciences, Cologne, Germany.BACKGROUND: The heterogeneity of tinnitus is a major challenge for tinnitus research. Even if a complex interaction of many factors is involved in the etiology of tinnitus, hearing loss (HL) has been identified as the most relevant etiologic factor. Here, we used a data-driven approach to identify patterns of hearing function in a large sample of tinnitus patients presenting in a tinnitus clinic.METHODS: Data from 2,838 patients presenting at the Tinnitus Center of the University Regensburg between 2007 and 2014 have been analyzed. Standard audiometric data were frequency-wise categorized in four categories [a: normal hearing (0-20 dB HL); b: moderate HL (25-50 dB HL; representing outer hair cell loss); c: severe HL (>50 dB HL; representing outer and inner hair cell loss); d: no data available] and entered in a latent class analysis, a statistical method to find subtypes of cases in multivariate categorical data. To validate the clinical relevance of the identified latent classes, they were compared with respect to clinical and demographic characteristics of their members.RESULTS: The classification algorithm identified eight distinct latent classes with an excellent separation. Patient classes differed with respect to demographic (e.g., age, gender) and clinical characteristics (e.g., tinnitus location, tinnitus severity, gradual, or abrupt onset, etc.).DISCUSSION: Our results demonstrate that data-driven categorization of hearing function seems to be a promising approach for profiling tinnitus patients, as it revealed distinct subtypes that reflect prototypic forms of HL and that differ in several relevant clinical characteristics.DOI: 10.3389/fneur.2017.00046PMCID: PMC5316929",pubmed,28265258,10.3389/fneur.2017.00046
towards a typology of specific language impairment,"Background: The population of children with specific language impairments (SLI) is heterogeneous. The present study was conducted to examine this heterogeneity more closely, by identifying and describing subgroups within the population of children with SLI in the Netherlands. Method: A broad battery of language tests and language-related cognitive tests were administered to 147 six-year-old and 136 eight-year-old children with SLI. Results: Factor analyses revealed 4 factors indicating 4 distinctive linguistic domains for both age samples: 1) lexical-semantic abilities, 2) auditory conceptualization, 3) verbal sequential memory and 4) speech production. These empirical findings were further validated by the positive correlations found between the language factors and the judgments of teachers and speech therapists. Finally, a cluster analysis revealed 4 distinct clusters of SLI children for each sample with specific language profiles based on the 4 factors. Results were nearly the same for both age samples. Conclusions: The language problems that emerged from the two samples of children with SLI could be described as falling into four types. Based on these language types, four subgroups of children with SLI could be distinguished, each with a specific profile. Some subgroups had severe problems on one specific type of language problem; others had severe problems in more than one type of language problem when compared to the other subgroups of the same age sample. The different profiles may indicate that a more dynamic approach is needed in intervention, considering the presence of both compensating and restricting factors within each child with SLI. © 2005 Association for Child Psychology and Psychiatry.",scopus,2-s2.0-33645893195,10.1111/j.1469-7610.2005.01454.x
asymmetric hearing during development the aural preference syndrome and treatment options,"84. Pediatrics. 2015 Jul;136(1):141-53. doi: 10.1542/peds.2014-3520. Epub 2015 Jun 8.Asymmetric Hearing During Development: The Aural Preference Syndrome and Treatment Options.Gordon K(1), Henkin Y(2), Kral A(3).Author information:(1)Archie's Cochlear Implant Laboratory, The Hospital for Sick Children, Department of Otolaryngology-Head and Neck Surgery, University of Toronto, Toronto, Canada; karen.gordon@utoronto.ca.(2)Hearing, Speech, and Language Center, Sheba Medical Center, Tel Hashomer, Department of Communication Disorders, Sackler Faculty of Medicine, Tel Aviv University, Tel Aviv, Israel; and.(3)Cluster of Excellence Hearing4all, Institute of AudioNeuroTechnology, Hannover, Germany; Department of Experimental Otology, ENT Clinics, School of Medicine, Hannover Medical University, Hannover, Germany; and School of Behavioral and Brain Sciences, The University of Texas at Dallas, Dallas, Texas.Deafness affects ∼2 in 1000 children and is one of the most common congenital impairments. Permanent hearing loss can be treated by fitting hearing aids. More severe to profound deafness is an indication for cochlear implantation. Although newborn hearing screening programs have increased the identification of asymmetric hearing loss, parents and caregivers of children with single-sided deafness are often hesitant to pursue therapy for the deaf ear. Delayed intervention has consequences for recovery of hearing. It has long been reported that asymmetric hearing loss/single-sided deafness compromises speech and language development and educational outcomes in children. Recent studies in animal models of deafness and in children consistently show evidence of an ""aural preference syndrome"" in which single-sided deafness in early childhood reorganizes the developing auditory pathways toward the hearing ear, with weaker central representation of the deaf ear. Delayed therapy consequently compromises benefit for the deaf ear, with slow rates of improvement measured over time. Therefore, asymmetric hearing needs early identification and intervention. Providing early effective stimulation in both ears through appropriate fitting of auditory prostheses, including hearing aids and cochlear implants, within a sensitive period in development has a cardinal role for securing the function of the impaired ear and for restoring binaural/spatial hearing. The impacts of asymmetric hearing loss on the developing auditory system and on spoken language development have often been underestimated. Thus, the traditional minimalist approach to clinical management aimed at 1 functional ear should be modified on the basis of current evidence.Copyright © 2015 by the American Academy of Pediatrics.DOI: 10.1542/peds.2014-3520",pubmed,26055845,10.1542/peds.2014-3520
the 1st clarity prediction challenge a machine learning challenge for hearing aid intelligibility prediction,"This paper reports on the design and outcomes of the 1st Clarity Prediction Challenge (CPC1) for predicting the intelligibility of hearing aid processed signals heard by individuals with a hearing impairment. The challenge was designed to promote the development of new intelligibility measures suitable for use in developing hearing aid algorithms. Participants were supplied with listening test data compromising 7233 responses from 27 individuals. Data was split between training and test sets in a manner that fostered a machine learning approach and allowed both closed-set (known listeners) and open-set (unseen listener/unseen system) evaluation. The paper provides a description of the challenge design including the datasets, the hearing aid algorithms applied, the listeners and the perceptual tests. The challenge attracted submissions from 15 systems. The results are reviewed and the paper summarises, compares and contrasts approaches. Copyright © 2022 ISCA.",scopus,2-s2.0-85128551580,10.21437/Interspeech.2022-10821
disrupted topological organization in white matter networks in unilateral sudden sensorineural hearing loss,"9,889,156, “Method for treating noise-induced hearing loss (NIHL)” and 9,919,008, “Methods for treating age-related hearing loss (ARHL)”. Both patents are based on the use of oral ACEMg, but currently, they are not involved in any trials testing this compound or any other commercial exploitation. The authors have no other relevant affiliations or financial involvement with any organization or entity with a financial interest in or financial conflict with the subject matter or materials discussed in the paper apart from those disclosed. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.687. Front Neurosci. 2021 Jul 12;15:666651. doi: 10.3389/fnins.2021.666651. eCollection 2021.Disrupted Topological Organization in White Matter Networks in Unilateral Sudden Sensorineural Hearing Loss.Zou Y(1)(2), Ma H(1)(2), Liu B(3), Li D(3), Liu D(1)(2), Wang X(4), Wang S(1)(2), Fan W(1)(2), Han P(1)(2).Author information:(1)Department of Radiology, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China.(2)Hubei Key Laboratory of Molecular Imaging, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China.(3)Department of Otorhinolaryngology, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China.(4)GE Healthcare, Shanghai, China.Sudden sensorineural hearing loss (SSNHL) is a sudden-onset hearing impairment that rapidly develops within 72 h and is mostly unilateral. Only a few patients can be identified with a defined cause by routine clinical examinations. Recently, some studies have shown that unilateral SSNHL is associated with alterations in the central nervous system. However, little is known about the topological organization of white matter (WM) networks in unilateral SSNHL patients in the acute phase. In this study, 145 patients with SSNHL and 91 age-, gender-, and education-matched healthy controls were evaluated using diffusion tensor imaging (DTI) and graph theoretical approaches. The topological properties of WM networks, including global and nodal parameters, were investigated. At the global level, SSNHL patients displayed decreased clustering coefficient, local efficiency, global efficiency, normalized clustering coefficient, normalized characteristic path length, and small-worldness and increased characteristic path length (p < 0.05) compared with healthy controls. At the nodal level, altered nodal centralities in brain regions involved the auditory network, visual network, attention network, default mode network (DMN), sensorimotor network, and subcortical network (p < 0.05, Bonferroni corrected). These findings indicate a shift of the WM network topology in SSNHL patients toward randomization, which is characterized by decreased global network integration and segregation and is reflected by decreased global connectivity and altered nodal centralities. This study could help us understand the potential pathophysiology of unilateral SSNHL.Copyright © 2021 Zou, Ma, Liu, Li, Liu, Wang, Wang, Fan and Han.DOI: 10.3389/fnins.2021.666651PMCID: PMC8312563",pubmed,34321993,10.3389/fnins.2021.666651
cochlear implant use following neonatal deafness influences the cochleotopic organization of the primary auditory cortex in cats,"483. J Comp Neurol. 2009 Jan 1;512(1):101-14. doi: 10.1002/cne.21886.Cochlear implant use following neonatal deafness influences the cochleotopic organization of the primary auditory cortex in cats.Fallon JB(1), Irvine DR, Shepherd RK.Author information:(1)The Bionic Ear Institute, Melbourne, Victoria, Australia 3002. jfallon@bionicear.orgElectrical stimulation of spiral ganglion neurons in a deafened cochlea, via a cochlear implant, provides a means of investigating the effects of the removal and subsequent restoration of afferent input on the functional organization of the primary auditory cortex (AI). We neonatally deafened 17 cats before the onset of hearing, thereby abolishing virtually all afferent input from the auditory periphery. In seven animals the auditory pathway was chronically reactivated with environmentally derived electrical stimuli presented via a multichannel intracochlear electrode array implanted at 8 weeks of age. Electrical stimulation was provided by a clinical cochlear implant that was used continuously for periods of up to 7 months. In 10 long-term deafened cats and three age-matched normal-hearing controls, an intracochlear electrode array was implanted immediately prior to cortical recording. We recorded from a total of 812 single unit and multiunit clusters in AI of all cats as adults using a combination of single tungsten and multichannel silicon electrode arrays. The absence of afferent activity in the long-term deafened animals had little effect on the basic response properties of AI neurons but resulted in complete loss of the normal cochleotopic organization of AI. This effect was almost completely reversed by chronic reactivation of the auditory pathway via the cochlear implant. We hypothesize that maintenance or reestablishment of a cochleotopically organized AI by activation of a restricted sector of the cochlea, as demonstrated in the present study, contributes to the remarkable clinical performance observed among human patients implanted at a young age.DOI: 10.1002/cne.21886PMCID: PMC2597008",pubmed,18972570,10.1002/cne.21886
knowledge attitude and management of hearing screening in children among family physicians in the kingdom of saudi arabia,"640. PLoS One. 2021 Aug 31;16(8):e0256647. doi: 10.1371/journal.pone.0256647. eCollection 2021.Knowledge, attitude and management of hearing screening in children among family physicians in the Kingdom of Saudi Arabia.Alqudah O(1), Alqudah S(2), Al-Bashaireh AM(3), Alharbi N(4), Alqudah AM(5).Author information:(1)Department of Community Health, King Fahad Medical City, Riyadh, Saudi Arabia.(2)Faculty of Applied Medical Sciences, Department of Rehabilitation Sciences, Jordan University of Science and Technology, Irbid, Jordan.(3)Faculty of Nursing, Department of Primary Care Nursing, Al-Ahliyya Amman University, Amman, Jordan.(4)Department of Community Health, Second Cluster, Riyadh, Saudi Arabia.(5)Graduate studies, Al-Balqa Applied University, Amman, Jordan.BACKGROUND: Early detection and management of hearing loss are important to develop ordinary speaking language and academic skills during childhood. Lack of knowledge by either parents or health care providers could hinder the process of hearing loss diagnosis, such that the intervention will be less effective. There is little evidence about the knowledge and practice of family physicians regarding hearing screening in Saudi Arabia and worldwide.OBJECTIVES: This study aimed to assess family physicians' knowledge, attitudes, and practices related to hearing loss in children. This in turn will help policy makers and educational institutions to establish and promote a program concerned with screening, diagnosis and intervention of paediatric hearing loss.METHODS: A cross-sectional descriptive study enrolled 133 family physicians working at primary health centres in Saudi Arabia from March 2020 to September 2020. A self-reported questionnaire was used to assess the knowledge, attitudes, and practices of family physicians concerning hearing loss in children.RESULTS: The majority of the participants were working under the umbrella of the Ministry of Health and around half of them did not screen any child for hearing loss. Despite that, 91.7% indicated the importance of neonatal hearing screening, 70.7% indicate infant candidacy for cochlear implant and only 33.1% know about the existence of the early hearing detection and intervention (EHDI) governmental program in kingdom of Saudi Arabia (KSA). Participants were able to identify factors associated with hearing loss such as a family history of hearing loss (85.6%), meningitis (75%) and craniofacial anomalies (51.5%). The most frequent specialists for patient referrals were ear nose and throat ENT (75.2%) and audiologists (67.7%).CONCLUSION: This study shows that family physicians have good general background about the benefits of EHDI programs and the management of hearing loss in the paediatric population. However, it also indicated insufficient knowledge in other domains of hearing loss, including assessments and the presence of the EHDI governmental program in KSA. Further actions on the involvement of family physicians in the process of neonatal hearing screening, diagnosis and intervention for hearing impairment are needed.DOI: 10.1371/journal.pone.0256647PMCID: PMC8407574",pubmed,34464417,10.1371/journal.pone.0256647
realtime sign language recognition using a multimodal deep learning approach,"Sign language recognition is an important area of research that aims to provide greater access to communication and information for individuals who are deaf or hard of hearing. In this paper, we present a new approach for real-time sign language recognition using a multimodal deep learning approach. The proposed approach integrates video and inertial sensor data for improved recognition accuracy and robustness.The proposed approach uses a convolutional neural network (CNN) to extract features from the video data and a Recurrent neural network (RNN) is used to capture the temporal dynamics of the sign language gestures.Sign language recognition systems use technologies such as computer vision, machine learning, and artificial intelligence to analyze and understand the gestures and movements of the signer. These systems have the potential to improve the quality of life for individuals who are deaf or hard of hearing by providing them with greater access to communication and information in a variety of settings.",ieee,,10.1109/ACCAI58221.2023.10199569
intensive treatment of auditory analysis in a 16yearold aphasic patient,"Aim of this treatment study was to improve speech comprehension of a 16-year-old patient with Wernicke's aphasia and a partial disorder of auditory analysis (""word sound deafness""). To this end, tasks addressing phoneme-grapheme correspondence with syllables were used as well as speech discrimination tasks with syllables, consonant clusters and neologisms. The patient showed significant improvement with trained items and generalization effects to untrained items. Furthermore, secondary improvements could be observed in tasks which are based on the auditory analysis (e.g. lexical decision). However, performance in an untrained control task (rime generation) did not change, indicating that the effects of therapy were specific. Moreover, the patient showed an enhanced self-monitoring, evidenced by an increased rate of self-corrections.",cinahl,9320547,0.2443/skv-s-2012-53020120601
an efficient approach for interpretation of indian sign language using machine learning,"Non-verbal communication involves the usage of Sign Language. The sign language is used by people with hearing / speech disabilities to express their thoughts and feelings. But normally, people find it difficult to understand the hand gestures of the specially challenged people as they do not know the meaning of the sign language gestures. Usually, a translator is needed when a speech / hearing impaired person wants to communicate with an ordinary person and vice versa. In order to enable the specially challenged people to effectively communicate with the people around them, a system that translates the Indian Sign Language (ISL) hand gestures of numbers (1-9), English alphabets (A-Z) and a few English words to understandable text and vice versa has been proposed in this paper. This is done using image processing techniques and Machine Learning algorithms. Different neural network classifiers are developed, tested and validated for their performance in gesture recognition and the most efficient classifier is identified.",ieee,,10.1109/ICSPC51351.2021.9451692
tone perception in mandarinspeaking school age children with otitis media with effusion,"89. PLoS One. 2017 Aug 22;12(8):e0183394. doi: 10.1371/journal.pone.0183394. eCollection 2017.Tone perception in Mandarin-speaking school age children with otitis media with effusion.Cai T(1), McPherson B(1), Li C(2), Yang F(3).Author information:(1)Division of Speech and Hearing Sciences, Faculty of Education, The University of Hong Kong, Hong Kong, China.(2)Department of Otorhinolaryngology, Shenzhen Children's Hospital, Shenzhen, China.(3)Department of Speech Therapy, Shenzhen Children's Hospital, Shenzhen, China.OBJECTIVES: The present study explored tone perception ability in school age Mandarin-speaking children with otitis media with effusion (OME) in noisy listening environments. The study investigated the interaction effects of noise, tone type, age, and hearing status on monaural tone perception, and assessed the application of a hierarchical clustering algorithm for profiling hearing impairment in children with OME.METHODS: Forty-one children with normal hearing and normal middle ear status and 84 children with OME with or without hearing loss participated in this study. The children with OME were further divided into two subgroups based on their severity and pattern of hearing loss using a hierarchical clustering algorithm. Monaural tone recognition was measured using a picture-identification test format incorporating six sets of monosyllabic words conveying four lexical tones under speech spectrum noise, with the signal-to-noise ratio (SNR) conditions ranging from -9 to -21 dB.RESULTS: Linear correlation indicated tone recognition thresholds of children with OME were significantly correlated with age and pure tone hearing thresholds at every frequency tested. Children with hearing thresholds less affected by OME performed similarly to their peers with normal hearing. Tone recognition thresholds of children with auditory status more affected by OME were significantly inferior to those of children with normal hearing or with minor hearing loss. Younger children demonstrated poorer tone recognition performance than older children with OME. A mixed design repeated-measure ANCOVA showed significant main effects of listening condition, hearing status, and tone type on tone recognition. Contrast comparisons revealed that tone recognition scores were significantly better under -12 dB SNR than under -15 dB SNR conditions and tone recognition scores were significantly worse under -18 dB SNR than those obtained under -15 dB SNR conditions. Tone 1 was the easiest tone to identify and Tone 3 was the most difficult tone to identify for all participants, when considering -12, -15, and -18 dB SNR as within-subject variables. The interaction effect between hearing status and tone type indicated that children with greater levels of OME-related hearing loss had more impaired tone perception of Tone 1 and Tone 2 compared to their peers with lesser levels of OME-related hearing loss. However, tone perception of Tone 3 and Tone 4 remained similar among all three groups. Tone 2 and Tone 3 were the most perceptually difficult tones for children with or without OME-related hearing loss in all listening conditions.CONCLUSIONS: The hierarchical clustering algorithm demonstrated usefulness in risk stratification for tone perception deficiency in children with OME-related hearing loss. There was marked impairment in tone perception in noise for children with greater levels of OME-related hearing loss. Monaural lexical tone perception in younger children was more vulnerable to noise and OME-related hearing loss than that in older children.DOI: 10.1371/journal.pone.0183394PMCID: PMC5568745",pubmed,28829840,10.1371/journal.pone.0183394
assessing the efficacy of asynchronous telehealthbased hearing screening and diagnostic services using automated audiometry in a rural south african school,"333. S Afr J Commun Disord. 2018 Jul 5;65(1):e1-e9. doi: 10.4102/sajcd.v65i1.582.Assessing the efficacy of asynchronous telehealth-based hearing screening and diagnostic services using automated audiometry in a rural South African school.Govender SM(1), Mars M.Author information:(1)Department of Telehealth, University of KwaZulu-Natal. samantha.govender@smu.ac.za. Asynchronous automated telehealth-based hearing screening and diagnostic testing can be used within the rural school context to identify and confirm hearing loss. Objective: The aims of the study were to evaluate the efficacy of an asynchronous telehealth-based service delivery model using automated technology for screening and diagnostic testing as well as to describe the prevalence, type and degree of hearing loss. Method: A comparative within-subject design was used. Frequency distributions, sensitivity, specificity scores as well as the positive and negative predictive values were calculated. Testing was conducted in a non-sound-treated classroom within a school environment on 73 participants (146 ears). The sensitivity and specificity rates were 65.2% and 100%, respectively. Diagnostic accuracy was 91.7% and the negative predictive values (NPV) and positive predictive values (PPV) were 93.8% and 100%, respectively. Results: Results revealed that 23 ears of 20 participants (16%) presented with hearing loss. Twelve per cent of ears presented with unilateral hearing impairment and 4% with bilateral hearing loss. Mild hearing loss was identified as most prevalent (8% of ears). Eight ears obtained false-negative results and presented with mild low- to mid-frequency hearing loss. The sensitivity rate for the study was low and was attributed to plausible reasons relating to test accuracy, child-related variables and mild low-frequency sensory-neural hearing loss. Conclusion: The study demonstrates that asynchronous telehealth-based automated hearing testing within the school context can be used to facilitate early identification of hearing loss; however, further research and development into protocol formulation, ongoing device monitoring and facilitator training is required to improve test sensitivity and ensure accuracy of results.DOI: 10.4102/sajcd.v65i1.582PMCID: PMC6111388",pubmed,30035608,10.4102/sajcd.v65i1.582
prediction of the functional status of the cochlear nerve in individual cochlear implant users using machine learning and electrophysiological measures,"562. Ear Hear. 2021 Jan/Feb;42(1):180-192. doi: 10.1097/AUD.0000000000000916.Prediction of the Functional Status of the Cochlear Nerve in Individual Cochlear Implant Users Using Machine Learning and Electrophysiological Measures.Skidmore J(1), Xu L(2)(3), Chao X(2)(3), Riggs WJ(1)(4), Pellittieri A(5), Vaughan C(1), Ning X(6), Wang R(2)(3), Luo J(2)(3), He S(1)(4).Author information:(1)Department of Otolaryngology-Head and Neck Surgery, The Ohio State University, Columbus, Ohio, USA.(2)Department of Otolaryngology-Head and Neck Surgery, Shandong Provincial ENT Hospital Affiliated to Shandong University, Jinan, Shandong, People's Republic of China.(3)Department of Auditory Implantation, Shandong ENT Hospital, Jinan, Shandong, People's Republic of China.(4)Department of Audiology, Nationwide Children's Hospital, Columbus, Ohio, USA.(5)The Cleveland Clinic, Cleveland, Ohio, USA.(6)Department of Biomedical Informatics, The Ohio State University, Columbus, Ohio, USA.OBJECTIVES: This study aimed to create an objective predictive model for assessing the functional status of the cochlear nerve (CN) in individual cochlear implant (CI) users.DESIGN: Study participants included 23 children with cochlear nerve deficiency (CND), 29 children with normal-sized CNs (NSCNs), and 20 adults with various etiologies of hearing loss. Eight participants were bilateral CI users and were tested in both ears. As a result, a total of 80 ears were tested in this study. All participants used Cochlear Nucleus CIs in their test ears. For each participant, the CN refractory recovery function and input/output (I/O) function were measured using electrophysiological measures of the electrically evoked compound action potential (eCAP) at three electrode sites across the electrode array. Refractory recovery time constants were estimated using statistical modeling with an exponential decay function. Slopes of I/O functions were estimated using linear regression. The eCAP parameters used as input variables in the predictive model were absolute refractory recovery time estimated based on the refractory recovery function, eCAP threshold, slope of the eCAP I/O function, and negative-peak (i.e., N1) latency. The output variable of the predictive model was CN index, an indicator for the functional status of the CN. Predictive models were created by performing linear regression, support vector machine regression, and logistic regression with eCAP parameters from children with CND and the children with NSCNs. One-way analysis of variance with post hoc analysis with Tukey's honest significant difference criterion was used to compare study variables among study groups.RESULTS: All three machine learning algorithms created two distinct distributions of CN indices for children with CND and children with NSCNs. Variations in CN index when calculated using different machine learning techniques were observed for adult CI users. Regardless of these variations, CN indices calculated using all three techniques in adult CI users were significantly correlated with Consonant-Nucleus-Consonant word and AzBio sentence scores measured in quiet. The five oldest CI users had smaller CN indices than the five youngest CI users in this study.CONCLUSIONS: The functional status of the CN for individual CI users was estimated by our newly developed analytical models. Model predictions of CN function for individual adult CI users were positively and significantly correlated with speech perception performance. The models presented in this study may be useful for understanding and/or predicting CI outcomes for individual patients.Copyright © 2020 Wolters Kluwer Health, Inc. All rights reserved.DOI: 10.1097/AUD.0000000000000916PMCID: PMC8156737",pubmed,32826505,10.1097/AUD.0000000000000916
combined effect of noise and handtransmitted vibration on noiseinduced hearing loss in the automobile manufacturing industry,"578. Zhonghua Lao Dong Wei Sheng Zhi Ye Bing Za Zhi. 2020 Jun 20;38(6):420-423. doi: 10.3760/cma.j.cn121094-20191009-00470.[Combined effect of noise and hand-transmitted vibration on noise-induced hearing loss in the automobile manufacturing industry].[Article in Chinese; Abstract available in Chinese from the publisher]Duan DP(1), Bai LX(1), Qiu CX(1), Huang TY(2), Tang SH(3), Liu YM(4).Author information:(1)Occupational Health Monitoring Center of Occupational Disease Prevention and Treatment Hospital, Guangzhou 510620, China.(2)Prevention and Control Department of Chronic Non Infectious Diseases, Guangzhou Center for Disease Control and Prevention, Guangzhou 510440, China.(3)Evaluation and Testing Center of Occupational Disease Prevention and Treatment Hospital, Guangzhou 510620, China.(4)Occupational Health Monitoring Center of Occupational Disease Prevention and Treatment Hospital, Guangzhou 510620, China; Institute of Occupational and Environmental Health, Guangzhou Medical University, Guangzhou 510180, China.Objective: To investigate the combined effect of noise and hand-transmitted vibration on noise-induced hearing loss (NIHL) in the automobile manufacturing industry. Methods: From September 2018 to January 2019, cluster sampling was used to select 998 workers in an automobile factory as study subjects, among whom 352 workers exposed to noise alone were enrolled as noise group, 342 workers exposed to noise and hand-transmitted vibration were enrolled as combined effect group, and 304 workers without exposure to occupational hazardous factors were enrolled as control group. A questionnaire survey and pure tone audiometry were performed for all study subjects. An analysis of variance was used for comparison of continuous data between groups, and the chi-square test was used for comparison of categorical data between groups; a ordinal polytomous logistic regression analysis was used to investigate the influencing factors for NIHL (with 0.05 as the inclusion criteria and 0.10 as the exclusion criteria for independent variables) . Results: There was a significant difference in L(Aeq, 8 h) between groups (P<0.05) ; the noise group and the combined effect group had a significantly higher L(Aeq, 8 h) than the control group (P<0.05) , while there was no significant difference in L(Aeq, 8 h) between the noise group and the combined effect group (P>0.05) . The control group had a significantly lower detection rate of hearing loss than the noise group and the combined effect group (P<0.0125) , and the combined effect group had a significantly higher detection rate of hearing loss than the noise group (P<0.0125) . The ordinal polytomous logistic regression analysis showed that after adjustment for confounding factors such as age, working years, sex, smoking, and drinking, both noise exposure and exposure to both noise and hand-transmitted vibration had an influence on workers' hearing (P<0.05) , and the workers exposed to both noise and hand-transmitted vibration had a higher risk of hearing loss than those exposed to noise alone. Conclusion: There may be a combined effect of noise and hand-transmitted vibration in the automobile manufacturing industry, which can increase the risk of NIHL in workers.Publisher: 目的： 探讨汽车制造业噪声和手传振动联合作用对噪声性听力损失（NIHL）的影响。 方法： 于2018年9月至2019年1月，采用整群抽样的方法，选择某大型汽车制造企业的998名员工为研究对象，以其中352名仅接触噪声的工人为噪声组，342名接触噪声和手传振动的工人为联合作用组，304名不接触职业病危害因素的员工为对照组。对研究对象进行问卷调查和纯音听力测试，计量资料组间差异用方差分析，计数资料组间差异用χ(2)检验；NIHL影响因素分析用有序多分类Logistic回归分析法（自变量纳入标准为0.05，剔除标准为0.10）。 结果： 各组间L(Aeq，8 h)比较，差异有统计学意义（P<0.05）；两两比较显示，噪声组、联合作用组L(Aeq，8 h)均高于对照组，差异均有统计学意义（P<0.05），噪声组与联合作用组L(Aeq，8 h)比较，差异无统计学意义（P>0.05）；对照组人群听力损失检出率分别低于噪声组和联合作用组（P<0.012 5），联合作用组人群听力损失检出率高于噪声组（P<0.012 5）。有序多分类Logistic回归分析结果显示：在排除年龄、工龄、性别、吸烟、饮酒等混杂因素影响后，噪声接触、手传振动与噪声联合接触均对工人听力有影响（P<0.05），其中，手传振动与噪声联合接触导致听力损失的风险高于单纯噪声接触。 结论： 汽车制造业中噪声和手传振动可能存在联合作用，可增加劳动者罹患NIHL的风险。.DOI: 10.3760/cma.j.cn121094-20191009-00470",pubmed,32629569,10.3760/cma.j.cn121094-20191009-00470
unilateral hearing during development hemispheric specificity in plastic reorganizations,"845. Front Syst Neurosci. 2013 Nov 27;7:93. doi: 10.3389/fnsys.2013.00093. eCollection 2013.Unilateral hearing during development: hemispheric specificity in plastic reorganizations.Kral A(1), Heid S(2), Hubka P(1), Tillein J(2).Author information:(1)Cluster of Excellence, Department of Experimental Otology, Institute of Audioneurotechnology, ENT Clinics, Hannover Medical School Hannover, Germany.(2)Cluster of Excellence, Department of Experimental Otology, Institute of Audioneurotechnology, ENT Clinics, Hannover Medical School Hannover, Germany ; Department of Physiology and Otolaryngology, J. W. Goethe University Frankfurt am Main, Germany.The present study investigates the hemispheric contributions of neuronal reorganization following early single-sided hearing (unilateral deafness). The experiments were performed on ten cats from our colony of deaf white cats. Two were identified in early hearing screening as unilaterally congenitally deaf. The remaining eight were bilaterally congenitally deaf, unilaterally implanted at different ages with a cochlear implant. Implanted animals were chronically stimulated using a single-channel portable signal processor for two to five months. Microelectrode recordings were performed at the primary auditory cortex under stimulation at the hearing and deaf ear with bilateral cochlear implants. Local field potentials (LFPs) were compared at the cortex ipsilateral and contralateral to the hearing ear. The focus of the study was on the morphology and the onset latency of the LFPs. With respect to morphology of LFPs, pronounced hemisphere-specific effects were observed. Morphology of amplitude-normalized LFPs for stimulation of the deaf and the hearing ear was similar for responses recorded at the same hemisphere. However, when comparisons were performed between the hemispheres, the morphology was more dissimilar even though the same ear was stimulated. This demonstrates hemispheric specificity of some cortical adaptations irrespective of the ear stimulated. The results suggest a specific adaptation process at the hemisphere ipsilateral to the hearing ear, involving specific (down-regulated inhibitory) mechanisms not found in the contralateral hemisphere. Finally, onset latencies revealed that the sensitive period for the cortex ipsilateral to the hearing ear is shorter than that for the contralateral cortex. Unilateral hearing experience leads to a functionally-asymmetric brain with different neuronal reorganizations and different sensitive periods involved.DOI: 10.3389/fnsys.2013.00093PMCID: PMC3841817",pubmed,24348345,10.3389/fnsys.2013.00093
an overview of propensity score matching methods for clustered data,"Propensity score matching is commonly used in observational studies to control for confounding and estimate the causal effects of a treatment or exposure. Frequently, in observational studies data are clustered, which adds to the complexity of using propensity score techniques. In this article, we give an overview of propensity score matching methods for clustered data, and highlight how propensity score matching can be used to account for not just measured confounders, but also unmeasured cluster level confounders. We also consider using machine learning methods such as generalized boosted models to estimate the propensity score and show that accounting for clustering when using these methods can greatly reduce the performance, particularly when there are a large number of clusters and a small number of subjects per cluster. In order to get around this we highlight scenarios where it may be possible to control for measured covariates using propensity score matching, while using fixed effects regression in the outcome model to control for cluster level covariates. Using simulation studies we compare the performance of different propensity score matching methods for clustered data across a number of different settings. Finally, as an illustrative example we apply propensity score matching methods for clustered data to study the causal effect of aspirin on hearing deterioration using data from the conservation of hearing study. © The Author(s) 2022.",scopus,2-s2.0-85142751947,10.1177/09622802221133556
physiologically motivated individual loudness model for normal hearing and hearing impaired listeners,"127. J Acoust Soc Am. 2018 Aug;144(2):917. doi: 10.1121/1.5050518.Physiologically motivated individual loudness model for normal hearing and hearing impaired listeners.Pieper I(1), Mauermann M(1), Oetting D(2), Kollmeier B(1), Ewert SD(1).Author information:(1)Medical Physics and Cluster of Excellence Hearing4All, Universität Oldenburg, Oldenburg, D-26111, Germany.(2)HörTech gGmbH and Cluster of Excellence Hearing4all, Oldenburg, Germany.A loudness model with a central gain is suggested to improve individualized predictions of loudness scaling data from normal hearing and hearing impaired listeners. The current approach is based on the loudness model of Pieper et al. [(2016). J. Acoust. Soc. Am. 139, 2896], which simulated the nonlinear inner ear mechanics as transmission-line model in a physical and physiological plausible way. Individual hearing thresholds were simulated by a cochlear gain reduction in the transmission-line model and linear attenuation (damage of inner hair cells) prior to an internal threshold. This and similar approaches of current loudness models that characterize the individual hearing loss were shown to be insufficient to account for individual loudness perception, in particular at high stimulus levels close to the uncomfortable level. An additional parameter, termed ""post gain,"" was introduced to improve upon the previous models. The post gain parameter amplifies the signal parts above the internal threshold and can better account for individual variations in the overall steepness of loudness functions and for variations in the uncomfortable level which are independent of the hearing loss. The post gain can be interpreted as a central gain occurring at higher stages as a result of peripheral deafferentation.DOI: 10.1121/1.5050518",pubmed,30180690,10.1121/1.5050518
mild noiseinduced hearing loss at young age affects temporal modulation transfer functions in adult cat primary auditory cortex,"Kittens were exposed for 2 h to a 1/3rd octave band of noise centered at 5 kHz and at 120 dB SPL. After the exposure, they were kept in a quiet room for at least 4 weeks, and until they were mature. The noise-exposed cats showed on average 16.5 dB higher ABR thresholds and 13.2 dB higher thresholds at the characteristic frequency (CF) than the control cats for frequencies between 4 and 16 kHz. The frequency-tuning curve bandwidth at 20 dB above threshold was significantly increased compared to controls in the CF region of the hearing loss. In noise-exposed cats, temporal modulation-transfer functions (tMTFs) to amplitude-modulated (AM) noise, but not to periodic click trains, showed a marked increase for modulation frequencies (MFs) below 6 Hz. The vectorstrength in noise-exposed cats increased for all modulation frequencies below 32 Hz for neurons with a CF in the range of the hearing loss. The tMTFs for AMnoise in the noise-exposed group were less band-pass compared to the controls, and in that sense the mild hearing loss could be considered as effectively reducing the central activation in the same way as a reduced sound pressure level. Effects of reduced central inhibition are visible in the broadening of frequency-tuning curves, and in the increased limiting rates for AMnoise. © 2006 Elsevier B.V. All rights reserved.",scopus,2-s2.0-33845622436,10.1016/j.heares.2006.09.016
phenotypes and clinical subgroups in vestibular migraine a crosssectional study with cluster analysis references,"Objective: The aim of this multicentric cross-sectional study was to collect phenotypes and clinical variability on a large sample of 244 patients enrolled in different university centers in Italy, trying to differentiate subtypes of VM. Background: VM is one of the most frequent episodic vertigo characterized by a great clinical variability for duration of attacks and accompanying symptoms. Diagnosis is based only on clinical history of episodic vertigo in 50% of cases associated with migrainous headache or photo/phonophobia. Methods: We enrolled in different university centers 244 patients affected by definite VM according to the criteria of the Barany Society between January 2022 and December 2022. An audiometric examination and a CNS MRI were performed before inclusion. Patients with low-frequency sensorineural hearing loss were not included, as well as patients with an MRI positive otherwise that for microischemic lesions. Patients were asked to characterize vestibular symptoms choosing among (multiple answers were allowed): internal vertigo, dizziness, visuo-vestibular symptoms/external vertigo; onset of vertigo and duration, neurovegetative, and cochlear accompanying symptoms (hearing loss, tinnitus, and fullness during attacks) were collected as well as migrainous headache and/or photo/phonophobia during vertigo; autoimmune disorders were also analyzed. A bedside examination was performed including study of spontaneous-positional nystagmus with infrared video goggles, post head shaking ny, skull vibration test, and video head impulse test. Results: We included 244 subjects, 181 were females (74.2%). The age of onset of the first vertigo was 36.6 +/- 14.5 while of the first headache was 23.2 +/- 10.1. A positive correlation has been found between the first headache and the first vertigo. The mean duration of vertigo attacks was 11 +/- 16 h. We carried on a cluster analysis to identify subgroups of patients with common clinical features. Four variables allowed to aggregate clusters: age of onset of vertigo, duration of vertigo attacks, presence of migrainous headache during vertigo, and presence of cochlear symptoms during vertigo. We identified 5 clusters: cluster 1/group 1 (23 subjects, 9.4%) characterized by longer duration of vertigo attacks; cluster 2/group 2 (52 subjects, 21.3%) characterized by absence of migrainous headache and cochlear symptoms during vertigo; cluster 3/group 3 (44 subjects, 18%) characterized by presence of cochlear symptoms during vertigo but not headache; cluster 4/group 4 (57 subjects, 23.4%) by the presence of both cochlear symptoms and migrainous headache during vertigo; cluster 5/group 5 (68 subjects, 27.9%) characterized by migrainous headache but no cochlear symptoms during vertigo. Conclusion: VM is with any evidence a heterogeneous disorder and clinical presentations exhibit a great variability. In VM, both symptoms orienting toward a peripheral mechanism (cochlear symptoms) and central ones (long lasting positional non-paroxysmal vertigo) may coexist. Our study is the first published trying to characterize subgroups of VM subjects, thus orienting toward different pathophysiological mechanisms. (PsycInfo Database Record (c) 2024 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc22&DO=10.1007%2fs10072-023-07116-w
artificial intelligence to support hearing loss diagnostics,,base,1ca093c245a226bbb3ba4db5a51894248dce81f3c64da32bbd0d4d7e31897a9f,
predicting the perceived sound quality of frequencycompressed speech,"436. PLoS One. 2014 Nov 17;9(11):e110260. doi: 10.1371/journal.pone.0110260. eCollection 2014.Predicting the perceived sound quality of frequency-compressed speech.Huber R(1), Parsa V(2), Scollie S(2).Author information:(1)Centre of Competence HörTech gGmbH, Oldenburg, Germany; Cluster of Excellence Hearing4All, Oldenburg and Hannover, Germany.(2)National Centre for Audiology, Western University, London, Canada.The performance of objective speech and audio quality measures for the prediction of the perceived quality of frequency-compressed speech in hearing aids is investigated in this paper. A number of existing quality measures have been applied to speech signals processed by a hearing aid, which compresses speech spectra along frequency in order to make information contained in higher frequencies audible for listeners with severe high-frequency hearing loss. Quality measures were compared with subjective ratings obtained from normal hearing and hearing impaired children and adults in an earlier study. High correlations were achieved with quality measures computed by quality models that are based on the auditory model of Dau et al., namely, the measure PSM, computed by the quality model PEMO-Q; the measure qc, computed by the quality model proposed by Hansen and Kollmeier; and the linear subcomponent of the HASQI. For the prediction of quality ratings by hearing impaired listeners, extensions of some models incorporating hearing loss were implemented and shown to achieve improved prediction accuracy. Results indicate that these objective quality measures can potentially serve as tools for assisting in initial setting of frequency compression parameters.DOI: 10.1371/journal.pone.0110260PMCID: PMC4234248",pubmed,25402456,10.1371/journal.pone.0110260
threedimensional imaging of intact porcine cochlea using tissue clearing and custombuilt lightsheet microscopy,"Hearing loss is a prevalent disorder that affects people of all ages. On top of the existing hearing aids and cochlear implants, there is a growing effort to regenerate functional tissues and restore hearing. However, studying and evaluating these regenerative medicine approaches in a big animal model (e.g. pigs) whose anatomy, physiology, and organ size are similar to a human is challenging. In big animal models, the cochlea is bulky, intricate, and veiled in a dense and craggy otic capsule. These facts complicate 3D microscopic analysis that is vital in the cochlea, where structure-function relation is time and again manifested. To allow 3D imaging of an intact cochlea of newborn and juvenile pigs with a volume up to ∼ 250 mm3, we adapted the BoneClear tissue clearing technique, which renders the bone transparent. The transparent cochleae were then imaged with cellular resolution and in a timely fashion, which prevented bubble formation and tissue degradation, using an adaptive custom-built light-sheet fluorescence microscope. The adaptive light-sheet microscope compensated for deflections of the illumination beam by changing the angles of the beam and translating the detection objective while acquiring images. Using this combination of techniques, macroscopic and microscopic properties of the cochlea were extracted, including the density of hair cells, frequency maps, and lower frequency limits. Consequently, the proposed platform could support the growing effort to regenerate cochlear tissues and assist with basic research to advance cures for hearing impairments. © 2020 Optical Society of America under the terms of the OSA Open Access Publishing Agreement",scopus,2-s2.0-85094324251,10.1364/BOE.402991
machine learning models for predicting sudden sensorineural hearing loss outcome a systematic review,,base,eb3215291a18f23128793edf022632920d7ca9ab8fe9de428adda9767fffa3c1,
sound source localization patterns and bilateral cochlear implants age at onset of deafness effects,"185. PLoS One. 2022 Feb 8;17(2):e0263516. doi: 10.1371/journal.pone.0263516. eCollection 2022.Sound source localization patterns and bilateral cochlear implants: Age at onset of deafness effects.Anderson SR(1), Jocewicz R(2), Kan A(3), Zhu J(4), Tzeng S(5), Litovsky RY(1).Author information:(1)Waisman Center, University of Wisconsin-Madison, Madison, Wisconsin, United States of America.(2)Department of Audiology, Stanford University, Stanford, California, United States of America.(3)School of Engineering, Macquarie University, New South Wales, Australia.(4)Department of Statistics, University of Wisconsin-Madison, Madison, Wisconsin, United States of America.(5)Department of Mathematics, National Sun Yat-sen University, Kaohsiung, Taiwan.The ability to determine a sound's location is critical in everyday life. However, sound source localization is severely compromised for patients with hearing loss who receive bilateral cochlear implants (BiCIs). Several patient factors relate to poorer performance in listeners with BiCIs, associated with auditory deprivation, experience, and age. Critically, characteristic errors are made by patients with BiCIs (e.g., medial responses at lateral target locations), and the relationship between patient factors and the type of errors made by patients has seldom been investigated across individuals. In the present study, several different types of analysis were used to understand localization errors and their relationship with patient-dependent factors (selected based on their robustness of prediction). Binaural hearing experience is required for developing accurate localization skills, auditory deprivation is associated with degradation of the auditory periphery, and aging leads to poorer temporal resolution. Therefore, it was hypothesized that earlier onsets of deafness would be associated with poorer localization acuity and longer periods without BiCI stimulation or older age would lead to greater amounts of variability in localization responses. A novel machine learning approach was introduced to characterize the types of errors made by listeners with BiCIs, making them simple to interpret and generalizable to everyday experience. Sound localization performance was measured in 48 listeners with BiCIs using pink noise trains presented in free-field. Our results suggest that older age at testing and earlier onset of deafness are associated with greater average error, particularly for sound sources near the center of the head, consistent with previous research. The machine learning analysis revealed that variability of localization responses tended to be greater for individuals with earlier compared to later onsets of deafness. These results suggest that early bilateral hearing is essential for best sound source localization outcomes in listeners with BiCIs.DOI: 10.1371/journal.pone.0263516PMCID: PMC8824335",pubmed,35134072,10.1371/journal.pone.0263516
slightmild sensorineural hearing loss in children,"OBJECTIVE. The goal was to determine the prevalence and effects of slight/mild bilateral sensorineural hearing loss among children in elementary school. METHODS.A cross-sectional, cluster-sample survey of 6581 children (response: 85%; grade 1: n = 3367; grade 5: n = 3214) in 89 schools in Melbourne, Australia, was performed. Slight/mild bilateral sensorineural hearing loss was defined as a lowfrequency pure-tone average across 0.5, 1, and 2 kHz and/or a high-frequency pure-tone average across 3, 4, and 6 kHz of 16 to 40 dB hearing level in the better ear, with air/bone-conduction gaps of <10 dB. Parents reported children's healthrelated quality of life and behavior. Each child with slight/mild bilateral sensorineural hearing loss, matched to 2 normally hearing children (low-frequency pure-tone average and high-frequency pure-tone average of ≤15 dB hearing level in both ears), completed standardized assessments. Whole-sample comparisons were adjusted for type of school, grade level, and gender, and matched-sample comparisons were adjusted for nonverbal IQ scores. RESULTS. Fifty-five children (0.88%) had slight/mild bilateral sensorineural hearing loss. Children with and without sensorineural hearing loss scored similarly in language (mean: 97.2 vs 99.7), reading (101.1 vs 102.8), behavior (8.4 vs 7.0), and parent- and child-reported child health-related quality of life (77.6 vs 80.0 and 76.1 vs 77.0, respectively), but phonologic short-term memory was poorer (91.0 vs 102.8) in the sensorineural hearing loss group. CONCLUSIONS. The prevalence of slight/mild bilateral sensorineural hearing loss was lower than reported in previous studies. There was no strong evidence that slight/mild bilateral sensorineural hearing loss affects adversely language, reading, behavior, or health-related quality of life in children who are otherwise healthy and of normal intelligence. Copyright © 2006 by the American Academy of Pediatrics.",scopus,2-s2.0-33750943514,10.1542/peds.2005-3168
noiseinduced haircell loss and total exposure energy analysis of a large data set,"246. J Acoust Soc Am. 2004 May;115(5 Pt 1):2207-20. doi: 10.1121/1.1689961.Noise-induced hair-cell loss and total exposure energy: analysis of a large data set.Harding GW(1), Bohne BA.Author information:(1)Department of Otolaryngology, Washington University School of Medicine, St. Louis, Missouri 63110, USA. hardingg@wustl.eduThe relation between total noise-exposure energy, recovery time, or rest during the exposure and amount of hair-cell loss was examined in 416 chinchillas. The exposures were octave bands of noise (OBN) with a center frequency of either 4 kHz at 47-108 dB sound pressure level (SPL) for 0.5 h to 36 d, or 0.5 kHz at 65-128 dB SPL for 3.5 h to 432 d. Recovery times varied from 0 to 365 d. With both OBNs, some animals were exposed on interrupted schedules. Hair-cell loss as a function of age in nonexposed animals (N = 117) was used to correct for sensory-cell loss due to aging. For both OBNs, the ears (N = 607) were separated into three subsets to characterize the primary hair-cell loss from noise and the secondary post-exposure loss and to determine if rest during the exposure decreased loss. Cluster and regression analyses were performed on data from the basal and apical halves of the cochlea to determine the specific rates for these three factors. It was found that: (1) when the OBN was above a critical level, there was no relation between total energy and hair-cell loss; (2) below a critical level, there were highly significant log-linear relations between total energy and hair-cell loss, but not at rates predicted by the equal-energy hypothesis; (3) rest periods during either OBN exposure reduced hair-cell loss; more so for the 4 kHz OBN than the 0.5 kHz OBN; (4) except for the highest exposure levels, the majority of outer hair cell loss from the 4 kHz OBN occurred after the exposure had terminated, while that from the 0.5 kHz OBN occurred during the exposure; and (5) a majority of the inner hair cell loss from both OBNs occurred post-exposure.DOI: 10.1121/1.1689961",pubmed,15139632,10.1121/1.1689961
classification of wideband tympanometry by deep transfer learning with data augmentation for automatic diagnosis of otosclerosis,"Otosclerosis is a common disease of the middle ear leading to stapedial fixation. Its rapid and non-invasive diagnosis could be achieved through wideband tympanometry (WBT), but the interpretation of the raw data provided by this tool is complex and time-consuming. Convolutional neural networks (CNN) could potentially be applied to this situation to help the clinicians categorize WBT data. A dataset containing 135 samples from 80 patients with otosclerosis and 55 controls was obtained. We designed a lightweight CNN to categorize samples into the otosclerosis and control. Receiver operating characteristic (ROC) analysis showed an area under the curve (AUC) of 0.95 ± 0.011, and the F1-score was 0.89 ± 0.031 (r=10). The performance was further improved by data augmentation schemes and transfer learning strategies (AUC: 0.97 ± 0.010, F1-score: 0.94 ± 0.016, p< 0.05, ANOVA). Finally, the most relevant diagnostic features employed by the CNN were assessed via the activation pattern heatmaps. These results are crucial for the visual interpretation of WBT graphic outputs which clinicians use in routine, and for a better understanding of the WBT signal in relation to the ossicular mechanics.  © 2013 IEEE.",scopus,2-s2.0-85112172026,10.1109/JBHI.2021.3093007
perceptual consequences of different signal changes due to binaural noise reduction do hearing loss and working memory capacity play a role,"424. Ear Hear. 2014 Sep-Oct;35(5):e213-27. doi: 10.1097/AUD.0000000000000054.Perceptual consequences of different signal changes due to binaural noise reduction: do hearing loss and working memory capacity play a role?Neher T(1), Grimm G, Hohmann V.Author information:(1)Medical Physics and Cluster of Excellence ""Hearing4all,"" Department of Medical Physics and Acoustics, Carl-von-Ossietzky University, Oldenburg, Germany.OBJECTIVES: In a previous study, ) investigated whether pure-tone average (PTA) hearing loss and working memory capacity (WMC) modulate benefit from different binaural noise reduction (NR) settings. Results showed that listeners with smaller WMC preferred strong over moderate NR even at the expense of poorer speech recognition due to greater speech distortion (SD), whereas listeners with larger WMC did not. To enable a better understanding of these findings, the main aims of the present study were (1) to explore the perceptual consequences of changes to the signal mixture, target speech, and background noise caused by binaural NR, and (2) to determine whether response to these changes varies with WMC and PTA.DESIGN: As in the previous study, four age-matched groups of elderly listeners (with N = 10 per group) characterized by either mild or moderate PTAs and either better or worse performance on a visual measure of WMC participated. Five processing conditions were tested, which were based on the previously used (binaural coherence-based) NR scheme designed to attenuate diffuse signal components at mid to high frequencies. The five conditions differed in terms of the type of processing that was applied (no NR, strong NR, or strong NR with restoration of the long-term stimulus spectrum) and in terms of whether the target speech and background noise were processed in the same manner or whether one signal was left unprocessed while the other signal was processed with the gains computed for the signal mixture. Comparison across these conditions allowed assessing the effects of changes in high-frequency audibility (HFA), SD, and noise attenuation and distortion (NAD). Outcome measures included a dual-task paradigm combining speech recognition with a visual reaction time (VRT) task as well as ratings of perceived effort and overall preference. All measurements were carried out using headphone simulations of a frontal target speaker in a busy cafeteria.RESULTS: Relative to no NR, strong NR was found to impair speech recognition and VRT performance slightly and to improve perceived effort and overall preference markedly. Relative to strong NR, strong NR with restoration of the long-term stimulus spectrum and thus HFA did not affect speech recognition, restored VRT performance to that achievable with no NR, and increased perceived effort and reduced overall preference markedly. SD had negative effects on speech recognition and perceived effort, particularly when both speech and noise were processed with the gains computed for the signal mixture. NAD had positive effects on speech recognition, perceived effort, and overall preference, particularly when the target speech was left unprocessed. VRT performance was unaffected by SD and NAD. None of the datasets exhibited any clear signs that response to the different signal changes varies with PTA or WMC.CONCLUSIONS: For the outcome measures and stimuli applied here, the present study provides little evidence that PTA or WMC affect response to changes in HFA, SD, and NAD caused by binaural NR. However, statistical power restrictions suggest further research is needed. This research should also investigate whether partial HFA restoration combined with some pre-processing that reduces co-modulation distortion results in a more favorable balance of the effects of binaural NR across outcome dimensions and whether NR strength has any influence on these results.DOI: 10.1097/AUD.0000000000000054",pubmed,25010636,10.1097/AUD.0000000000000054
smartphonebased system for learning and inferring hearing aid settings,"103. J Am Acad Audiol. 2016 Oct;27(9):732-749. doi: 10.3766/jaaa.15099.Smartphone-Based System for Learning and Inferring Hearing Aid Settings.Aldaz G(1), Puria S(2), Leifer LJ(1).Author information:(1)Department of Mechanical Engineering, Center for Design Research, Stanford University, Stanford, CA.(2)Mechanics and Computation Division, Department of Mechanical Engineering, Stanford University, Stanford, CA.BACKGROUND: Previous research has shown that hearing aid wearers can successfully self-train their instruments' gain-frequency response and compression parameters in everyday situations. Combining hearing aids with a smartphone introduces additional computing power, memory, and a graphical user interface that may enable greater setting personalization. To explore the benefits of self-training with a smartphone-based hearing system, a parameter space was chosen with four possible combinations of microphone mode (omnidirectional and directional) and noise reduction state (active and off). The baseline for comparison was the ""untrained system,"" that is, the manufacturer's algorithm for automatically selecting microphone mode and noise reduction state based on acoustic environment. The ""trained system"" first learned each individual's preferences, self-entered via a smartphone in real-world situations, to build a trained model. The system then predicted the optimal setting (among available choices) using an inference engine, which considered the trained model and current context (e.g., sound environment, location, and time).PURPOSE: To develop a smartphone-based prototype hearing system that can be trained to learn preferred user settings. Determine whether user study participants showed a preference for trained over untrained system settings.RESEARCH DESIGN: An experimental within-participants study. Participants used a prototype hearing system-comprising two hearing aids, Android smartphone, and body-worn gateway device-for ∼6 weeks.STUDY SAMPLE: Sixteen adults with mild-to-moderate sensorineural hearing loss (HL) (ten males, six females; mean age = 55.5 yr). Fifteen had ≥6 mo of experience wearing hearing aids, and 14 had previous experience using smartphones.INTERVENTION: Participants were fitted and instructed to perform daily comparisons of settings (""listening evaluations"") through a smartphone-based software application called Hearing Aid Learning and Inference Controller (HALIC). In the four-week-long training phase, HALIC recorded individual listening preferences along with sensor data from the smartphone-including environmental sound classification, sound level, and location-to build trained models. In the subsequent two-week-long validation phase, participants performed blinded listening evaluations comparing settings predicted by the trained system (""trained settings"") to those suggested by the hearing aids' untrained system (""untrained settings"").DATA COLLECTION AND ANALYSIS: We analyzed data collected on the smartphone and hearing aids during the study. We also obtained audiometric and demographic information.RESULTS: Overall, the 15 participants with valid data significantly preferred trained settings to untrained settings (paired-samples t test). Seven participants had a significant preference for trained settings, while one had a significant preference for untrained settings (binomial test). The remaining seven participants had nonsignificant preferences. Pooling data across participants, the proportion of times that each setting was chosen in a given environmental sound class was on average very similar. However, breaking down the data by participant revealed strong and idiosyncratic individual preferences. Fourteen participants reported positive feelings of clarity, competence, and mastery when training via HALIC.CONCLUSIONS: The obtained data, as well as subjective participant feedback, indicate that smartphones could become viable tools to train hearing aids. Individuals who are tech savvy and have milder HL seem well suited to take advantages of the benefits offered by training with a smartphone.American Academy of AudiologyDOI: 10.3766/jaaa.15099PMCID: PMC5266590",pubmed,27718350,10.3766/jaaa.15099
sign language finger alphabet recognition from gaborpca representation of hand gestures,"During recent years a large number of computer aided applications have been developed to help the disabled people. This has improved the communication between the able and the hearing impaired community. An intelligent signed alphabet recognizer can work as an aiding agent to translate the signs to words (and also sentences) and vice versa. To achieve this goal few steps to be followed, among which the first complicated task is to recognize the sign-language alphabets from hand gesture images. In this paper, we propose a system that is able to recognize American Sign Language (ASL) alphabets from hand gesture with average 93.23% accuracy. The classification is performed with fuzzy-c-mean clustering on a lower dimensional data which is acquired from the Principle Component Analysis (PCA) of Gabor representation of hand gesture images. Out of the top 20 Principle Components (PCs) the best combination of PCs is determined by finding the best fuzzy cluster for the corresponding PCs of the training data. The best result is obtained from the combination of the fourth to seventh principle components.",ieee,2160-1348,10.1109/ICMLC.2007.4370514
recurrent neural network model for identifying neurological auditory disorder,"This chapter discusses neurological disorders related to the auditory cortex of the human brain. For example, cortical deafness might be considered a neurological disorder. Sometimes cortical deafness occurs due to a major stroke or injury in the cortical tissues around the temporal lobe. Patients with these types of cortical disorders may not be able to perceive or understand acoustic stimuli. Speech recognition is a sub-domain of natural language processing and machine learning (ML) that enables recognition and understanding of acoustic information in machines parallel to the human auditory cortex. This type of artificial acoustic information processing is possible with the help of a recurrent neural network (RNN), which is one of the main pillars of deep learning (DL). An RNN predicts the frequency and pattern of sound waves by the technique of backpropagation, which is a fundamental phenomenon of recurrences in the detailed architecture of a well-defined RNN. In this chapter, we identify cortical processing ambiguities due to neurological disorders. An RNN can efficiently learn various temporal as well as spatial relationships of speech by using mechanisms such as gated recurrent units (GRUs) and long short-term memory (LSTM) with sequence controlling. This is only possible by using the advanced techniques of sound and speech recognition that are fully adopted by RNNs in the broad domain of cognitive auditory information processing and natural language processing such as audio segmentation, phonetic recognition, attention mechanism, and so on. © 2023 Elsevier Inc. All rights reserved.",scopus,2-s2.0-85142820773,10.1016/B978-0-323-90277-9.00103-6
unsupervised neural networks for speech perception with cochlear implant systems for the profoundly deaf,"Recently we have proposed a new speech processing concept for Cochlear Implant (C O - systems. The concept is based on speaker independent signal representation and a neural net classifier which can be combined with the well known CI- speech- coding- strategies. This paper describes some new simulation results: For every speech input frame a 4- dimensional feature vector has been extracted by employing a relative spectral perceptual linear predictive (RASTA-PLP) technique. To classify the feature vectors into so called “auditory related units (ARU)” we applied the self-organizing Kohonen neural net. The best matching ARU's will directly control the synthesis of a “alphabet” of patient adapted stimulus patterns. Simulation results show that the Kohonen algorithm finds representative clusters in the feature vector space for different net dimensions. A discussion of the results and a overview of present experiments with deaf patients will be given. © 1995, Springer Verlag. All rights reserved.",scopus,2-s2.0-84947440929,10.1007/3-540-59497-3_210
singleunit data for sensory neuroscience responses from the auditory nerve of youngadult and aging gerbils,"668. Sci Data. 2024 Apr 22;11(1):411. doi: 10.1038/s41597-024-03259-3.Single-unit data for sensory neuroscience: Responses from the auditory nerve of young-adult and aging gerbils.Heeringa AN(1).Author information:(1)Research Centre Neurosensory Science and Cluster of Excellence ""Hearing4all"", Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg, Carl von Ossietzky Straße 9-11, 26129, Oldenburg, Germany. amarins.nieske.heeringa@uni-oldenburg.de.This dataset was collected to study the functional consequences of age-related hearing loss for the auditory nerve, which carries acoustic information from the periphery to the central auditory system. Using high-impedance glass electrodes, raw voltage traces and spike times were recorded from more than one thousand single fibres of the auditory nerve of young-adult, middle-aged, and old Mongolian gerbils raised in a quiet environment. The dataset contains not only responses to simple acoustic stimuli to characterize the fibres, but also to more complex stimuli, such as speech logatomes in background noise and Schroeder-phase stimuli. A software toolbox is provided to search through the dataset, to plot various analysed outcomes, and to give insight into the analyses. This dataset may serve as a valuable resource to test further hypotheses about age-related hearing loss. Additionally, it can aid in optimizing available computational models of the auditory system, which can contribute to, or eventually even fully replace, animal experiments.© 2024. The Author(s).DOI: 10.1038/s41597-024-03259-3",pubmed,38649691,10.1038/s41597-024-03259-3
a neural network approach to the prediction of pure tone thresholds with distortion product emissions,"438. Ear Nose Throat J. 1994 Nov;73(11):812-3, 817-23.A neural network approach to the prediction of pure tone thresholds with distortion product emissions.Kimberley BP(1), Kimberley BM, Roth L.Author information:(1)Hearing Research Laboratory, Calgary, Alberta, Canada.Distortion Product Emission (DPE) growth functions, demographic data, and pure tone thresholds were recorded in 229 normal-hearing and hearing-impaired ears. Half of the data set (115 ears) was used to train a set of neural networks to map DPE and demographic features to pure tone thresholds at six frequencies in the audiometric range. The six networks developed from this training process were then used to predict pure tone thresholds in the remaining 114-ear data set. When normal pure tone threshold was defined as a threshold less than 20 dB HL, frequency-specific prediction accuracy varied from 57% (correct classification of hearing impairment at 1025 Hz) to 100% (correct classification of hearing impairment at 2050 Hz). Overall prediction accuracy was 90% for impaired pure tone thresholds and 80% for normal pure tone thresholds. This neural network approach was found to be more accurate than discriminant analysis in the prediction of pure tone thresholds. It is concluded that a general strategy exists whereby DPE measures can accurately categorize pure tone thresholds as normal or impaired in large populations of ears with purely cochlear hearing dysfunction.",pubmed,7828474,
predicting depression from hearing loss using artificial intelligence,,base,7830a35815abc859a842a02284f1e47f3cdb2748554de6adfcb9d6478c9bde1a,
deep intracochlear injection of triamcinoloneacetonide with an inner ear catheter in patients with residual hearing,"783. Front Neurosci. 2023 Jul 26;17:1202429. doi: 10.3389/fnins.2023.1202429. eCollection 2023.Deep intracochlear injection of triamcinolone-acetonide with an inner ear catheter in patients with residual hearing.Prenzler NK(1), Salcher R(1), Lenarz T(1)(2), Gaertner L(1), Lesinski-Schiedat A(1), Warnecke A(1)(2).Author information:(1)Department of Otorhinolaryngology, Head and Neck Surgery, Hannover Medical School, Hanover, Germany.(2)Cluster of Excellence ""Hearing 4 All"" (DFG Exc. 2177), Hannover Medical School, Hanover, Germany.INTRODUCTION: In a previous study, an inner ear catheter was used to deliver low- and high-dose steroids into the cochlea prior to cochlear implant electrode insertion. With this approach, more apical regions of the cochlea could be reached and a reduction of electrode impedances in the short term was achieved in cochlear implant recipients. Whether intracochlear application of drugs via the catheter is a safe method also for patients with residual hearing has not been investigated hitherto. The aim of the present study was therefore to investigate the effect of intracochlear triamcinolone application in cochlear implant recipients with residual hearing.PATIENTS AND METHODS: Patients with residual hearing were administered triamcinolone-acetonide (4 mg/ml; n = 10) via an inner ear catheter just prior to insertion of a MED-EL FLEX28 electrode. Impedances were measured at defined time points (intra-operatively, post-operatively and at first fitting) and retrospectively compared with a control group (no steroid application) and low- and high-dose group. Hearing thresholds were measured preoperatively, 3 days after surgery and at first fitting by pure tone audiometry. Pre- to postoperative hearing loss was determined at first fitting and compared to results from a previous study.RESULTS: The median hearing loss after implantation (125-1,500 Hz) was 20.6 dB. Four patients (40%) showed a median hearing loss of less than 15 dB, three patients (30%) between 15 and 30 dB and three patients (30%) more than 30 dB. The median hearing loss was similar to the results obtained from our previous study showing a median hearing loss of 24 dB when using FLEX28 electrode arrays.CONCLUSION: No difference in residual hearing loss was found when comparing application of triamcinolone-acetonide using an inner ear catheter prior to the insertion of a FLEX28 electrode array to the use of the FLEX28 electrode array without the catheter. Thus, we conclude that application of drugs to the cochlea with an inner ear catheter could be a feasible approach in patients with residual hearing.Copyright © 2023 Prenzler, Salcher, Lenarz, Gaertner, Lesinski-Schiedat and Warnecke.DOI: 10.3389/fnins.2023.1202429PMCID: PMC10410142",pubmed,37564369,10.3389/fnins.2023.1202429
realtime fmri neurofeedback  a promising tool for treating tinnitusxxxv world congress of audiology april 1013 2022 warsaw poland,"Chronic tinnitus is often associated with cognitive and emotional problems, such as excessive stress, sleeping disorders and relaxation, anxiety disorders. We assumed that application of real time fMRI neurofeedback helps to reduce stress and negative feelings caused by tinnitus. Haller et al. made the first attempts to treat tinnitus with rt-fMRI-neurofeedback in 2010. Based on the results of studies showing that disruption of the auditory cortex with TMS promotes noise reduction (Kleinjung et al. 2005) Haller et al. conducted an experiment on which people suffering from tinnitus have undergone therapy aimed at reducing the activity of the auditory cortex. Surveys (TFIs) carried out 2 weeks after training showed improvement in 2 out of 6 subjects. More recent studies on the effectiveness of auditory cortex deactivation training in tinnitus therapy conducted in two experimental groups including continuous and intermittent neurofeedback showed greater effectiveness (at trend level) of continuous training measured using the TFI scale immediately after training and 6 weeks later (Emmert et al. 2017). The above results indicate the limited effectiveness of tinnitus therapy by reducing the activity of the auditory cortex with rtfMRI- neurofeedback therapy. This is probably due to both the undetermined etiology of this disease and the early developmental stage of the therapy used. Recent reviews indicate the efficacy of rt-fMRI-neurofeedback in changing brain activity in targeted areas in approximately 60% of studies (Alkoby et al. 2018; Thibault et al. 2018) while behavioral changes are observed in about 40% of studies (Thibault et al. 2018), and clinical improvement of patients in 20--30% (Thibault et al. 2018). The relatively high effectiveness of modulation of brain activity using rt-fMRI-neurofeedback gives the possibility of a causal study of neuronal mechanisms in both a healthy brain and pathological conditions, and the study of the impact of the activity of various structures on improving the functioning of patients (e.g. amygdala in tinnitus [Davies'a et al. 2017]). Mechanisms of rt-fMRI-neurofeedback are particularly interesting from both a cognitive and clinical point of view. Understanding these mechanisms would make it possible to consciously modulate the activity of brain structures and their impact on the causes and symptoms of studied disorders. To this aim, by using the new possibilities resulting from the continuous development of new rt-fMRI-neurofeedback methods (training individualization, process training sessions, the use of machine learning methods) and the availability of open software (openNFT, opendecnef), we have undertaken research aimed at understanding the mechanisms of neurofeedback and the possibilities transfer of brain autoregulation skills between modalities of stimuli and feedback signals used. This work is proof of concept study.",cinahl,2083389X,
perceptual dimensions underlying tinnituslike sounds,"Purpose: The goal of this study was to establish the perceptual underpinnings of the terms that are commonly used by patients when describing the quality of their tinnitus. Method: Using a free-classification method, 15 subjects with normal hearing placed 60 different tinnitus-like sounds into similarity clusters on a grid. Multidimensional scaling, hierarchical clustering, and acoustic analyses were used to determine the acoustic underpinnings of the perceptual dimensions and perceptual similarity. Results: Multidimensional scaling revealed three different perceptual dimensions (pitch, modulation depth + spectral elements, and envelope rate). Hierarchical clustering revealed five explicit similarity clusters: tonal, steady noise, pulsatile, lowfrequency fluctuating noise, and high-frequency fluctuating. Conclusions: Results are consistent with tinnitus perceptions falling into a small set of categories that can be characterized by their acoustics. As a result, there is the potential to develop different tools to assess tinnitus using a variety of different sounds.",cinahl,10924388,10.1044/2020_JSLHR-19-00327
relationship between binaural highfrequency mean hearing threshold and hypertension in female worker exposed to noise,"300. Zhonghua Lao Dong Wei Sheng Zhi Ye Bing Za Zhi. 2021 May 20;39(5):354-358. doi: 10.3760/cma.j.cn121094-20200413-00185.[Relationship between binaural high-frequency mean hearing threshold and hypertension in female worker exposed to noise].[Article in Chinese; Abstract available in Chinese from the publisher]Guo JY(1), Dong GH(2), Rong X(3), Luo HC(4), Liu YM(3).Author information:(1)School of Public Health, Sun Yat-sen University, Guangzhou 510080, China Guangzhou Occupational Disease Prevention and Treatment Hospital, Guangzhou 510620, China.(2)School of Public Health, Sun Yat-sen University, Guangzhou 510080, China.(3)Guangzhou Occupational Disease Prevention and Treatment Hospital, Guangzhou 510620, China.(4)Guangzhou Emergency Management Bureau, Guangzhou 510060, China.Objective: To explore the relationship between the binaural high-frequency mean hearing threshold and the hypertension of female workers exposed to noise, and to understand the application significance of the binaural high-frequency mean hearing threshold as an internal effect indicator of the risk of hypertension in female workers exposed to noise. Methods: From January to December 2018, a total of 20882 female workers exposed to noise in Guangzhou were selected by cluster sampling. Pure tone audiometry, blood pressure, age and length of service were collected. Trend test was used to evaluate the effects of exposure to noise and binaural high-frequency mean hearing threshold on blood pressure. Binary logistic regression model was used to evaluate the risk of hypertension associated with exposure to noise and binaural high-frequency mean hearing threshold. Results: The detection rate of normal hearing threshold, mild hearing loss and severe hearing loss was 80.73% (16858/20882) , 16.21% (3384/20882) and 3.06% (640/20882) respectively. The prevalence of hypertension was 6.04% (1018/16858) in normal hearing group, 10.28% (348/3384) in patients with high frequency mild hearing loss, and 11.25% (72/640) in patients with high frequency severe hearing loss. There was a linear relationship between the increase of working age and high-frequency mean hearing threshold and the increase of systolic and diastolic blood pressure (P< 0.05) . Compared with those exposed to noise for less than 1 year, the risk of hypertension in female workers with 7-9 years and more than 9 years was decreased (OR= 0.79, 0.75, P<0.05) . Compared with normal hearing group, the risk of hypertension in high frequency mild hearing loss group was increased (OR=1.31, P<0.05) . Conclusion: The increase in the binaural high-frequency mean hearing threshold of female workers exposed to noise can increase the blood pressure level and the risk of hypertension, and attention should be paid to female workers with high-frequency mild hearing loss.Publisher: 目的： 探讨双耳高频平均听阈与噪声作业女工高血压的关系，了解双耳高频平均听阈作为噪声作业女工发生高血压风险内效应指标的应用意义。 方法： 于2018年1至12月，采取整群抽样的方法，将广州市的20 882名在岗噪声作业女工纳入研究，收集其纯音测听值、血压、年龄及工龄等信息。运用趋势χ(2)检验评估接触噪声工龄和双耳高频平均听阈对血压的影响，运用二元logistic回归模型评估接触噪声工龄和双耳高频平均听阈相关的高血压风险。 结果： 噪声作业女工双耳高频平均听阈正常听力检出率为80.73%（16 858/20 882）,高频轻度听力损失检出率为16.21%（3 384/20 882），高频重度听力损失检出率为3.06%（640/20 882）。正常听力组中高血压患病率为6.04%（1 018/16 858）,高频轻度听力损失患者中高血压患病率为10.28%（348/3 384）,高频重度听力损失患者中高血压患病率为11.25%（72/640）。随着接触噪声工龄和双耳高频平均听阈增加，收缩压和舒张压呈升高趋势（P<0.05）。与接触噪声工龄<1年组比较，7~9年和≥9年组女工发生高血压风险均下降（OR=0.79、0.75，P<0.05）；与正常听力组比较，高频轻度听力损失组发生高血压风险增加（OR=1.31，P<0.05）。 结论： 噪声作业女工双耳高频平均听阈增加可能导致血压水平和高血压风险增加，应重点关注高频轻度听力损失女工。.DOI: 10.3760/cma.j.cn121094-20200413-00185",pubmed,34074080,10.3760/cma.j.cn121094-20200413-00185
analytical methods for correlated data arising from multicenter hearing studies,"87. Stat Med. 2022 Nov 20;41(26):5335-5348. doi: 10.1002/sim.9572. Epub 2022 Sep 20.Analytical methods for correlated data arising from multicenter hearing studies.Sheng Y(1), Yang C(2), Curhan S(3)(4), Curhan G(2)(3)(4)(5), Wang M(1)(2)(3)(4).Author information:(1)Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, Massachusetts, USA.(2)Department of Epidemiology, Harvard T.H. Chan School of Public Health, Boston, Massachusetts, USA.(3)Harvard Medical School, Boston, Massachusetts, USA.(4)Channing Division of Network Medicine, Department of Medicine, Brigham and Women's Hospital, Boston, Massachusetts, USA.(5)Renal Division, Department of Medicine, Brigham and Women's Hospital, Boston, Massachusetts, USA.In epidemiological hearing studies, estimating the association between exposures and hearing loss using audiometrically-assessed hearing measurements is challenging due to the complex correlation structure in the clustered data, with clusters formed by the two ears of the same individual and the testing site and audiologist. We propose a linear mixed-effects model to take into account the multilevel correlation structures of the data. Both theoretically and in simulation studies, we compare single-ear linear regression models commonly used in published hearing loss studies with the proposed both-ears linear mixed models properly accounting for the multi-level correlations. Our findings include (1) when there are only participant-level covariates, the worse-ear linear regression models produce unbiased but typically less efficient estimators than the both-ear and average-ear approaches; (2) when there are ear-level confounders, the worse-ear method may lead to biased estimators and the average-ear method produces unbiased but typically less efficient estimators than the both-ear method; (3) the both-ear method may gain efficiency when additionally adjusting for testing sites and audiologists. As an illustrative example, we applied the single-ear and both-ear methods to assess aspirin-hearing association in the Nurses' Health Study II.© 2022 John Wiley & Sons Ltd.DOI: 10.1002/sim.9572PMCID: PMC9588694",pubmed,36125070,10.1002/sim.9572
automatic internal auditory canal segmentation using a weakly supervised 3d unet,"Cochlear implants (CIs) are neural prosthetics used to improve hearing in patients with severe-to-profound hearing loss. After implantation, the process of fine-tuning the implant for a specific patient is expedited if the audiologist has tools to approximate which auditory nerve fiber regions are being stimulated by the implant's electrode array. Auditory nerves travel from the cochlea where the prosthetic is implanted to the brain via the internal auditory canal (IAC). In this paper, we present a method for segmenting the IAC in a CT image using weakly supervised 3D UNets. Our approach is to train a U-Net with a custom loss function to refine a localization provided by a previously proposed active-shape-model-based IAC segmentation method. Preliminary results indicate that our proposed approach is successful in refining IAC localization, which is an important step towards accurate auditory nerve fiber localization for neural activation modeling.  © 2022 SPIE.",scopus,2-s2.0-85131959117,10.1117/12.2611897
pilot study on the use of data mining to identify cochlear implant candidates,"313. Cochlear Implants Int. 2018 May;19(3):142-146. doi: 10.1080/14670100.2018.1425274. Epub 2018 Jan 19.Pilot study on the use of data mining to identify cochlear implant candidates.Grisel JJ(1), Schafer E(2), Lam A(3), Griffin T(4).Author information:(1)a Head & Neck Surgical Associates , 1 Burnside Dr., Wichita Falls , TX 76310 , USA.(2)b University of North Texas , 1155 Union Circle #305010 Denton , TX 76203 , USA.(3)c Auditory Implant Initiative , 1 Burnside Dr., Wichita Falls , TX 76310 , USA.(4)d Midwestern State University , 3410 Taft Blvd., Wichita Falls , TX 76308 , USA.OBJECTIVES: The goal of this pilot study was to determine the clinical utility of data-mining software that screens for cochlear implant (CI) candidacy.METHODS: The Auditory Implant Initiative developed a software module that screens for CI candidates via integration with a software system (Noah 4) that serves as a depository for hearing test data. To identify candidates, patient audiograms from one practice were exported into the screening module. Candidates were tracked to determine if any eventually underwent implantation.RESULTS: After loading 4836 audiograms from the Noah 4 system, the screening module identified 558 potential CI candidates. After reviewing the data for the potential candidates, 117 were targeted and invited to an educational event. Following the event, a total of six candidates were evaluated, and two were implanted.DISCUSSION: This objective approach to identifying candidates has the potential to address the gross underutilization of CIs by removing any bias or lack of knowledge regarding the management of severe to profound sensorineural hearing loss with CIs.CONCLUSION: The screening module was an effective tool for identifying potential CI candidates at one ENT practice. On a larger scale, the screening module has the potential to impact thousands of CI candidates worldwide.DOI: 10.1080/14670100.2018.1425274",pubmed,29347892,10.1080/14670100.2018.1425274
environmentspecific noise suppression for improved speech intelligibility by cochlear implant users,"453. J Acoust Soc Am. 2010 Jun;127(6):3689-95. doi: 10.1121/1.3365256.Environment-specific noise suppression for improved speech intelligibility by cochlear implant users.Hu Y(1), Loizou PC.Author information:(1)Department of Electrical Engineering, University of Texas-Dallas, Richardson, Texas 75080, USA. huy@uwm.eduAttempts to develop noise-suppression algorithms that can significantly improve speech intelligibility in noise by cochlear implant (CI) users have met with limited success. This is partly because algorithms were sought that would work equally well in all listening situations. Accomplishing this has been quite challenging given the variability in the temporal/spectral characteristics of real-world maskers. A different approach is taken in the present study focused on the development of environment-specific noise suppression algorithms. The proposed algorithm selects a subset of the envelope amplitudes for stimulation based on the signal-to-noise ratio (SNR) of each channel. Binary classifiers, trained using data collected from a particular noisy environment, are first used to classify the mixture envelopes of each channel as either target-dominated (SNR>or=0 dB) or masker-dominated (SNR<0 dB). Only target-dominated channels are subsequently selected for stimulation. Results with CI listeners indicated substantial improvements (by nearly 44 percentage points at 5 dB SNR) in intelligibility with the proposed algorithm when tested with sentences embedded in three real-world maskers. The present study demonstrated that the environment-specific approach to noise reduction has the potential to restore speech intelligibility in noise to a level near to that attained in quiet.DOI: 10.1121/1.3365256PMCID: PMC2896410",pubmed,20550267,10.1121/1.3365256
can multifrequency tympanometry be used in the diagnosis of menieres disease a systematic review and metaanalysis,"729. J Clin Med. 2024 Mar 4;13(5):1476. doi: 10.3390/jcm13051476.Can Multifrequency Tympanometry Be Used in the Diagnosis of Meniere's Disease? A Systematic Review and Meta-Analysis.Tsilivigkos C(1), Vitkos EN(2), Ferekidis E(1), Warnecke A(3)(4).Author information:(1)First Department of Otolaryngology, Hippokration General Hospital, National and Kapodistrian University of Athens, 15772 Athens, Greece.(2)Department of Oral and Maxillofacial Surgery, George Papanikolaou General Hospital, 56429 Thessaloniki, Greece.(3)Department of Otorhinolaryngology-Head and Neck Surgery, Hannover Medical School, 30625 Hanover, Germany.(4)Cluster of Excellence Hearing4all, German Research Foundation, 30625 Hannover, Germany.(1) Background: Ménière's disease (MD) is a disease of the inner ear, presenting with episodes of vertigo, hearing loss, and tinnitus.The aim of this study is to examine the role of multifrequency tympanometry (MFT) in the diagnosis of MD. (2) Methods: A systematic review of MEDLINE (via PubMed), Scopus, Google Scholar, and the Cochrane Library was performed, aligned with the PRISMA guidelines. Only studies that directly compare ears affected by Ménière's disease with unaffected or control ears were included. Random-effects model meta-analyses were performed. (3) Results: Seven prospective case-control studies reported a total of 899 ears, 282 of which were affected by Ménière's disease (affected ears-AE), 197 unaffected ears in patients with MD (UE), and 420 control ears (CE) in healthy controls. No statistically significant differences between the groups were observed regarding resonant frequency (RF). The pure tone audiometry average of the lower frequencies (PTA basic) was significantly greater in affected ears when compared with unaffected ears. The conductance tympanogram at 2 kHz revealed a statistically significantly greater G width of 2 kHz in the affected ears when compared to both unaffected and control ears, while control ears had a statistically significant lesser G width of 2 kHz compared to both the other two groups. (4) Conclusions: MFT, and specifically G width at 2 kHz, could be an important tool in the diagnosis of MD.DOI: 10.3390/jcm13051476PMCID: PMC10932169",pubmed,38592318,10.3390/jcm13051476
an integrated knowledge translation experience use of the network of pediatric audiologists of canada to facilitate the development of the university of western ontario pediatric audiological monitoring protocol uwo pedamp v10,"673. Trends Amplif. 2011 Mar-Jun;15(1):34-56. doi: 10.1177/1084713811417634.An integrated knowledge translation experience: use of the Network of Pediatric Audiologists of Canada to facilitate the development of the University of Western Ontario Pediatric Audiological Monitoring Protocol (UWO PedAMP v1.0).Moodie ST(1), Bagatto MP, Miller LT, Kothari A, Seewald R, Scollie SD.Author information:(1)National Centre for Audiology, Faculty of Health Sciences, University of Western Ontario, London, Ontario, Canada. sheila@nca.uwo.caPediatric audiologists lack evidence-based, age-appropriate outcome evaluation tools with well-developed normative data that could be used to evaluate the auditory development and performance of children aged birth to 6 years with permanent childhood hearing impairment. Bagatto and colleagues recommend a battery of outcome tools that may be used with this population. This article provides results of an evaluation of the individual components of the University of Western Ontario Pediatric Audiological Monitoring Protocol (UWO PedAMP) version 1.0 by the audiologists associated with the Network of Pediatric Audiologists of Canada. It also provides information regarding barriers and facilitators to implementing outcome measures in clinical practice. Results indicate that when compared to the Parents' Evaluation of Aural/Oral Performance of Children (PEACH) Diary, audiologists found the PEACH Rating Scale to be a more clinically feasible evaluation tool to implement in practice from a time, task, and consistency of use perspective. Results also indicate that the LittlEARS(®) Auditory Questionnaire could be used to evaluate the auditory development and performance of children aged birth to 6 years with permanent childhood hearing impairment (PCHI). The most cited barrier to implementation is time. The result of this social collaboration was the creation of a knowledge product, the UWO PedAMP v1.0, which has the potential to be useful to audiologists and the children and families they serve.DOI: 10.1177/1084713811417634PMCID: PMC4040833",pubmed,22194315,10.1177/1084713811417634
effect of a ci programming fitting tool with artificial intelligence in experienced cochlear implant patients,"138. Otol Neurotol. 2023 Mar 1;44(3):209-215. doi: 10.1097/MAO.0000000000003810. Epub 2023 Jan 18.Effect of a CI Programming Fitting Tool with Artificial Intelligence in Experienced Cochlear Implant Patients.Wathour J(1), Govaerts PJ, Lacroix E, Naïma D(1).Author information:(1)Cliniques universitaires Saint-Luc, Avenue Hippocrate 100, 1200, Brussels, Belgium.OBJECTIVE: Cochlear implants (CIs) are the treatment of choice for patients with severe to profound hearing loss. The hearing results, however, considerably vary across patients. This may partly be due to variability in the CI fitting. We investigated the effect of FOX, a software tool to program CIs using artificial intelligence (AI), on hearing outcomes.METHODS: Forty-seven experienced CI patients who came to our tertiary CI center for their annual follow-up between 2017 and 2020 were recruited for this study. They received a new CI map created by the AI software tool. CI parameters and auditory outcomes obtained with this new map were compared with those of the initial manual map after 15 days of take-home experience. Within-patient differences were assessed. At the end of the study, the patients were offered a choice to continue using the AI map or to revert to their old manual map.RESULTS: Several auditory outcomes improved with the AI map, namely, pure tone audiometric threshold at 6,000 Hz (median improvement 10 dB, range = -20 to 50 dB, Z = -2.608, p = 0.008), phonemic discrimination scores (median improvement 10%, range = 0% to 30%, Z = -4.061, p = 0.001), and soft-intensity (median improvement of 10%, range = -20% to 90%, Z = -4.412, p < 0.001) to normal-intensity (median improvement of 10%, range = -30% to 60%, Z = -3.35, p < 0.001) speech audiometric scores.CONCLUSION: The AI-assisted CI mapping model as a potential assistive tool may improve audiological outcomes for experienced CI patients, including high-frequency pure tone audiometry and audiometric speech scores at low and normal presentation levels.Clinical trial registration: NCT03700268.Copyright © 2023, Otology & Neurotology, Inc.DOI: 10.1097/MAO.0000000000003810",pubmed,36728126,10.1097/MAO.0000000000003810
how hearing impairment affects sentence comprehension using eye fixations to investigate the duration of speech processing,"442. Trends Hear. 2015 Apr 24;19:2331216515584149. doi: 10.1177/2331216515584149.How hearing impairment affects sentence comprehension: using eye fixations to investigate the duration of speech processing.Wendt D(1), Kollmeier B(2), Brand T(2).Author information:(1)Medizinische Physik, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany Hearing Systems, Department of Electrical Engineering, Technical University of Denmark, Kgs. Lyngby, Denmark wendt@elektro.dtu.dk.(2)Medizinische Physik, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany Cluster of Excellence Hearing4all, Oldenburg, Germany.The main objective of this study was to investigate the extent to which hearing impairment influences the duration of sentence processing. An eye-tracking paradigm is introduced that provides an online measure of how hearing impairment prolongs processing of linguistically complex sentences; this measure uses eye fixations recorded while the participant listens to a sentence. Eye fixations toward a target picture (which matches the aurally presented sentence) were measured in the presence of a competitor picture. Based on the recorded eye fixations, the single target detection amplitude, which reflects the tendency of the participant to fixate the target picture, was used as a metric to estimate the duration of sentence processing. The single target detection amplitude was calculated for sentence structures with different levels of linguistic complexity and for different listening conditions: in quiet and in two different noise conditions. Participants with hearing impairment spent more time processing sentences, even at high levels of speech intelligibility. In addition, the relationship between the proposed online measure and listener-specific factors, such as hearing aid use and cognitive abilities, was investigated. Longer processing durations were measured for participants with hearing impairment who were not accustomed to using a hearing aid. Moreover, significant correlations were found between sentence processing duration and individual cognitive abilities (such as working memory capacity or susceptibility to interference). These findings are discussed with respect to audiological applications.© The Author(s) 2015.DOI: 10.1177/2331216515584149PMCID: PMC4409940",pubmed,25910503,10.1177/2331216515584149
selecting appropriate tests to assess the benefits of bilateral amplification with hearing aids,"226. Trends Hear. 2016 Jul 26;20:2331216516658239. doi: 10.1177/2331216516658239.Selecting Appropriate Tests to Assess the Benefits of Bilateral Amplification With Hearing Aids.van Schoonhoven J(1), Schulte M(2), Boymans M(3), Wagener KC(2), Dreschler WA(3), Kollmeier B(4).Author information:(1)Department of Clinical and Experimental Audiology, Academic Medical Centre, Amsterdam, The Netherlands jvanschoonhoven@amc.nl.(2)Hörzentrum Oldenburg GmbH, Oldenburg, Germany Cluster of Excellence Hearing4all, Oldenburg, Germany.(3)Department of Clinical and Experimental Audiology, Academic Medical Centre, Amsterdam, The Netherlands.(4)Hörzentrum Oldenburg GmbH, Oldenburg, Germany Cluster of Excellence Hearing4all, Oldenburg, Germany Medizinische Physik, Carl-von-Ossietzky Universität Oldenburg, Germany.The aim of this study was to investigate the effect of bilateral hearing aids (HA) in subjects with mild and moderate-to-severe hearing loss. This study was designed as a within-subject feasibility study. Bilateral HA use was assessed using different laboratory tests on speech reception, listening effort, noise tolerance, and localization. All data were evaluated with bilateral and unilateral HA fittings. Forty experienced bilateral HA users were included with hearing impairment ranging from mild to moderate-to-severe. Subjects were stratified into two groups based on the degree of hearing loss. Speech reception in noise, listening effort, and localization tests showed a bilateral benefit for the moderate-to-severely hearing-impaired subjects. A bilateral benefit was also observed for listening effort in the mildly hearing-impaired group. The assessment of listening effort shows promise as a measure of bilateral HA benefit for mild hearing impairment. Localization and speech reception in noise tests provide additional value for larger losses. The next step is to compare experienced unilateral with bilateral HA users.© The Author(s) 2016.DOI: 10.1177/2331216516658239PMCID: PMC4964154",pubmed,27460871,10.1177/2331216516658239
neural synchrony in ventral cochlear nucleus neuron populations is not mediated by intrinsic processes but is stimulus induced implications for auditory brainstem implants,"574. J Neural Eng. 2009 Dec;6(6):065003. doi: 10.1088/1741-2560/6/6/065003. Epub 2009 Oct 23.Neural synchrony in ventral cochlear nucleus neuron populations is not mediated by intrinsic processes but is stimulus induced: implications for auditory brainstem implants.Shivdasani MN(1), Mauger SJ, Rathbone GD, Paolini AG.Author information:(1)School of Psychological Science, La Trobe University, Bundoora, VIC 3086, Australia.The aim of this investigation was to elucidate if neural synchrony forms part of the spike time-based theory for coding of sound information in the ventral cochlear nucleus (VCN) of the auditory brainstem. Previous research attempts to quantify the degree of neural synchrony at higher levels of the central auditory system have indicated that synchronized firing of neurons during presentation of an acoustic stimulus could play an important role in coding complex sound features. However, it is unknown whether this synchrony could in fact arise from the VCN as it is the first station in the central auditory pathway. Cross-correlation analysis was conducted on 499 pairs of multiunit clusters recorded in the urethane-anesthetized rat VCN in response to pure tones and combinations of two tones to determine the presence of neural synchrony. The shift predictor correlogram was used as a measure for determining the synchrony owing to the effects of the stimulus. Without subtraction of the shift predictor, over 65% of the pairs of multiunit clusters exhibited significant correlation in neural firing when the frequencies of the tones presented matched their characteristic frequencies (CFs). In addition, this stimulus-evoked neural synchrony was dependent on the physical distance between electrode sites, and the CF difference between multiunit clusters as the number of correlated pairs dropped significantly for electrode sites greater than 800 microm apart and for multiunit cluster pairs with a CF difference greater than 0.5 octaves. However, subtraction of the shift predictor correlograms from the raw correlograms resulted in no remaining correlation between all VCN pairs. These results suggest that while neural synchrony may be a feature of sound coding in the VCN, it is stimulus induced and not due to intrinsic neural interactions within the nucleus. These data provide important implications for stimulation strategies for the auditory brainstem implant, which is used to provide functional hearing to the profoundly deaf through electrical stimulation of the VCN.DOI: 10.1088/1741-2560/6/6/065003",pubmed,19850978,10.1088/1741-2560/6/6/065003
a shift of treatment approach in speech language pathology services for children with speech sound disorders  a single case study of an intense intervention based on nonlinear phonology and motorlearning principles,"Even though there are documented benefits of direct intensive intervention for children with speech sound disorders (SSDs), the intensity given at Swedish Speech Language Pathology services rarely exceeds once a week. Also, indirect therapy approaches are commonly employed. The purpose of the present case study was to investigate the effects of an intensive specialist therapy, based on non-linear phonological analysis and motor learning principles. The participant was a boy aged 4:10 years with severe SSD, who previously had received indirect therapy from age 3 with, very limited results. A single subject ABA design was used. At baseline, whole word match was 0%, Word shape CV match was 39% and PCC was 22, 7%. He had no multisyllabic words, no consonant clusters and no established coronals. Intervention was given 4 days weekly for 3 weeks in two periods with a 7-week intervening break and a post therapy assessments. Therapy was focused on establishing multisyllabic words, iambic stress pattern, clusters and coronals with the principle of using already established elements for targeting new elements. At post therapy assessment, whole word match was 39%, word shape CV match was 71% and PCC 69.1%. Multisyllabic words (86%), coronals (82%) and word initial clusters (80%) were established. Without being targeted, back vowels were also present and segment timing improved. The strong treatment effects of this study demonstrate that at least severe cases of SSD require the clinical knowledge and skills that only a SLP can provide and that frequent direct therapy is both beneficial and needed.",cinahl,2699206,10.1080/02699206.2018.1552990
music education for the deaf and hard of hearing a literature review to understand the methods used in music teaching,This research aims to identify the main scientific productions on the theme of music education for the deaf in various article bases and to find the themes used for teaching music to the deaf. It seeks to analyze them and thus propose a new method using artificial intelligence to meet the needs of the deaf person and make them independent in learning.,ieee,2166-0727,10.23919/CISTI58278.2023.10211595
first clinical experiences with a direct acoustic cochlear stimulator in comparison to preoperative fitted conventional hearing aids,"466. Otol Neurotol. 2013 Dec;34(9):1711-8. doi: 10.1097/MAO.0000000000000225.First clinical experiences with a direct acoustic cochlear stimulator in comparison to preoperative fitted conventional hearing aids.Busch S(1), Kruck S, Spickers D, Leuwer R, Hoth S, Praetorius M, Plinkert PK, Mojallal H, Schwab B, Maier H, Lenarz T.Author information:(1)*Department of Otolaryngology, and †Cluster of Excellence Hearing4all, Medical University Hannover, Hannover; ‡Department of Oto-Rhino-Laryngology, HELIOS Hospital Krefeld, Krefeld; and §Department of Otolaryngology, Head and Neck Surgery, University of Heidelberg, Heidelberg, Germany.OBJECTIVE: Patients with moderate-to-severe mixed hearing losses (MHLs) are hard to provide sufficient benefit with currently available conventional hearing aids. Here, the long-term safety of a direct acoustic cochlear stimulator (DACS) and the effectiveness compared with conventional ""high-performance"" hearing aids were investigated.STUDY DESIGN: Prospective, within patient reference, nonrandomized, interventional multicenter clinical study performed at these 3 centers: Medical University Hannover, University of Heidelberg, and Helios Hospital Krefeld.PATIENTS AND INTERVENTION: Ten otosclerosis patients with severe-to-profound MHL were preoperatively fitted with state-of-the-art conventional hearing aids (HA). After 2 months of testing conventional HA, 9 of the patients decided to be implanted with a DACS.MAIN OUTCOME MEASURES: Air conduction (AC) and bone conduction (BC) aided and unaided thresholds, speech discrimination before and after implantation and at 3, 6, and 12 months after activation. The subjective benefit was assessed by the Abbreviated Profile of Hearing Aid Benefit (APHAB).RESULTS: Preoperative hearing thresholds were preserved over the 12 month observation time after activation. Average functional gain (0.5-4 kHz) achieved with conventional HA was 47 dB compared with 56 dB with the DACS. Speech-in-noise tests revealed a lower SNR for DACS (3.1 dB) than for the HA (6.6 dB) and patients were more satisfied with the DACS.CONCLUSION: The DACS significantly improved hearing, speech intelligibility, and satisfaction in patients with a severe-to-profound mixed hearing loss and can be considered a safe and useful alternative to conventional hearing aids.DOI: 10.1097/MAO.0000000000000225",pubmed,24232068,10.1097/MAO.0000000000000225
the effect of ventilation tubes on language development in infants with otitis media with effusion a randomized trial,"646. Pediatrics. 2000 Sep;106(3):E42.The effect of ventilation tubes on language development in infants with otitis media with effusion: A randomized trial.Rovers MM(1), Straatman H, Ingels K, van der Wilt GJ, van den Broek P, Zielhuis GA.Author information:(1)Department of Otorhinolaryngology, University Medical Center Saint Radboud, Nijmegen, The Netherlands. m.rovers@mie.kun.nlOBJECTIVE: To study the effectiveness of ventilation tubes on the language development in infants with persistent otitis media with effusion (OME). All existing studies addressed children 3 years of age or older. Currently, OME is detected and treated with ventilation tubes at a younger age. Because of the critical relationship between age, hearing, and language development, we conducted a study of the effects of ventilation tubes on language development in infants 1 to 2 years old with persistent OME.DESIGN: A multicenter, randomized, controlled trial (embedded in a cohort) with 2 treatment arms: 1) treatment with ventilation tubes (VT group; n = 93); or 2) with a period of watchful waiting (WW group; n = 94). Hearing loss and expressive and comprehensive language were assessed every 6 months, while tympanometry and otoscopy were performed every 3 months. Other factors with potential influence on language development were also included: adenoidectomy, hospital, attending day care, sex, age at randomization, educational level of the mother, upper respiratory infections, and the native country of the parents and older siblings. The trial was designed to allow for the detection of a mean difference in language development of 3 months or more between children allocated to the VT and WW groups.RESULTS: No relevant differences were found in expressive or comprehensive language between the 2 groups after adjustment for educational level of the mother, IQ of the child, and differences at baseline. A principal component analysis showed that in the VT group, the children with frequent complaints improved 1.6 months more in comprehensive language than those with no or some complaints. The children with favorable language stimulation, however, did not improve more than the children with less favorable stimulation. No differences were found for expressive language among the various clusters. The probability to improve >3 months in comprehensive language was.48 (95% confidence interval [CI]:.29-.68) for children with highly educated mothers versus.09 (95% CI:.02-.30) for children whose mothers had a low educational level. In the WW group, these changes were.30 (95% CI:.14-.53) and.14 (95% CI:.04-.35), respectively. The probability to improve >4 months in expressive language was.52 (95% CI:.32-.71) for children with highly educated mothers versus.06 (95% CI:.01-.31) for children whose mothers had a low educational level. In the WW group these changes were.42 (95% CI:.23-.64) and.11 (95% CI:.03-.35), respectively. In addition, there were delays in expressive language in both groups compared with their age expected values. The comprehensive language of the children who were effusion-free during the follow-up (n = 54) improved 1.5 months (95% CI: -.2-3.2) more than that of the children who had persistent effusion during the entire follow-up (n = 28). No differences were found for expressive language development. Disregarding the intervention contrast, improvements in hearing seemed to be related to improvements in language development, especially in verbal comprehension.DISCUSSION: In this study, we used the Reynell, Schlichting, and Lexi tests to study the relation between early persistent OME and language development. These tests are directly related to normal language, widely accepted, and validated. It cannot be ruled out that more specific measures such as auditory perception tests would have produced more differences between groups, but the focus was on general language development. A total of 10 children in the WW group received treatment with ventilation tubes during follow-up. A further 11 children dropped out during the trial. A sensitivity analysis with the 10 children who received ventilation tubes did not change the results, and baseline differences were not found between the 11 children who dropped out and those who completed the trial.CONCLUSIONS: In the total group of infants with persistent OME, ventilation tubes did not h",pubmed,10969126,
hearing loss after radiation and chemotherapy for central nervous system and head and neck tumors in children,"Purpose/objective(s): Hearing loss (HL) is a serious secondary effect of treatment for head and neck (H&N) and central nervous system (CNS) tumors in children. Radiation and platinum chemotherapy independently increase the risk of HL; however, combined modality treatment is routinely used and the effect of chemoradiation on HL risk is not well-studied. Using chemotherapy and cochlear radiation (RT) doses, we created a model that predicts for HL, including a nomogram that can be used in the clinical setting to calculate the risk of clinically-significant HL.Materials/methods: In this single institution retrospective study, 171 patients with H&N or CNS tumors were treated with radiation, with or without chemotherapy and had longitudinal (≥2) audiological evaluation. SIOP-Boston (SIOP) and Chang grades were assigned to 2,420 hearing assessments of 342 ears; analyses using SIOP grades are presented here. For the nomogram, SIOP grade ≥3 was considered clinically-significant HL (requiring hearing aids). An Andersen-Gill model for recurrent events was used to obtain inverse intensity weights to account for factors explaining the number of assessments. Multivariable weighted ordinal logistic regression was fitted to evaluate the effect of clinicopathologic features on HL. Clustered robust standard errors were calculated to account for intra-patient correlation.Results: Patients underwent a median of 6 (range 2-23) assessments over a median of 3.1 years from diagnosis to last audiogram (range 0.1-15.2 years). Cisplatin was given to 63% of patients and 34% received carboplatin. The mean cochlear doses on the right and left were 36.8 Gy (standard deviation [SD] 16.5) and 37.0 Gy (SD 16.2), respectively. Multivariable regression revealed that mean cochlear dose (odds ratio [OR] 1.04 per Gy, 95% confidence interval [CI] 1.02-1.05, P < 0.001), time since RT (OR 1.2 per year, 95% CI 1.2-1.3, P < 0.001), cisplatin use (OR 5.33, 95% CI 2.9-9.9, P < 0.001), and carboplatin use (OR 2.3, 95% CI 1.27-4.17, P = 0.006) were associated with increasing SIOP grade of HL; age at RT, hydrocephalus, surgery, amifostine, and laterality were not correlated with HL. There was no synergistic effect of RT and cisplatin (interaction term, P = 0.4) or RT and carboplatin (interaction term, P = 0.9). Cumulative incidence of high-frequency HL (> 4 kHz) was > 50% at 5 years post-RT in those who received mean dose > 20 Gy to the cochlea, while incidence of HL across all frequencies continued to increase beyond 5 years post-RT.Conclusion: RT and chemotherapy have an additive, not synergistic, effect on HL risk. Mean cochlear radiation dose, time since radiation, and platinum chemotherapy use were associated with HL. This modelling can guide survivorship care by multidisciplinary pediatric oncology teams with respect to audiology follow-up in an effort to that ensure suitable educational accommodations and assistive devices are in place.",cinahl,3603016,10.1016/j.ijrobp.2021.07.655
classification of auditory brainstem responses through symbolic pattern discovery,"155. Artif Intell Med. 2016 Jun;70:12-30. doi: 10.1016/j.artmed.2016.05.001. Epub 2016 May 24.Classification of auditory brainstem responses through symbolic pattern discovery.Molina ME(1), Perez A(2), Valente JP(3).Author information:(1)Department of Languages, Information Systems and Software Engineering, School of Computer Engineering, Technical University of Madrid, Campus de Montegancedo, s/n, Boadilla del Monte, Madrid 28660, Spain. Electronic address: me.molina@alumnos.upm.es.(2)Department of Languages, Information Systems and Software Engineering, School of Computer Engineering, Technical University of Madrid, Campus de Montegancedo, s/n, Boadilla del Monte, Madrid 28660, Spain. Electronic address: aurora@fi.upm.es.(3)Department of Languages, Information Systems and Software Engineering, School of Computer Engineering, Technical University of Madrid, Campus de Montegancedo, s/n, Boadilla del Monte, Madrid 28660, Spain. Electronic address: jpvalente@fi.upm.es.INTRODUCTION: Numeric time series are present in a very wide range of domains, including many branches of medicine. Data mining techniques have proved to be useful for knowledge discovery in this type of data and for supporting decision-making processes.OBJECTIVES: The overall objective is to classify time series based on the discovery of frequent patterns. These patterns will be discovered in symbolic sequences obtained from the time series data by means of a temporal abstraction process.METHODS: Firstly, we transform numeric time series into symbolic time sequences, where the symbols aim to represent the relevant domain concepts. These symbols can be defined using either public or expert domain knowledge. Then we apply a symbolic pattern discovery technique to the output symbolic sequences. This technique identifies the subsequences frequently found in a population group. These subsequences (patterns) are representative of population groups. Finally, we employ a classification technique based on the identified patterns in order to classify new individuals. Thanks to the inclusion of domain knowledge, the classification results can be explained using domain terminology. This makes the results easier to interpret for the domain specialist (physician).RESULTS: This method has been applied to brainstem auditory evoked potentials (BAEPs) time series. Preliminary experiments were carried out to analyse several aspects of the method including the best configuration of the pattern discovery technique parameters. We then applied the method to the BAEPs of 83 individuals belonging to four classes (healthy, conductive hearing loss, vestibular schwannoma-brainstem involvement and vestibular schwannoma-8th-nerve involvement). According to the results of the cross-validation, overall accuracy was 99.4%, sensitivity (recall) was 97.6% and specificity was 100% (no false positives).CONCLUSION: The proposed method effectively reduces dimensionality. Additionally, if the symbolic transformation includes the right domain knowledge, the method arguably outputs a data representation that denotes the relevant domain concepts more clearly. The method is capable of finding patterns in BAEPs time series and is very accurate at correctly predicting whether or not new patients have an auditory-related disorder.Copyright © 2016 Elsevier B.V. All rights reserved.DOI: 10.1016/j.artmed.2016.05.001",pubmed,27431034,10.1016/j.artmed.2016.05.001
the aging cochlea towards unraveling the functional contributions of strial dysfunction and synaptopathy,"96. Hear Res. 2019 May;376:111-124. doi: 10.1016/j.heares.2019.02.015. Epub 2019 Mar 2.The aging cochlea: Towards unraveling the functional contributions of strial dysfunction and synaptopathy.Heeringa AN(1), Köppl C(2).Author information:(1)Cluster of Excellence 'Hearing4all' and Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg, 26129, Oldenburg, Germany.(2)Cluster of Excellence 'Hearing4all' and Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg, 26129, Oldenburg, Germany. Electronic address: christine.koeppl@uni-oldenburg.de.Strial dysfunction is commonly observed as a key consequence of aging in the cochlea. A large body of animal research, especially in the quiet-aged Mongolian gerbil, shows specific histopathological changes in the cochlear stria vascularis and the putatively corresponding effects on endocochlear potential and auditory nerve responses. However, recent work suggests that synaptopathy, or the loss of inner hair cell-auditory nerve fiber synapses, also presents as a consequence of aging. It is now believed that the loss of synapses is the earliest age-related degenerative event. The present review aims to integrate classic and novel research on age-related pathologies of the inner ear. First, we summarize current knowledge on age-related strial dysfunction and synaptopathy. We describe how these cochlear pathologies fit into the categories for presbyacusis, as first defined by Schuknecht in the '70s. Further, we discuss how strial dysfunction and synaptopathy affect sound coding by the auditory nerve and how they can be experimentally induced to study their specific contributions to age-related hearing deficits. As such, we aim to give an overview of the current literature on age-related cochlear pathologies and hope to inspire further research on the role of cochlear aging in age-related hearing deficits.Copyright © 2019 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2019.02.015",pubmed,30862414,10.1016/j.heares.2019.02.015
clinical verification of a hearing aid with artificial intelligence,"In summary, it is crucial to remember that we observed differences between the standard digital hearing instruments and those built on a platform of parallel processing on measures of performance in background noise and complex listening situations. While standard instruments will use single pieces of information in trying to predict the auditory environment, Oticon Syncro uses parallel processing to analyze multiple processing options and select the best solution. The underlying processing strategy is to maximize the speech-to-noise ratio at all times and thereby optimize speech understanding.",scopus,2-s2.0-15944415798,10.1097/01.HJ.0000286116.92711.77
sensory cognitive and linguistic factors in the early academic performance of elementary school children the bentoniu project,"Standardized sensory, perceptual, linguistic, intellectual, and cognitive tests were administered to 470 children, approximately 96% of the students entering the first grade in the four elementary schools of Benton County, Indiana, over a 3-year period (1995-1997). The results of 36 tests and subtests administered to entering first graders were well described by a 4-factor solution. These factors and the tests that loaded most heavily on them were reading-related skills (phonological awareness, letter and word identification); visual cognition (visual perceptual abilities, spatial perception, visual memory); verbal cognition (language development, vocabulary, verbal concepts); and speech processing (the ability to understand speech under difficult listening conditions). A cluster analysis identified 9 groups of children, each with a different profile of scores on the 4 factors. Within these groups, the proportion of students with unsatisfactory reading achievement in the first 2 years of elementary school (as reflected in teacher-assigned grades) varied from 3% to 40%. The profiles of factor scores demonstrated the primary influence of the reading-related skills factor on reading achievement and also on other areas of academic performance. The second strongest predictor of reading and mathematics grades was the visual cognition factor, followed by the verbal cognition factor. The speech processing factor was the weakest predictor of academic achievement, accounting for less than 1% of the variance in reading achievement. This project was a collaborative effort of the Benton Community School Corporation and a multidisciplinary group of investigators from Indiana University.",cinahl,222194,10.1177/002221940303600209
selfmotion with hearing impairment and directional hearing aids,"12. Trends Hear. 2022 Jan-Dec;26:23312165221078707. doi: 10.1177/23312165221078707.Self-motion with Hearing Impairment and (Directional) Hearing Aids.Hendrikse MME(1), Eichler T(1), Hohmann V(1), Grimm G(1).Author information:(1)Auditory Signal Processing and Cluster of Excellence ""Hearing4all"", Department of Medical Physics and Acoustics, University of Oldenburg, Oldenburg, Germany.When listening to a sound source in everyday situations, typical movement behavior is highly individual and may not result in the listener directly facing the sound source. Behavioral differences can affect the performance of directional algorithms in hearing aids, as was shown in previous work by using head movement trajectories of normal-hearing (NH) listeners in acoustic simulations for noise-suppression performance predictions. However, the movement behavior of hearing-impaired (HI) listeners with or without hearing aids may differ, and hearing-aid users might adapt their self-motion to improve the performance of directional algorithms. This work investigates the influence of hearing impairment on self-motion, and the interaction of hearing aids with self-motion. In order to do this, the self-motion of three HI participant groups----aided with an adaptive differential microphone (ADM), aided without ADM, and unaided-was measured and compared to previously measured self-motion data from younger and older NH participants. Self-motion was measured in virtual audiovisual environments (VEs) in the laboratory, and the signal-to-noise ratios (SNRs) and SNR improvement of the ADM resulting from the head movements of the participants were estimated using acoustic simulations. HI participants did almost all of the movement with their head and less with their eyes compared to NH participants, which led to a 0.3 dB increase in estimated SNR and to differences in estimated SNR improvement of the ADM. However, the self-motion of the HI participants aided with ADM was similar to that of other HI participants, indicating that the ADM did not cause listeners to adapt their self-motion.DOI: 10.1177/23312165221078707PMCID: PMC8966140",pubmed,35341403,10.1177/23312165221078707
multiple effects of childhood deafness on cortical activity in children receiving bilateral cochlear implants simultaneously references,"Objective: Auditory development is disrupted without normal hearing but might proceed to some extent depending on the type and onset of deafness. We therefore hypothesized that activity in the auditory cortex would be highly variable in children who are deaf. Methods: To answer this, activity in the deaf brain was evoked by electrical pulses from newly provided bilateral cochlear implants (CIs) in 72 children (n = 144 responses). Results: Responses were categorized by visual inspection into 3 main types which were validated by principal component cluster analyses; 49% had a negative amplitude wave similar to that previously reported in pre-term infants, 26% were dominated by a positive peak typical of responses in young normal hearing children and experienced paediatric CI users, 25% were novel multi-peaked responses. No significant demographic differences, including duration and onset of deafness, were found between response types. However, children with severe biallelic mutations of GJB-2 showed predominately negative peak type responses (79%) as compared with their peers without these mutations who had a more equal distribution between cortical response types. Conclusion: Cortical development in children who are deaf is heterogeneous but can be better predicted when the genotype is known to be a GJB-2 mutation. Significance: Remediation of childhood deafness seeks to restore normal development and function of central auditory functions and thus may need to be tailored to account for effects specific to the aetiology of deafness. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc10&DO=10.1016%2fj.clinph.2010.10.037
automatic subtitle synchronization and positioning system dedicated to deaf and hearing impaired people,"In this paper, we introduce a subtitle synchronization and positioning system designed to increase the accessibility of deaf and hearing impaired people to multimedia documents. The main contributions of the paper concern: a novel synchronization algorithm able to robustly align, without any human intervention, the closed caption with the audio transcript and a timestamp refinement technique that adjusts the subtitle segments duration with respect to the audiovisual recommendations. Finally, we introduce a novel method that performs a high level understanding of the multimedia content, in order to determine the subtitle optimal positions, within the video frame, such that they do not overlap with other relevant textual information. The experimental evaluation performed on a large dataset of 30 videos taken from the French national television validates the approach with average accuracy scores superior to 90% regardless on the video genre. The subjective evaluation of the proposed subtitle synchronization and positioning system, performed with actual hearing impaired people, demonstrates the effectiveness of our approach.",ieee,2169-3536,10.1109/ACCESS.2021.3119201
largescale electrophysiology and deep learning reveal distorted neural signal dynamics after hearing loss,"65. Elife. 2023 May 10;12:e85108. doi: 10.7554/eLife.85108.Large-scale electrophysiology and deep learning reveal distorted neural signal dynamics after hearing loss.Sabesan S(1), Fragner A(2), Bench C(1), Drakopoulos F(1), Lesica NA(1).Author information:(1)Ear Institute, University College London, London, United Kingdom.(2)Perceptual Technologies, London, United Kingdom.Update of    doi: 10.1101/2022.10.04.510811.Listeners with hearing loss often struggle to understand speech in noise, even with a hearing aid. To better understand the auditory processing deficits that underlie this problem, we made large-scale brain recordings from gerbils, a common animal model for human hearing, while presenting a large database of speech and noise sounds. We first used manifold learning to identify the neural subspace in which speech is encoded and found that it is low-dimensional and that the dynamics within it are profoundly distorted by hearing loss. We then trained a deep neural network (DNN) to replicate the neural coding of speech with and without hearing loss and analyzed the underlying network dynamics. We found that hearing loss primarily impacts spectral processing, creating nonlinear distortions in cross-frequency interactions that result in a hypersensitivity to background noise that persists even after amplification with a hearing aid. Our results identify a new focus for efforts to design improved hearing aids and demonstrate the power of DNNs as a tool for the study of central brain structures.© 2023, Sabesan et al.DOI: 10.7554/eLife.85108PMCID: PMC10202456",pubmed,37162188,10.7554/eLife.85108
the derivedband envelope following response and its sensitivity to sensorineural hearing deficits,"71. Hear Res. 2020 Jul;392:107979. doi: 10.1016/j.heares.2020.107979. Epub 2020 Apr 29.The derived-band envelope following response and its sensitivity to sensorineural hearing deficits.Keshishzadeh S(1), Garrett M(2), Vasilkov V(3), Verhulst S(4).Author information:(1)Hearing Technology @ WAVES, Department of Information Technology, Ghent University, Technologiepark 126, Zwijnaarde, 9052, Belgium. Electronic address: sarineh.keshishzadeh@ugent.be.(2)Medizinische Physik and Cluster of Excellence Hearing4all, Department of Medical Physics and Acoustics, University of Oldenburg, Carl-von-Ossietzky Strasse 9-11, 26120, Oldenburg, Germany. Electronic address: markus.garrett@uni-oldenburg.de.(3)Hearing Technology @ WAVES, Department of Information Technology, Ghent University, Technologiepark 126, Zwijnaarde, 9052, Belgium. Electronic address: viacheslav.vasilkov@ugent.be.(4)Hearing Technology @ WAVES, Department of Information Technology, Ghent University, Technologiepark 126, Zwijnaarde, 9052, Belgium. Electronic address: s.verhulst@ugent.be.The envelope following response (EFR) has been proposed as a non-invasive marker of synaptopathy in animal models. However, its amplitude is affected by the spread of basilar-membrane excitation and other coexisting sensorineural hearing deficits. This study aims to (i) improve frequency specificity of the EFR by introducing a derived-band EFR (DBEFR) technique and (ii) investigate the effect of lifetime noise exposure, age and outer-hair-cell (OHC) damage on DBEFR magnitudes. Additionally, we adopt a modelling approach to validate the frequency-specificity of the DBEFR and test how different aspects of sensorineural hearing loss affect peripheral generators. The combined analysis of simulations and experimental data proposes that the DBEFRs extracted from the [2-6]-kHz frequency band is a sensitive and frequency-specific measure of synaptopathy in humans. Individual variability in DBEFR magnitudes among listeners with normal audiograms was explained by their self-reported amount of experienced lifetime noise-exposure and corresponded to amplitude variability predicted by synaptopathy. Older listeners consistently had reduced DBEFR magnitudes in comparison to young normal-hearing listeners, in correspondence to how age-induced synaptopathy affects EFRs and compromises temporal envelope encoding. To a lesser degree, OHC damage was also seen to affect the DBEFR magnitude, hence the DBEFR metric should ideally be combined with a sensitive marker of OHC damage to offer a differential diagnosis of synaptopathy in listeners with impaired audiograms.Copyright © 2020 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2020.107979",pubmed,32447097,10.1016/j.heares.2020.107979
a realtime convolutional neural network based speech enhancement for hearing impaired listeners using smartphone,This paper presents a Speech Enhancement (SE) technique based on multi-objective learning convolutional neural network to improve the overall quality of speech perceived by Hearing Aid (HA) users. The proposed method is implemented on a smartphone as an application that performs real-time SE. This arrangement works as an assistive tool to HA. A multi-objective learning architecture including primary and secondary features uses a mapping-based convolutional neural network (CNN) model to remove noise from a noisy speech spectrum. The algorithm is computationally fast and has a low processing delay which enables it to operate seamlessly on a smartphone. The steps and the detailed analysis of real-time implementation are discussed. The proposed method is compared with existing conventional and neural network-based SE techniques through speech quality and intelligibility metrics in various noisy speech conditions. The key contribution of this paper includes the realization of CNN SE model on a smartphone processor that works seamlessly with HA. The experimental results demonstrate significant improvements over the state-of-the-art techniques and reflect the usability of the developed SE application in noisy environments.,ieee,2169-3536,10.1109/ACCESS.2019.2922370
artificial neural networkassisted classification of hearing prognosis of sudden sensorineural hearing loss with vertigo,"This study aimed to determine the impact on hearing prognosis of the coherent frequency with high magnitude-squared wavelet coherence (MSWC) in video head impulse test (vHIT) among patients with sudden sensorineural hearing loss with vertigo (SSNHLV) undergoing high-dose steroid treatment. This study was a retrospective cohort study. SSNHLV patients treated at our referral center from December 2016 to December 2020 were examined. The cohort comprised 64 patients with SSNHLV undergoing high-dose steroid treatment. MSWC was measured by calculating the wavelet coherence analysis (WCA) at various frequencies from a vHIT. The hearing prognosis were analyzed using a multivariable Cox regression model and convolution neural network (CNN) of WCA. There were 64 patients with a male-to-female ratio of 1:1.67. The greater highest coherent frequency of the posterior semicircular canal (SCC) was associated with the complete recovery (CR) of hearing. After adjustment for other factors, the result remained robust (hazard ratio [HR] 2.11, 95% confidence interval [CI] 1.86-2.35). In the feature extraction with Resnet-50 and proceeding SVM in the horizontal image cropping style, the classification accuracy [STD] for (CR vs. partial + no recovery [PR + NR]), (over-sampling of CR vs. PR + NR), (extensive data extraction of CR vs. PR + NR), and (interpolation of time series of CR vs. PR + NR) were 83.6% [7.4], 92.1% [6.8], 88.9% [7.5], and 91.6% [6.4], respectively. The high coherent frequency of the posterior SCC was a significantly independent factor that was associated with good hearing prognosis in the patients who have SSNHLV. WCA may be provided with comprehensive ability in vestibulo-ocular reflex (VOR) evaluation. CNN could be utilized to classify WCA, predict treatment outcomes, and facilitate vHIT interpretation. Feature extraction in CNN with proceeding SVM and horizontal cropping style of wavelet coherence plot performed better accuracy and offered more stable model for hearing outcomes in patients with SSNHLV than pure CNN classification. Clinical and Translational Impact Statement - High coherent frequency in vHIT results in good hearing outcomes in SSNHLV and facilitates AI classification. © 2013 IEEE.",scopus,2-s2.0-85148443718,10.1109/JTEHM.2023.3242339
identification of childhood hearing impairment in uusimaa county finland,"495. Int J Pediatr Otorhinolaryngol. 1996 Jan;34(1-2):45-51. doi: 10.1016/0165-5876(95)01238-9.Identification of childhood hearing impairment in Uusimaa County, Finland.Marttila TI(1), Karikoski JO.Author information:(1)Department of Audiology, Ear, Nose and Throat Clinic Helsinki University Central Hospital, Finland.The purpose was to report the identification age of the hard-of-hearing children born between 1 January 1973 and 31 December 1990. The subjects comprised all children (353) fitted with hearing aid in an age-matched target population of 270 726 persons in Uusimaa County including Helsinki. The age of identification was studied in three groups; pure tone average (0.5, 1 and 2 kHz) > or = 30 dB, > or = 35 dB and > or = 50 dB HL enabling comparison with the identification ages reported in the literature. In the first group the median identification age was 3.6 years (mean 4.2), in the second 2.9 years (mean 3.8) and in the third 2.1 years (mean 2.8). The first group was identified significantly later than the third one (P = 0.004). The second group differed from the third significantly in detection age as well (P = 0.004). The severity of hearing impairment correlated highly with the detection age (r = -0.69. P < 0.0001). The data clustered at the ages of 1-2.5 years (hearing level 90-120 dB) and at 4-8 years (30-60 dB).DOI: 10.1016/0165-5876(95)01238-9",pubmed,8770672,10.1016/0165-5876(95)01238-9
an effectively causal deep learning algorithm to increase intelligibility in untrained noises for hearingimpaired listeners,"232. J Acoust Soc Am. 2021 Jun;149(6):3943. doi: 10.1121/10.0005089.An effectively causal deep learning algorithm to increase intelligibility in untrained noises for hearing-impaired listeners.Healy EW(1), Tan K(2), Johnson EM(1), Wang D(2).Author information:(1)Department of Speech and Hearing Science, The Ohio State University, Columbus, Ohio 43210, USA.(2)Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio 43210, USA.Real-time operation is critical for noise reduction in hearing technology. The essential requirement of real-time operation is causality-that an algorithm does not use future time-frame information and, instead, completes its operation by the end of the current time frame. This requirement is extended currently through the concept of ""effectively causal,"" in which future time-frame information within the brief delay tolerance of the human speech-perception mechanism is used. Effectively causal deep learning was used to separate speech from background noise and improve intelligibility for hearing-impaired listeners. A single-microphone, gated convolutional recurrent network was used to perform complex spectral mapping. By estimating both the real and imaginary parts of the noise-free speech, both the magnitude and phase of the estimated noise-free speech were obtained. The deep neural network was trained using a large set of noises and tested using complex noises not employed during training. Significant algorithm benefit was observed in every condition, which was largest for those with the greatest hearing loss. Allowable delays across different communication settings are reviewed and assessed. The current work demonstrates that effectively causal deep learning can significantly improve intelligibility for one of the largest populations of need in challenging conditions involving untrained background noises.DOI: 10.1121/10.0005089PMCID: PMC8186949",pubmed,34241481,10.1121/10.0005089
an unsupervised noise classification smartphone app for hearing improvement devices,This paper presents an app for running a previously developed unsupervised noise classifier in realtime on smartphone/tablet platforms. The steps taken to enable the development of this app are discussed. The app is utilized to carry out field testing of the unsupervised classification of actual encountered noise environments without any prior training and without specifying the number of noise classes or clusters. Two objective measures of cluster purity and normalized mutual information are considered to examine the performance of the app in the field with the user acting as the identifier of the ground truth classes. The results obtained indicate the effectiveness of this real-time smartphone app for carrying out the environmental noise classification in an unsupervised manner.,ieee,2473-716X,10.1109/SPMB.2017.8257031
the effect of hearing loss on agerelated differences in neural distinctiveness,"658. Neuropsychol Dev Cogn B Aging Neuropsychol Cogn. 2023 Jun 12:1-19. doi: 10.1080/13825585.2023.2223904. Online ahead of print.The effect of hearing loss on age-related differences in neural distinctiveness.Guerreiro MJS(1)(2), Puschmann S(1), Eck J(3), Rienäcker F(4), Van Gerven PWM(5), Thiel CM(1)(2).Author information:(1)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky University of Oldenburg, Oldenburg, Germany.(2)Cluster of Excellence ""Hearing4all"", Carl von Ossietzky University of Oldenburg, Oldenburg, Germany.(3)Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University, Maastricht, The Netherlands.(4)Department of Neuropsychology and Psychopharmacology, Faculty of Psychology and Neuroscience, Maastricht University, Maastricht, The Netherlands.(5)Department of Educational Development & Research, School of Health Professions Education (SHE), Faculty of Health, Medicine and Life Sciences, Maastricht University, Maastricht, The Netherlands.Age differences in cognitive performance have been shown to be overestimated if age-related hearing loss is not taken into account. Here, we investigated the role of age-related hearing loss on age differences in functional brain organization by assessing its impact on previously reported age differences in neural differentiation. To this end, we analyzed the data of 36 younger adults, 21 older adults with clinically normal hearing, and 21 older adults with mild-to-moderate hearing loss who had taken part in a functional localizer task comprising visual (i.e., faces, scenes) and auditory stimuli (i.e., voices, music) while undergoing functional magnetic resonance imaging. Evidence for reduced neural distinctiveness in the auditory cortex was observed only in older adults with hearing loss relative to younger adults, whereas evidence for reduced neural distinctiveness in the visual cortex was observed both in older adults with normal hearing and in older adults with hearing loss relative to younger adults. These results indicate that age-related dedifferentiation in the auditory cortex is exacerbated by age-related hearing loss.DOI: 10.1080/13825585.2023.2223904",pubmed,37306610,10.1080/13825585.2023.2223904
the genetic architecture of noiseinduced hearing loss evidence for a genebyenvironment interaction,"124. G3 (Bethesda). 2016 Oct 13;6(10):3219-3228. doi: 10.1534/g3.116.032516.The Genetic Architecture of Noise-Induced Hearing Loss: Evidence for a Gene-by-Environment Interaction.Lavinsky J(1), Ge M(2), Crow AL(3), Pan C(4), Wang J(2), Salehi P(2), Myint A(2), Eskin E(5), Allayee H(3), Lusis AJ(6), Friedman RA(7).Author information:(1)Tina and Rick Caruso Department of Otolaryngology, Zilkha Neurogenetic Institute Graduate Program in Surgical Sciences, Federal University of Rio Grande do Sul, Porto Alegre, Rio Grande do Sul, Brazil.(2)Tina and Rick Caruso Department of Otolaryngology, Zilkha Neurogenetic Institute.(3)Department of Preventive Medicine and Institute for Genetic Medicine, USC Keck School of Medicine, University of Southern California, Los Angeles, California 90033.(4)Department of Human Genetics.(5)Department of Computer Science.(6)Department of Microbiology, Immunology, and Molecular Genetics, University of California, Los Angeles, California 90024.(7)Tina and Rick Caruso Department of Otolaryngology, Zilkha Neurogenetic Institute rick.friedman@med.usc.edu.The discovery of environmentally specific genetic effects is crucial to the understanding of complex traits, such as susceptibility to noise-induced hearing loss (NIHL). We describe the first genome-wide association study (GWAS) for NIHL in a large and well-characterized population of inbred mouse strains, known as the Hybrid Mouse Diversity Panel (HMDP). We recorded auditory brainstem response (ABR) thresholds both pre and post 2-hr exposure to 10-kHz octave band noise at 108 dB sound pressure level in 5-6-wk-old female mice from the HMDP (4-5 mice/strain). From the observation that NIHL susceptibility varied among the strains, we performed a GWAS with correction for population structure and mapped a locus on chromosome 6 that was statistically significantly associated with two adjacent frequencies. We then used a ""genetical genomics"" approach that included the analysis of cochlear eQTLs to identify candidate genes within the GWAS QTL. In order to validate the gene-by-environment interaction, we compared the effects of the postnoise exposure locus with that from the same unexposed strains. The most significant SNP at chromosome 6 (rs37517079) was associated with noise susceptibility, but was not significant at the same frequencies in our unexposed study. These findings demonstrate that the genetic architecture of NIHL is distinct from that of unexposed hearing levels and provide strong evidence for gene-by-environment interactions in NIHL.Copyright © 2016 Lavinsky et al.DOI: 10.1534/g3.116.032516PMCID: PMC5068943",pubmed,27520957,10.1534/g3.116.032516
machine learning based dynamic band selection for splitting auditory signals to reduce inner ear hearing losses,"Quality of hearing has been severely impacted due to signal losses occurs in the human inner ear explicitly in the region of cochlea. Loudness recruitment, degraded frequency selectivity and auditory masking are the major outward effects of inner ear hearing losses. Splitting auditory signals into frequency bands and presenting dichotically to both ears became a comprehensive solution to reduce inner ear hearing losses. However, these methods divide input signal into the fix number of frequency bands, this limits their applicability where signals have large variations in their spectral characteristics. To address this challenge, we have proposed machine learning based intelligent band selection algorithm to split auditory signals dynamically. Proposed algorithm analyze input speech signal based on spectral characteristics to determine the optimum number of bands required to effectively present major acoustic cues of the signal. Further, dynamic splitting algorithm efficiently divides signal for dichotic presentation. Proposed method has been examined on large number of subjects from different age groups and gender having cochlear hearing impairment. Qualitative and quantitative assessment shown significant improvement in the recognition score with substantial reduction in the response time. © 2023 International Journal on Recent and Innovation Trends in Computing and Communication. All rights reserved.",scopus,2-s2.0-85167673345,10.17762/ijritcc.v11i6.7059
analysis of compressive properties of the bioaid hearing aid algorithm,"237. Int J Audiol. 2018 Jun;57(sup3):S130-S138. doi: 10.1080/14992027.2017.1378931. Epub 2017 Sep 25.Analysis of compressive properties of the BioAid hearing aid algorithm.Clark NR(1), Lecluyse W(2), Jürgens T(3).Author information:(1)a Mimi Hearing Technologies GmbH , Berlin , Germany.(2)b Department of Children, Young People and Education , University Campus Suffolk , Ipswich , UK , and.(3)c Medizinische Physik, Forschungszentrum Neurosensorik, and Cluster of Excellence ""Hearing4all"" , Carl-von-Ossietzky Universität Oldenburg , Oldenburg , Germany.OBJECTIVE: This technical paper describes a biologically inspired hearing aid algorithm based on a computer model of the peripheral auditory system simulating basilar membrane compression, reflexive efferent feedback and its resulting properties.DESIGN: Two evaluations were conducted on the core part of the algorithm, which is an instantaneous compression sandwiched between the attenuation and envelope extraction processes of a relatively slow feedback compressor.STUDY SAMPLE: The algorithm's input/output (I/O) function was analysed for different stationary (ambient) sound levels, and the algorithm's response to transient sinusoidal tone complexes was analysed and contrasted to that of a reference dynamic compressor.RESULTS: The algorithm's emergent properties are: (1) the I/O function adapts to the average sound level such that processing is linear for levels close to the ambient sound level and (2) onsets of transient signals are marked across time and frequency.CONCLUSION: Adaptive linearisation and onset marking, as inherent compressive features of the algorithm, provide potentially beneficial features to hearing-impaired listeners with a relatively simple circuit. The algorithm offers a new, biological perspective on hearing aid amplification.DOI: 10.1080/14992027.2017.1378931",pubmed,28942716,10.1080/14992027.2017.1378931
spatial speechinnoise performance in bimodal and singlesided deaf cochlear implant users,"46. Trends Hear. 2019 Jan-Dec;23:2331216519858311. doi: 10.1177/2331216519858311.Spatial Speech-in-Noise Performance in Bimodal and Single-Sided Deaf Cochlear Implant Users.Williges B(1), Wesarg T(2), Jung L(2), Geven LI(3), Radeloff A(3), Jürgens T(1)(4).Author information:(1)1 Medical Physics and Cluster of Excellence ""Hearing4all,"" Carl von Ossietzky University of Oldenburg, Germany.(2)2 Department of Otorhinolaryngology - Head and Neck Surgery, Faculty of Medicine, Medical Center - University of Freiburg, University of Freiburg, Germany.(3)3 Department of Otorhinolaryngology, Head and Neck Surgery, Carl von Ossietzky University of Oldenburg, Germany.(4)4 Institute of Acoustics, University of Applied Sciences Lübeck, Germany.This study compared spatial speech-in-noise performance in two cochlear implant (CI) patient groups: bimodal listeners, who use a hearing aid contralaterally to support their impaired acoustic hearing, and listeners with contralateral normal hearing, i.e., who were single-sided deaf before implantation. Using a laboratory setting that controls for head movements and that simulates spatial acoustic scenes, speech reception thresholds were measured for frontal speech-in-stationary noise from the front, the left, or the right side. Spatial release from masking (SRM) was then extracted from speech reception thresholds for monaural and binaural listening. SRM was found to be significantly lower in bimodal CI than in CI single-sided deaf listeners. Within each listener group, the SRM extracted from monaural listening did not differ from the SRM extracted from binaural listening. In contrast, a normal-hearing control group showed a significant improvement in SRM when using two ears in comparison to one. Neither CI group showed a binaural summation effect; that is, their performance was not improved by using two devices instead of the best monaural device in each spatial scenario. The results confirm a ""listening with the better ear"" strategy in the two CI patient groups, where patients benefited from using two ears/devices instead of one by selectively attending to the better one. Which one is the better ear, however, depends on the spatial scenario and on the individual configuration of hearing loss.DOI: 10.1177/2331216519858311PMCID: PMC6669847",pubmed,31364496,10.1177/2331216519858311
conductive hearing loss estimated from wideband acoustic immittance measurements in ears with otitis media with effusion,"613. Ear Hear. 2023 Jul-Aug 01;44(4):721-731. doi: 10.1097/AUD.0000000000001317. Epub 2022 Dec 29.Conductive Hearing Loss Estimated From Wideband Acoustic Immittance Measurements in Ears With Otitis Media With Effusion.Merchant GR(1), Neely ST.Author information:(1)Center for Hearing Research, Boys Town National Research Hospital, Omaha, Nebraska, USA.OBJECTIVES: Previous work has shown that wideband acoustic immittance (WAI) is sensitive to the volume of effusion present in ears with otitis media with effusion (OME). Prior work also demonstrates that the volume of the effusion appears to drive, or at least play a significant role in, how much conductive hearing loss (CHL) a child has due to a given episode of OME. Given this association, the goal of this work was to determine how well CHL could be estimated directly from WAI in ears with OME.DESIGN: Sixty-three ears from a previously published study on OME (ages 9 months to 11 years, 2 months) were grouped based on effusion volume (full, partial, or clear) determined during tympanostomy tube placement surgery and compared with age-matched normal control ears. Audiometric thresholds were obtained for a subset of the 34 ears distributed across the four groups. An electrical-analog model of ear-canal acoustics and middle-ear mechanics was fit to the measured WAI from individual ears. Initial estimates of CHL were derived from either (1) average absorbance or (2) the model component thought to represent damping in the ossicular chain.RESULTS: The analog model produced good fits for all effusion-volume groups. The two initial CHL estimates were both well correlated (87% and 81%) with the pure-tone average hearing thresholds used to define the CHL. However, in roughly a third of the ears (11/34), the estimate based on damping was too large by nearly a factor of two. This observation motivated improved CHL estimates.CONCLUSIONS: Our CHL estimation method can estimate behavioral audiometric thresholds (CHL) within a margin of error that is small enough to be clinically meaningful. The importance of this finding is increased by the challenges associated with behavioral audiometric testing in pediatric populations, where OME is the most common. In addition, the discovery of two clusters in the damping-related CHL estimate suggests the possible existence of two distinctly different types of ears: pressure detectors and power detectors.Copyright © 2022 Wolters Kluwer Health, Inc. All rights reserved.DOI: 10.1097/AUD.0000000000001317PMCID: PMC10271999",pubmed,36607739,10.1097/AUD.0000000000001317
a novel speechprocessing strategy incorporating tonal information for cochlear implants,"456. IEEE Trans Biomed Eng. 2004 May;51(5):752-60. doi: 10.1109/TBME.2004.826597.A novel speech-processing strategy incorporating tonal information for cochlear implants.Lan N(1), Nie KB, Gao SK, Zeng FG.Author information:(1)Department of Biokinesiology and Physical Therapy, University of Soutern California, CHP-155, Los Angeles, CA 90089, USA. ninglan@usc.eduGood performance in cochlear implant users depends in large part on the ability of a speech processor to effectively decompose speech signals into multiple channels of narrow-band electrical pulses for stimulation of the auditory nerve. Speech processors that extract only envelopes of the narrow-band signals (e.g., the continuous interleaved sampling (CIS) processor) may not provide sufficient information to encode the tonal cues in languages such as Chinese. To improve the performance in cochlear implant users who speak tonal language, we proposed and developed a novel speech-processing strategy, which extracted both the envelopes of the narrow-band signals and the fundamental frequency (F0) of the speech signal, and used them to modulate both the amplitude and the frequency of the electrical pulses delivered to stimulation electrodes. We developed an algorithm to extract the fundatmental frequency and identified the general patterns of pitch variations of four typical tones in Chinese speech. The effectiveness of the extraction algorithm was verified with an artificial neural network that recognized the tonal patterns from the extracted F0 information. We then compared the novel strategy with the envelope-extraction CIS strategy in human subjects with normal hearing. The novel strategy produced significant improvement in perception of Chinese tones, phrases, and sentences. This novel processor with dynamic modulation of both frequency and amplitude is encouraging for the design of a cochlear implant device for sensorineurally deaf patients who speak tonal languages.DOI: 10.1109/TBME.2004.826597",pubmed,15132501,10.1109/TBME.2004.826597
predicting acoustic hearing preservation following cochlear implant surgery using machine learning,"151. Laryngoscope. 2024 Feb;134(2):926-936. doi: 10.1002/lary.30894. Epub 2023 Jul 14.Predicting Acoustic Hearing Preservation Following Cochlear Implant Surgery Using Machine Learning.Zeitler DM(1)(2), Buchlak QD(3)(4), Ramasundara S(3), Farrokhi F(1)(5), Esmaili N(3)(6).Author information:(1)Neuroscience Institute, Virginia Mason Franciscan Health, Seattle, Washington, USA.(2)Department of Otolaryngology-Head Neck Surgery, Section of Otology/Neurotology, Virginia Mason Franciscan Health, Seattle, Washington, USA.(3)School of Medicine, University of Notre Dame Australia, Sydney, New South Wales, Australia.(4)Department of Neurosurgery, Monash Health, Melbourne, Victoria, Australia.(5)Department of Neurosurgery, Virginia Mason Franciscan Health, Seattle, Washington, USA.(6)Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, New South Wales, Australia.OBJECTIVES: The aim of the study was to train and test supervised machine-learning classifiers to predict acoustic hearing preservation after CI using preoperative clinical data.STUDY DESIGN: Retrospective predictive modeling study of prospectively collected single-institution CI dataset.METHODS: One hundred and seventy-five patients from a REDCap database including 761 patients >18 years who underwent CI and had audiometric testing preoperatively and one month after surgery were included. The primary outcome variable was the lowest quartile change in acoustic hearing at one month after CI using various formulae (standard pure tone average, SPTA; low-frequency PTA, LFPTA). Analysis involved applying multivariate logistic regression to detect statistical associations and training and testing supervised learning classifiers. Classifier performance was assessed with numerous metrics including area under the receiver operating characteristic curve (AUC) and Matthews correlation coefficient (MCC).RESULTS: Lowest quartile change (indicating hearing preservation) in SPTA was positively associated with a history of meningitis, preoperative LFPTA, and preoperative SPTA. Lowest quartile change in SPTA was negatively associated with sudden hearing loss, noise exposure, aural fullness, and abnormal anatomy. Lowest quartile change in LFPTA was positively associated with preoperative LFPTA. Lowest quartile change in LFPTA was negatively associated with tobacco use. Random forest demonstrated the highest mean classification performance on the validation dataset when predicting each of the outcome variables.CONCLUSIONS: Machine learning demonstrated utility for predicting preservation of residual acoustic hearing in patients undergoing CI surgery, and the detected associations facilitated the interpretation of our machine-learning models. The models and statistical associations together may be used to facilitate improvements in shared clinical decision-making and patient outcomes.LEVEL OF EVIDENCE: 3 Laryngoscope, 134:926-936, 2024.© 2023 The American Laryngological, Rhinological and Otological Society, Inc.DOI: 10.1002/lary.30894",pubmed,37449725,10.1002/lary.30894
noise trauma induced plastic changes in brain regions outside the classical auditory pathway,"132. Neuroscience. 2016 Feb 19;315:228-45. doi: 10.1016/j.neuroscience.2015.12.005. Epub 2015 Dec 14.Noise trauma induced plastic changes in brain regions outside the classical auditory pathway.Chen GD(1), Sheppard A(2), Salvi R(2).Author information:(1)Center for Hearing and Deafness, SUNY at Buffalo, Buffalo, NY 14214, USA. Electronic address: gchen7@buffalo.edu.(2)Center for Hearing and Deafness, SUNY at Buffalo, Buffalo, NY 14214, USA.The effects of intense noise exposure on the classical auditory pathway have been extensively investigated; however, little is known about the effects of noise-induced hearing loss on non-classical auditory areas in the brain such as the lateral amygdala (LA) and striatum (Str). To address this issue, we compared the noise-induced changes in spontaneous and tone-evoked responses from multiunit clusters (MUC) in the LA and Str with those seen in auditory cortex (AC) in rats. High-frequency octave band noise (10-20 kHz) and narrow band noise (16-20 kHz) induced permanent threshold shifts at high-frequencies within and above the noise band but not at low frequencies. While the noise trauma significantly elevated spontaneous discharge rate (SR) in the AC, SRs in the LA and Str were only slightly increased across all frequencies. The high-frequency noise trauma affected tone-evoked firing rates in frequency and time-dependent manner and the changes appeared to be related to the severity of noise trauma. In the LA, tone-evoked firing rates were reduced at the high-frequencies (trauma area) whereas firing rates were enhanced at the low-frequencies or at the edge-frequency dependent on severity of hearing loss at the high frequencies. The firing rate temporal profile changed from a broad plateau to one sharp, delayed peak. In the AC, tone-evoked firing rates were depressed at high frequencies and enhanced at the low frequencies while the firing rate temporal profiles became substantially broader. In contrast, firing rates in the Str were generally decreased and firing rate temporal profiles become more phasic and less prolonged. The altered firing rate and pattern at low frequencies induced by high-frequency hearing loss could have perceptual consequences. The tone-evoked hyperactivity in low-frequency MUC could manifest as hyperacusis whereas the discharge pattern changes could affect temporal resolution and integration.Copyright © 2015 IBRO. Published by Elsevier Ltd. All rights reserved.DOI: 10.1016/j.neuroscience.2015.12.005PMCID: PMC5327920",pubmed,26701290,10.1016/j.neuroscience.2015.12.005
prediction of hearing prognosis of large vestibular aqueduct syndrome based on the pytorch deep learning model,"In order to compare magnetic resonance imaging (MRI) findings of patients with large vestibular aqueduct syndrome (LVAS) in the stable hearing loss (HL) group and the fluctuating HL group, this paper provides reference for clinicians' early intervention. From January 2001 to January 2016, patients with hearing impairment diagnosed as LVAS in infancy in the Department of Otorhinolaryngology, Head and Neck Surgery, Children's Hospital of Fudan University were collected and divided into the stable HL group (n = 29) and the fluctuating HL group (n = 30). MRI images at initial diagnosis were collected, and various deep learning neural network training models were established based on PyTorch to classify and predict the two series. Vgg16_bn, vgg19_bn, and ResNet18, convolutional neural networks (CNNs) with fewer layers, had favorable effects for model building, with accs of 0.9, 0.8, and 0.85, respectively. ResNet50, a CNN with multiple layers and an acc of 0.54, had relatively poor effects. The GoogLeNet-trained model performed best, with an acc of 0.98. We conclude that deep learning-based radiomics can assist doctors in accurately predicting LVAS patients to classify them into either fluctuating or stable HL types and adopt differentiated treatment methods.  © 2022 Bo Duan et al.",scopus,2-s2.0-85128802331,10.1155/2022/4814577
agerelated hearing loss aquaporin 4 gene expression changes in the mouse cochlea and auditory midbrain references,"Presbycusis-age-related hearing loss, is the number one communication disorder, and one of the top three chronic medical conditions of our aged population. Aquaporins, particularly aquaporin 4 (Aqp4), are membrane proteins with important roles in water and ion flux across cell membranes, including cells of the inner ear and pathways of the brain used for hearing. To more fully understand the biological bases of presbycusis, 39 CBA mice, a well studied animal model of presbycusis, underwent non-invasive hearing testing as a function of sound frequency (auditory brainstem response-ABR thresholds, and distortion-product otoacoustic emission-DPOAE magnitudes), and were clustered into four groups based on age and hearing ability. Aqp4 gene expression, as determined by genechip microarray analysis and quantitative real-time PCR, was compared to the young adult control group in the three older groups: middle aged with good hearing, old age with mild presbycusis, and old age with severe presbycusis. Linear regression and ANOVA showed statistically significant changes in Aqp4 gene expression and ABR and DPOAE hearing status in the cochlea and auditory midbrain-inferior colliculus. Down-regulation in the cochlea was seen, and an initial down-, then up-regulation was discovered for the inferior colliculus Aqp4 expression. It is theorized that these changes in Aqp4 gene expression represent an age-related disruption of ion flux in the fluids of the cochlea that are responsible for ionic gradients underlying sound transduction in cochlear hair cells necessary for hearing. In regard to central auditory processing at the level of the auditory midbrain, aquaporin gene expression changes may affect neurotransmitter cycling involving supporting cells, thus impairing complex sound neural processing with age. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc8&DO=10.1016%2fj.brainres.2008.11.070
vestibular mapping in ramsayhunt syndrome and idiopathic sudden sensorineural hearing loss,"Purpose: To observe vestibular impairment patterns in patients with Ramsay Hunt syndrome with dizziness (RHS_D) and sudden sensorineural hearing loss with dizziness (SSNHL_D) using hierarchical cluster analysis (HCA) to interpret results with possible mechanisms. Methods: The data of 30 RHS_D and 81 SSNHL_D patients from January 2017 to August 2022 in a single tertiary referral center were retrospectively analyzed. Video head impulse test (vHIT) and vestibular evoked myogenic potential (VEMP) were used for vestibular analysis of peripheral vestibular organs, and the results of vHIT and VEMP were analyzed. HCA was used to analyze vestibular impairment patterns. Results: In RHS_D patients, the lateral semicircular canal (LSCC) was the most impaired semicircular canal (SCC), followed by the anterior semicircular canal (ASCC) and the posterior semicircular canal (PSCC), and the utricle was more impaired than the saccule. In SSNHL_D patients, the PSCC was the most impaired SCC, followed by the LSCC and the ASCC, and the utricle was more impaired than the saccule. In HCA of RHS_D patients, the ASCC and utricle were initially clustered, followed by the LSCC, PSCC and saccule in order. In the HCA of SSNHL_D patients, the PSCC was solely merged and independently clustered. Conclusion: There were different patterns of vestibular impairments between RHS_D and SSNHL_D patients. The vestibular analysis and HCA results of SSNHL_D showed tendency of skip lesion, which could be explained by vascular pathophysiology. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",scopus,2-s2.0-85160221678,10.1007/s00405-023-08029-2
use of noninvasive measures to predict cochlear synapse counts,"198. Hear Res. 2018 Dec;370:113-119. doi: 10.1016/j.heares.2018.10.006. Epub 2018 Oct 13.Use of non-invasive measures to predict cochlear synapse counts.Bramhall NF(1), McMillan GP(2), Kujawa SG(3), Konrad-Martin D(2).Author information:(1)VA RR&D National Center for Rehabilitative Auditory Research (NCRAR), VA Portland Health Care System, Portland, OR, 97239, USA; Department of Otolaryngology/Head & Neck Surgery, Oregon Health & Science University, Portland, OR, 97239, USA. Electronic address: naomi.bramhall@va.gov.(2)VA RR&D National Center for Rehabilitative Auditory Research (NCRAR), VA Portland Health Care System, Portland, OR, 97239, USA; Department of Otolaryngology/Head & Neck Surgery, Oregon Health & Science University, Portland, OR, 97239, USA.(3)Eaton-Peabody Laboratories, Massachusetts Eye & Ear Infirmary, Boston, MA, 02114, USA; Department of Otolaryngology, Harvard Medical School, Boston, MA, 02115, USA.Cochlear synaptopathy, the loss of synaptic connections between inner hair cells (IHCs) and auditory nerve fibers, has been documented in animal models of aging, noise, and ototoxic drug exposure, three common causes of acquired sensorineural hearing loss in humans. In each of these models, synaptopathy begins prior to changes in threshold sensitivity or loss of hair cells; thus, this underlying injury can be hidden behind a normal threshold audiogram. Since cochlear synaptic loss cannot be directly confirmed in living humans, non-invasive assays will be required for diagnosis. In animals with normal auditory thresholds, the amplitude of wave 1 of the auditory brainstem response (ABR) is highly correlated with synapse counts. However, synaptopathy can also co-occur with threshold elevation, complicating the use of the ABR alone as a diagnostic measure. Using an age-graded series of mice and a partial least squares regression approach to model structure-function relationships, this study shows that the combination of a small number of ABR and distortion product otoacoustic emission (DPOAE) measurements can predict synaptic ribbon counts at various cochlear frequencies to within 1-2 synapses per IHC of their true value. In contrast, the model, trained using the age-graded series of mice, overpredicted synapse counts in a small sample of young noise-exposed mice, perhaps due to differences in the underlying pattern of damage between aging and noise-exposed mice. These results provide partial validation of a noninvasive approach to identify synaptic/neuronal loss in humans using ABRs and DPOAEs.Published by Elsevier B.V.DOI: 10.1016/j.heares.2018.10.006PMCID: PMC7161203",pubmed,30366194,10.1016/j.heares.2018.10.006
apps filter out background noise to improve hearing,,cinahl,10859586,
a multidimensional characterization of the neurocognitive architecture underlying agerelated temporal speech processing references,"Healthy aging is often associated with speech comprehension difficulties in everyday life situations despite a pure-tone hearing threshold in the normative range. Drawing on this background, we used a multidimensional approach to assess the functional and structural neural correlates underlying age-related temporal speech processing while controlling for pure-tone hearing acuity. Accordingly, we combined structural magnetic resonance imaging and electroencephalography, and collected behavioral data while younger and older adults completed a phonetic categorization and discrimination task with consonant-vowel syllables varying along a voice-onset time continuum. The behavioral results confirmed age-related temporal speech processing singularities which were reflected in a shift of the boundary of the psychometric categorization function, with older adults perceiving more syllable characterized by a short voice-onset time as /ta/ compared to younger adults. Furthermore, despite the absence of any between-group differences in phonetic discrimination abilities, older adults demonstrated longer N100/P200 latencies as well as increased P200 amplitudes while processing the consonant-vowel syllables varying in voice-onset time. Finally, older adults also exhibited a divergent anatomical gray matter infrastructure in bilateral auditory-related and frontal brain regions, as manifested in reduced cortical thickness and surface area. Notably, in the younger adults but not in the older adult cohort, cortical surface area in these two gross anatomical clusters correlated with the categorization of consonant-vowel syllables characterized by a short voice-onset time, suggesting the existence of a critical gray matter threshold that is crucial for consistent mapping of phonetic categories varying along the temporal dimension. Taken together, our results highlight the multifaceted dimensions of age-related temporal speech processing characteristics, and pave the way toward a better understanding of the relationships between hearing, speech and the brain in older age. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc22&DO=10.1016%2fj.neuroimage.2023.120285
exploring differences in speech processing among older hearingimpaired listeners with or without hearing aid experience eyetracking and fmri measurements,"666. Front Neurosci. 2019 May 3;13:420. doi: 10.3389/fnins.2019.00420. eCollection 2019.Exploring Differences in Speech Processing Among Older Hearing-Impaired Listeners With or Without Hearing Aid Experience: Eye-Tracking and fMRI Measurements.Habicht J(1), Behler O(1), Kollmeier B(1), Neher T(2).Author information:(1)Medizinische Physik and Cluster of Excellence ""Hearing4all"", Oldenburg University, Oldenburg, Germany.(2)Institute of Clinical Research, University of Southern Denmark, Odense, Denmark.Recently, evidence has been accumulating that untreated hearing loss can lead to neurophysiological changes that affect speech processing abilities in noise. To shed more light on how aiding may impact these effects, this study explored the influence of hearing aid (HA) experience on the cognitive processes underlying speech comprehension. Eye-tracking and functional magnetic resonance imaging (fMRI) measurements were carried out with acoustic sentence-in-noise (SiN) stimuli complemented by pairs of pictures that either correctly (target picture) or incorrectly (competitor picture) depicted the sentence meanings. For the eye-tracking measurements, the time taken by the participants to start fixating the target picture (the 'processing time') was measured. For the fMRI measurements, brain activation inferred from blood-oxygen-level dependent responses following sentence comprehension was measured. A noise-only condition was also included. Groups of older hearing-impaired individuals matched in terms of age, hearing loss, and working memory capacity with (eHA; N = 13) or without (iHA; N = 14) HA experience participated. All acoustic stimuli were presented via earphones with individual linear amplification to ensure audibility. Consistent with previous findings, the iHA group had significantly longer (poorer) processing times than the eHA group, despite no differences in speech recognition performance. Concerning the fMRI measurements, there were indications of less brain activation in some right frontal areas for SiN relative to noise-only stimuli in the eHA group compared to the iHA group. Together, these results suggest that HA experience leads to faster speech-in-noise processing, possibly related to less recruitment of brain regions outside the core sentence-comprehension network. Follow-up research is needed to substantiate the findings related to changes in cortical speech processing with HA use.DOI: 10.3389/fnins.2019.00420PMCID: PMC6509414",pubmed,31130836,10.3389/fnins.2019.00420
audiogene computerbased prediction of genetic factors involved in nonsyndromic hearing impairment,"AudioGene is a software system developed at the University of Iowa to classify and predict gene mutations that indicate causal or increased risk factors of disease. We focus on a concise example — the most likely genetic causes of a particular form of inherited hearing loss — ADNSHL. Whereas the cost and throughput involved in the collection of genomic data have advanced dramatically during the past decade, gathering and interpreting clinical information regarding disease diagnosis remains slow, costly and error-prone. AudioGene employs machine-learning techniques in an iterative procedure to prioritize probable genetic risk factors of disease, which are then verified with a molecular (wet lab) assay. In our current implementation AudioGene achieves 67% first-choice accuracy (versus 23% using a majority classifier). When the top three choices are considered, accuracy increases to 83%. This has numerous implications for reducing the cost of genetic screening as well as increasing the power of novel gene discovery efforts. While AudioGene is focused on hearing loss, the design and underlying mechanisms are generalizable to many other diseases including heart disease, cancer and mental illness.",ieee,2161-5330,10.1109/AICCSA.2011.6126605
multivariate dpoae metrics for identifying changes in hearing perspectives from ototoxicity monitoring references,"Distortion-product otoacoustic emissions (DPOAEs) provide a window into real-time cochlear mechanical function. Yet, relationships between the changes in DPOAE metrics and auditory sensitivity are still poorly understood. Explicating these relationships might support the use of DPOAEs in hearing conservation programs (HCPs) for detecting early damage leading to noise-induced hearing loss (NIHL) so that mitigating steps might be taken to limit any lasting damage. This report describes the development of DPOAE-based statistical models to assess the risk of hearing loss from cisplatin treatment among cancer patients. Ototoxicity risk assessment (ORA) models were constructed using a machine learning paradigm in which partial least squares and leave-one-out cross-validation were applied, yielding optimal screening algorithms from a set of known risk factors for ototoxicity and DPOAE changes from pre-exposure baseline measures. Single DPOAE metrics alone were poorer indicators of the risk of ototoxic hearing shifts than the best performing multivariate models. This finding suggests that multivariate approaches applied to the use of DPOAEs in a HCP, will improve the ability of DPOAE measures to identify ears with noise-induced mechanical damage and/or hearing loss at each monitoring interval. This prediction must be empirically assessed in noise-exposed subjects. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc11&DO=10.3109%2f14992027.2011.635713
individual differences reveal correlates of hidden hearing deficits,"305. J Neurosci. 2015 Feb 4;35(5):2161-72. doi: 10.1523/JNEUROSCI.3915-14.2015.Individual differences reveal correlates of hidden hearing deficits.Bharadwaj HM(1), Masud S(2), Mehraei G(3), Verhulst S(4), Shinn-Cunningham BG(2).Author information:(1)Center for Computational Neuroscience and Neural Technology and Department of Biomedical Engineering, Boston University, Boston, Massachusetts 02215, hari@nmr.mgh.harvard.edu.(2)Center for Computational Neuroscience and Neural Technology and Department of Biomedical Engineering, Boston University, Boston, Massachusetts 02215.(3)Center for Computational Neuroscience and Neural Technology and Program in Speech and Hearing Biosciences and Technology, Harvard University-Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, and.(4)Center for Computational Neuroscience and Neural Technology and Cluster of Excellence Hearing4all and Medizinische Physik, Department of Medical Physics and Acoustics, Oldenburg University, D-26111 Oldenburg, Germany.Clinical audiometry has long focused on determining the detection thresholds for pure tones, which depend on intact cochlear mechanics and hair cell function. Yet many listeners with normal hearing thresholds complain of communication difficulties, and the causes for such problems are not well understood. Here, we explore whether normal-hearing listeners exhibit such suprathreshold deficits, affecting the fidelity with which subcortical areas encode the temporal structure of clearly audible sound. Using an array of measures, we evaluated a cohort of young adults with thresholds in the normal range to assess both cochlear mechanical function and temporal coding of suprathreshold sounds. Listeners differed widely in both electrophysiological and behavioral measures of temporal coding fidelity. These measures correlated significantly with each other. Conversely, these differences were unrelated to the modest variation in otoacoustic emissions, cochlear tuning, or the residual differences in hearing threshold present in our cohort. Electroencephalography revealed that listeners with poor subcortical encoding had poor cortical sensitivity to changes in interaural time differences, which are critical for localizing sound sources and analyzing complex scenes. These listeners also performed poorly when asked to direct selective attention to one of two competing speech streams, a task that mimics the challenges of many everyday listening environments. Together with previous animal and computational models, our results suggest that hidden hearing deficits, likely originating at the level of the cochlear nerve, are part of ""normal hearing.""Copyright © 2015 the authors 0270-6474/15/352161-12$15.00/0.DOI: 10.1523/JNEUROSCI.3915-14.2015PMCID: PMC4402332",pubmed,25653371,10.1523/JNEUROSCI.3915-14.2015
objectification of intracochlear electrocochleography using machine learning,"Introduction: Electrocochleography (ECochG) measures inner ear potentials in response to acoustic stimulation. In patients with cochlear implant (CI), the technique is increasingly used to monitor residual inner ear function. So far, when analyzing ECochG potentials, the visual assessment has been the gold standard. However, visual assessment requires a high level of experience to interpret the signals. Furthermore, expert-dependent assessment leads to inconsistency and a lack of reproducibility. The aim of this study was to automate and objectify the analysis of cochlear microphonic (CM) signals in ECochG recordings. Methods: Prospective cohort study including 41 implanted ears with residual hearing. We measured ECochG potentials at four different electrodes and only at stable electrode positions (after full insertion or postoperatively). When stimulating acoustically, depending on the individual residual hearing, we used three different intensity levels of pure tones (i.e., supra-, near-, and sub-threshold stimulation; 250–2,000 Hz). Our aim was to obtain ECochG potentials with differing SNRs. To objectify the detection of CM signals, we compared three different methods: correlation analysis, Hotelling's T2 test, and deep learning. We benchmarked these methods against the visual analysis of three ECochG experts. Results: For the visual analysis of ECochG recordings, the Fleiss' kappa value demonstrated a substantial to almost perfect agreement among the three examiners. We used the labels as ground truth to train our objectification methods. Thereby, the deep learning algorithm performed best (area under curve = 0.97, accuracy = 0.92), closely followed by Hotelling's T2 test. The correlation method slightly underperformed due to its susceptibility to noise interference. Conclusions: Objectification of ECochG signals is possible with the presented methods. Deep learning and Hotelling's T2 methods achieved excellent discrimination performance. Objective automatic analysis of CM signals enables standardized, fast, accurate, and examiner-independent evaluation of ECochG measurements. Copyright © 2022 Schuerch, Wimmer, Dalbert, Rummel, Caversaccio, Mantokoudis and Weder.",scopus,2-s2.0-85138304832,10.3389/fneur.2022.943816
what keeps older adults with hearing impairment from adopting hearing aids,"504. Trends Hear. 2018 Jan-Dec;22:2331216518809737. doi: 10.1177/2331216518809737.What Keeps Older Adults With Hearing Impairment From Adopting Hearing Aids?Tahden MAS(1)(2), Gieseler A(1)(2), Meis M(1)(3)(4), Wagener KC(1)(3)(4), Colonius H(1)(2).Author information:(1)1 Cluster of Excellence 'Hearing4all', University of Oldenburg, Germany.(2)2 Cognitive Psychology Lab, Department of Psychology, University of Oldenburg, Germany.(3)3 Hörzentrum Oldenburg GmbH, Germany.(4)4 HörTech gGmbH, Oldenburg, Germany.The aim of this study was to compare elderly individuals who are hearing impaired but inexperienced in using hearing aids (hearing aid non-users; HA-NU) with their aided counterparts (hearing aid users; HA-U) across various auditory and non-auditory measures in order to identify differences that might be associated with the low hearing aid uptake rate. We have drawn data of 72 HA-NU and 139 HA-U with a mild-to-moderate hearing loss, and matched these two groups on the degree of hearing impairment, age, and sex. First, HA-NU and HA-U were compared across 65 auditory, cognitive, health-specific, and socioeconomic test measures as well as measures assessing technology commitment. Second, a logistic regression approach was performed to identify relevant predictors for using hearing aids. Finally, we conducted a sensitivity analysis for the matching approach. Group comparisons indicated that HA-NU perceive their hearing problem as less severe than their aided counterparts. Furthermore, HA-NU showed worse technology commitment and lower socioeconomic status than HA-U. The logistic regression revealed self-reported hearing performance, technology commitment, and the socioeconomic and health status as the most important predictors for using hearing aids.DOI: 10.1177/2331216518809737PMCID: PMC6243636",pubmed,30451099,10.1177/2331216518809737
plasticity of auditory cortex associated with sensorineural hearing loss in adult c57bl6j mice,"345. J Comp Neurol. 1993 Mar 15;329(3):402-11. doi: 10.1002/cne.903290310.Plasticity of auditory cortex associated with sensorineural hearing loss in adult C57BL/6J mice.Willott JF(1), Aitkin LM, McFadden SL.Author information:(1)Department of Psychology, Northern Illinois University, DeKalb 60115.The representation of frequency was mapped in the primary auditory cortex (AI) of C57BL/6J (C57) mice during young adulthood (1.5-2 months) when hearing is optimal, and at 3, 6, and 12 months of age, a period during which progressive, high frequency, sensorineural hearing loss occurs in this strain. Maps were also obtained from CBA/CaJ mice which retain good hearing as they age. In AI of young adult C57 mice and CBA mice, characteristic frequencies (CFs) of multiple-unit clusters were easily identified with extracellular recordings, and a general tonotopic organization was observed from dorsal (high frequency) to ventral and caudal (low frequency). In individual cases there appeared to be deviations from the above tonotopic organization, despite the fact that inbred mice are genetically invariant. As progressive loss of high frequency sensitivity ensued peripherally, a substantially increased representation of middle frequencies was observed in AI. There was no apparent change in the surface area of the auditory cortex despite the elimination of high frequencies, and virtually the entire auditory cortex became devoted to the middle frequencies (especially 10-13 kHz) for which sensitivity remained high. Similar age-related changes were not observed in normal-hearing CBA mice. These findings indicate that plasticity in the representation of frequency in AI is associated with high frequency hearing loss in C57 mice.DOI: 10.1002/cne.903290310",pubmed,8459051,10.1002/cne.903290310
immunolabeling and counting ribbon synapses in young adult and aged gerbil cochleae,"30. J Vis Exp. 2022 Apr 21;(182). doi: 10.3791/63874.Immunolabeling and Counting Ribbon Synapses in Young Adult and Aged Gerbil Cochleae.Steenken F(1), Bovee S(1), Köppl C(2).Author information:(1)Cluster of Excellence ""Hearing4all"" and Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg.(2)Cluster of Excellence ""Hearing4all"" and Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg; christine.koeppl@uol.de.The loss of ribbon synapses connecting inner hair cells and afferent auditory nerve fibers is assumed to be one cause of age-related hearing loss. The most common method for detecting the loss of ribbon synapses is immunolabeling because it allows for quantitative sampling from several tonotopic locations in an individual cochlea. However, the structures of interest are buried deep inside the bony cochlea. Gerbils are used as an animal model for age-related hearing loss. Here, routine protocols for fixation, immunolabeling gerbil cochlear whole mounts, confocal imaging, and quantifying ribbon synapse numbers and volumes are described. Furthermore, the particular challenges associated with obtaining good material from valuable aging individuals are highlighted. Gerbils are euthanized and either perfused cardiovascularly, or their tympanic bullae are carefully dissected out of the skull. The cochleae are opened at the apex and base and directly transferred to the fixative. Irrespective of the initial method, the cochleae are postfixed and subsequently decalcified. The tissue is then labeled with primary antibodies against pre- and postsynaptic structures and hair cells. Next, the cochleae are incubated with secondary fluorescence-tagged antibodies that are specific against their respective primary ones. The cochleae of aged gerbils are then treated with an autofluorescence quencher to reduce the typically substantial background fluorescence of older animals' tissues. Finally, cochleae are dissected into 6-11 segments. The entire cochlear length is reconstructed such that specific cochlear locations can be reliably determined between individuals. Confocal image stacks, acquired sequentially, help visualize hair cells and synapses at the chosen locations. The confocal stacks are deconvolved, and the synapses are either counted manually using ImageJ, or more extensive quantification of synaptic structures is carried out with image analysis procedures custom-written in Matlab.DOI: 10.3791/63874",pubmed,35532259,10.3791/63874
individual classification of single trial eeg traces to discriminate brain responses to speech with different signaltonoise ratios,"390. Annu Int Conf IEEE Eng Med Biol Soc. 2018 Jul;2018:987-990. doi: 10.1109/EMBC.2018.8512491.Individual Classification of Single Trial EEG Traces to Discriminate Brain responses to Speech with Different Signal-to-Noise Ratios.Cabrera AF, Petersen EB, Graversen C, Sorensen AT, Lunner T, Rank ML.To gain knowledge of listening effort in adverse situations, it is important to know how the brain processes speech with different signal-to-noise ratios (SNR). To investigate this, we conducted a study with 33 hearing impaired individuals, whose electroencephalographic (EEG) signals were recorded while listening to sentences presented in high and low levels of background noise. To discriminate between these two conditions, features from the 64-channel EEG recordings were extracted using the power spectrum obtained by a Fast Fourier Transform. Features vectors were selected on an individual basis by using the statistical R2 approach. The selected features were then classified by a Support Vector Machine with a nonlinear kernel, and the classification results were validated using a leave-one-out strategy, and presented an average classification accuracy over all 33 subjects of 83% (SD=6.4%). The most discriminative features were selected in the high-beta (19-30 Hz) and gamma (30-45 Hz) bands. These results suggest that specific brain oscillations are involved in addressing background noise during speech stimuli, which may reflect differences in cognitive load between the conditions of low and high background noise.DOI: 10.1109/EMBC.2018.8512491",pubmed,30440556,10.1109/EMBC.2018.8512491
efficient twomicrophone speech enhancement using basic recurrent neural network cell for hearing and hearing aids,"184. J Acoust Soc Am. 2020 Jul;148(1):389. doi: 10.1121/10.0001600.Efficient two-microphone speech enhancement using basic recurrent neural network cell for hearing and hearing aids.Shankar N(1), Bhat GS(1), Panahi IMS(1).Author information:(1)Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, Texas 75080, USA.This work presents a two-microphone speech enhancement (SE) framework based on basic recurrent neural network (RNN) cell. The proposed method operates in real-time, improving the speech quality and intelligibility in noisy environments. The RNN model trained using a simple feature set-real and imaginary parts of the short-time Fourier transform (STFT) are computationally efficient with a minimal input-output processing delay. The proposed algorithm can be used in any stand-alone platform such as a smartphone using its two inbuilt microphones. The detailed operation of the real-time implementation on the smartphone is presented. The developed application works as an assistive tool for hearing aid devices (HADs). Speech quality and intelligibility test results are used to compare the proposed algorithm to existing conventional and neural network-based SE methods. Subjective and objective scores show the superior performance of the developed method over several conventional methods in different noise conditions and low signal to noise ratios (SNRs).DOI: 10.1121/10.0001600PMCID: PMC7928060",pubmed,32752751,10.1121/10.0001600
voxelwise analysis of diffusion tensor imaging for clinical outcome of cochlear implantation retrospective study,"707. Clin Exp Otorhinolaryngol. 2012 Apr;5 Suppl 1(Suppl 1):S37-42. doi: 10.3342/ceo.2012.5.S1.S37. Epub 2012 Apr 30.Voxel-wise analysis of diffusion tensor imaging for clinical outcome of cochlear implantation: retrospective study.Chang Y(1), Lee HR, Paik JS, Lee KY, Lee SH.Author information:(1)Department of Molecular Medicine, Kyungpook National University School of Medicine, Daegu, Korea.OBJECTIVES: To evaluate retrospectively, the possible difference in diffusion tensor imaging (DTI) metric of fractional anisotropy (FA) between good and poor surgical outcome cochlear implantation (CI) patients using investigator-independent voxel-wise analysis.METHODS: Eighteen patients (11 males, 7 females; mean age, 5.9 years) with profound sensorineural hearing loss underwent DTI scans using a 3.0 Tesla magnetic resonance scanner. Among the 18 patients, 10 patients with categories of auditory performance (CAP) score over 6 were classified into the good outcome group and 8 patients with CAP score below 6 were classified into the poor outcome group. The diffusion tensor scalar measure was calculated from the eigenvalues of the tensor on a voxel-by-voxel basis from each subject and two-sample t-test evaluation between good and poor outcome subjects were performed for each voxel of FA values, across the entire brain, with a voxel-wise intensity threshold of P<0.0005 (uncorrected) and a contiguous cluster size of 64 voxels. Individual values of FA were measured by using the region-of-interest based analysis for correlation analysis with CAP scores, open sentence and open word scores.RESULTS: Two-sample t-test evaluation using SPM voxel-wise analysis found significantly higher FA values at the several brain areas including Broca's area, genu of the corpus callosum, and auditory tract in good outcome subjects compared to poor outcome subjects. Correlation analyses between FA and CAP scores, open sentence and open word scores revealed strong correlations at medial geniculate nucleus, Broca's area, genu of the corpus callosum and auditory tract.CONCLUSION: Investigator-independent voxel-based analysis of DTI image demonstrated that good outcome subjects showed better neural integrity at brain areas associated with language and auditory functions, suggesting that the conservation of microstructural integrity of these brain areas is important. Preoperative functional imaging may be helpful for CI.DOI: 10.3342/ceo.2012.5.S1.S37PMCID: PMC3369980",pubmed,22701772,10.3342/ceo.2012.5.S1.S37
consonant similarity judgments by normal and hearingimpaired listenersdp   mar 1980,"Attempted to identify listener strategies or perceptual modes that might be adopted by hearing-impaired listeners when making similarity judgments among pairs of speech sounds. An attempt was also made to describe the relationship between similarity judgments and auditory confusions for such listeners. 15 21-45 yr old Ss (5 normal hearing, 5 with bilateral acquired hearing loss, and 5 with congenital hearing loss) provided similarity ratings and recognition responses to consonant pairs. Resulting similarity judgments were organized into a variety of similarity matrices and analyzed, using multidimensional scaling, hierarchical clustering, and traditional descriptive and interpretative statistics. The analyses of the similarity ratings between consonants showed that hearing-impaired Ss applied phonemic labels to the stimuli and based their ratings on these labels rather than on the unlabeled acoustic characteristics of the speech sounds. Analysis of the recognition data indicated that those consonants that were most confused were not necessarily the most conceptually similar to the listener. (36 ref) (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc2&DO=10.1044%2fjshr.2301.162
cochlear implant evaluation prognosis estimation by data mining system,"286. J Int Adv Otol. 2016 Apr;12(1):1-7. doi: 10.5152/iao.2016.510.Cochlear Implant Evaluation: Prognosis Estimation by Data Mining System.Guerra-Jiménez G(1), Ramos De Miguel Á, Falcón González JC, Borkoski Barreiro SA, Pérez Plasencia D, Ramos Macías Á.Author information:(1)Department of Ear Nose Throat, Complejo Hospitalario Insular Materno Infantil, Las Palmas de GC, Spain. gloriaguerraj@gmail.com.OBJECTIVE: Prediction of speech recognition (SR) and quality of life (QoL) outcomes after cochlear implantation is one of the most important challenges for otologists. By sifting through very large amounts of data, data mining reveals trends, patterns, and relationships that might otherwise have remained undetected. There are identifiable pre-implantational factors that condition the cochlear implantation outcome. Our objective is to design a data mining system to predict and classify cochlear implant (CI) predictable benefits in terms of SR and QoL in each patient.MATERIALS AND METHODS: This is an observational study of CI users for at least one year. Audiological benefits and its relation to QoL are analyzed using the Glasgow Benefit Inventory (GBI) and the Specific Questionnaire (SQ). Sociodemographic and medical variables are processed in SPSS Statistics 19.0, MatLab® and Weka®. Classifiers are designed using the nearest neighbour and decision tree algorithms. Estimators are created by linear logistic regression.RESULTS: A total of 29 patients (mean age, 55.3 years; 52% female and 48% male) including 48% unilateral CI users and 51% bimodal CI users were included in the study. GBI improved by 36 points and SQ by 1.7 (p<0.05). Using Nearest Neighbour (IB1) algorithm for classifiers, interesting attributes were identified for SR and SQ result classification (success rate: 80.7%). Decision tree algorithm (J48) showed influencing variables for GBI (success rate: 81%). Estimators by linear logistic regression analysis disclosed a precision of 85%, 68%, and 71% for SR, GBI, and SQ, respectively.CONCLUSION: Our study proposes a systematized system to classify and estimate SR and QoL improvement based on our initial evaluation to complement decision making and patients' information.DOI: 10.5152/iao.2016.510",pubmed,27340975,10.5152/iao.2016.510
aim in mni res disease 122,"Ménière’s disease (MD) is difficult to diagnose objectively and evaluate the treatment outcomes. Although pure tone audiometry is the only objective test included in the diagnostic criteria, inner ear MRI technique, which was recently developed to visualize endolymphatic hydrops (EH), is useful for the diagnosis of MD. However, analyzing methods are reported to be diverse, and sometimes, they are timeconsuming and complicated. In recent years, the rapidly developing field of artificial intelligence (AI) showed outstanding performance in image recognition. In particular, convolutional neural network (CNN) based on deep learning plays a remarkable role in today’s medical field, where imaging analysis is critical. We developed a CNN-based deep learning model called INHEARIT (INner ear Hydrops Estimation via ARtificial InTelligence) for automatic calculation of EH ratio in a segmented region of the cochlea and vestibule. The model can generate results that are highly consistent with those generated by manual calculation more quickly. This automated analysis of inner ear MRI using deep learning would be useful for diagnosis and follow-up of MD. It is also expected to be widely used in differential diagnosis of various EH-related diseases. © Springer Nature Switzerland AG 2022.",scopus,2-s2.0-85159013557,10.1007/978-3-030-64573-1_271
psychometric properties and factor structure of a new scale to measure hyperacusis introducing the inventory of hyperacusis symptoms,"797. Ear Hear. 2018 Sep/Oct;39(5):1025-1034. doi: 10.1097/AUD.0000000000000583.Psychometric Properties and Factor Structure of a New Scale to Measure Hyperacusis: Introducing the Inventory of Hyperacusis Symptoms.Greenberg B(1)(2), Carlos M(1).Author information:(1)School of Clinical Psychology, American School of Professional Psychology at Argosy University, Alameda, California, USA.(2)Adult Outpatient Psychiatry Department, California Pacific Medical Center, San Francisco, California.OBJECTIVES: Despite increasing interest in hyperacusis and other disorders of auditory sensitivity, there is still a lack of valid, standardized assessment tools to measure symptom severity, treatment outcomes, and diagnostic differentiation. Accordingly, this study sought to create a new scale that is reliable, valid, brief, and easy to score with the purpose of filling this gap.DESIGN: Original items were constructed through review of currently existing models of hyperacusis measurement, as well as qualitative data collected from professional audiologists and individuals reporting heightened audiological sensitivity with tinnitus. An initial 26-item scale yielded sound reliability and validity properties. Refinement based on review of initial data resulted in a 25-question second version with a maximum score of 100. A total of 450 completed survey protocols were analyzed from 469 refined Inventory of Hyperacusis Symptoms (IHS) administrations collected online, representing individuals from 37 countries with a mean age of 34.8 years.RESULTS: Internal consistency reliability analysis yielded a Cronbach's α of 0.93, indicating excellent reliability. Furthermore, the IHS showed sound convergent validity with established measures of quality of life, anxiety, and depression in bivariate correlation analysis of Pearson's r. Factor analysis revealed a dimensional structure containing five factors, which were designated psychosocial impact, emotional arousal, functional impact, general loudness, and communication. Analysis of variance between perceived global hyperacusis severity categories provided a preliminary framework for scoring thresholds. Although the level of hearing loss did not correlate with IHS scores, increased tinnitus symptoms were a significant factor in predicting hyperacusis distress and severity.CONCLUSIONS: These initial results demonstrated sound statistical properties of the IHS and usefulness as a hyperacusis measurement tool in research and clinical practice. Factor structure and scale dimensions allow for differentiation between subtypes of loudness, annoyance, fear, and pain based on responses to clusters of specific items within the dimensional factor structure of the scale, and may thus prove useful in clinical practice and research.DOI: 10.1097/AUD.0000000000000583",pubmed,29742543,10.1097/AUD.0000000000000583
characterization of synthetic health data using rulebased artificial intelligence models,"The aim of this study is to apply and characterize eXplainable AI (XAI) to assess the quality of synthetic health data generated using a data augmentation algorithm. In this exploratory study, several synthetic datasets are generated using various configurations of a conditional Generative Adversarial Network (GAN) from a set of 156 observations related to adult hearing screening. A rule-based native XAI algorithm, the Logic Learning Machine, is used in combination with conventional utility metrics. The classification performance in different conditions is assessed: models trained and tested on synthetic data, models trained on synthetic data and tested on real data, and models trained on real data and tested on synthetic data. The rules extracted from real and synthetic data are then compared using a rule similarity metric. The results indicate that XAI may be used to assess the quality of synthetic data by (i) the analysis of classification performance and (ii) the analysis of the rules extracted on real and synthetic data (number, covering, structure, cut-off values, and similarity). These results suggest that XAI can be used in an original way to assess synthetic health data and extract knowledge about the mechanisms underlying the generated data.",ieee,2168-2208,10.1109/JBHI.2023.3236722
optimized loudnessfunction estimation for categorical loudness scaling data,"373. Hear Res. 2014 Oct;316:16-27. doi: 10.1016/j.heares.2014.07.003. Epub 2014 Jul 21.Optimized loudness-function estimation for categorical loudness scaling data.Oetting D(1), Brand T(2), Ewert SD(2).Author information:(1)Project Group Hearing, Speech and Audio Technology of the Fraunhofer IDMT and Cluster of Excellence Hearing4all, Marie-Curie-Str. 2, 26129 Oldenburg, Germany; Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, 26111 Oldenburg, Germany. Electronic address: dirk.oetting@idmt.fraunhofer.de.(2)Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, 26111 Oldenburg, Germany.Individual loudness perception can be assessed using categorical loudness scaling (CLS). The procedure does not require any training and is frequently used in clinics. The goal of this study was to investigate different methods of loudness-function estimation from CLS data in terms of their test-retest behaviour and to suggest an improved method compared to Brand and Hohmann (2002) for adaptive CLS. Four different runs of the CLS procedure were conducted using 13 normal-hearing and 11 hearing-impaired listeners. The following approaches for loudness-function estimation (fitting) by minimising the error between the data and loudness function were compared: Errors were defined both in level and in loudness direction, respectively. The hearing threshold level (HTL) was extracted from CLS by splitting the responses into an audible and an inaudible category. The extracted HTL was used as a fixed starting point of the loudness function. The uncomfortable loudness level (UCL) was estimated if presentation levels were not sufficiently high to yield responses in the upper loudness range, as often observed in practise. Compared to the original fitting method, the modified estimation of the HTL was closer to the pure-tone audiometric threshold. Results of a computer simulation for UCL estimation showed that the estimation error was reduced for data sets with sparse or absent responses in the upper loudness range. Overall, the suggested modifications lead to a better test-retest behaviour. If CLS data are highly consistent over the whole loudness range, all fitting methods lead to almost equal loudness functions. A considerable advantage of the suggested fitting method is observed for data sets where the responses either show high standard deviations or where responses are not present in the upper loudness range. Both cases regularly occur in clinical practice.Copyright © 2014 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2014.07.003",pubmed,25058812,10.1016/j.heares.2014.07.003
breaking barriers the evolution of sign language detection with artificial intelligence,"Since the beginning of time, God has gifted us with the ability to hear and speak. People with hearing loss, on the other hand, use sign language as an alternative way of communication that consists of a collection of human actions that reflect a certain expression. Communication between verbal and nonverbal groups has grown difficult because members of the communities are unable to comprehend the meaning of sign language. This study describes the construction of a real-time sign language detection system utilizing a deep learning approach and combining artificial intelligence technology with object detection. This translator will be able to recognize hand motions and translate them into text output in the alphabet using object identification and comprehensive machine vision. This translator serves as a medium for disabled people to communicate and share their ideas with the rest of the community.",ieee,,10.1109/IMCOM60618.2024.10418440
intelligent smartphone audiometry,"The continuously aging population in the majority of industrialized nations challenges the healthcare providers to maintain quality of healthcare services at status quo. To make health care sustainable, the paradigm Ambient Intelligence can be used to exploit information and telecommunication technology for the development of autonomous healthcare services. Following this approach, we design and implement a virtual audiologist dubbed ViA, performing air-conduction and bone-conduction hearing tests based on a standard in contrast to existing solutions. The software platform is Android. In this paper, we present the system architecture of ViA with focus on the Android platform and discuss calibration issues. First test results in office environment indicate that a large range of hearing thresholds can be reproduced on different smartphones within a band of 9 dB. ViA has the potential to identify individuals with correctable hearing loss without putting extra load on the healthcare providers. © Springer International Publishing AG 2017.",scopus,2-s2.0-85021684392,10.1007/978-3-319-61118-1_15
brain stem responses evoked by stimulation with an auditory brain stem implant in children with cochlear nerve aplasia or hypoplasia,"Objectives: The inclusion criteria for an auditory brain stem implant (ABI) have been extended beyond the traditional, postlingually deafened adult with Neurofibromatosis type 2, to include children who are born deaf due to cochlear nerve aplasia or hypoplasia and for whom a cochlear implant is not an option. Fitting the ABI for these new candidates presents a challenge, and intraoperative electrically evoked auditory brain stem responses (EABRs) may assist in the surgical placement of the electrode array over the dorsal and ventral cochlear nucleus in the brain stem and in the postoperative programming of the device. This study had four objectives: (1) to characterize the EABR by stimulation of the cochlear nucleus in children, (2) to establish whether there are any changes between the EABR recorded intraoperatively and again just before initial behavioral testing with the device, (3) to establish whether there is evidence of morphology changes in the EABR depending on the site of stimulation with the ABI, and (4) to investigate how the EABR relates to behavioral measurements and the presence of auditory and nonauditory sensations perceived with the ABI at initial device activation. Design: Intra- and postoperative EABRs were recorded from six congenitally deaf children with ABIs, four boys and two girls, mean age 4.2 yrs (range 3.2 to 5.0 yrs). The ABI was stimulated at nine different bipolar sites on the array, and the EABRs recorded were analyzed with respect to the morphology and peak latency with site of stimulation for each recording session. The relationship between the EABR waveforms and the presence or absence of auditory electrodes at initial device activation was investigated. The EABR threshold levels were compared with the behavioral threshold (T) and comfortably loud (C) levels of stimulation required at initial device activation. Results: EABRs were elicited from all children on both test occasions. Responses contained a possible combination of one to three peaks from a total of four identifiable peaks with mean latencies of 1.04, 1.81, 2.61, and 3.58 msecs, respectively. The presence of an EABR was a good predictor of an auditory response; however, the absence of the EABR was poor at predicting a site with no auditory response. The morphology of EABRs often varied with site of stimulation and between EABR test occasions. Postoperatively, there was a trend for P1, P3, and P4 to be present at the lateral end of the array and P2 at the medial end of the array. Behavioral T and C levels showed a good correlation with postoperative EABR thresholds but a poor correlation with intraoperative EABR thresholds. Conclusions: The presence of an intraoperative EABR was a good indicator for the location of electrodes on the ABI array that provided auditory sensations. The morphology of the EABR was often variable within and between test sessions. The postoperative EABR thresholds did correlate with the behavioral T and C levels and could be used to assist with initial device fitting.",cinahl,1960202,10.1097/AUD.0b013e3181fc9f17
prevalence of hearing impairment in mahabubnagar district telangana state india,"187. Ear Hear. 2019 Jan/Feb;40(1):204-212. doi: 10.1097/AUD.0000000000000599.Prevalence of Hearing Impairment in Mahabubnagar District, Telangana State, India.Bright T(1), Mactaggart I(1), Kuper H(1), Murthy GV(1)(2), Polack S(1).Author information:(1)International Centre for Evidence in Disability, London School of Hygiene & Tropical Medicine, London, United Kingdom.(2)South Asia Centre for Disability Inclusive Development and Research, Indian Institute of Public Health Hyderabad, Public Health Foundation of India, Hyderabad, India.OBJECTIVES: To estimate the prevalence of hearing impairment in Mahabubnagar district, Telangana state, India.METHODS: A population-based prevalence survey of hearing impairment was undertaken in 2014. Fifty-one clusters of 80 people aged 6 months and older were selected using probability-proportionate-to-size sampling. A two-stage hearing screening was conducted using otoacoustic emissions on all participants followed by pure-tone audiometry on those aged 4 years and older who failed otoacoustic emissions. Cases of hearing impairment were defined using the World Health Organization definition of disabling hearing impairment: a pure-tone average of thresholds at 500, 1000, 2000, and 4000 Hz of ≥41 dB HL for adults and ≥31 dB HL for children based on the better ear. Possible causes of hearing impairment were ascertained by a certified audiologist. Reported hearing difficulties were also measured in this survey and compared with audiometry results.RESULTS: Three thousand five hundred seventy-three people were examined (response rate 87%), of whom 52% were female. The prevalence of disabling hearing impairment was 4.5% [95% confidence interval (CI) = 3.8 to 5.3). Disabling hearing impairment prevalence increased with age from 0.4% in those aged 4 to 17 years (95% CI = 0.2 to 1.1) to 34.7% (95% CI = 28.7 to 41.1) in those aged older than 65 years. No difference in prevalence was seen by sex. Ear examination suggested that the possible cause of disabling hearing impairment was chronic suppurative otitis media for 6.9% of cases and dry perforation for 5.6% cases. For the vast majority of people with disabling hearing impairment, a possible cause could not be established. The overall prevalence of reported or proxy reported hearing impairment was 2.6% (95% CI = 2.0 to 3.4), and this ranged from 0.6% (95% CI = 0.08 to 4.4) in those aged 0 to 3 years to 14.4% (95% CI = 9.8 to 20.7) in those aged older than 65 years.CONCLUSIONS: Disabling hearing impairment in Telangana State is common, affecting approximately 1 in 23 people overall and a third of people aged older than 65 years. These findings suggest that there are a substantial number of individuals with hearing impairment who could potentially benefit from improved access to low-cost interventions.DOI: 10.1097/AUD.0000000000000599",pubmed,29782444,10.1097/AUD.0000000000000599
analysis of phonological criteria in egyptian arabic speaking children using cochlear implant,"Objectives: The purpose of this study is to assess the most common segmental and supra-segmental phonological criteria of the Egyptian Arabic speaking children using CI. This may lead to; better understanding of speech progress and planning individualized therapy programs for these children. Methods: This study included 43 children using cochlear implant (23 males and 20 females), from the clients of the phoniatric unit of ORL Department Zagazig University, at the period from September 2017 to April 2019. The age ranged between 4 to 10 years old. All children had assessments of their language and speech features (phonological patterns, segmental and supra-segmental) and speech intelligibility, then the results were collected and statistically analyzed. Results: The participants of the study exhibited many types of developmental patterns; e.g., Cluster reduction, final consonant deletions, assimilation and substitutions. There were also fewer incidences of non-developmental phonological patterns. The sequence of acquisition of segmental phonological development revealed the following sequence: Bilabial sounds acquired first (oral /b/ before nasal /m/), then lingu-alveolar, then fricatives, then velar and back sounds and lastly laterals and glides. All studied segmental, supra-segmental features and speech intelligibility were correlated with the CI usage period. Conclusion: The speech of the Egyptian CI children shows many developmental phonological patterns as well as non-developmental ones. The sequence of phonemic development revealed that anterior sounds precede posterior ones, oral sounds precede nasal ones and stops precede fricatives. Glides and laterals showed very late acquisition. All segmental and supra-segmental disturbances improved gradually with regular use of CI and attending speech therapy plans. © 2019 Elsevier B.V.",scopus,2-s2.0-85072172835,10.1016/j.ijporl.2019.109637
perception of polyphony with cochlear implants for 2 and 3 simultaneous pitches,"472. Otol Neurotol. 2014 Mar;35(3):431-6. doi: 10.1097/MAO.0000000000000255.Perception of polyphony with cochlear implants for 2 and 3 simultaneous pitches.Penninger RT(1), Kludt E, Limb CJ, Leman M, Dhooge I, Buechner A.Author information:(1)*Institute for Psychoacoustics and Electronic Music, University Ghent, Ghent, Belgium; †Cluster of Excellence ""Hearing4all"" Department of Otolaryngology, Medical University of Hannover, Hannover, Germany; ‡Department of Otolaryngology-Head and Neck Surgery, Johns Hopkins Hospital, Baltimore, Maryland, U.S.A.; §Peabody Conservatory of Music, Baltimore, Maryland, U.S.A.; and ∥Department of Otolaryngology, University Hospital Ghent, Ghent, Belgium.HYPOTHESIS: It was hypothesized that cochlear implant (CI) subjects would be able to correctly identify 1, 2, and 3 simultaneous pitches through direct electrical stimulation. We further hypothesized that the location on the implant array and the fundamental frequency of the pitches would have an impact on the performance.BACKGROUND: ""They gave me back speech but not music"" is a sentence commonly heard by CI subjects. One of the reasons is that in music, multiple streams are frequently played at the same time, which is an essential feature of harmony. Current CI speech processors do not allow CI users to perceive such complex polyphonic sounds.METHODS: In the present study, the authors assessed the ability of CI subjects to perceive simultaneous modulation frequencies based on direct electrical stimulation. Ten CI subjects were asked to identify 1, 2, and 3 simultaneous pitches applied on different electrodes using sinusoidal amplitude modulation. All stimuli were loudness balanced before the actual identification task.RESULTS: Subjects were able to identify 1, 2, and 3 simultaneous pitches. The further the distance between the 2 electrodes, the better was the performance in the 2-pitch condition. The distance between the modulation frequencies had a significant effect on the performance in the 2-and 3-pitch condition.CONCLUSION: Subjects are able to identify complex polyphonic stimuli based on the number of active electrodes. The additional polyphonic rate pitch cue improves performance in some conditions.DOI: 10.1097/MAO.0000000000000255",pubmed,24518404,10.1097/MAO.0000000000000255
speech experience shapes the speechreading network and subsequent deafness facilitates it,"441. Brain. 2009 Oct;132(Pt 10):2761-71. doi: 10.1093/brain/awp159. Epub 2009 Jun 16.Speech experience shapes the speechreading network and subsequent deafness facilitates it.Suh MW(1), Lee HJ, Kim JS, Chung CK, Oh SH.Author information:(1)Department of Otorhinolaryngology, Seoul National University College of Medicine, Yongon-Dong, Chongno-Gu, Seoul, South Korea.Speechreading is a visual communicative skill for perceiving speech. In this study, we tested the effects of speech experience and deafness on the speechreading neural network in normal hearing controls and in two groups of deaf patients who became deaf either before (prelingual deafness) or after (postlingual deafness) auditory language acquisition. Magnetic signals from the cerebral cortex were recorded using a 306-channel magnetoencephalographic system. During magnetoencephalographic measurements, subjects were asked to perform a speechreading task from video clips of a female speaker either pronouncing syllables (speechreading condition) or showing closed-mouth movement. The sources of the evoked fields were modelled using equivalent current dipoles, the origins of which were fitted to the intracranial space based on magnetic resonance imaging findings. During the speechreading condition, the latency of auditory cortex activation was shorter in the postlingual deafness group than in the normal hearing control group. This parameter negatively correlated with speechreading scores measured clinically. Furthermore, as the duration of deafness increased, the latency of auditory cortex activation decreased exponentially. However, no such correlation was found in the prelingual deafness group which differed significantly from the two other groups in this respect. The latency of auditory cortex activation was significantly longer in the prelingual deafness group than in the two other groups. Thus, auditory experience may be crucial for the development of a normal neural network for speechreading. The pre-existing speechreading network in the postlingual deafness group is made more efficient by speeding up the neural response.DOI: 10.1093/brain/awp159",pubmed,19531532,10.1093/brain/awp159
machine learning approaches to intelligent sign language recognition and classification,"Sign language is a great visual communication technique for those who have auditory or speech impairments. The deaf and dumb people have long relied on sign language recognition (SLR) to communicate and integrate into society. This research uses Indian Sign Language to identify elementary sign-language gestures in images/videos and compares machine language methods. Images are pre-processed and feature-extracted to improve the performance of deployed models. The goal is to create a system that uses an efficient classifier to deliver reliable hand sign-language gesture recognition. For the recognition of the Indian Sign Language dataset for the Sign Language Translation and Recognition ISL-CSLTR database, the accuracy and precision of classification methods are analysed and compared. When compared to the decision tree and KNN models, the Random Forest model had a greater accuracy of 84% and 83% precision. We also got 77% Recall and a 0.7 F Score according to this study. The tool used for evaluating our work in python.  © 2023 Inderscience Enterprises Ltd.",scopus,2-s2.0-85161863224,10.1504/IJSSE.2023.131232
automatic recognition of auditory brainstem response characteristic waveform based on bidirectional long shortterm memory,"603. Front Med (Lausanne). 2021 Jan 11;7:613708. doi: 10.3389/fmed.2020.613708. eCollection 2020.Automatic Recognition of Auditory Brainstem Response Characteristic Waveform Based on Bidirectional Long Short-Term Memory.Chen C(1), Zhan L(2), Pan X(1), Wang Z(1), Guo X(1), Qin H(2), Xiong F(2), Shi W(2), Shi M(2), Ji F(2), Wang Q(2), Yu N(2), Xiao R(1)(3).Author information:(1)School of Computer and Communication Engineering, University of Science & Technology Beijing, Beijing, China.(2)College of Otolaryngology Head and Neck Surgery, National Clinical Research Center for Otolaryngologic Diseases, Key Lab of Hearing Science, Ministry of Education, Beijing Key Lab of Hearing Impairment for Prevention and Treatment, Chinese PLA General Hospital, Beijing, China.(3)Institute of Artificial Intelligence, University of Science and Technology Beijing, Beijing, China.Background: Auditory brainstem response (ABR) testing is an invasive electrophysiological auditory function test. Its waveforms and threshold can reflect auditory functional changes in the auditory centers in the brainstem and are widely used in the clinic to diagnose dysfunction in hearing. However, identifying its waveforms and threshold is mainly dependent on manual recognition by experimental persons, which could be primarily influenced by individual experiences. This is also a heavy job in clinical practice. Methods: In this work, human ABR was recorded. First, binarization is created to mark 1,024 sampling points accordingly. The selected characteristic area of ABR data is 0-8 ms. The marking area is enlarged to expand feature information and reduce marking error. Second, a bidirectional long short-term memory (BiLSTM) network structure is established to improve relevance of sampling points, and an ABR sampling point classifier is obtained by training. Finally, mark points are obtained through thresholding. Results: The specific structure, related parameters, recognition effect, and noise resistance of the network were explored in 614 sets of ABR clinical data. The results show that the average detection time for each data was 0.05 s, and recognition accuracy reached 92.91%. Discussion: The study proposed an automatic recognition of ABR waveforms by using the BiLSTM-based machine learning technique. The results demonstrated that the proposed methods could reduce recording time and help doctors in making diagnosis, suggesting that the proposed method has the potential to be used in the clinic in the future.Copyright © 2021 Chen, Zhan, Pan, Wang, Guo, Qin, Xiong, Shi, Shi, Ji, Wang, Yu and Xiao.DOI: 10.3389/fmed.2020.613708PMCID: PMC7829202",pubmed,33505982,10.3389/fmed.2020.613708
predictive models for cochlear implant outcomes performance generalizability and the impact of cohort size,"318. Trends Hear. 2021 Jan-Dec;25:23312165211066174. doi: 10.1177/23312165211066174.Predictive models for cochlear implant outcomes: Performance, generalizability, and the impact of cohort size.Shafieibavani E(1), Goudey B(1)(2), Kiral I(1), Zhong P(1), Jimeno-Yepes A(1), Swan A(1), Gambhir M(1), Buechner A(3), Kludt E(3), Eikelboom RH(4)(5)(6), Sucher C(4)(5), Gifford RH(7), Rottier R(8), Plant K(8), Anjomshoa H(1)(9).Author information:(1)127113IBM Research Australia, Southbank, Victoria, Australia.(2)School of Computing and Information Systems, University of Melbourne, Parkville, Victoria, Australia.(3)9177Medizinische Hochschule Hannover, Hannover, Niedersachsen, Germany.(4)104182Ear Science Institute Australia, Subiaco, Western Australia, Australia.(5)Ear Sciences Centre, The University of Western Australia, Nedlands, Western Australia, Australia.(6)Department of Speech Language Pathology and Audiology, University of Pretoria, South Africa.(7)Department of Hearing and Speech Sciences, Vanderbilt University Medical Center, Nashville, TN, United States of America.(8)104148Cochlear Limited, New South Wales, Australia.(9)School of Mathematics and Statistics, University of Melbourne, Parkville, Victoria, Australia.While cochlear implants have helped hundreds of thousands of individuals, it remains difficult to predict the extent to which an individual's hearing will benefit from implantation. Several publications indicate that machine learning may improve predictive accuracy of cochlear implant outcomes compared to classical statistical methods. However, existing studies are limited in terms of model validation and evaluating factors like sample size on predictive performance. We conduct a thorough examination of machine learning approaches to predict word recognition scores (WRS) measured approximately 12 months after implantation in adults with post-lingual hearing loss. This is the largest retrospective study of cochlear implant outcomes to date, evaluating 2,489 cochlear implant recipients from three clinics. We demonstrate that while machine learning models significantly outperform linear models in prediction of WRS, their overall accuracy remains limited (mean absolute error: 17.9-21.8). The models are robust across clinical cohorts, with predictive error increasing by at most 16% when evaluated on a clinic excluded from the training set. We show that predictive improvement is unlikely to be improved by increasing sample size alone, with doubling of sample size estimated to only increasing performance by 3% on the combined dataset. Finally, we demonstrate how the current models could support clinical decision making, highlighting that subsets of individuals can be identified that have a 94% chance of improving WRS by at least 10% points after implantation, which is likely to be clinically meaningful. We discuss several implications of this analysis, focusing on the need to improve and standardize data collection.DOI: 10.1177/23312165211066174PMCID: PMC8764462",pubmed,34903103,10.1177/23312165211066174
influencing factors of hearing habits on hearing loss and related symptoms among medical students ,"Background: Recently,many studies have assessed the hearing loss (HL) of elderly people,people across occupational groups,and infants,but few have assessed this issue among medical students in China.Objective: To investigate the status of HL and hearing related symptoms among medical students,and to explore the effect of hearing habits on them.Methods: A total of 1 882 students of four-year (freshman-junior students) and five-year (freshman-senior students) graduation systems were selected by cluster sampling in Medical School,Hangzhou Normal University from March to May 2017.The questionnaire survey and pure-tone hearing tests(PTT)were conducted.The questionnaire comprised three parts:demographic characteristics (age,sex,major,grade),the hearing related symptoms(the occurrence of tinnitus,earache and aural fullness in the last year),and hearing habits(whether to use headphones,type of headphones,frequency of headphone usage,time of headphone usage,time of playing games with headphones,maximum volume of headphone,mode of cell phone conversation,noise environment to improve headphone volume,sleeping with headphones listening to music/radio,frequency of going entertainment places).The sound-pressure levels of background noise measured at a dedicated sound-isolating room in the university laboratory building were below 30 dB.PTT were measured using a calibrated audiometer (AT235) with standard headphones (TDH-39P).The speech-frequency HL (>25 dB)and high-frequency HL (>25 dB) were assessed.Factors influencing HL and hearing related symptoms were analyzed by multivariate Logistic regression models.Results: A total of 1 882 questionnaires were sent out,1 882 questionnaires were recovered,1 882 questionnaires were valid,with an effective recovery rate of 100.0%.Pure-tone averages of the right ear were higher than those of the left ear at hearing frequency of 0.125,0.250,0.500,1.000,2.000,4.000,6.000 kHz(P<0.05).The number of students with speech frequency hearing loss (> 25 dB) was 32 (1.7%),high frequency hearing loss (> 25 dB) was 62 (3.3%),tinnitus was 913 (48.5%),earache was 533 (28.3%) and aural fullness was 502 (26.7%).Among 1 664 medical students(88.4%)who used headphones,there were 963 (57.9%),610 (36.6%) and 91(5.5%) medical students used earplug,in-ear headphones or headset,respectively.The number of students with frequency of using earphones > 2 times/d was 513(27.3%).Duration of using earphones was >1.0 h/d among 289 medical students(15.4%).Duration of playing games with using earphones was ≥1 h/d among 547 medical students(29.1%).Maximum volume of using earphones was ≥60% among 70 medical students(3.7%).The number of students making phone calls by earphones was 521(27.6%).Probability to increase earphone volume in a noisy environment was ≥50% among 1 142 medical students(60.7%).Frequency of sleeping with earphones while listening to music or broadcast was ≥2 times/week among 158 medical students(8.4%).Frequency of attending entertainment venues was >3 times/month among 36 medical students(1.9%).The speech-frequency HL and high-frequency HL rates of the tinnitus group were higher than those in the non-tinnitus group(P<0.05).The speech-frequency HL and high-frequency HL rates did not show any significant difference among ear pain,and aural fullness(P>0.05).Sex〔female:OR=0.467,95%CI(0.267,0.816)〕was a factor affecting speech-frequency HL among medical students(P<0.05).Probability to increase earphone volume in a noisy environment〔≥50%:OR=1.567,95%CI(1.282,1.915)〕 was a factor influencing tinnitus among medical students(P<0.05).Frequency of using earphones〔>2 times/d:OR=1.443,95%CI(1.151,1.810)〕,and duration of using earphones 〔0.5-1.0 h/time:OR=1.447,95%CI(1.151,1.818);>1.0 h/times:OR=1.648,95%CI(1.214,2.237)〕were factors of ear pain among medical students(P<0.05).Sex 〔female:OR=1.338,95%CI(1.061,1.686)〕,duration of playing games with earphones〔≥1 h/d:OR=1.315,95%CI(1.053,1.642)〕,probability to increase earphone volume in a noisy environment〔≥50%:OR=1.398,95%CI(1.126,1.735)〕,and frequency of attending entertainment venues 〔>3 times/month:OR=3.324,95%CI(1.686,6.554)〕were factors for aural fullness among medical students(P<0.05).Conclusion: The hearing status is not optimistic among medical students.The hearing status of the right ear is worse than that of the left ear.Improper usage of earphones and exposure to damaging sound levels at noisy entertainment venue can cause hearing damage.Bad hearing habits have long-term influence on the hearing of medical students.We suggest that qualified schools add PTT to the entrance examination,and strengthen the health education related to hearing. Copyright © 2019 by the Chinese General Practice.",scopus,2-s2.0-85075317542,10.12114/j.issn.1007-9572.2019.00.102
connexin 26 null mice exhibit spiral ganglion degeneration that can be blocked by bdnf gene therapy,"432. Hear Res. 2014 Mar;309:124-35. doi: 10.1016/j.heares.2013.11.009. Epub 2013 Dec 12.Connexin 26 null mice exhibit spiral ganglion degeneration that can be blocked by BDNF gene therapy.Takada Y(1), Beyer LA(2), Swiderski DL(2), O'Neal AL(2), Prieskorn DM(2), Shivatzki S(3), Avraham KB(3), Raphael Y(4).Author information:(1)Kresge Hearing Research Institute, Department of Otolaryngology - Head and Neck Surgery, University of Michigan, 1150 West. Medical Center Dr., Ann Arbor, MI 48109-5648, USA; Department of Otolaryngology, Kansai Medical University, 2-3-1, Shinmachi, Hirakata, Osaka 573-1191, Japan.(2)Kresge Hearing Research Institute, Department of Otolaryngology - Head and Neck Surgery, University of Michigan, 1150 West. Medical Center Dr., Ann Arbor, MI 48109-5648, USA.(3)Department of Human Molecular Genetics and Biochemistry, Sackler Faculty of Medicine and Sagol School of Neuroscience, Tel Aviv University, Tel Aviv 69978, Israel.(4)Kresge Hearing Research Institute, Department of Otolaryngology - Head and Neck Surgery, University of Michigan, 1150 West. Medical Center Dr., Ann Arbor, MI 48109-5648, USA. Electronic address: yoash@umich.edu.Mutations in the connexin 26 gene (GJB2) are the most common genetic cause of deafness, leading to congenital bilateral non-syndromic sensorineural hearing loss. Here we report the generation of a mouse model for a connexin 26 (Cx26) mutation, in which cre-Sox10 drives excision of the Cx26 gene from non-sensory cells flanking the auditory epithelium. We determined that these conditional knockout mice, designated Gjb2-CKO, have a severe hearing loss. Immunocytochemistry of the auditory epithelium confirmed absence of Cx26 in the non-sensory cells. Histology of the organ of Corti and the spiral ganglion neurons (SGNs) performed at ages 1, 3, or 6 months revealed that in Gjb2-CKO mice, the organ of Corti began to degenerate in the basal cochlear turn at an early stage, and the degeneration rapidly spread to the apex. In addition, the density of SGNs in Rosenthal's canal decreased rapidly along a gradient from the base of the cochlea to the apex, where some SGNs survived until at least 6 months of age. Surviving neurons often clustered together and formed clumps of cells in the canal. We then assessed the influence of brain derived neurotrophic factor (BDNF) gene therapy on the SGNs of Gjb2-CKO mice by inoculating Adenovirus with the BDNF gene insert (Ad.BDNF) into the base of the cochlea via the scala tympani or scala media. We determined that over-expression of BDNF beginning around 1 month of age resulted in a significant rescue of neurons in Rosenthal's canal of the cochlear basal turn but not in the middle or apical portions. This data may be used to design therapies for enhancing the SGN physiological status in all GJB2 patients and especially in a sub-group of GJB2 patients where the hearing loss progresses due to ongoing degeneration of the auditory nerve, thereby improving the outcome of cochlear implant therapy in these ears.Copyright © 2013 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2013.11.009PMCID: PMC3946535",pubmed,24333301,10.1016/j.heares.2013.11.009
hand gesture recognition using ai algorithm for hearing impaired,"HRI can be challenging to facilitate communication between humans and robots. This is because robots cannot understand human language without a mediator, and this causes problems for HRI when it comes to hearing impaired patients, and elderly persons. In today's world HGR plays an important role and the HGR system is widely used in many applications throughout the world. HGR can help establish transmission between humans and machines by helping these groups of people. ML is part of AI that focuses on the evolution of a system that rely on data. The main issue of HGR is that the machine doesn't recognize the human language directly and human machine interaction is in need of media for conveying which can be recognized by machine and as well as humans, to help the hearing impaired persons and elderly persons, so HGR as transmitting media is required to give instructions to computer. This project is based on HGR as an input method using two clustering methods, Fuzzy C Means clustering and Decision K Means clustering. The Euclidean distance will be used to compare the different data elements in order to get an output at the end of the project. The output had higher accuracies compared to other algorithms.",ieee,,10.1109/ICIRCA54612.2022.9985748
intelligent hearing system using assistive technology for hearingimpaired patients,"Hearing is an important sense of coexistence, but majority of people taken it for granted unless it is weakened or lost. In this paper, an Assistive Intelligent Hearing Aid System (AIHAS) is proposed that supports hearing impaired patients and allow them to live a normal life. The patients will be required to wear smart glasses equipped with bone conduction technology and wirelessly connected with an application running on patient's smartphone. The AIHAS is designed to: (1) detect multiple ear damages, (2) evaluates the degree of patient's hearing loss, (3) having smart filters with two different modes based on surrounding environment, and (4) assist children with articulation development. The patients will be able to choose between two types of filters i.e. Quiet Room (QR) and Noisy Room (NR) filter depending on the surrounding environment. Additionally, an Auditory Assistive mode is also added to AIHAS to train and assist children with speech disorders caused by hearing impairment at a younger age. The AIHAS also allows interfacing with a smartwatch for easier system access. The system prototype has been developed and tested with multiple patients. The proposed AIHAS is an intelligent, low cost, reliable and a portable solution. © 2018 IEEE.",scopus,2-s2.0-85062082218,10.1109/IEMCON.2018.8615021
objective speech intelligibility prediction using a deep learning model with continuous speechevoked cortical auditory responses,"Auditory prostheses provide an opportunity for rehabilitation of hearing-impaired patients. Speech intelligibility can be used to estimate the extent to which the auditory prosthesis improves the user’s speech comprehension. Although behavior-based speech intelligibility is the gold standard, precise evaluation is limited due to its subjectiveness. Here, we used a convolutional neural network to predict speech intelligibility from electroencephalography (EEG). Sixty-four–channel EEGs were recorded from 87 adult participants with normal hearing. Sentences spectrally degraded by a 2-, 3-, 4-, 5-, and 8-channel vocoder were used to set relatively low speech intelligibility conditions. A Korean sentence recognition test was used. The speech intelligibility scores were divided into 41 discrete levels ranging from 0 to 100%, with a step of 2.5%. Three scores, namely 30.0, 37.5, and 40.0%, were not collected. The speech features, i.e., the speech temporal envelope (ENV) and phoneme (PH) onset, were used to extract continuous-speech EEGs for speech intelligibility prediction. The deep learning model was trained by a dataset of event-related potentials (ERP), correlation coefficients between the ERPs and ENVs, between the ERPs and PH onset, or between ERPs and the product of the multiplication of PH and ENV (PHENV). The speech intelligibility prediction accuracies were 97.33% (ERP), 99.42% (ENV), 99.55% (PH), and 99.91% (PHENV). The models were interpreted using the occlusion sensitivity approach. While the ENV models’ informative electrodes were located in the occipital area, the informative electrodes of the phoneme models, i.e., PH and PHENV, were based on the occlusion sensitivity map located in the language processing area. Of the models tested, the PHENV model obtained the best speech intelligibility prediction accuracy. This model may promote clinical prediction of speech intelligibility with a comfort speech intelligibility test. Copyright © 2022 Na, Joo, Trang, Quan and Woo.",scopus,2-s2.0-85137807462,10.3389/fnins.2022.906616
transcranial alternating current stimulation with the thetaband portion of the temporallyaligned speech envelope improves speechinnoise comprehension,"Transcranial alternating current stimulation with the speech envelope can modulate the comprehension of speech in noise. The modulation stems from the theta- but not the delta-band portion of the speech envelope, and likely reflects the entrainment of neural activity in the theta frequency band, which may aid the parsing of the speech stream. The influence of the current stimulation on speech comprehension can vary with the time delay between the current waveform and the audio signal. While this effect has been investigated for current stimulation based on the entire speech envelope, it has not yet been measured when the current waveform follows the theta-band portion of the speech envelope. Here, we show that transcranial current stimulation with the speech envelope filtered in the theta frequency band improves speech comprehension as compared to a sham stimulus. The improvement occurs when there is no time delay between the current and the speech stimulus, as well as when the temporal delay is comparatively short, 90 ms. In contrast, longer delays, as well as negative delays, do not impact speech-in-noise comprehension. Moreover, we find that the improvement of speech comprehension at no or small delays of the current stimulation is consistent across participants. Our findings suggest that cortical entrainment to speech is most influenced through current stimulation that follows the speech envelope with at most a small delay. They also open a path to enhancing the perception of speech in noise, an issue that is particularly important for people with hearing impairment. © Copyright © 2020 Keshavarzi and Reichenbach.",scopus,2-s2.0-85086433988,10.3389/fnhum.2020.00187
computer assisted data collection in vestibular disorders,"607. Acta Otolaryngol Suppl. 1995;520 Pt 1:205-6. doi: 10.3109/00016489509125229.Computer assisted data collection in vestibular disorders.Kentala E(1), Pyykkö I, Auramo Y, Juhola M.Author information:(1)Department of Otolaryngology, University Hospital of Helsinki, Finland.We have developed an interactive database for vertigo than can be used to assist in the diagnostic procedure and to store the data in a form of a database. The database offers the possibility to split and reunite the collected information in a desired way. The database contains detailed information about patient history, symptoms and findings in neurotological, audiological and imaging tests. The symptoms are classified into three sets of questions: vertigo (including postural instability), hearing loss and tinnitus, and provoking factors. Confounding disorders are screened. The neurotological tests involve saccades, smooth pursuit, posturography and caloric test. In addition, findings in specified antibody testing, clinical neurotological tests. MRI, brain stem audiometry and electrocochleography are included. The input information can be applied in an expert system ONE for vertigo work-up. The database is user-friendly. Besides diagnostic purposes the database is excellent for research purposes, and combined with the expert system it works as a tutorial guide for medical students.DOI: 10.3109/00016489509125229",pubmed,8749120,10.3109/00016489509125229
agerelated deficits in binaural hearing contribution of peripheral and central effects,"51. J Neurosci. 2024 Apr 17;44(16):e0963222024. doi: 10.1523/JNEUROSCI.0963-22.2024.Age-Related Deficits in Binaural Hearing: Contribution of Peripheral and Central Effects.Tolnai S(1)(2), Weiß M(3)(4)(5), Beutelmann R(6)(2), Bankstahl JP(4), Bovee S(6)(2), Ross TL(4), Berding G(3)(4), Klump GM(1)(2).Author information:(1)Animal Physiology and Behavior Group, Department of Neuroscience, School of Medicine and Health Sciences, Carl von Ossietzky University of Oldenburg, Oldenburg 26111, Germany sandra.tolnai@uol.de georg.klump@uol.de.(2)Cluster of Excellence ""Hearing4all"", Oldenburg 26111, Germany.(3)Cluster of Excellence ""Hearing4all"", Hannover 30625, Germany.(4)Department of Nuclear Medicine, Hannover Medical School, Hannover 30625, Germany.(5)The Calcium Signalling Group, Department of Biochemistry and Molecular Cell Biology, University Medical Center Hamburg-Eppendorf, Hamburg 20246, Germany.(6)Animal Physiology and Behavior Group, Department of Neuroscience, School of Medicine and Health Sciences, Carl von Ossietzky University of Oldenburg, Oldenburg 26111, Germany.Pure-tone audiograms often poorly predict elderly humans' ability to communicate in everyday complex acoustic scenes. Binaural processing is crucial for discriminating sound sources in such complex acoustic scenes. The compromised perception of communication signals presented above hearing threshold has been linked to both peripheral and central age-related changes in the auditory system. Investigating young and old Mongolian gerbils of both sexes, an established model for human hearing, we demonstrate age-related supra-threshold deficits in binaural hearing using behavioral, electrophysiological, anatomical, and imaging methods. Binaural processing ability was measured as the binaural masking level difference (BMLD), an established measure in human psychophysics. We tested gerbils behaviorally with ""virtual headphones,"" recorded single-unit responses in the auditory midbrain and evaluated gross midbrain and cortical responses using positron emission tomography (PET) imaging. Furthermore, we obtained additional measures of auditory function based on auditory brainstem responses, auditory-nerve synapse counts, and evidence for central inhibitory processing revealed by PET. BMLD deteriorates already in middle-aged animals having normal audiometric thresholds and is even worse in old animals with hearing loss. The magnitude of auditory brainstem response measures related to auditory-nerve function and binaural processing in the auditory brainstem also deteriorate. Furthermore, central GABAergic inhibition is affected by age. Because the number of synapses in the apical turn of the inner ear was not reduced in middle-aged animals, we conclude that peripheral synaptopathy contributes little to binaural processing deficits. Exploratory analyses suggest increased hearing thresholds, altered binaural processing in the brainstem and changed central GABAergic inhibition as potential contributors.Copyright © 2024 the authors.DOI: 10.1523/JNEUROSCI.0963-22.2024PMCID: PMC11026345",pubmed,38395618,10.1523/JNEUROSCI.0963-22.2024
efficacy of parentchild interaction therapy on anxiety symptoms in cochlear implanted deaf children15th international conference on cochlear implants and other implantable auditory technologies 27th jun 2018  30th jun 2018 antwerp belgium,"Introduction: The aim of this study was to investigate the effectiveness of parent-child interaction therapy on the anxiety symptoms of cochlear implanted deaf children. Materials and Methods: This is an experimental study designed as pretest-posttest with control group in the form of random assignment to two experimental and control groups. The statistical population of the present study included deaf children aged 7 to 11 years old with cochlear implants referring to Tehran cochlear implantation centers in 2017. Among these centers, the Pejvak auditoryverbal rehabilitation center was selected by cluster sampling. The subjects were sampled among the referents with required characteristics through a targeted method. The deaf children (n= 30) with implanted cochlea were randomly divided into two experimental (n= 15) and control (n = 15) groups. The parents of these children completed the Achenbach questionnaire/parent form (CBCL) and anxiety subscale. Data were analyzed by covariance test. Results: Comparing the mean values of the experimental and control groups showed that average anxiety symptoms in the experimental group decreased significantly compared to the control group in the post-test. Conclusion: Based on the results of this study, it can be concluded that parent-child interaction therapy has an effect on the anxiety symptoms of cochlear implanted deaf children.",cinahl,2083389X,
temporal hyperprecision of brainstem neurons alters spatial sensitivity of binaural auditory processing with cochlear implants,"839. Front Neurosci. 2023 Jan 4;16:1021541. doi: 10.3389/fnins.2022.1021541. eCollection 2022.Temporal hyper-precision of brainstem neurons alters spatial sensitivity of binaural auditory processing with cochlear implants.Müller M(1), Hu H(2)(3), Dietz M(2)(3), Beiderbeck B(1), Ferreiro DN(4)(5), Pecka M(1)(4).Author information:(1)Graduate School of Systemic Neurosciences, Ludwig-Maximilians-Universität, Munich, Germany.(2)Department of Medical Physics and Acoustics, Carl von Ossietzky University of Oldenburg, Oldenburg, Germany.(3)Cluster of Excellence ""Hearing4All"", Universität Oldenburg, Oldenburg, Germany.(4)Section of Neurobiology, Faculty of Biology, LMU Biocenter, Ludwig-Maximilians-Universität, Munich, Germany.(5)Department of General Psychology and Education, Ludwig-Maximilians-Universität, Munich, Germany.The ability to localize a sound source in complex environments is essential for communication and navigation. Spatial hearing relies predominantly on the comparison of differences in the arrival time of sound between the two ears, the interaural time differences (ITDs). Hearing impairments are highly detrimental to sound localization. While cochlear implants (CIs) have been successful in restoring many crucial hearing capabilities, sound localization via ITD detection with bilateral CIs remains poor. The underlying reasons are not well understood. Neuronally, ITD sensitivity is generated by coincidence detection between excitatory and inhibitory inputs from the two ears performed by specialized brainstem neurons. Due to the lack of electrophysiological brainstem recordings during CI stimulation, it is unclear to what extent the apparent deficits are caused by the binaural comparator neurons or arise already on the input level. Here, we use a bottom-up approach to compare response features between electric and acoustic stimulation in an animal model of CI hearing. Conducting extracellular single neuron recordings in gerbils, we find severe hyper-precision and moderate hyper-entrainment of both the excitatory and inhibitory brainstem inputs to the binaural comparator neurons during electrical pulse-train stimulation. This finding establishes conclusively that the binaural processing stage must cope with highly altered input statistics during CI stimulation. To estimate the consequences of these effects on ITD sensitivity, we used a computational model of the auditory brainstem. After tuning the model parameters to match its response properties to our physiological data during either stimulation type, the model predicted that ITD sensitivity to electrical pulses is maintained even for the hyper-precise inputs. However, the model exhibits severely altered spatial sensitivity during electrical stimulation compared to acoustic: while resolution of ITDs near midline was increased, more lateralized adjacent source locations became inseparable. These results directly resemble recent findings in rodent and human CI listeners. Notably, decreasing the phase-locking precision of inputs during electrical stimulation recovered a wider range of separable ITDs. Together, our findings suggest that a central problem underlying the diminished ITD sensitivity in CI users might be the temporal hyper-precision of inputs to the binaural comparator stage induced by electrical stimulation.Copyright © 2023 Müller, Hu, Dietz, Beiderbeck, Ferreiro and Pecka.DOI: 10.3389/fnins.2022.1021541PMCID: PMC9846145",pubmed,36685222,10.3389/fnins.2022.1021541
an algorithm to increase intelligibility for hearingimpaired listeners in the presence of a competing talker,"311. J Acoust Soc Am. 2017 Jun;141(6):4230. doi: 10.1121/1.4984271.An algorithm to increase intelligibility for hearing-impaired listeners in the presence of a competing talker.Healy EW(1), Delfarah M(2), Vasko JL(1), Carter BL(1), Wang D(2).Author information:(1)Department of Speech and Hearing Science, The Ohio State University, Columbus, Ohio 43210, USA.(2)Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio 43210, USA.Individuals with hearing impairment have particular difficulty perceptually segregating concurrent voices and understanding a talker in the presence of a competing voice. In contrast, individuals with normal hearing perform this task quite well. This listening situation represents a very different problem for both the human and machine listener, when compared to perceiving speech in other types of background noise. A machine learning algorithm is introduced here to address this listening situation. A deep neural network was trained to estimate the ideal ratio mask for a male target talker in the presence of a female competing talker. The monaural algorithm was found to produce sentence-intelligibility increases for hearing-impaired (HI) and normal-hearing (NH) listeners at various signal-to-noise ratios (SNRs). This benefit was largest for the HI listeners and averaged 59%-points at the least-favorable SNR, with a maximum of 87%-points. The mean intelligibility achieved by the HI listeners using the algorithm was equivalent to that of young NH listeners without processing, under conditions of identical interference. Possible reasons for the limited ability of HI listeners to perceptually segregate concurrent voices are reviewed as are possible implementation considerations for algorithms like the current one.DOI: 10.1121/1.4984271PMCID: PMC5464956",pubmed,28618817,10.1121/1.4984271
use of a deep recurrent neural network to reduce wind noise effects on judged speech intelligibility and sound quality,"344. Trends Hear. 2018 Jan-Dec;22:2331216518770964. doi: 10.1177/2331216518770964.Use of a Deep Recurrent Neural Network to Reduce Wind Noise: Effects on Judged Speech Intelligibility and Sound Quality.Keshavarzi M(1), Goehring T(2), Zakis J(3), Turner RE(4), Moore BCJ(1).Author information:(1)1 Department of Psychology, University of Cambridge, Cambridge, UK.(2)2 MRC Cognition and Brain Sciences Unit, University of Cambridge, Cambridge, UK.(3)3 Blamey and Saunders Hearing Pty Ltd, East Melbourne, Victoria, Australia.(4)4 Department of Engineering, University of Cambridge, Cambridge, UK.Despite great advances in hearing-aid technology, users still experience problems with noise in windy environments. The potential benefits of using a deep recurrent neural network (RNN) for reducing wind noise were assessed. The RNN was trained using recordings of the output of the two microphones of a behind-the-ear hearing aid in response to male and female speech at various azimuths in the presence of noise produced by wind from various azimuths with a velocity of 3 m/s, using the ""clean"" speech as a reference. A paired-comparison procedure was used to compare all possible combinations of three conditions for subjective intelligibility and for sound quality or comfort. The conditions were unprocessed noisy speech, noisy speech processed using the RNN, and noisy speech that was high-pass filtered (which also reduced wind noise). Eighteen native English-speaking participants were tested, nine with normal hearing and nine with mild-to-moderate hearing impairment. Frequency-dependent linear amplification was provided for the latter. Processing using the RNN was significantly preferred over no processing by both subject groups for both subjective intelligibility and sound quality, although the magnitude of the preferences was small. High-pass filtering (HPF) was not significantly preferred over no processing. Although RNN was significantly preferred over HPF only for sound quality for the hearing-impaired participants, for the results as a whole, there was a preference for RNN over HPF. Overall, the results suggest that reduction of wind noise using an RNN is possible and might have beneficial effects when used in hearing aids.DOI: 10.1177/2331216518770964PMCID: PMC5949931",pubmed,29708061,10.1177/2331216518770964
dispositional optimism dysphoria health and coping with hearing impairment in elderly adultsdp   marapr 1995,"Investigated the psychometric validity of and associations among measures of dispositional optimism, dysphoria, health, and coping in a sample of 68 hearing impaired adults (mean age 69.6 yrs), who were recently supplied with hearing aids. Cluster analysis was also attempted to investigate the presence of coping categories, and the characteristics of these obtained subgroups. Measures included the Hearing Coping Assessment, the Hearing Questions (HQ), the Life Orientation Test, the Beck Depression Inventory, and a subscale from the Goteborg Quality of Life. Psychometric analyses revealed high reliability in terms of Cronbach's alpha and split-half r's. Significant intercorrelations were found between several measures, but not with pure-tone audiometry (.5, 1, 2, and 3 kHz). Three clusters were identified interpreted as (1) high copers, (2) copers with moderate psychological and somatic complaints, and (3) low copers. (French abstract) (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc3&DO=10.3109%2f00206099509071900
can you hear out the melody testing musical scene perception in young normalhearing and older hearingimpaired listeners,"250. Trends Hear. 2020 Jan-Dec;24:2331216520945826. doi: 10.1177/2331216520945826.Can You Hear Out the Melody? Testing Musical Scene Perception in Young Normal-Hearing and Older Hearing-Impaired Listeners.Siedenburg K(1), Röttges S(1), Wagener KC(2), Hohmann V(1)(2).Author information:(1)Department of Medical Physics and Acoustics and Cluster of Excellence Hearing4all, Carl von Ossietzky University of Oldenburg.(2)Hörzentrum Oldenburg GmbH & Hörtech gGmbH, Oldenburg, Germany.It is well known that hearing loss compromises auditory scene analysis abilities, as is usually manifested in difficulties of understanding speech in noise. Remarkably little is known about auditory scene analysis of hearing-impaired (HI) listeners when it comes to musical sounds. Specifically, it is unclear to which extent HI listeners are able to hear out a melody or an instrument from a musical mixture. Here, we tested a group of younger normal-hearing (yNH) and older HI (oHI) listeners with moderate hearing loss in their ability to match short melodies and instruments presented as part of mixtures. Four-tone sequences were used in conjunction with a simple musical accompaniment that acted as a masker (cello/piano dyads or spectrally matched noise). In each trial, a signal-masker mixture was presented, followed by two different versions of the signal alone. Listeners indicated which signal version was part of the mixture. Signal versions differed either in terms of the sequential order of the pitch sequence or in terms of timbre (flute vs. trumpet). Signal-to-masker thresholds were measured by varying the signal presentation level in an adaptive two-down/one-up procedure. We observed that thresholds of oHI listeners were elevated by on average 10 dB compared with that of yNH listeners. In contrast to yNH listeners, oHI listeners did not show evidence of listening in dips of the masker. Musical training of participants was associated with a lowering of thresholds. These results may indicate detrimental effects of hearing loss on central aspects of musical scene perception.DOI: 10.1177/2331216520945826PMCID: PMC7502688",pubmed,32895034,10.1177/2331216520945826
a spiking neuron model of cortical correlates of sensorineural hearing loss spontaneous firing synchrony and tinnitus,"428. Neural Comput. 2006 Dec;18(12):2942-58. doi: 10.1162/neco.2006.18.12.2942.A spiking neuron model of cortical correlates of sensorineural hearing loss: Spontaneous firing, synchrony, and tinnitus.Dominguez M(1), Becker S, Bruce I, Read H.Author information:(1)melidomi@yahoo.comHearing loss due to peripheral damage is associated with cochlear hair cell damage or loss and some retrograde degeneration of auditory nerve fibers. Surviving auditory nerve fibers in the impaired region exhibit elevated and broadened frequency tuning, and the cochleotopic representation of broadband stimuli such as speech is distorted. In impaired cortical regions, increased tuning to frequencies near the edge of the hearing loss coupled with increased spontaneous and synchronous firing is observed. Tinnitus, an auditory percept in the absence of sensory input, may arise under these circumstances as a result of plastic reorganization in the auditory cortex. We present a spiking neuron model of auditory cortex that captures several key features of cortical organization. A key assumption in the model is that in response to reduced afferent excitatory input in the damaged region, a compensatory change in the connection strengths of lateral excitatory and inhibitory connections occurs. These changes allow the model to capture some of the cortical correlates of sensorineural hearing loss, including changes in spontaneous firing and synchrony; these phenomena may explain central tinnitus. This model may also be useful for evaluating procedures designed to segregate synchronous activity underlying tinnitus and for evaluating adaptive hearing devices that compensate for selective hearing loss.DOI: 10.1162/neco.2006.18.12.2942",pubmed,17052154,10.1162/neco.2006.18.12.2942
momentary analysis of tinnitus considering the patient,"Ecological momentary assessment is a valuable research technique meant to capture real-time data and contextualize disease. While more common in neuropsychiatric research, this methodology is exceptionally fit for tinnitus. Tinnitus has been shown to be affected by many patient-level and environment-specific factors. From an individual’s baseline anxiety to the level of ambient noise in their environment, the level of bother experienced by those with tinnitus can vary widely. Only assessing tinnitus within a clinical environment can distort the true impact of the disease. Ecological data can minimize bias while generating an individualistic picture of the burden being experienced by the patient. Individual data can also compliment new research methods rooted in precision medicine, providing clearer, better-suited treatments for each patient on the tinnitus spectrum. © 2020, Springer Nature Switzerland AG.",scopus,2-s2.0-85114369803,10.1007/7854_2020_176
recognition of indian sign language using machine learning algorithms,"Auditory perception enables an individual to sense sound vibrations caused due to the variations in pressure present. In accordance with WHO, 6.3% of the Indian population suffers from hearing disability. People that suffer from hearing impairment communicate using hand gestures. Unfortunately, the vast majority of the people in India are not aware of the semantics of these gestures. To bridge the gap between the people suffering from hearing disabilities and those who are not, we have proposed an Indian sign language Recognition system using machine learning algorithm techniques. Our method utilizes several images of people demonstrating the alphabets in Indian Sign Language. These images are pre-processed, and further, we utilize these obtained images for training and testing our Machine Learning Algorithms. Out of all the six machine learning algorithms that we used, Random Forest Machine Learning Algorithm gave the highest accuracy of 98.44%. © 2021 IEEE",scopus,2-s2.0-85126197310,10.1109/SPIN52536.2021.9566141
predicting and explaining hearing aid usage using encoderdecoder with attention mechanism and shap,"Understanding the factors that contribute to optimal hearing aid fitting and hearing aid user experiences is crucial in order to increase the satisfaction and quality of life of hearing loss patients, as well as reduce societal and financial burdens. This work proposes a novel framework that uses Encoder-decoder with attention mechanism (attn-ED) for predicting future hearing aid usage and SHAP to explain the factors contributing to this prediction. It has been demonstrated in experiments that attn-ED performs well at predicting future hearing aid usage, and that SHAP can be utilized to calculate the contribution of different factors affecting hearing aid usage. This framework aims to establish confidence that AI models can be utilized in the medical domain with the use of XAI methods. Moreover, the proposed framework can also assist clinicians in determining the nature of interventions.",ieee,,10.1109/SITIS57111.2022.00053
detection of different types of ear diseases in infants using deep learning for early treatment,"The loss or inability of auditory sense is the most common sensory organ deficiency. Out of every 1000 new-borns, 5–6 are deaf or hard of hearing. They will not be detected until they are two or three years old, after which permanent trauma would have happened. One of the most prominent reasons for deafness in infants is due to ear diseases. Early detection of ear diseases is the top priority to prevent deafness in infants. This research work has developed and applied modified deep learning architecture such as AlexNet, mini GoogLeNet, and LeNet and compared the training by the three architectures. Of the three, AlexNet architecture detected and identified the presence of different ear ailments such as acute otitis media, glue ear, safe CSOM, otomycosis, other infections and also checked whether the ear is normal, with high accuracy.",ieee,,10.1109/ICSSAS57918.2023.10331703
neural network models of tinnitus,"263. Prog Brain Res. 2007;166:125-40. doi: 10.1016/S0079-6123(07)66011-7.Neural network models of tinnitus.Husain FT(1).Author information:(1)Brain Imaging and Modeling Section, National Institute on Deafness and Other Communication Disorders, NIH, Bethesda, MD 20892, USA. husainf@nidcd.nih.govIn this chapter we review the relatively recent effort on the part of neuroscientists to use computational neural network modeling to investigate the neural basis of subjective tinnitus. There are advantages and challenges in using a modeling framework to understand this complex auditory disorder. The foremost challenge to modeling a subjective condition such as tinnitus is the evaluation of the occurrence of tinnitus in the model. We propose comparing measures of the model's activities (simulated neuronal activity, behavioral activity, or neuroimaging activity) with experimental data obtained from studies of tinnitus in humans and animals; strong agreement with experimental data will provide support for the validity of the simulation of tinnitus in a particular model. A major advantage of neural network modeling is that it allows experimentation not possible in animals. Models make it possible to evaluate the contribution of different neural mechanisms affecting tinnitus in a principled manner. A model makes predictions that can be tested by experiments thus leading to the designing of focused experiments. We review several neural models of tinnitus and discuss published findings from simulations using these models. We conclude with a proposed scheme for investigating tinnitus that combines neural network modeling with brain imaging experiments.DOI: 10.1016/S0079-6123(07)66011-7",pubmed,17956777,10.1016/S0079-6123(07)66011-7
dopaminergic and cholinergic innervation in the mouse cochlea after noiseinduced or agerelated synaptopathy,"598. Hear Res. 2022 Sep 1;422:108533. doi: 10.1016/j.heares.2022.108533. Epub 2022 May 21.Dopaminergic and cholinergic innervation in the mouse cochlea after noise-induced or age-related synaptopathy.Grierson KE(1), Hickman TT(2), Liberman MC(3).Author information:(1)Eaton-Peabody Laboratories, Massachusetts Eye and Ear, Boston, MA, 02114 USA; Dept of Otolaryngology-Head & Neck Surgery, Harvard Medical School, Boston, MA, 02115 USA; Hearing Research Lab, Garvan Institute of Medical Research, Darlinghurst, NSW, 2010, AUS.(2)Eaton-Peabody Laboratories, Massachusetts Eye and Ear, Boston, MA, 02114 USA; Dept of Otolaryngology-Head & Neck Surgery, Harvard Medical School, Boston, MA, 02115 USA. Electronic address: tyler_hickman@meei.harvard.edu.(3)Eaton-Peabody Laboratories, Massachusetts Eye and Ear, Boston, MA, 02114 USA; Dept of Otolaryngology-Head & Neck Surgery, Harvard Medical School, Boston, MA, 02115 USA.Cochlear synaptopathy, the loss of or damage to connections between auditory-nerve fibers (ANFs) and inner hair cells (IHCs), is a prominent pathology in noise-induced and age-related hearing loss. Here, we investigated if degeneration of the olivocochlear (OC) efferent innervation is also a major aspect of the synaptopathic ear, by quantifying the volume and spatial organization of its cholinergic and dopaminergic components, using antibodies to vesicular acetylcholine transporter (VAT) and tyrosine hydroxylase (TH), respectively. CBA/CaJ male mice were examined 1 day to 8 months after a synaptopathic noise exposure, and compared to unexposed age-matched controls and unexposed aged mice at 24-28 months. In normal ears, cholinergic lateral (L)OC terminals were denser in the apical half of the cochlea and on the modiolar side of the inner hair cells (IHCs), where ANFs of low-spontaneous rate are typically found, while dopaminergic terminals were more common in the basal third of the cochlea and, re the IHC axes, were offset towards the habenula with respect to cholinergic terminals. The noise had only small and transient effects on the density of LOC innervation, its spatial organization around the IHC axes, or the extent to which TH and VAT signal were colocalized. The synaptopathic noise also had relatively small and transient effects on cholinergic innervation density in the outer hair cell (OHC) area, which normally peaks in the 16 kHz region and falls monotonically towards higher and lower frequencies. In contrast, in the aged ears, there was massive degeneration of OHC efferents, especially in the apical half of the cochlea, where there was also significant loss of OHCs. In the IHC area, there was significant loss of cholinergic terminals in both apical and basal regions and of dopaminergic innervation in the basal half. Furthermore, the cholinergic terminals in the aged ears spread from their normal clustering near the IHC basolateral pole, where the ANF synapses are found, to positions up and down the IHC somata and regions of the neuropil closer to the habenula. This apparent migration was most striking in the apex, where the hair cell pathology was greatest, and may be a harbinger of impending hair cell death.Copyright © 2022. Published by Elsevier B.V.DOI: 10.1016/j.heares.2022.108533",pubmed,35671600,10.1016/j.heares.2022.108533
singleended prediction of listening effort using deep neural networks,"367. Hear Res. 2018 Mar;359:40-49. doi: 10.1016/j.heares.2017.12.014. Epub 2017 Dec 27.Single-ended prediction of listening effort using deep neural networks.Huber R(1), Krüger M(2), Meyer BT(3).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4all, Carl-von-Ossietzky Universität Oldenburg, Carl-von-Ossietzky-Str. 9-11, 26129 Oldenburg, Germany. Electronic address: Rainer.Huber@uni-oldenburg.de.(2)Hörzentrum Oldenburg, Marie-Curie-Str. 2, 26129 Oldenburg, Germany. Electronic address: Melanie.Krueger@hoerzentrum-oldenburg.de.(3)Medizinische Physik and Cluster of Excellence Hearing4all, Carl-von-Ossietzky Universität Oldenburg, Carl-von-Ossietzky-Str. 9-11, 26129 Oldenburg, Germany. Electronic address: Bernd.Meyer@uni-oldenburg.de.The effort required to listen to and understand noisy speech is an important factor in the evaluation of noise reduction schemes. This paper introduces a model for Listening Effort prediction from Acoustic Parameters (LEAP). The model is based on methods from automatic speech recognition, specifically on performance measures that quantify the degradation of phoneme posteriorgrams produced by a deep neural net: Noise or artifacts introduced by speech enhancement often result in a temporal smearing of phoneme representations, which is measured by comparison of phoneme vectors. This procedure does not require a priori knowledge about the processed speech, and is therefore single-ended. The proposed model was evaluated using three datasets of noisy speech signals with listening effort ratings obtained from normal hearing and hearing impaired subjects. The prediction quality was compared to several baseline models such as the ITU-T standard P.563 for single-ended speech quality assessment, the American National Standard ANIQUE+ for single-ended speech quality assessment, and a single-ended SNR estimator. In all three datasets, the proposed new model achieved clearly better prediction accuracies than the baseline models; correlations with subjective ratings were above 0.9. So far, the model is trained on the specific noise types used in the evaluation. Future work will be concerned with overcoming this limitation by training the model on a variety of different noise types in a multi-condition way in order to make it generalize to unknown noise types.Copyright © 2018 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2017.12.014",pubmed,29373159,10.1016/j.heares.2017.12.014
preferred strength of noise reduction for normally hearing and hearingimpaired listeners,"33. Trends Hear. 2023 Jan-Dec;27:23312165231211437. doi: 10.1177/23312165231211437.Preferred Strength of Noise Reduction for Normally Hearing and Hearing-Impaired Listeners.Houben R(1), Reinten I(2), Dreschler WA(2), Mathijssen R(3), Dijkstra TMH(4)(5)(6).Author information:(1)Pento Audiological Centre, Amersfoort, The Netherlands.(2)Clinical and Experimental Audiology, Amsterdam UMC location AMC, Amsterdam, The Netherlands.(3)Embedded Systems Innovation, TNO, Eindhoven, The Netherlands.(4)Institute for Computing and Information Sciences, Radboud University Nijmegen, Nijmegen, The Netherlands.(5)Department of Women's Health, University Clinic Tübingen, Tübingen, Germany.(6)Institute for Translational Bioinformatics, University Clinic Tübingen, Tübingen, Germany.Preference for noise reduction (NR) strength differs between individuals. The purpose of this study was (1) to investigate whether hearing loss influences this preference, (2) to find the number of distinct settings required to classify participants in similar groups based on their preference for NR strength, and (3) to estimate the number of paired comparisons needed to predict to which preference group a participant belongs. A paired comparison paradigm was used in which participants listened to pairs of speech-in-noise stimuli processed by NR with 10 different strength settings. Participants indicated their preferred sound sample. The 30 participants were divided into three groups according to hearing status (normal hearing, mild hearing loss, and moderate hearing loss). The results showed that (1) participants with moderate hearing loss preferred stronger NR than participants with normal hearing; (2) cluster analysis based solely on the preference for NR strength showed that the data could be described well by dividing the participants into three preference clusters; (3) the appropriate cluster membership could be found with 15 paired comparisons. We conclude that on average, a higher hearing loss is related to a preference for stronger NR, at least for our NR algorithm and our participants. The results show that it might be possible to use a limited set of pre-set NR strengths that can be chosen clinically. For our NR one might use three settings: no NR, intermediate NR, and strong NR. Paired comparisons might be used to find the optimal one of the three settings.DOI: 10.1177/23312165231211437PMCID: PMC10666719",pubmed,37990543,10.1177/23312165231211437
indication of direct acoustical cochlea stimulation in comparison to cochlear implants,"403. Hear Res. 2016 Oct;340:185-190. doi: 10.1016/j.heares.2016.01.016. Epub 2016 Feb 4.Indication of direct acoustical cochlea stimulation in comparison to cochlear implants.Kludt E(1), Büchner A(2), Schwab B(2), Lenarz T(2), Maier H(2).Author information:(1)Cluster of Excellence Hearing4all, Germany; Dept. of Otolaryngology, Medical University Hannover, Hannover, Germany. Electronic address: Kludt.Eugen@MH-Hannover.de.(2)Cluster of Excellence Hearing4all, Germany; Dept. of Otolaryngology, Medical University Hannover, Hannover, Germany.The new implantable hearing system Codacs™ was designed to close the treatment gap between active middle ear implants and cochlear implants in cases of severe-to-profound mixed hearing loss. The Codacs™ actuator is attached to conventional stapes prosthesis during the implantation and thereby provides acoustical stimulation through a stapedotomy to the cochlea. Cochlear implants (CIs) on the other hand are an established treatment option for profoundly deaf patients including mixed hearing losses that are possible candidates for the Codacs™. In this retrospective study, we compared the clinical outcome of 25 patients with the Codacs™ (≥3 month post-activation) to 54 CI patients (two years post-activation) with comparable pre-operative bone conduction (BC) thresholds that were potential candidates for both categories of devices. The word recognition score (Freiburg monosyllables test) in quiet was significantly (p < 0.05) better in the Codacs™ than in the corresponding CI patients for average pre-operative bone conduction below 60 dB HL and equal in patients with a pre-operative BC PTA between 60 and 70 dB HL. Speech in noise intelligibility (HSM sentences test at +10 dB SNR) was significantly (p < 0.001) better in Codacs™ (80% median) than in CI patients (25% median) in all tested groups. Our results indicate for patients with sufficient cochlear reserve that speech intelligibility in noise with the Codacs™ hearing implant is significantly better than with a CI. Further, results in Codacs™ were better predictable, encouraging the extension of the indication to patients with less cochlear reserve than reported here.Copyright © 2016 The Authors. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2016.01.016",pubmed,26836967,10.1016/j.heares.2016.01.016
barn owls have ageless ears,"111. Proc Biol Sci. 2017 Sep 27;284(1863):20171584. doi: 10.1098/rspb.2017.1584.Barn owls have ageless ears.Krumm B(1), Klump G(1), Köppl C(1), Langemann U(2).Author information:(1)Cluster of Excellence 'Hearing4all', Animal Physiology and Behaviour Group, Department of Neuroscience, School of Medicine and Health Sciences, University of Oldenburg, 26111 Oldenburg, Germany.(2)Cluster of Excellence 'Hearing4all', Animal Physiology and Behaviour Group, Department of Neuroscience, School of Medicine and Health Sciences, University of Oldenburg, 26111 Oldenburg, Germany ulrike.langemann@uni-oldenburg.de.We measured the auditory sensitivity of the barn owl (Tyto alba), using a behavioural Go/NoGo paradigm in two different age groups, one younger than 2 years (n = 4) and another more than 13 years of age (n = 3). In addition, we obtained thresholds from one individual aged 23 years, three times during its lifetime. For computing audiograms, we presented test frequencies of between 0.5 and 12 kHz, covering the hearing range of the barn owl. Average thresholds in quiet were below 0 dB sound pressure level (SPL) for frequencies between 1 and 10 kHz. The lowest mean threshold was -12.6 dB SPL at 8 kHz. Thresholds were the highest at 12 kHz, with a mean of 31.7 dB SPL. Test frequency had a significant effect on auditory threshold but age group had no significant effect. There was no significant interaction between age group and test frequency. Repeated threshold estimates over 21 years from a single individual showed only a slight increase in thresholds. We discuss the auditory sensitivity of barn owls with respect to other species and suggest that birds, which generally show a remarkable capacity for regeneration of hair cells in the basilar papilla, are naturally protected from presbycusis.© 2017 The Author(s).DOI: 10.1098/rspb.2017.1584PMCID: PMC5627212",pubmed,28931742,10.1098/rspb.2017.1584
sixty years of frequencydomain monaural speech enhancement from traditional to deep learning methods,"67. Trends Hear. 2023 Jan-Dec;27:23312165231209913. doi: 10.1177/23312165231209913.Sixty Years of Frequency-Domain Monaural Speech Enhancement: From Traditional to Deep Learning Methods.Zheng C(1)(2), Zhang H(1)(2), Liu W(1)(2), Luo X(1)(2), Li A(1)(2), Li X(1)(2), Moore BCJ(3).Author information:(1)Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences, Beijing, China.(2)University of Chinese Academy of Sciences, Beijing, China.(3)Cambridge Hearing Group, Department of Psychology, University of Cambridge, Cambridge, UK.Frequency-domain monaural speech enhancement has been extensively studied for over 60 years, and a great number of methods have been proposed and applied to many devices. In the last decade, monaural speech enhancement has made tremendous progress with the advent and development of deep learning, and performance using such methods has been greatly improved relative to traditional methods. This survey paper first provides a comprehensive overview of traditional and deep-learning methods for monaural speech enhancement in the frequency domain. The fundamental assumptions of each approach are then summarized and analyzed to clarify their limitations and advantages. A comprehensive evaluation of some typical methods was conducted using the WSJ + Deep Noise Suppression (DNS) challenge and Voice Bank + DEMAND datasets to give an intuitive and unified comparison. The benefits of monaural speech enhancement methods using objective metrics relevant for normal-hearing and hearing-impaired listeners were evaluated. The objective test results showed that compression of the input features was important for simulated normal-hearing listeners but not for simulated hearing-impaired listeners. Potential future research and development topics in monaural speech enhancement are suggested.DOI: 10.1177/23312165231209913PMCID: PMC10658184",pubmed,37956661,10.1177/23312165231209913
modulations of restingstatic functional connectivity on insular by electroacupuncture in subjective tinnitus,"Objective: To explore the modulations of electroacupuncture in subjective tinnitus (ST) by comparing the difference of functional connectivity (FC) in ST patients and healthy volunteers between the insular (INS) and the whole brain region. Methods: A total of 34 ST patients were selected into electroacupuncture group (EG) and 34 age- and sex-matched normal subjects were recruited into control group (CG). The EG received acupuncture at SI19 (Tinggong), GB11 (Touqiaoyin), TE17 (Yifeng), GV20 (Baihui), GV15 (Yamen), GV14 (Dazhui), SJ13 (Zhongzhu), among which the points of SI19 and GB11 were connected to the electroacupuncture instrument with the density wave of 2/50 Hz, and 3 treatments per week for 10 sessions in total. The severity of tinnitus was evaluated by Tinnitus Handicap Inventory (THI), the hearing status was recorded using pure tone audiometry, and resting-state functional magnetic resonance imaging (rs-fMRI) was performed on the brain before and after treatment, the CG received no intervention yet only rs-fMRI data were collected. Results: With the electroacupuncture treatment, the total THI score, average air conduction threshold of patients of EG were significantly lower than before (p < 0.01), and the total effective rate was 88.24%. Compared with CG, FC of ST patients between INS and left superior temporal gyrus and right hippocampal significantly decreased before treatment, while FC of ST patients between INS and right superior frontal gyrus, left middle frontal gyrus and right anterior cuneus significantly decreased after treatment (voxel p < 0.001, cluster p < 0.05, corrected with GRF). FC of ST patients between the INS and right middle frontal gyrus, left superior frontal gyrus and right paracentral lobule showed a significant decrease after treatment (voxel p < 0.001, cluster p < 0.05, corrected with GRF). In addition, THI score in EG was negatively correlated with the reduction of FC value in INS-left superior frontal gyrus before treatment (r = −0.41, p = 0.017). Therefore, this study suggests that abnormal FC of INS may be one of the significant central mechanisms of ST patients and can be modulated by electroacupuncture. Discussion: Electroacupuncture treatment can effectively reduce or eliminate tinnitus symptoms in ST patients and improve the hearing by decreasing FC between the INS and the frontal and temporal brain regions. Copyright © 2024 Zha, Zhang, Shi, Cheng, Rong, Yu, Liu, Xue, Ye, Yang, Qiu and Yang.",scopus,2-s2.0-85189620759,10.3389/fneur.2024.1373390
an examination of clinical uptake factors for remote hearing aid support a concept mapping study with audiologists,"337. Int J Audiol. 2021 Apr;60(sup1):S13-S22. doi: 10.1080/14992027.2020.1795281. Epub 2020 Aug 4.An examination of clinical uptake factors for remote hearing aid support: a concept mapping study with audiologists.Glista D(1)(2), O'Hagan R(2), Moodie S(1)(2), Scollie S(1)(2).Author information:(1)The School of Communication Sciences and Disorders, Faculty of Health Sciences, The University of Western Ontario, London, Canada.(2)The National Centre for Audiology, The University of Western Ontario, London, Canada.OBJECTIVE: To develop a conceptual framework around the factors that influence audiologists in the clinical uptake of remote follow-up hearing aid support services.DESIGN: A purposive sample of 42 audiologists, stratified according to client-focus of either paediatric or adult, were recruited from professional associations in Ontario, Canada, as members of the six-step, participatory-based concept mapping process. Analyses included multidimensional scaling and hierarchical cluster analysis.RESULTS: Six main themes emerged from this research according to overall level of importance: (1) technology and infrastructure; (2) audiologist-centred considerations; (3) hearing healthcare regulations; (4) client-centred considerations; (5) clinical implementation considerations; and (6) financial considerations. Subthemes were identified at the group-level and by subgroup. These highlight the importance of TECH factors (accessible Technology, Easy to use, robust Connection, and Help available), as well as the multi-faceted nature of the perceived attitudes/aptitudes across stakeholders.CONCLUSION: Findings can be utilised in tailored planning and development efforts to support future research, knowledge dissemination, best-practice protocol/guideline development, and related training to assist in the clinical uptake of remote follow-up hearing aid support services, across variable practice contexts.DOI: 10.1080/14992027.2020.1795281",pubmed,32749182,10.1080/14992027.2020.1795281
towards effective assessment of normal hearing function from abr using a timevariant sweeptone stimulus approach,"167. Physiol Meas. 2021 May 13;42(4). doi: 10.1088/1361-6579/abcdf2.Towards effective assessment of normal hearing function from ABR using a time-variant sweep-tone stimulus approach.Jiang Y(1)(2)(3), Samuel OW(1)(3), Zhang H(1)(2)(3), Chen S(1)(2)(3), Li G(1)(2)(3).Author information:(1)CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology (SIAT), Chinese Academy of Sciences (CAS), and the SIAT Branch, Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, People's Republic of China.(2)Shenzhen College of Advanced Technology, University of Chinese Academy of Sciences, Shenzhen, People's Republic of China.(3)Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen, People's Republic of China.Objective. The auditory brainstem response (ABR) audiometry is a means of assessing the functional status of the auditory neural pathway in the clinic. The conventional click ABR test lacks good neural synchrony and it mainly evaluates high-frequency hearing while the common tone-burst ABR test only detects hearing loss of a certain frequency at a time. Additionally, the existing chirp stimuli are designed based on average data of cochlear characteristics, ignoring individual differences amongst subjects.Approach. Therefore, this study designed a new stimulus approach based on a sweep-tone concept with a time variant and spectrum characteristics that could be customized based on an individual's cochlear characteristics. To validate the efficiency of the proposed method, we compared its performance with the click and tone-bursts using ABR recordings from 11 normal-hearing adults.Main results. Experimental results showed that the proposed sweep-tone ABR achieved a higher amplitude compared with those elicited by the click and tone-bursts. When the stimulus level or rate was varied, the sweep-tone ABR consistently elicited a larger response than the corresponding click ABR. Moreover, the sweep-tone ABR appeared earlier than the click ABR under the same conditions. Specifically, the mean wave V peak-to-peak amplitude of the sweep-tone ABR was 1.3 times that of the click ABR at 70 dB nHL (normal hearing level) and a rate of 20 s-1, in which the former saved 40% of test time.Significance. In summary, the proposed sweep-tone approach is found to be more efficient than the traditional click and tone-burst in eliciting ABR.© 2021 Institute of Physics and Engineering in Medicine.DOI: 10.1088/1361-6579/abcdf2",pubmed,33238252,10.1088/1361-6579/abcdf2
prevention of noiseinduced hearing loss in vivo continuous application of insulinlike growth factor 1 and its effect on inner ear synapses auditory function and perilymph proteins,"215. Int J Mol Sci. 2022 Dec 24;24(1):291. doi: 10.3390/ijms24010291.Prevention of Noise-Induced Hearing Loss In Vivo: Continuous Application of Insulin-like Growth Factor 1 and Its Effect on Inner Ear Synapses, Auditory Function and Perilymph Proteins.Malfeld K(1)(2), Armbrecht N(1), Pich A(3), Volk HA(2), Lenarz T(1)(4), Scheper V(1)(4).Author information:(1)Department of Otolaryngology, Hannover Medical School, Carl-Neuberg-Str. 1, 30625 Hannover, Germany.(2)Department of Small Animal Medicine and Surgery, University of Veterinary Medicine Hannover, Foundation, 30559 Hannover, Germany.(3)Core Facility Proteomics, Hannover Medical School, Carl-Neuberg-Str. 1, 30625 Hannover, Germany.(4)Cluster of Excellence ""Hearing4all"", German Research Foundation (DFG; ""Deutsche Forschungsgemeinschaft""), Hannover Medical School, Carl-Neuberg-Str. 1, 30625 Hannover, Germany.As noise-induced hearing loss (NIHL) is a leading cause of occupational diseases, there is an urgent need for the development of preventive and therapeutic interventions. To avoid user-compliance-based problems occurring with conventional protection devices, the pharmacological prevention is currently in the focus of hearing research. Noise exposure leads to an increase in reactive oxygen species (ROS) in the cochlea. This way antioxidant agents are a promising option for pharmacological interventions. Previous animal studies reported preventive as well as therapeutic effects of Insulin-like growth factor 1 (IGF-1) in the context of NIHL. Unfortunately, in patients the time point of the noise trauma cannot always be predicted, and additive effects may occur. Therefore, continuous prevention seems to be beneficial. The present study aimed to investigate the preventive potential of continuous administration of low concentrations of IGF-1 to the inner ear in an animal model of NIHL. Guinea pigs were unilaterally implanted with an osmotic minipump. One week after surgery they received noise trauma, inducing a temporary threshold shift. Continuous IGF-1 delivery lasted for seven more days. It did not lead to significantly improved hearing thresholds compared to control animals. Quite the contrary, there is a hint for a higher noise susceptibility. Nevertheless, changes in the perilymph proteome indicate a reduced damage and better repair mechanisms through the IGF-1 treatment. Thus, future studies should investigate delivery methods enabling continuous prevention but reducing the risk of an overdosage.DOI: 10.3390/ijms24010291PMCID: PMC9820558",pubmed,36613734,10.3390/ijms24010291
regional homogeneity and functional connectivity in restingstate brain activity in tinnitus patients,"Objective: Subjective tinnitus is characterized by the perception of sound in the absence of any external auditory stimuli. This perceived sound may be related to altered intrinsic neural activity generated along the central auditory pathway. This retrospective study was designed to investigate regional homogeneity and functional connectivity in the resting-state brain activity of patients with tinnitus. Methods: We recruited tinnitus patients with normal hearing or mild hearing loss (n = 17) and age-matched healthy controls (n = 20), and examined regional homogeneity and functional connectivity in resting-state brain activity using resting-state functional magnetic resonance imaging data. The present study protocol was approved by the Institutional Review Board on Experimental Ethics at Sun Yat-sen University, China (approval No. SYSEC-KY-KS-2019-083). Results: Compared with normal controls, patients with tinnitus had significantly decreased regional homogeneity in the anterior lobe of the cerebellum and increased homogeneity in the inferior frontal gyrus (P < 0.05 corrected at a cluster-level). In addition, tinnitus patients showed enhanced functional connectivity between the inferior frontal gyrus and the ventral striatum and midbrain, as well as increased connectivity between the cerebellum and ventromedial prefrontal cortex (P < 0.05 corrected at a cluster-level). We also found decreased connectivity between the cerebellum and the anterior insula compared with controls (P < 0.05 corrected at a cluster-level). Conclusion: Abnormal connectivity in non-auditory brain structures, particularly those related to emotion processing, may be associated with tinnitus persistence. Copyright © 2020 The Chinese Medical Association, Published by Wolters Kluwer Health, Inc",scopus,2-s2.0-85137970014,10.1097/JBR.0000000000000047
response profiles of murine spiral ganglion neurons on multielectrode arrays,"827. J Neural Eng. 2016 Feb;13(1):016011. doi: 10.1088/1741-2560/13/1/016011. Epub 2015 Dec 14.Response profiles of murine spiral ganglion neurons on multi-electrode arrays.Hahnewald S(1), Tscherter A, Marconi E, Streit J, Widmer HR, Garnham C, Benav H, Mueller M, Löwenheim H, Roccio M, Senn P.Author information:(1)Inner Ear Research Laboratory, University Departments of Clinical Research and Otorhinolaryngology, Head & Neck Surgery, Inselspital, University of Bern, Switzerland. Regenerative Neuroscience Cluster, Department of Clinical Research, University of Bern, Switzerland.OBJECTIVE: Cochlear implants (CIs) have become the gold standard treatment for deafness. These neuroprosthetic devices feature a linear electrode array, surgically inserted into the cochlea, and function by directly stimulating the auditory neurons located within the spiral ganglion, bypassing lost or not-functioning hair cells. Despite their success, some limitations still remain, including poor frequency resolution and high-energy consumption. In both cases, the anatomical gap between the electrode array and the spiral ganglion neurons (SGNs) is believed to be an important limiting factor. The final goal of the study is to characterize response profiles of SGNs growing in intimate contact with an electrode array, in view of designing novel CI devices and stimulation protocols, featuring a gapless interface with auditory neurons.APPROACH: We have characterized SGN responses to extracellular stimulation using multi-electrode arrays (MEAs). This setup allows, in our view, to optimize in vitro many of the limiting interface aspects between CIs and SGNs.MAIN RESULTS: Early postnatal mouse SGN explants were analyzed after 6-18 days in culture. Different stimulation protocols were compared with the aim to lower the stimulation threshold and the energy needed to elicit a response. In the best case, a four-fold reduction of the energy was obtained by lengthening the biphasic stimulus from 40 μs to 160 μs. Similarly, quasi monophasic pulses were more effective than biphasic pulses and the insertion of an interphase gap moderately improved efficiency. Finally, the stimulation with an external electrode mounted on a micromanipulator showed that the energy needed to elicit a response could be reduced by a factor of five with decreasing its distance from 40 μm to 0 μm from the auditory neurons.SIGNIFICANCE: This study is the first to show electrical activity of SGNs on MEAs. Our findings may help to improve stimulation by and to reduce energy consumption of CIs and thereby contribute to the development of fully implantable devices with better auditory resolution in the future.DOI: 10.1088/1741-2560/13/1/016011",pubmed,26656212,10.1088/1741-2560/13/1/016011
hearing threshold distribution and effect of screening in a populationbased german sample,"83. Int J Audiol. 2016;55(2):110-25. doi: 10.3109/14992027.2015.1084054. Epub 2015 Sep 29.Hearing threshold distribution and effect of screening in a population-based German sample.von Gablenz P(1), Holube I(1).Author information:(1)a Institute of Hearing Technology and Audiology and Cluster of Excellence ""Hearing4All"" , Oldenburg , Germany.OBJECTIVE: To establish the status of hearing in adults in Germany and the effects of screening for noise, tinnitus, ear diseases, and general health on the distribution of hearing threshold levels (HTL) DESIGN: A cross-sectional epidemiological study conducted between 2010 and 2012 in two middle-sized cities.STUDY SAMPLE: A total of 1903 adults aged 18 to 97 years from a randomized sample drawn from the local registration offices and stratified for age and gender.RESULTS: Dispersion and distribution of HTL data observed in the population-based sample are well in line with international results. However, median HTL tend to be better than in most recent international studies. Screening for ""otological normality"" improves the median HTL overall by 3 dB in males and 1 dB in females. This effect is strongly age-dependent in males and far less pronounced in females. While by and large HTL medians of females in the screened sample meet the values expected by ISO 7029:2000, HTL medians of males in middle and higher age cohorts are better than expected, especially in the frequencies above 2 kHz.CONCLUSIONS: This study supports international findings that in males, the age-related decrease in hearing sensitivity at high frequencies is smaller than described by ISO 7029:2000.DOI: 10.3109/14992027.2015.1084054",pubmed,26418731,10.3109/14992027.2015.1084054
a neural network model for optimizing vowel recognition by cochlear implant listeners,"468. IEEE Trans Neural Syst Rehabil Eng. 2001 Mar;9(1):42-8. doi: 10.1109/7333.918275.A neural network model for optimizing vowel recognition by cochlear implant listeners.Chang CH(1), Anderson GT, Loizou PC.Author information:(1)Applied Science Department at the University of Arkansas at Little Rock, 72204, USA.Due to the variability in performance among cochlear implant (CI) patients, it is becoming increasingly important to find ways to optimally fit patients with speech processing strategies. This paper proposes an approach based on neural networks, which can be used to automatically optimize the performance of CI patients. The neural network model is implemented in two stages. In the first stage, a neural network is trained to mimic the CI patient's performance on the vowel identification task. The trained neural network is then used in the second stage to adjust a free parameter to improve vowel recognition performance for each individual patient. The parameter examined in this study was a weighting function applied to the compressed channel amplitudes extracted from a 6-channel continuous interleaved sampling (CIS) strategy. Two types of weighting functions were examined, one which assumed channel interaction, and one which assumed no interaction between channels. Results showed that the neural network models closely matched the performance of five Med-EI/CIS-Link implant patients. The resulting weighting functions obtained after neural network training improved vowel performance, with the larger improvement (4%) attained by the weighting function which modeled channel interaction.DOI: 10.1109/7333.918275",pubmed,11482362,10.1109/7333.918275
automatic detection and classification of hearing loss conditions using an artificial neural network approach,"The auditory dysfunction is one of the most frequent disabilities, this condition can be diagnosed with an electroencephalogram modality called auditory evoked potentials (AEP). In this paper, we present a machine learning implementation to automatically detect and classify hearing loss conditions based on features extracted from synthetically generated brainstem auditory evoked potentials, a necessity given the scarcity of full-fledged datasets. The approach is based on a multi-player perceptron, which has demonstrated to be a useful and powerful tool in this domain. Preliminary results show very encouraging results, with accuracy results above 90% for a variety of hearing loss conditions; this system is to be deployed as hardware implementation for creating an affordable and portable medical device, as reported in previous work. © 2019, Springer Nature Switzerland AG.",scopus,2-s2.0-85068345233,10.1007/978-3-030-21077-9_21
left hemisphere fractional anisotropy increase in noiseinduced tinnitus a diffusion tensor imaging dti study of white matter tracts in the brain,"397. Hear Res. 2014 Mar;309:8-16. doi: 10.1016/j.heares.2013.10.005. Epub 2013 Nov 8.Left hemisphere fractional anisotropy increase in noise-induced tinnitus: a diffusion tensor imaging (DTI) study of white matter tracts in the brain.Benson RR(1), Gattu R(2), Cacace AT(3).Author information:(1)Center for Neurological Studies, Novi, MI, USA.(2)Department of Radiology, Wayne State University School of Medicine, Detroit, MI, USA.(3)Department of Communication Sciences & Disorders, Wayne State University, 207 Rackham, 60 Farnsworth, Detroit, MI 48202, USA. Electronic address: cacacea@wayne.edu.Diffusion tensor imaging (DTI) is a contemporary neuroimaging modality used to study connectivity patterns and microstructure of white matter tracts in the brain. The use of DTI in the study of tinnitus is a relatively unexplored methodology with no studies focusing specifically on tinnitus induced by noise exposure. In this investigation, participants were two groups of adults matched for etiology, age, and degree of peripheral hearing loss, but differed by the presence or absence (+/-) of tinnitus. It is assumed that matching individuals on the basis of peripheral hearing loss, allows for differentiating changes in white matter microstructure due to hearing loss from changes due to the effects of chronic tinnitus. Alterations in white matter tracts, using the fractional anisotropy (FA) metric, which measures directional diffusion of water, were quantified using tract-based spatial statistics (TBSS) with additional details provided by in vivo probabilistic tractography. Our results indicate that 10 voxel clusters differentiated the two groups, including 9 with higher FA in the group with tinnitus. A decrease in FA was found for a single cluster in the group with tinnitus. However, seven of the 9 clusters with higher FA were in left hemisphere thalamic, frontal, and parietal white matter. These foci were localized to the anterior thalamic radiations and the inferior and superior longitudinal fasciculi. The two right-sided clusters with increased FA were located in the inferior fronto-occipital fasciculus and superior longitudinal fasciculus. The only decrease in FA for the tinnitus-positive group was found in the superior longitudinal fasciculus of the left parietal lobe.Copyright © 2013 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2013.10.005",pubmed,24212050,10.1016/j.heares.2013.10.005
dispositional optimism dysphoria health and coping with hearing impairment in elderly adults original paper,"Sixty-eight elderly hearing impaired subjects were interviewed and completed self-report measures on hearing disability, dispositional optimism, dysphoria, and general health. The measures used were the Hearing Coping Assessment (HCA), the Hearing Questions (HQ), the Life Orientation Test (LOT), The Beck Depression Inventory (BDI), and a subscale from the Göteborg Quality of Life (GQL). Psychometric analyses of HCA, HQ, LOT, BDI, and GQL revealed high reliability in terms of Cronbach's Alpha and split-half r's. Significant inter-correlations were found between several measures, but not with pure-tone audiometry (0.5,1,2, and 3 kHz). Cluster analysis was used to identify subgroups in the sample. As a result three clusters were identified interpreted as 'high copers', 'copers with moderate psychological and somatic complaints', and 'low copers'. Results from the cluster analyses were confirmed by using two different clustering methods and by between-cluster comparisons on the HQ, which had not been used to obtain the clusters. © 1995 Informa UK Ltd All rights reserved: reproduction in whole or part not permitted.",scopus,2-s2.0-0029155978,10.3109/00206099509071900
database description russian fricatives recorded in 198 real speech sentences from 59 speakers,"840. Data Brief. 2023 May 11;48:109205. doi: 10.1016/j.dib.2023.109205. eCollection 2023 Jun.Database description: Russian fricatives recorded in 198 real speech sentences from 59 speakers.Ulrich N(1).Author information:(1)Lab Dynamics of Language UMR 5596, CNRS and University Lyon 2, France.This speech dataset is primarily designed to investigate linguistic and speaker information in fricative sounds in Russian. Acoustic recordings were obtained from 59 students (30 females and 29 males) between 18 and 30 years. Eighteen participants were recorded in a second session. The participants were born and lived since their early childhood in St. Petersburg. The participants did not report any speech or hearing impairment. The recording sessions were conducted at the phonetic laboratory of the Phonetic Institute in St. Petersburg, in an audiometric booth using the recording program Speech-Recorder version 3.28.0 at a sample rate of 44.1 kHz (16-bit encoding). During the recordings, a clip-on microphone (Sennheiser MKE 2-P) was placed at a distance of 15cm from the speakers' mouth and connected through an audio interface (Zoom U-22) to a laptop computer. The participants were instructed to read 198 randomized sentences from a computer screen. The fricatives [f], [s], [ʃ], [x], [v], [z], [ʒ], [sʲ], [ɕ], [vʲ], [zʲ] were embedded into those sentences. Two sentence structures were designed to obtain each real-word lexemes produced in three different contexts. The first type of sentence is a so-called carrier sentence with the structure of ""She said ""X"" and not ""Y"" "". Minimal pairs of real words, containing one of the 11 tested fricatives were placed in both ""X"" and ""Y"" positions. The second type of pre-designed sentence was a natural language sentence including each of the lexemes. All raw audio files were first automatically pre-processed by applying the online tool Munich Automatic Segmentation system. Then, the files of the first recording session were filtered below 80 and above 20050 Hz, and the boundaries were manually corrected using Praat. The dataset consists of 22,561 fricative tokens. The number of observations per sound differs across categories, because of their natural distribution. The dataset is made available as a collection of audio files in wav format along with companion Praat TextGrid files for each sentence. Target fricatives are furthermore available as individual wav files. The whole dataset can be accessed with the DOI https://doi.org/10.48656/4q9c-gz16. Additionally, the experimental design allows the investigation of other sound categories. The number of speakers recorded gives further possibilities for phonetic-oriented speaker identification studies.© 2023 The Author(s).DOI: 10.1016/j.dib.2023.109205PMCID: PMC10293980",pubmed,37383770,10.1016/j.dib.2023.109205
auditory inspired machine learning techniques can improve speech intelligibility and quality for hearingimpaired listeners,"129. J Acoust Soc Am. 2017 Mar;141(3):1985. doi: 10.1121/1.4977197.Auditory inspired machine learning techniques can improve speech intelligibility and quality for hearing-impaired listeners.Monaghan JJ(1), Goehring T(1), Yang X(1), Bolner F(2), Wang S(1), Wright MC(1), Bleeck S(1).Author information:(1)Institute of Sound and Vibration Research, University of Southampton, Southampton, United Kingdom.(2)ExpORL, Katholieke Universiteit Leuven, Leuven, Belgium.Machine-learning based approaches to speech enhancement have recently shown great promise for improving speech intelligibility for hearing-impaired listeners. Here, the performance of three machine-learning algorithms and one classical algorithm, Wiener filtering, was compared. Two algorithms based on neural networks were examined, one using a previously reported feature set and one using a feature set derived from an auditory model. The third machine-learning approach was a dictionary-based sparse-coding algorithm. Speech intelligibility and quality scores were obtained for participants with mild-to-moderate hearing impairments listening to sentences in speech-shaped noise and multi-talker babble following processing with the algorithms. Intelligibility and quality scores were significantly improved by each of the three machine-learning approaches, but not by the classical approach. The largest improvements for both speech intelligibility and quality were found by implementing a neural network using the feature set based on auditory modeling. Furthermore, neural network based techniques appeared more promising than dictionary-based, sparse coding in terms of performance and ease of implementation.DOI: 10.1121/1.4977197",pubmed,28372043,10.1121/1.4977197
perceived industrial deafness and hearing loss among people in a small queensland rural community,"396. Southeast Asian J Trop Med Public Health. 2005 Jul;36(4):1048-56.Perceived industrial deafness and hearing loss among people in a small Queensland rural community.Jirojwong S(1), Joubert D, Anastasi S; Wowan/Dululu Community Volunteer Group Inc.Author information:(1)Faculty of Arts, Health and Sciences, Central Queensland University, Rockhampton QLD, Australia. s.jirojwong@cqu.edu.auThis paper aims to describe chronic diseases including hearing loss reported by people in a small rural community. It will present the results of audiometric screening among a group of people in this community and their self reported risk factors of hearing loss. Different risk factors experienced by men and women will be compared. Two surveys were conducted in a small Queensland rural community. The first survey gathered information relating to chronic diseases among 604 people using a telephone interview method. The second survey assessed the level of hearing among 64 people who presented themselves for audiometric screening, their history of exposure to loud noise and their previous use of hearing protective measures. A higher rate of ""industrial deafness"" was reported (110.75 per 1,000 population) than the 1995 National rate (95.2 per 1,000 population). Of 64 people who attended the audiometric assessment, 60 (93.8%) had some level of hearing loss using the 2000 International Standard of hearing level (ISO 7029: 2000) taking age and gender into account. However, 15 persons (23.4%) perceived that they had good hearing. When compared to ISO 7029: 2000 standard, men and women had a similar pattern of hearing loss. Compared to men, a lower percentage of women were exposed to different sources of loud noise and were less likely to use hearing protection devices.",pubmed,16295567,
an application of mapping neural networks and a digital signal processor for cochlear neuroprostheses,"401. Biol Cybern. 1993;68(6):545-52. doi: 10.1007/BF00200814.An application of mapping neural networks and a digital signal processor for cochlear neuroprostheses.Zadák J(1), Unbehauen R.Author information:(1)Lehrstuhl für Allgemeine und Theoretische Elektrotechnik, Universität Erlangen-Nürnberg, Germany.Cochlear neuroprostheses strive to restore the sensation of hearing to patients with a profound sensorineural deafness. They exhibit a stimulation of the surviving auditory nerve neurons by electrical currents delivered through electrodes placed on or within the cochlea. The present article describes a new method for an efficient derivation of the required information from the incoming speech signal necessary for the implant stimulation. Also some realization aspects of the new approach are addressed. In the new strategy, a multilayer neural network is employed in the formant frequency estimation having some suitable speech signal descriptors as particular input signals. The proposed method allows us a fast formant frequency estimation necessary for the implant stimulation. With the developed strategy, the prosthesis can be adjusted to the environment which the patient is supposed to live in. Moreover, the neural network concept offers us an alternative for dealing with the areas of neural loss or ""holes"" in the frequency map of the patient's ear.DOI: 10.1007/BF00200814",pubmed,8324062,10.1007/BF00200814
different neuroanatomical correlates for temporal and spectral suprathreshold auditory tasks and speech in noise recognition in older adults with hearing impairment references,"Varying degrees of pure-tone hearing loss in older adults are differentially associated with cortical volume (CV) and thickness (CT) within and outside of the auditory pathway. This study addressed the question to what degree supra-threshold auditory performance (i.e., temporal compression and frequency selectivity) as well as speech in noise (SiN) recognition are associated with neurostructural correlates in a sample of 59 healthy older adults with mild to moderate pure-tone hearing loss. Using surface-based morphometry on T1-weighted MRI images, CT, CV, and surface area (CSA) of several regions-of-interest were obtained. The results showed distinct neurostructural patterns for the different tasks in terms of involved regions as well as morphometric parameters. While pure-tone averages (PTAs) positively correlated with CT in a right hemisphere superior temporal sulcus and gyrus cluster, supra-threshold auditory perception additionally extended significantly to CV and CT in left and right superior temporal clusters including Heschl's gyrus and sulcus, the planum polare and temporale. For SiN recognition, we found significant correlations with an auditory-related CT cluster and furthermore with language-related areas in the prefrontal cortex. Taken together, our results show that different auditory abilities are differently associated with cortical morphology in older adults with hearing impairment. Still, a common pattern is that greater PTAs and poorer supra-threshold auditory performance as well as poorer SiN recognition are all related to cortical thinning and volume loss but not to changes in CSA. These results support the hypothesis that mostly CT undergoes alterations in the context of auditory decline, while CSA remains stable. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc22&DO=10.1111%2fejn.15922
intensive treatment of auditory analysis in a 16yearold aphasic patient hochfrequentes training der auditiven analyse bei aphasie,"Aim of this treatment study was to improve speech comprehension of a 16-year-old patient with Wernicke's aphasia and a partial disorder of auditory analysis (""word sound deafness""). To this end, tasks addressing phoneme-grapheme correspondence with syllables were used as well as speech discrimination tasks with syllables, consonant clusters and neologisms. The patient showed significant improvement with trained items and generalization effects to untrained items. Furthermore, secondary improvements could be observed in tasks which are based on the auditory analysis (e.g. lexical decision). However, performance in an untrained control task (rime generation) did not change, indicating that the effects of therapy were specific. Moreover, the patient showed an enhanced self-monitoring, evidenced by an increased rate of self-corrections. Copyright © Schulz-Kirchner Verlag, Idstein.",scopus,2-s2.0-84869756015,10.2443/skv-s-2012-53020120601
vehicle noise comparison of loudness ratings in the field and the laboratory,"21, 2017.81. Int J Audiol. 2024 Feb;63(2):117-126. doi: 10.1080/14992027.2022.2147867. Epub 2022 Dec 13.Vehicle noise: comparison of loudness ratings in the field and the laboratory.Llorach G(1)(2)(3), Oetting D(1)(2), Vormann M(1)(2), Meis M(1)(2), Hohmann V(1)(2)(3).Author information:(1)Development, Hörzentrum Oldenburg gGmbH, Oldenburg, Germany.(2)Cluster of Excellence Hearing4All, Department of Medical Physics and Acoustics, University of Oldenburg, Oldenburg, Germany.(3)Auditory Signal Processing, Department of Medical Physics and Acoustics, University of Oldenburg, Oldenburg, Germany.OBJECTIVE: Distorted loudness perception is one of the main complaints of hearing aid users. Measuring loudness perception in the clinic as experienced in everyday listening situations is important for loudness-based hearing aid fitting. Little research has been done comparing loudness perception in the field and in the laboratory.DESIGN: Participants rated the loudness in the field and in the laboratory of 36 driving actions. The field measurements were recorded with a 360° camera and a tetrahedral microphone. The recorded stimuli, which are openly accessible, were presented in three conditions in the laboratory: 360° video recordings with a head-mounted display, video recordings with a desktop monitor and audio-only.STUDY SAMPLES: Thirteen normal-hearing participants and 18 hearing-impaired participants with hearing aids.RESULTS: The driving actions were rated as louder in the laboratory than in the field for the condition with a desktop monitor and for the audio-only condition. The less realistic a laboratory condition was, the more likely it was for a participant to rate a driving action as louder. The field-laboratory loudness differences were bigger for louder sounds.CONCLUSION: The results of this experiment indicate the importance of increasing realism and immersion when measuring loudness in the clinic.DOI: 10.1080/14992027.2022.2147867",pubmed,36512479,10.1080/14992027.2022.2147867
expediting the identification of impaired channels in cochlear implants via analysis of speechbased confusion matrices,"454. IEEE Trans Biomed Eng. 2007 Dec;54(12):2193-204. doi: 10.1109/tbme.2007.908336.Expediting the identification of impaired channels in cochlear implants via analysis of speech-based confusion matrices.Remus JJ(1), Throckmorton CS, Collins LM.Author information:(1)Department of Electrical and Computer Engineering, Duke University, Durham, NC 27708-0291, USA.There is significant variability in the benefit provided by cochlear implants to severely deafened individuals. The reasons why some subjects exhibit low speech recognition scores are unknown; however, underlying physiological or psychophysical factors may be involved. Certain phenomena, such as indiscriminable electrodes and nonmonotonic pitch rankings, might hint at limitations in the ability of individual channels in the cochlear implant and/or sensorineural pathway to convey speech information. In this paper, four approaches for analyzing the results of a simple listening test using speech stimuli are investigated for the purpose of targeting channels of concern in order for follow-on psychophysical experiments to correctly identify channels performing in an ""impaired"" or anomalous manner. Listening tests were first conducted with normal-hearing subjects and acoustic models simulating channel-specific anomalies. Results indicate that these proposed analyses perform significantly better than chance in providing information about the location of anomalous channels. Vowel and consonant confusion matrices from six cochlear implant subjects were also analyzed to test the robustness of the proposed analyses to variability intrinsic to cochlear implant data. The current study suggests that confusion matrix analyses have the potential to expedite the identification of impaired channels by providing preliminary information prior to exhaustive psychophysical testing.DOI: 10.1109/tbme.2007.908336",pubmed,18075035,10.1109/tbme.2007.908336
with you  indian sign language detection and alert system,"The global deaf, non-speaking, and hard-of-hearing community relies heavily on sign language to facilitate communication. This paper will delve into the development of a detection system for the Indian Sign Language (ISL) that puts convenience and inclusivity first for this group. To accomplish this goal, the system uses a Long Short-Term Memory (LSTM) artificial neural network. Currently, the system can recognize 6 dynamic signs, 6 emergency signs, and 26 static ISL alphabets. The emergency signals use certain non-verbal cues to automatically dial the designated emergency services number. The emergency's type, the user's IP address, and their location are all included in the message that is delivered. By enabling communication through ISL, this system has been created to provide accessibility for the community of people with hearing disabilities. The artificial neural network enables the system to acquire the capability of learning and identifying various signals. Additionally, the use of emergency indicators can be extremely important in circumstances when verbal communication is impossible as the system can immediately notify emergency services and give them the information they need, to deliver quick aid. Overall, the hearing-impaired people in India may benefit greatly from this system's contributions to their safety and well-being.",ieee,,10.1109/ESCI59607.2024.10497366
development of novel musical stimuli to investigate the perception of musical emotions in individuals with hearing loss,"577. J Korean Med Sci. 2023 Mar 27;38(12):e82. doi: 10.3346/jkms.2023.38.e82.Development of Novel Musical Stimuli to Investigate the Perception of Musical Emotions in Individuals With Hearing Loss.Lee J(1)(2), Han JH(1)(2), Lee HJ(1)(2)(3).Author information:(1)Laboratory of Brain & Cognitive Sciences for Convergence Medicine, Hallym University College of Medicine, Anyang, Korea.(2)Ear and Interaction Center, Doheun Institute for Digital Innovation in Medicine (D.I.D.I.M.), Hallym University Medical Center, Anyang, Korea.(3)Department of Otorhinolaryngology, Hallym University College of Medicine, Chuncheon, Korea. hyojlee@hallym.ac.kr.BACKGROUND: Many studies have examined the perception of musical emotion using excerpts from familiar music that includes highly expressed emotions to classify emotional choices. However, using familiar music to study musical emotions in people with acquired hearing loss could produce ambiguous results as to whether the emotional perception is due to previous experiences or listening to the current musical stimuli. To overcome this limitation, we developed new musical stimuli to study emotional perception without the effects of episodic memory.METHODS: A musician was instructed to compose five melodies with evenly distributed pitches around 1 kHz. The melodies were created to express the emotions of happy, sad, angry, tender, and neutral. To evaluate whether these melodies expressed the intended emotions, two methods were applied. First, we classified the expressed emotions of melodies with selected musical features from 60 features using genetic algorithm-based k-nearest neighbors. Second, forty-four people with normal hearing participated in an online survey regarding the emotional perception of music based on dimensional and discrete approaches to evaluate the musical stimuli set.RESULTS: Twenty-four selected musical features produced classification for intended emotions with an accuracy of 76%. The results of the online survey in the normal hearing (NH) group showed that the intended emotions were selected significantly more often than the others. K-means clustering analysis revealed that melodies with arousal and valence ratings corresponded to representative quadrants of interest. Additionally, the applicability of the stimuli was tested in 4 individuals with high-frequency hearing loss.CONCLUSION: By applying the individuals with NH, the musical stimuli were shown to classify emotions with high accuracy, as expressed. These results confirm that the set of musical stimuli can be used to study the perceived emotion in music, demonstrating the validity of the musical stimuli, independent of innate musical bias such as due to episodic memory. Furthermore, musical stimuli could be helpful for further studying perceived musical emotion in people with hearing loss because of the controlled pitch for each emotion.© 2023 The Korean Academy of Medical Sciences.DOI: 10.3346/jkms.2023.38.e82PMCID: PMC10042730",pubmed,36974396,10.3346/jkms.2023.38.e82
simulation system of cochlear implant,"Different types of hearing loss can be treated according to the etiology and degree. Commonly used methods include drug therapy, surgical treatment, wearing hearing aids and implanting electrical stimulation equipment. Among them, cochlear implant is an effective way to restore the hearing perception ability of patients with very severe deafness and total deafness, and it is the most commonly used electrical stimulation implant device. Cochlear implant is still not widely used in China because of its high price and long training period. In order to facilitate speech training and improve speech perception ability of cochlear implant, this paper designs a speech training and speech simulation system of cochlear implant. The designed software and hardware system is a simulation test platform, with low price, simple test mode, and potential huge market value and application value. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.",scopus,2-s2.0-85101524891,10.1007/978-3-030-66785-6_32
predicting consonant recognition and confusions in normalhearing listeners,"740. J Acoust Soc Am. 2017 Feb;141(2):1051. doi: 10.1121/1.4976054.Predicting consonant recognition and confusions in normal-hearing listeners.Zaar J(1), Dau T(1).Author information:(1)Hearing Systems Group, Department of Electrical Engineering, Technical University of Denmark, DK-2800 Kongens Lyngby, Denmark.The perception of consonants in background noise has been investigated in various studies and was shown to critically depend on fine details in the stimuli. In this study, a microscopic speech perception model is proposed that represents an extension of the auditory signal processing model by Dau, Kollmeier, and Kohlrausch [(1997). J. Acoust. Soc. Am. 102, 2892-2905]. The model was evaluated based on the extensive consonant perception data set provided by Zaar and Dau [(2015). J. Acoust. Soc. Am. 138, 1253-1267], which was obtained with normal-hearing listeners using 15 consonant-vowel combinations mixed with white noise. Accurate predictions of the consonant recognition scores were obtained across a large range of signal-to-noise ratios. Furthermore, the model yielded convincing predictions of the consonant confusion scores, such that the predicted errors were clustered in perceptually plausible confusion groups. The large predictive power of the proposed model suggests that adaptive processes in the auditory preprocessing in combination with a cross-correlation based template-matching back end can account for some of the processes underlying consonant perception in normal-hearing listeners. The proposed model may provide a valuable framework, e.g., for investigating the effects of hearing impairment and hearing-aid signal processing on phoneme recognition.DOI: 10.1121/1.4976054",pubmed,28253684,10.1121/1.4976054
sensorineural hearing loss affects functional connectivity of the auditory cortex parahippocampal gyrus and inferior prefrontal gyrus in tinnitus patients,"683. Front Neurosci. 2022 Apr 1;16:816712. doi: 10.3389/fnins.2022.816712. eCollection 2022.Sensorineural Hearing Loss Affects Functional Connectivity of the Auditory Cortex, Parahippocampal Gyrus and Inferior Prefrontal Gyrus in Tinnitus Patients.Chen J(1), Zhao Y(1), Zou T(1), Wen X(1), Zhou X(1), Yu Y(1), Liu Z(1), Li M(1).Author information:(1)Department of Otolaryngology, The First People's Hospital of Foshan, Foshan, China.BACKGROUND: Tinnitus can interfere with a patient's speech discrimination, but whether tinnitus itself or the accompanying sensorineural hearing loss (SNHL) causes this interference is still unclear. We analyzed event-related electroencephalograms (EEGs) to observe auditory-related brain function and explore the possible effects of SNHL on auditory processing in tinnitus patients.METHODS: Speech discrimination scores (SDSs) were recorded in 21 healthy control subjects, 24 tinnitus patients, 24 SNHL patients, and 27 patients with both SNHL and tinnitus. EEGs were collected under an oddball paradigm. Then, the mismatch negativity (MMN) amplitude and latency, the clustering coefficient and average path length of the whole network in the tinnitus and SNHL groups were compared with those in the control group. Additionally, we analyzed the intergroup differences in functional connectivity among the primary auditory cortex (AC), parahippocampal gyrus (PHG), and inferior frontal gyrus (IFG).RESULTS: SNHL patients with or without tinnitus had lower SDSs than the control subjects. Compared with control subjects, tinnitus patients with or without SNHL had decreased MMN amplitudes, and SNHL patients had longer MMN latencies. Tinnitus patients without SNHL had a smaller clustering coefficient and a longer whole-brain average path length than the control subjects. SNHL patients with or without tinnitus had a smaller clustering coefficient and a longer average path length than patients with tinnitus alone. The connectivity strength from the AC to the PHG and IFG was lower on the affected side in tinnitus patients than that in control subjects; the connectivity strength from the PHG to the IFG was also lower on the affected side in tinnitus patients than that in control subjects. However, the connectivity strength from the IFG to the AC was stronger in tinnitus patients than that in the control subjects. In SNHL patients with or without tinnitus, these changes were magnified.CONCLUSION: Changes in auditory processing in tinnitus patients do not influence SDSs. Instead, SNHL might cause the activity of the AC, PHG and IFG to change, resulting in impaired speech recognition in tinnitus patients with SNHL.Copyright © 2022 Chen, Zhao, Zou, Wen, Zhou, Yu, Liu and Li.DOI: 10.3389/fnins.2022.816712PMCID: PMC9011051",pubmed,35431781,10.3389/fnins.2022.816712
machine learning technique reveals prognostic factors of vibrant soundbridge for conductive or mixed hearing loss patients,,base,6d5e172708d488f744dfcb2aa66e224591f39dbaa0cbd4d4f5caa2eae8960d6f,
decoding hearingrelated changes in older adults spatiotemporal neural processing of speech using machine learning,"659. Front Neurosci. 2020 Jul 16;14:748. doi: 10.3389/fnins.2020.00748. eCollection 2020.Decoding Hearing-Related Changes in Older Adults' Spatiotemporal Neural Processing of Speech Using Machine Learning.Mahmud MS(1), Ahmed F(1), Al-Fahad R(1), Moinuddin KA(1), Yeasin M(1), Alain C(2)(3)(4), Bidelman GM(5)(6)(7).Author information:(1)Department of Electrical and Computer Engineering, The University of Memphis, Memphis, TN, United States.(2)Rotman Research Institute-Baycrest Centre for Geriatric Care, Toronto, ON, Canada.(3)Department of Psychology, University of Toronto, Toronto, ON, Canada.(4)Institute of Medical Sciences, University of Toronto, Toronto, ON, Canada.(5)Institute for Intelligent Systems, University of Memphis, Memphis, TN, United States.(6)School of Communication Sciences and Disorders, University of Memphis, Memphis, TN, United States.(7)Department of Anatomy and Neurobiology, University of Tennessee Health Science Center, Memphis, TN, United States.Speech perception in noisy environments depends on complex interactions between sensory and cognitive systems. In older adults, such interactions may be affected, especially in those individuals who have more severe age-related hearing loss. Using a data-driven approach, we assessed the temporal (when in time) and spatial (where in the brain) characteristics of cortical speech-evoked responses that distinguish older adults with or without mild hearing loss. We performed source analyses to estimate cortical surface signals from the EEG recordings during a phoneme discrimination task conducted under clear and noise-degraded conditions. We computed source-level ERPs (i.e., mean activation within each ROI) from each of the 68 ROIs of the Desikan-Killiany (DK) atlas, averaged over a randomly chosen 100 trials without replacement to form feature vectors. We adopted a multivariate feature selection method called stability selection and control to choose features that are consistent over a range of model parameters. We use parameter optimized support vector machine (SVM) as a classifiers to investigate the time course and brain regions that segregate groups and speech clarity. For clear speech perception, whole-brain data revealed a classification accuracy of 81.50% [area under the curve (AUC) 80.73%; F1-score 82.00%], distinguishing groups within ∼60 ms after speech onset (i.e., as early as the P1 wave). We observed lower accuracy of 78.12% [AUC 77.64%; F1-score 78.00%] and delayed classification performance when speech was embedded in noise, with group segregation at 80 ms. Separate analysis using left (LH) and right hemisphere (RH) regions showed that LH speech activity was better at distinguishing hearing groups than activity measured in the RH. Moreover, stability selection analysis identified 12 brain regions (among 1428 total spatiotemporal features from 68 regions) where source activity segregated groups with >80% accuracy (clear speech); whereas 16 regions were critical for noise-degraded speech to achieve a comparable level of group segregation (78.7% accuracy). Our results identify critical time-courses and brain regions that distinguish mild hearing loss from normal hearing in older adults and confirm a larger number of active areas, particularly in RH, when processing noise-degraded speech information.Copyright © 2020 Mahmud, Ahmed, Al-Fahad, Moinuddin, Yeasin, Alain and Bidelman.DOI: 10.3389/fnins.2020.00748PMCID: PMC7378401",pubmed,32765215,10.3389/fnins.2020.00748
cochlear aging disrupts the correlation between spontaneous rate and soundlevel coding in auditory nerve fibers,"691. J Neurophysiol. 2023 Sep 1;130(3):736-750. doi: 10.1152/jn.00090.2023. Epub 2023 Aug 16.Cochlear aging disrupts the correlation between spontaneous rate- and sound-level coding in auditory nerve fibers.Heeringa AN(1)(2), Teske F(2), Ashida G(1)(2), Köppl C(1)(2).Author information:(1)Cluster of Excellence ""Hearing4all,"" Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg, Oldenburg, Germany.(2)Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg, Oldenburg, Germany.The spiking activity of auditory nerve fibers (ANFs) transmits information about the acoustic environment from the cochlea to the central auditory system. Increasing age leads to degeneration of cochlear tissues, including the sensory hair cells and stria vascularis. Here, we aim to identify the functional effects of such age-related cochlear pathologies of ANFs. Rate-level functions (RLFs) were recorded from single-unit ANFs of young adult (n = 52, 3-12 months) and quiet-aged (n = 24, >36 months) Mongolian gerbils of either sex. RLFs were used to determine sensitivity and spontaneous rates (SRs) and were classified into flat-saturating, sloping-saturating, and straight categories, as previously established. A physiologically based cochlear model, adapted for the gerbil, was used to simulate the effects of cochlear degeneration on ANF physiology. In ANFs tuned to low frequencies (<3.5 kHz), SR was lower in those of aged gerbils, while an age-related loss of low-SR fibers was evident in ANFs tuned to high frequencies. These changes in SR distribution did not affect the typical SR versus sensitivity correlation. The distribution of RLF types among low-SR fibers, however, shifted toward that of high-SR fibers, specifically showing more fast-saturating and fewer sloping-saturating RLFs. A modeled striatal degeneration, which affects the combined inner hair cell and synaptic output, reduced SR but left RLF type unchanged. An additional reduced basilar membrane gain, which decreased sensitivity, explained the changed RLF types. Overall, the data indicated age-related changes in the characteristics of single ANFs that blurred the established relationships between SR and RLF types.NEW & NOTEWORTHY Auditory nerve fibers, which connect the cochlea to the central auditory system, change their encoding of sound level in aged gerbils. In addition to a general shift to higher levels, indicative of decreased sensitivity, level coding was also differentially affected in fibers with low- and high-spontaneous rates. Loss of low-spontaneous rate fibers, combined with a general decrease of spontaneous rate, further blurs the categorization of auditory nerve fiber types in the aged gerbil.DOI: 10.1152/jn.00090.2023",pubmed,37584075,10.1152/jn.00090.2023
personalization of hearing aid fitting based on adaptive dynamic range optimization,"117. Sensors (Basel). 2022 Aug 12;22(16):6033. doi: 10.3390/s22166033.Personalization of Hearing Aid Fitting Based on Adaptive Dynamic Range Optimization.Ni A(1), Akbarzadeh S(1), Lobarinas E(2), Kehtarnavaz N(1).Author information:(1)Department of Electrical and Computer Engineering, University of Texas at Dallas, Richardson, TX 75080-3021, USA.(2)Callier Center for Communication Disorders, University of Texas at Dallas, Richardson, TX 75080-3021, USA.Adaptive dynamic range optimization (ADRO) is a hearing aid fitting rationale which involves adjusting the gains in a number of frequency bands by using a series of rules. The rules reflect the comparison of the estimated percentile occurrences of the sound levels with the audibility and comfort hearing levels of a person suffering from hearing loss. In the study reported in this paper, a previously developed machine learning method was utilized to personalize the ADRO fitting in order to provide an improved hearing experience as compared to the standard ADRO hearing aid fitting. The personalization was carried out based on the user preference model within the framework of maximum likelihood inverse reinforcement learning. The testing of ten subjects with hearing loss was conducted, which indicated that the personalized ADRO was preferred over the standard ADRO on average by about 10 times. Furthermore, a word recognition experiment was conducted, which showed that the personalized ADRO had no adverse impact on speech understanding as compared to the standard ADRO.DOI: 10.3390/s22166033PMCID: PMC9414822",pubmed,36015791,10.3390/s22166033
prediction of longterm speech outcome in prelingual deaf subjects after ci using restingstate pet15th international conference on cochlear implants and other implantable auditory technologies 27th jun 2018  30th jun 2018 antwerp belgium,"There are numerous patient factors affecting the outcome of cochlear implantation (CI). However, preoperative functional status of the cerebral cortex has only been investigated in small numbers of patients. Hence, the current study was performed to reveal functional neuroimaging signatures of speech outcome after CI in prelingually deaf patients using resting-state FDG-PET big-data based machine learning approach and to suggest a outcome prediction model based on cortical predictors of CI outcome. A total of 111 prelingually deaf children underwent pre-CI resting-state FDG-PET. This FDG-PET was used to predict post-CI 3 year speech outcome with regard to open set word and sentence test under auditory-only (A- only) and audiovisual (AV) conditions. FDG-PET data was preprocessed with MarsBaR toolbox for region of interest (ROI) analysis, and 90 cerebral cortical ROIs were used for the analysis. For statistical analysis, LASSO (Least Absolute Shrinkage and Selection Operator) regression analysis using average glucose metabolism of 90 ROIs with regard to post-CI 3 year open set word and sentence scores. In prelingually deaf CI users, activations of the suprerior temporal gyrus, supramarginal gyrus, and inferior frontal gyrus were predictors of higher post-CI 3 year speech outcome under the A-only condition. Meanwhile, under A-V condition, an additional activation of the anterior cingulate gyrus was necessary to show better speech outcome. In prelingually deaf CI users, activations of the ventral attention network and prefrontal top-down modulator are important to better process language under the A-only condition. Under A-V condition, an additional activation of the salience network is necessary to better understand multimodal information. Taken together, FDGPET- based machine learning using LASSO could predict CI outcome in prelingually deaf subjects, functional neuroimaging- based outcome prediction may be of help for precision medicine in CI subjects.",cinahl,2083389X,
late electricallyevoked compound action potentials as markers for acute microlesions of spiral ganglion neurons,"737. Hear Res. 2022 Jan;413:108057. doi: 10.1016/j.heares.2020.108057. Epub 2020 Aug 18.Late electrically-evoked compound action potentials as markers for acute micro-lesions of spiral ganglion neurons.Konerding W(1), Arenberg JG(2), Kral A(3), Baumhoff P(4).Author information:(1)Department of Experimental Otology, Hannover Medical School, Stadtfelddamm 34, 30625 Hannover, Germany. Electronic address: konerding.wiebke@mh-hannover.de.(2)Massachusetts Eye and Ear Infirmary, Harvard Medical School, 25 Shattuck Street, Boston, MA 02115, USA. Electronic address: Julie_Arenberg@MEEI.HARVARD.EDU.(3)Department of Experimental Otology, Hannover Medical School, Stadtfelddamm 34, 30625 Hannover, Germany; Cluster of Excellence ""Hearing4all"", Germany. Electronic address: kral.andrej@mh-hannover.de.(4)Department of Experimental Otology, Hannover Medical School, Stadtfelddamm 34, 30625 Hannover, Germany. Electronic address: baumhoff.peter@mh-hannover.de.Cochlear implants (CIs) are the treatment of choice for profoundly hearing impaired people. It has been proposed that speech perception in CI users is influenced by the neural health (deafferentation, demyelination and degeneration) of the cochlea, which may be heterogeneous along an individual cochlea. Several options have been put forward to account for these local differences in neural health when fitting the speech processor settings, however with mixed results. The interpretation of the results is hampered by the fact that reliable markers of locally restricted changes in spiral ganglion neuron (SGN) health are lacking. The aim of the study was (i) to establish mechanical micro-lesions in the guinea pig as a model of heterogeneous SGN deafferentation and degeneration and (ii) to assess potential electrophysiological markers that can also be used in human subjects. First, we defined the extent of micro-lesions in normal hearing animals using acoustically-evoked compound action potentials (aCAPs); second, we measured electrically-evoked CAPs (eCAPs) before and after focal lesioning in neomycin-deafened and implanted animals. Therefore, we inserted guinea pig adjusted 6-contact CIs through a cochleostomy in the scala tympani. The eCAP was recorded from a ball electrode at the round window niche in response to monopolar or bipolar, 50 µs/phase biphasic pulses of alternating anodic- and cathodic-leading polarity. To exclude the large electrical artifact from the analysis, we focused on the late eCAP component. We systematically isolated the eCAP parameter that showed local pre- versus post-lesion changes and lesion-target specificity. Histological evaluation of the cleared cochleae revealed focal damage of an average size of 0.0036 mm3 with an apical-basal span of maximal 440 µm. We found that the threshold of the late N2P2 eCAP component was significantly elevated after lesioning when stimulating at basal (near the lesion), but not apical (distant to the lesion) CI contacts. To circumvent the potentially conflicting influence of the apical-basal gradient in eCAP thresholds, we used the polarity effect (PE=cathodic-anodic) as a relative measure. During monopolar stimulation, but not bipolar stimulation, the PE was sensitive to the lesion target and showed significantly better cathodic than anodic thresholds after soma lesions. We conclude that the difference in N2P2 thresholds in response to cathodic versus anodic-leading monopolar stimulation corresponds to the presence of SGN soma damage, and may therefore be a marker for SGN loss. We consider this electrophysiological estimate of local neural health a potentially relevant tool for human applications because of the temporal separation from the stimulation artifact and possible implementation into common eCAP measurements.Copyright © 2020. Published by Elsevier B.V.DOI: 10.1016/j.heares.2020.108057",pubmed,32883545,10.1016/j.heares.2020.108057
a systematic review of machine learning approaches for classifying indian sign language gestures and facial expressions,"Indian Sign Language is the primary mode of communication known to persons who use Indian Sign Language for people who have hearing or language deficits. Different types of machine learning models are used to broaden the scope of communication for those with impairments and illiteracy. There are numerous machine learning models for analyzing gestures, postures, and facial recognition in Indian Sign Language for single-handed and double-handed signals. The present study on hand gestures, recognition, and translation intends to build an essential foundation for developing a platform to facilitate communication for the s pecially-abled with anyone. Machine learning algorithms generally focus on letter recognition or a few fundamental indicators. Communication is essential for exchanging ideas, thoughts, and feelings. Sign language is a kind of communication that uses hand motions. This is aimed toward those with impairments such as muteness and deafness. Machine learning, a branch of artificial intelligence, will aid in identifying various hand motions and predicting the language created by those inputs based on those inputs[2]. Sign language has a grammar that is unique from and independent of English. When compared to English, SL allows for far more freedom in word order. Tense is marked morphologically on verbs in English, but SL (like many other languages, such as Indian Sign Language) communicates tense lexically using temporal adverbs. The structure of ISL and English differs at the phonological level as well. Signed languages, like spoken languages, include a degree of sublexical structure that includes segments and combinatorial rules; however, phonological elements are manual rather than vocal. The way spatial information is conveyed in English and ISL differs substantially. All Deaf people are illiterate in written English. As an output, the SL text can be produced. SL is just physically executed English, where English and SL share the identical linguistic structure-that one is a straight encoding of the other. Many software designers mistakenly believe that deaf users can always access printed the English language in a user interface. Many designers feel that if auditory information is also supplied as written English, the deaf user's demands will be addressed. Prepositions such as 'in,' 'on,' and 'under' are used to indicate locative information in English, as in many other spoken languages. On the other hand, SL encodes locative and motion information via verbal classifier formulations in which hand shape morphemes define item type, and the location of the hands in signing space schematically depicts the spatial relationship between two things. Thus, English and ASL differ significantly in phonological, morphological, and syntactic areas.  © 2022 IEEE.",scopus,2-s2.0-85146492330,10.1109/ICIRCA54612.2022.9985747
microrna profiling as a methodology to diagnose mnires disease potential application of machine learning,"Objective: Diagnosis and treatment of Ménière’s disease remains a significant challenge because of our inability to understand what is occurring on a molecular level. MicroRNA (miRNA) perilymph profiling is a safe methodology and may serve as a “liquid biopsy” equivalent. We used machine learning (ML) to evaluate miRNA expression profiles of various inner ear pathologies to predict diagnosis of Ménière’s disease. Study Design: Prospective cohort study. Setting: Tertiary academic hospital. Subjects and Methods: Perilymph was collected during labyrinthectomy (Ménière’s disease, n = 5), stapedotomy (otosclerosis, n = 5), and cochlear implantation (sensorineural hearing loss [SNHL], n = 9). miRNA was isolated and analyzed with the Affymetrix miRNA 4.0 array. Various ML classification models were evaluated with an 80/20 train/test split and cross-validation. Permutation feature importance was performed to understand miRNAs that were critical to the classification models. Results: In terms of miRNA profiles for conductive hearing loss versus Ménière’s, 4 models were able to differentiate and identify the 2 disease classes with 100% accuracy. The top-performing models used the same miRNAs in their decision classification model but with different weighted values. All candidate models for SNHL versus Ménière’s performed significantly worse, with the best models achieving 66% accuracy. Ménière’s models showed unique features distinct from SNHL. Conclusions: We can use ML to build Ménière’s-specific prediction models using miRNA profile alone. However, ML models were less accurate in predicting SNHL from Ménière’s, likely from overlap of miRNA biomarkers. The power of this technique is that it identifies biomarkers without knowledge of the pathophysiology, potentially leading to identification of novel biomarkers and diagnostic tests. © American Academy of Otolaryngology–Head and Neck Surgery Foundation 2020.",scopus,2-s2.0-85087982820,10.1177/0194599820940649
altered restingstate network connectivity patterns for predicting attentional function in deaf individuals an eeg study,"Multiple aspects of brain development are influenced by early sensory loss such as deafness. Despite growing evidence of changes in attentional functions for prelingual profoundly deaf, the brain mechanisms underlying these attentional changes remain unclear. This study investigated the relationships between differential attention and the resting-state brain network difference in deaf individuals from the perspective of brain network connectivity. We recruited 36 deaf individuals and 34 healthy controls (HC). We recorded each participant's resting-state electroencephalogram (EEG) and the event-related potential (ERP) data from the Attention Network Test (ANT). The coherence (COH) method and graph theory were used to build brain networks and analyze network connectivity. First, the ERPs of analysis in task states were investigated. Then, we correlated the topological properties of the network functional connectivity with the ERPs. The results revealed a significant correlation between frontal-occipital connection in the resting state and the amplitude of alert N1 amplitude in the alpha band. Specifically, clustering coefficients and global and local efficiency correlate negatively with alert N1 amplitude, whereas the characteristic path length positively correlates with alert N1 amplitude. In addition, deaf individuals exhibited weaker frontal-occipital connections compared to the HC group. In executive control, the deaf group had longer reaction times and larger P3 amplitudes. However, the orienting function did not significantly differ from the HC group. Finally, the alert N1 amplitude in the ANT task for deaf individuals was predicted using a multiple linear regression model based on resting-state EEG network properties. Our results suggest that deafness affects the performance of alerting and executive control while orienting functions develop similarly to hearing individuals. Furthermore, weakened frontal-occipital connections in the deaf brain are a fundamental cause of altered alerting functions in the deaf. These results reveal important effects of brain networks on attentional function from the perspective of brain connections and provide potential physiological biomarkers to predicting attention. © 2023",scopus,2-s2.0-85146478626,10.1016/j.heares.2023.108696
hearing loss adaptivity of machine learning based compressive sensing speech enhancement for hearing aids,"Hearing aids are needed to compensate for various auditory losses in human speech cognition. Though many techniques have for proposed for speech enhancement for hearing aids, they are tuned for a particular loss and not robust against many losses. Most existing speech enhancement techniques lack adaptivity to various noises. This work proposes a machine learning speech enhancement technique based on compressive sensing. The proposed technique adapts its sensing to obtain noised reduced speech characteristics and then amplifies it for different hearing loss. The effectiveness of proposed solution is tested against different hearing loss and the solution is found to perform well in terms subjective and objective speech quality metrics  © 2022 IEEE.",scopus,2-s2.0-85147436527,10.1109/ICCUBEA54992.2022.10011011
feasibility and acceptability of training community health workers in ear and hearing care in malawi a cluster randomised controlled trial,"837. BMJ Open. 2017 Oct 11;7(10):e016457. doi: 10.1136/bmjopen-2017-016457.Feasibility and acceptability of training community health workers in ear and hearing care in Malawi: a cluster randomised controlled trial.Mulwafu W(1), Kuper H(2), Viste A(3), Goplen FK(3).Author information:(1)Department of Surgery, College of Medicine Blantyre Malawi, Blantyre, Malawi.(2)Department of Clinical Research, The London School of Hygiene & Tropical Medicine, London, UK.(3)Haukeland Universitetssjukehus, Bergen, Norway.OBJECTIVE: To assess the feasibility and acceptability of training community health workers (CHWs) in ear and hearing care, and their ability to identify patients with ear and hearing disorders.DESIGN: Cluster randomised controlled trial (RCT).SETTING: Health centres in Thyolo district, Malawi.PARTICIPANTS: Ten health centres participated, 5 intervention (29 CHWs) and 5 control (28 CHWs).INTERVENTION: Intervention CHWs received 3 days of training in primary ear and hearing care, while among control CHWs, training was delayed for 6 months. Both groups were given a pretest that assessed knowledge about ear and hearing care, only the intervention group was given the posttest on the third day of training. The intervention group was given 1 month to identify patients with ear and hearing disorders in their communities, and these people were screened for hearing disorders by ear, nose and throat clinical specialists.OUTCOME MEASURES: Primary outcome measure was improvement in knowledge of ear and hearing care among CHWs after the training. Secondary outcome measures were number of patients with ear or hearing disorders identified by CHWs and number recorded at health centres during routine activities, and the perceived feasibility and acceptability of the intervention.RESULTS: The average overall correct answers increased from 55% to 68% (95% CI 65 to 71) in the intervention group (p<0.001). A total of 1739 patients with potential ear and hearing disorders were identified by CHWs and 860 patients attended the screening camps, of whom 400 had hearing loss (73 patients determined through bilateral fail on otoacoustic emissions, 327 patients through audiometry). Where cause could be determined, the most common cause of ear and hearing disorders was chronic suppurative otitis media followed by impacted wax. The intervention was perceived as feasible and acceptable to implement.CONCLUSIONS: Training was effective in improving the knowledge of CHW in ear and hearing care in Malawi and allowing them to identify patients with ear and hearing disorders. This intervention could be scaled up to other CHWs in low-income and middle-income countries.TRIAL REGISTRATION NUMBER: Pan African Clinical Trial Registry (201705002285194); Results.© Article author(s) (or their employer(s) unless otherwise stated in the text of the article) 2017. All rights reserved. No commercial use is permitted unless otherwise expressly granted.DOI: 10.1136/bmjopen-2017-016457PMCID: PMC5652500",pubmed,29025832,10.1136/bmjopen-2017-016457
directional processing and noise reduction in hearing aids individual and situational influences on preferred setting,"173. J Am Acad Audiol. 2016 Sep;27(8):628-46. doi: 10.3766/jaaa.15062.Directional Processing and Noise Reduction in Hearing Aids: Individual and Situational Influences on Preferred Setting.Neher T(1)(2), Wagener KC(3)(2), Fischer RL(4).Author information:(1)Medizinische Physik, Oldenburg University, Oldenburg, Germany.(2)Cluster of Excellence Hearing4all, Oldenburg, Germany.(3)Hörzentrum Oldenburg GmbH, Oldenburg, Germany.(4)Sivantos GmbH, Erlangen, Germany.BACKGROUND: A better understanding of individual differences in hearing aid (HA) outcome is a prerequisite for more personalized HA fittings. Currently, knowledge of how different user factors relate to response to directional processing (DIR) and noise reduction (NR) is sparse.PURPOSE: To extend a recent study linking preference for DIR and NR to pure-tone average hearing thresholds (PTA) and cognitive factors by investigating if (1) equivalent links exist for different types of DIR and NR, (2) self-reported noise sensitivity and personality can account for additional variability in preferred DIR and NR settings, and (3) spatial target speech configuration interacts with individual DIR preference.RESEARCH DESIGN: Using a correlational study design, overall preference for different combinations of DIR and NR programmed into a commercial HA was assessed in a complex speech-in-noise situation and related to PTA, cognitive function, and different personality traits.STUDY SAMPLE: Sixty experienced HA users aged 60-82 yr with controlled variation in PTA and working memory capacity took part in this study. All of them had participated in the earlier study, as part of which they were tested on a measure of ""executive control"" tapping into cognitive functions such as working memory, mental flexibility, and selective attention.DATA COLLECTION AND ANALYSIS: Six HA settings based on unilateral (within-device) or bilateral (across-device) DIR combined with inactive, moderate, or strong single-microphone NR were programmed into a pair of behind-the-ear HAs together with individually prescribed amplification. Overall preference was assessed using a free-field simulation of a busy cafeteria situation with either a single frontal talker or two talkers at ±30° azimuth as the target speech. In addition, two questionnaires targeting noise sensitivity and the ""Big Five"" personality traits were administered. Data were analyzed using multiple regression analyses and repeated-measures analyses of variance with a focus on potential interactions between the HA settings and user factors.RESULTS: Consistent with the earlier study, preferred HA setting was related to PTA and executive control. However, effects were weaker this time. Noise sensitivity and personality did not interact with HA settings. As expected, spatial target speech configuration influenced preference, with bilateral and unilateral DIR ""winning"" in the single- and two-talker scenario, respectively. In general, participants with higher PTA tended to more strongly prefer bilateral DIR than participants with lower PTA.CONCLUSIONS: Although the current study lends some support to the view that PTA and cognitive factors affect preferred DIR and NR setting, it also indicates that these effects can vary across noise management technologies. To facilitate more personalized HA fittings, future research should investigate the source of this variability.American Academy of Audiology.DOI: 10.3766/jaaa.15062",pubmed,27564441,10.3766/jaaa.15062
simultaneous masking between electric and acoustic stimulation in cochlear implant users with residual lowfrequency hearing,"288. Hear Res. 2017 Sep;353:185-196. doi: 10.1016/j.heares.2017.06.014. Epub 2017 Jun 30.Simultaneous masking between electric and acoustic stimulation in cochlear implant users with residual low-frequency hearing.Krüger B(1), Büchner A(2), Nogueira W(3).Author information:(1)Department of Otolaryngology, Hannover Medical School, Cluster of Excellence Hearing4all, Hannover, Germany. Electronic address: Krueger.benjamin@mh-hannover.de.(2)Department of Otolaryngology, Hannover Medical School, Cluster of Excellence Hearing4all, Hannover, Germany. Electronic address: Buechner.Andreas@mh-hannover.de.(3)Department of Otolaryngology, Hannover Medical School, Cluster of Excellence Hearing4all, Hannover, Germany. Electronic address: NogueiraVazquez.Waldo@mh-hannover.de.Ipsilateral electric-acoustic stimulation (EAS) is becoming increasingly important in cochlear implant (CI) treatment. Improvements in electrode designs and surgical techniques have contributed to improved hearing preservation during implantation. Consequently, CI implantation criteria have been expanded toward people with significant residual low-frequency hearing, who may benefit from the combined use of both the electric and acoustic stimulation in the same ear. However, only few studies have investigated the mutual interaction between electric and acoustic stimulation modalities. This work characterizes the interaction between both stimulation modalities using psychophysical masking experiments and cone beam computer tomography (CBCT). Two psychophysical experiments for electric and acoustic masking were performed to measure the hearing threshold elevation of a probe stimulus in the presence of a masker stimulus. For electric masking, the probe stimulus was an acoustic tone while the masker stimulus was an electric pulse train. For acoustic masking, the probe stimulus was an electric pulse train and the masker stimulus was an acoustic tone. Five EAS users, implanted with a CI and ipsilateral residual low-frequency hearing, participated in the study. Masking was determined at different electrodes and different acoustic frequencies. CBCT scans were used to determine the individual place-pitch frequencies of the intracochlear electrode contacts by using the Stakhovskaya place-to-frequency transformation. This allows the characterization of masking as a function of the difference between electric and acoustic stimulation sites, which we term the electric-acoustic frequency difference (EAFD). The results demonstrate a significant elevation of detection thresholds for both experiments. In electric masking, acoustic-tone thresholds increased exponentially with decreasing EAFD. In contrast, for the acoustic masking experiment, threshold elevations were present regardless of the tested EAFDs. Based on the present findings, we conclude that there is an asymmetry between the electric and the acoustic masker modalities. These observations have implications for the design and fitting of EAS sound-coding strategies.Copyright © 2017 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2017.06.014",pubmed,28688755,10.1016/j.heares.2017.06.014
the application of improved chqs for mass epidemiology study on hearing impairment,"858. Lin Chuang Er Bi Yan Hou Tou Jing Wai Ke Za Zhi. 2010 Jan;24(1):19-20, 24.[The application of improved CHQS for mass epidemiology study on hearing impairment].[Article in Chinese]Liu C(1), Xing G, Xu X, Chen Z, Zhou H, Wang D, Tian H, Bu X.Author information:(1)Department of Otorhinolaryngology, the First Affiliated Hospital of Nanjing Medical University, Nanjing, 210029, China.OBJECTIVE: To develop and evaluate the improved Chinese hearing questionnaire for school children (CHQS) for mass epidemiology study on hearing impairment in China.METHOD: Using the probability proportion to size (PPS) method, 8412 residents were investigated in 40 clusters in Jiangsu province with the WHO ear diseases and hearing disorders survey protocol. 87.9% of the residents aged 7 years and over answered the questionnaire and accepted the pure tone audiometry.RESULT: The prevalence of hearing impairment was 12.9% by the questionnaire. Compared with ""golden standard"" (pure tone audiometry), Sen = 58.5%, Spe = 96.7%, PV+ = 78.9%, PV- = 91.7%, overall accuracy = 90.0%. The sensitivity for women was higher than men.CONCLUSION: The questionnaire produced high efficiency and specificity values. It could be used in mass hearing screening, particularly in remote and rural area, although the sensitivity was as low as most questionnaires.",pubmed,20235451,
auditory attention tracking states in a cocktail party environment can be decoded by deep convolutional neural networks,"261. J Neural Eng. 2020 Jun 12;17(3):036013. doi: 10.1088/1741-2552/ab92b2.Auditory attention tracking states in a cocktail party environment can be decoded by deep convolutional neural networks.Tian Y, Ma L.OBJECTIVE: A deep convolutional neural network (CNN) is a method for deep learning (DL). It has a powerful ability to automatically extract features and is widely used in classification tasks with scalp electroencephalogram (EEG) signals. However, the small number of samples and low signal-to-noise ratio involved in scalp EEG with low spatial resolution constitute a limitation that might restrict potential brain-computer interface (BCI) applications that are based on the CNN model. In the present study, a novel CNN model with source-spatial feature images (SSFIs) as the input is proposed to decode auditory attention tracking states in a cocktail party environment.APPROACH: We first extract SSFIs using rhythm entropy and weighted minimum norm estimation. Next, we develop a CNN model with three convolutional layers. Furthermore, we estimate the performance of the proposed model via generalized performance, alternative models that deleted or replaced a model's component, and loss curves. Finally, we use a deep transfer model with fine-tuning for a low (poor) behavioral performance group (L-group).MAIN RESULTS: Based on cortical activity reconstructions from the scalp EEGs, the classification accuracy (CA) of the proposed model is 80.4% (chance level: 52.5%), which is superior to that achieved by scalp EEG. Additionally, the performance of the proposed model is more stable when compared to alternative models that delete or replace specific model components. The proposed model identifies the difference between two auditory attention tracking states (successful versus unsuccessful) at an early stage with a short time window (250 ms after target offset). Furthermore, we propose a deep transfer learning model to improve the classification for the L-group. With this model, the CA of the L-group significantly increase by 5.3%.SIGNIFICANCE: Our proposed model improves the performance of a decoder for auditory attention tracking, which could be suitable for relieving the difficulty with the attentional modulation of individual's neural responses. It provides a novel communication channel with auditory cognitive BCI for patients with attention and hearing impairment.DOI: 10.1088/1741-2552/ab92b2",pubmed,32403093,10.1088/1741-2552/ab92b2
loudness and pitch perception using dynamically compensated virtual channels,"312. Hear Res. 2017 Feb;344:223-234. doi: 10.1016/j.heares.2016.11.017. Epub 2016 Dec 7.Loudness and pitch perception using Dynamically Compensated Virtual Channels.Nogueira W(1), Litvak LM(2), Landsberger DM(3), Büchner A(4).Author information:(1)Medical University Hannover, Cluster of Excellence ""Hearing4all"", Hannover, Germany. Electronic address: nogueiravazquez.waldo@mh-hannover.de.(2)Advanced Bionics LLC, Valencia, CA, USA.(3)New York University School of Medicine, New York, NY, USA.(4)Medical University Hannover, Cluster of Excellence ""Hearing4all"", Hannover, Germany.Reducing power consumption is important for the development of smaller cochlear implant (CI) speech processors. Simultaneous electrode stimulation may improve power efficiency by minimizing the required current applied to a given electrode. Simultaneous in-phase stimulation on adjacent electrodes (i.e. virtual channels) can be used to elicit pitch percepts intermediate to the ones provided by each of the physical electrodes in isolation. Virtual channels are typically implemented in monopolar stimulation mode, producing broad excitation patterns. Focused stimulation may reduce the excitation patterns, but is inefficient in terms of power consumption. To create a more power efficient virtual channel, we developed the Dynamically Compensated Virtual Channel (DC-VC) using four adjacent electrodes. The two central electrodes are current steered using the coefficient α (0<α<1 ) whereas the two flanking electrodes are used to focus/unfocus the stimulation with the coefficient σ (-1<σ<1). With increasing values of σ, power can be saved at the potential expense of generating broader electric fields. Additionally, reshaping the electric fields might also alter place pitch coding. The goal of the present study is to investigate the tradeoff between place pitch encoding and power savings using simultaneous electrode stimulation in the DC-VC configuration. A computational model and psychophysical experiments in CI users have been used for that purpose. Results from 10 adult Advanced Bionics CI users have been collected. Results show that the required current to produce comfortable levels is significantly reduced with increasing σ as predicted by the computational model. Moreover, no significant differences in the estimated number of discriminable steps were detected for the different values of σ. From these results, we conclude that DC-VCs can reduce power consumption without decreasing the number of discriminable place pitch steps.Copyright © 2016. Published by Elsevier B.V.DOI: 10.1016/j.heares.2016.11.017PMCID: PMC5421637",pubmed,27939418,10.1016/j.heares.2016.11.017
neural signatures of working memory in agerelated hearing loss,"36. Neuroscience. 2020 Mar 1;429:134-142. doi: 10.1016/j.neuroscience.2019.12.046. Epub 2020 Jan 11.Neural Signatures of Working Memory in Age-related Hearing Loss.Rosemann S(1), Thiel CM(2).Author information:(1)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, 26111 Oldenburg, Germany; Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, 26111 Oldenburg, Germany. Electronic address: Stephanie.rosemann@uni-oldenburg.de.(2)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, 26111 Oldenburg, Germany; Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, 26111 Oldenburg, Germany.Age-related hearing loss affects the ability to hear high frequencies and therefore leads to difficulties in understanding speech, particularly under adverse listening conditions. This decrease in hearing can be partly compensated by the recruitment of executive functions, such as working memory. The compensatory effort may, however, lead to a decrease in available neural resources compromising cognitive abilities. We here aim to investigate whether mild to moderate hearing loss impacts prefrontal functions and related executive processes and whether these are related to speech-in-noise perception abilities. Nineteen hard of hearing and nineteen age-matched normal-hearing participants performed a working memory task to drive prefrontal activity, which was gauged with functional magnetic resonance imaging. In addition, speech-in-noise understanding, cognitive flexibility and inhibition control were assessed. Our results showed no differences in frontoparietal activation patterns and working memory performance between normal-hearing and hard of hearing participants. The behavioral assessment of further executive functions, however, provided evidence of lower cognitive flexibility in hard of hearing participants. Cognitive flexibility and hearing abilities further predicted speech-in-noise perception. We conclude that neural and behavioral signatures of working memory are intact in mild to moderate hearing loss. Moreover, cognitive flexibility seems to be closely related to hearing impairment and speech-in-noise perception and should, therefore, be investigated in future studies assessing age-related hearing loss and its implications on prefrontal functions.Copyright © 2020 IBRO. Published by Elsevier Ltd. All rights reserved.DOI: 10.1016/j.neuroscience.2019.12.046",pubmed,31935488,10.1016/j.neuroscience.2019.12.046
the prevalence of central auditory processing disorder in elementary school students of kerman iran,"Objectives This study aimed to determine the prevalence of central auditory processing disorder (CAPD) in elementary school students in Kerman, Iran, during 2018-2019. Materials & Methods This cross-sectional study was conducted on 1369 elementary school students in Kerman. These students were selected by cluster sampling from different areas of Kerman and then screened using the Buffalo Model Questionnaire (BMQ). Based on the data obtained from the questionnaire, normal children were excluded from the study. Then, children with suspected central auditory processing disorder (CAPD) underwent ear exams and were excluded from the study in case of abnormal results in the tympanic membrane examination (rapture-effusion). The remaining subjects underwent peripheral audiometry evaluation, and children with abnormal audiometry were excluded from the study. Finally, the remaining children with suspicious screening results, a normal examination, and normal audiometry underwent a specific test to detect Central auditory processing disorder. Data analysis was carried out using SPSS software. Results One thousand three hundred sixty-nine primary school students with a mean age of 9.15 ±2.63 years enrolled in this study. 52%% of students were male. 8.03% of them had CAPD. A statistically significant relationship was found between the prevalence of CAPD and gender (P<0.001), place of residence (P<0.001), history of middle ear inflammation (P<0.001) and history of head injury. Conclusion The quality of life of these students with CAPD can be improved via timely recognition of CAPD and the provision of appropriate preventive and therapeutic facilities. © 2023 The Authors.",scopus,2-s2.0-85183915429,10.22037/ijcn.v17i1.33821
speech enhancement based on neural networks improves speech intelligibility in noise for cochlear implant users,"158. Hear Res. 2017 Feb;344:183-194. doi: 10.1016/j.heares.2016.11.012. Epub 2016 Nov 30.Speech enhancement based on neural networks improves speech intelligibility in noise for cochlear implant users.Goehring T(1), Bolner F(2), Monaghan JJ(3), van Dijk B(4), Zarowski A(5), Bleeck S(3).Author information:(1)ISVR, University of Southampton, University Rd, Southampton SO17 1BJ, United Kingdom. Electronic address: goehring.tobias@gmail.com.(2)ExpORL, KU Leuven, O&N II Herestraat 49, 3000 Leuven, Belgium; Cochlear Technology Centre, Schaliënhoevedreef 20 I, 2800 Mechelen, Belgium.(3)ISVR, University of Southampton, University Rd, Southampton SO17 1BJ, United Kingdom.(4)Cochlear Technology Centre, Schaliënhoevedreef 20 I, 2800 Mechelen, Belgium.(5)European Institute for ORL-HNS, Sint Augustinus Hospital, Oosterveldlaan 24, 2610 Wilrijk, Belgium.Speech understanding in noisy environments is still one of the major challenges for cochlear implant (CI) users in everyday life. We evaluated a speech enhancement algorithm based on neural networks (NNSE) for improving speech intelligibility in noise for CI users. The algorithm decomposes the noisy speech signal into time-frequency units, extracts a set of auditory-inspired features and feeds them to the neural network to produce an estimation of which frequency channels contain more perceptually important information (higher signal-to-noise ratio, SNR). This estimate is used to attenuate noise-dominated and retain speech-dominated CI channels for electrical stimulation, as in traditional n-of-m CI coding strategies. The proposed algorithm was evaluated by measuring the speech-in-noise performance of 14 CI users using three types of background noise. Two NNSE algorithms were compared: a speaker-dependent algorithm, that was trained on the target speaker used for testing, and a speaker-independent algorithm, that was trained on different speakers. Significant improvements in the intelligibility of speech in stationary and fluctuating noises were found relative to the unprocessed condition for the speaker-dependent algorithm in all noise types and for the speaker-independent algorithm in 2 out of 3 noise types. The NNSE algorithms used noise-specific neural networks that generalized to novel segments of the same noise type and worked over a range of SNRs. The proposed algorithm has the potential to improve the intelligibility of speech in noise for CI users while meeting the requirements of low computational complexity and processing delay for application in CI devices.Copyright © 2016 The Authors. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2016.11.012PMCID: PMC5256482",pubmed,27913315,10.1016/j.heares.2016.11.012
the severity of infection determines the localization of damage and extent of sensorineural hearing loss in experimental pneumococcal meningitis,"362. J Neurosci. 2016 Jul 20;36(29):7740-9. doi: 10.1523/JNEUROSCI.0554-16.2016.The Severity of Infection Determines the Localization of Damage and Extent of Sensorineural Hearing Loss in Experimental Pneumococcal Meningitis.Perny M(1), Roccio M(2), Grandgirard D(3), Solyga M(1), Senn P(4), Leib SL(5).Author information:(1)Neuroinfection Laboratory, Institute for Infectious Diseases, University of Bern, 3001 Bern, Switzerland, Laboratory of Inner Ear Research, Department of Clinical Research, University of Bern and University Department of Otorhinolaryngology, Head & Neck Surgery, Inselspital, 3008 Bern, Switzerland, Cluster for Regenerative Neuroscience, Department of Clinical Research, University of Bern, 3008 Bern, Switzerland.(2)Laboratory of Inner Ear Research, Department of Clinical Research, University of Bern and University Department of Otorhinolaryngology, Head & Neck Surgery, Inselspital, 3008 Bern, Switzerland, Cluster for Regenerative Neuroscience, Department of Clinical Research, University of Bern, 3008 Bern, Switzerland.(3)Neuroinfection Laboratory, Institute for Infectious Diseases, University of Bern, 3001 Bern, Switzerland, Cluster for Regenerative Neuroscience, Department of Clinical Research, University of Bern, 3008 Bern, Switzerland.(4)Laboratory of Inner Ear Research, Department of Clinical Research, University of Bern and University Department of Otorhinolaryngology, Head & Neck Surgery, Inselspital, 3008 Bern, Switzerland, Department of Otorhinolaryngology, Head and Neck Surgery, University Hospital Geneva, 1205 Geneva, Switzerland, and Cluster for Regenerative Neuroscience, Department of Clinical Research, University of Bern, 3008 Bern, Switzerland.(5)Neuroinfection Laboratory, Institute for Infectious Diseases, University of Bern, 3001 Bern, Switzerland, Cluster for Regenerative Neuroscience, Department of Clinical Research, University of Bern, 3008 Bern, Switzerland stephen.leib@ifik.unibe.ch.Hearing loss is an important sequela of pneumococcal meningitis (PM), occurring in up to 30% of survivors. The role of the severity of infection on hearing function and pathomorphological consequences in the cochlea secondary to PM have not been investigated to date. Using a well-established model of PM, we systematically investigated the functional hearing outcome and the long-term fate of neurosensory cells in the cochlea, i.e., hair cells and spiral ganglion neurons (SGNs), with a focus on their tonotopic distribution. Intracisternal infection of infant rats with increasing inocula of Streptococcus pneumoniae resulted in a dose-dependent increase in CSF levels of interleukin-1β, interleukin-6, tumor necrosis factor α, interleukin-10, and interferon-γ in acute disease. The severity of long-term hearing loss at 3 weeks after infection, measured by auditory brainstem response recordings, correlated to the initial inoculum dose and to the levels of proinflammatory cytokines determined in the acute phase of PM. Quantitative cochlear histomorphology revealed a significant loss of SGNs and outer hair cells that strongly correlated to the level of infection, with the most severe damage occurring in the basal part of the cochlea. Inner hair cells (IHCs) were not significantly affected throughout the entire cochlea. However, surviving IHCs lost synaptic connectivity to remaining SGNs in all cochlear regions. These findings provide evidence that the inoculum concentration, i.e., severity of infection, is the major determinant of long-term morphological cell pathologies in the cochlea and functional hearing loss.SIGNIFICANCE STATEMENT: Hearing loss is a neurofunctional deficit occurring in up to 30% of patients surviving pneumococcal meningitis (PM). Here, we analyze the correlation between the severity of infection and the inflammatory response in the CSF, the tonotopic distribution of neurosensory pathologies in the cochlea, and the long-term hearing function in a rat model of pneumococcal meningitis. Our study identifies the severity of infection as the key determinant of long-term hearing loss, underlining the importance of the prompt institution of antibiotic therapy in patients suffering from PM. Furthermore, our findings reveal in detail the spatial loss of cochlear neurosensory cells, providing new insights into the pathogenesis of meningitis-associated hearing loss that reveal new starting points for the development of otoprotective therapies.Copyright © 2016 the authors 0270-6474/16/367740-10$15.00/0.DOI: 10.1523/JNEUROSCI.0554-16.2016PMCID: PMC6705551",pubmed,27445150,10.1523/JNEUROSCI.0554-16.2016
prevalence of preexisting hearing loss among patients with drugresistant tuberculosis in south africa,"115. Am J Audiol. 2020 Jun 8;29(2):199-205. doi: 10.1044/2020_AJA-19-00103. Epub 2020 Apr 22.Prevalence of Pre-Existing Hearing Loss Among Patients With Drug-Resistant Tuberculosis in South Africa.Hong H(1)(2), Dowdy DW(3), Dooley KE(4), Francis HW(5), Budhathoki C(1), Han HR(1)(6), Farley JE(1)(2).Author information:(1)The REACH Initiative, Johns Hopkins University School of Nursing, Baltimore, MD.(2)Johns Hopkins University School of Nursing, Baltimore, MD.(3)Departments of Epidemiology and International Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, MD.(4)Divisions of Clinical Pharmacology and Infectious Disease, Johns Hopkins University School of Medicine, Baltimore, MD.(5)Division of Head and Neck Surgery and Communication Sciences, Duke University School of Medicine, Durham, NC.(6)Center for Cardiovascular and Chronic Care, The Johns Hopkins University, Baltimore, MD.Purpose Hearing loss, resulting from aminoglycoside ototoxicity, is common among patients with drug-resistant tuberculosis (DR-TB). Those with pre-existing hearing loss are at particular risk of clinically important hearing loss with aminoglycoside-containing treatment than those with normal hearing at baseline. This study aimed to identify factors associated with pre-existing hearing loss among patients being treated for DR-TB in South Africa. Method Cross-sectional analysis nested within a cluster-randomized trial data across 10 South African TB hospitals. Patients ≥ 13 years old received clinical and audiological evaluations before DR-TB treatment initiation. Results Of 936 patients, average age was 35 years. One hundred forty-two (15%) reported pre-existing auditory symptoms. Of 482 patients tested by audiometry, 290 (60%) had pre-existing hearing loss. The prevalence of pre-existing hearing loss was highest among patients ≥ 50 years (adjusted prevalence ratio [aPrR] for symptoms 5.53, 95% confidence interval (CI) [3.63, 8.42]; aPrR for audiometric hearing loss 1.63, 95% CI [1.31, 2.03] compared to age 13-18 years) and among those with a prior history of second-line TB treatment (aPrR for symptoms 1.73, 95% CI [1.66, 1.80]; PrR for audiometric hearing loss 1.33, 95% CI [1.03, 1.73]). Having HIV with cluster of differentiation 4 cell count < 200 cells/mm3 and malnutrition were risk factors but did not reach statistical significance in adjusted analyses. Conclusion Pre-existing hearing loss is common among patients presenting for DR-TB treatment in South Africa, and those older than the age of 50 years or who had prior second-line TB treatment history were at highest risk.DOI: 10.1044/2020_AJA-19-00103PMCID: PMC7839025",pubmed,32320639,10.1044/2020_AJA-19-00103
objective source selection in blind source separation of aeps in children with cochlear implants,"486. Annu Int Conf IEEE Eng Med Biol Soc. 2007;2007:6224-7. doi: 10.1109/IEMBS.2007.4353777.Objective source selection in Blind Source Separation of AEPs in children with Cochlear Implants.Castañeda-Villa N(1), James CJ.Author information:(1)ISVR, University of Southampton, SO171BJ, UK. ncv@soton.ac.ukMulti-channel Auditory Evoked Potentials (AEPs) are a useful methodology for evaluating the auditory performance of children with Cochlear Implants (CIs). These recordings are generally contaminated, not only with well known physiological artifacts (blinking, muscle) and line noise etc., but also by CI artifact. The CI induces an artifact in the recording at the electrodes in the temporal lobe area (where it is implanted) when specific tones are presented, this artifact in particular makes the detection and analysis of AEPs much more challenging. This paper evaluates the convenience of using Blind Source Separation (BSS) and Independent Component Analysis (ICA) in order to identify the AEPs from ongoing recordings and to isolate the artifact when testing a child with a CI. We propose a new procedure to elicit an objective differentiation between the independent components (ICs) related to the AEPs and CI artifact; two concepts are fundamental in this procedure Mutual Information (MI) and Clustering. Finally, the variability of three BSS/ICA algorithms is assessed; in order to determine which one is more convenient to isolate the respective ICs of interest. Temporal decorrelation based ICA showed the least change in the estimation of both the AEPs and the CI artifact; this has allowed for considerable autonomy in the construction of relevant, consistent clusters.DOI: 10.1109/IEMBS.2007.4353777",pubmed,18003443,10.1109/IEMBS.2007.4353777
bridging the divide brain and behavior in developmental language disorder,"Developmental language disorder (DLD) is a heterogenous neurodevelopmental disorder that affects a child’s ability to comprehend and/or produce spoken and/or written language, yet it cannot be attributed to hearing loss or overt neurological damage. It is widely believed that some combination of genetic, biological, and environmental factors influences brain and language development in this population, but it has been difficult to bridge theoretical accounts of DLD with neuroimaging findings, due to heterogeneity in language impairment profiles across individuals and inconsistent neuroimaging findings. Therefore, the purpose of this overview is two-fold: (1) to summarize the neuroimaging literature (while drawing on findings from other language-impaired populations, where appropriate); and (2) to briefly review the theoretical accounts of language impairment patterns in DLD, with the goal of bridging the disparate findings. As will be demonstrated with this overview, the current state of the field suggests that children with DLD have atypical brain volume, laterality, and activation/connectivity patterns in key language regions that likely contribute to language difficulties. However, the precise nature of these differences and the underlying neural mechanisms contributing to them remain an open area of investigation. © 2023 by the authors.",scopus,2-s2.0-85178270290,10.3390/brainsci13111606
characteristics of brain glucose metabolism and metabolic connectivity in noiseinduced hearing loss,"10 2017 216 972 B4.44. Sci Rep. 2023 Dec 11;13(1):21889. doi: 10.1038/s41598-023-48911-x.Characteristics of brain glucose metabolism and metabolic connectivity in noise-induced hearing loss.Shin S(1), Nam HY(2).Author information:(1)Department of Nuclear Medicine, Samsung Changwon Hospital, Sungkyunkwan University School of Medicine, Changwon, Republic of Korea.(2)Department of Nuclear Medicine, Samsung Changwon Hospital, Sungkyunkwan University School of Medicine, Changwon, Republic of Korea. octobre23@hanmail.net.The purpose of this study was to evaluate the differences in cerebral glucose metabolism and metabolic connectivity between noise-induced hearing loss (NIHL) subjects and normal subjects. Eighty-nine subjects who needed close observation for NIHL or were diagnosed with NIHL and 89 normal subjects were enrolled. After pre-processing of positron emission tomography images including co-registration, spatial normalization, and smoothing, a two-sample t-test was conducted to compare cerebral glucose metabolism between the two groups. To evaluate metabolic connectivity between two groups, BRAPH-BRain Analysis using graPH theory, a software package to perform graph theory analysis of the brain connectome was used. NIHL subjects showed hypometabolism compared to normal subjects in both insulae (x - 38, y - 18, z 4; × 42, y - 12, z 4) and right superior temporal gyrus (× 44, y 16, z - 20). No brain regions showed hypermetabolism in the NIHL subjects. In metabolic connectivity analysis, NIHL subjects showed decreased average strength, global efficiency, local efficiency, and mean clustering coefficient when compared with normal subjects. Decreased glucose metabolism and metabolic connectivity in NIHL subject might reflect decreased auditory function. It might be characteristic of sensorineural hearing loss.© 2023. The Author(s).DOI: 10.1038/s41598-023-48911-xPMCID: PMC10713681",pubmed,38081979,10.1038/s41598-023-48911-x
time series clustering analysis exploring the patterns of hearing loss in adults 50 years old and above in cheshire and merseyside ics 20132022 an interactive map application ,,base,724d899471958f0788805b76b94d2f10cfd10ebe0996dfe178df325e3c4ac82f,
time series clustering analysis exploring the patterns of hearing loss in adults 50 years old and above in cheshire and merseyside ics 20132022 an interactive map application,,base,32cf6167bdc4424da3db0ffc66022ca5d378b1900b8aecb9e0ade41a692ae837,
rhythmic training improves temporal anticipation and adaptation abilities in children with hearing loss during verbal interaction,"Purpose: In this study, we investigate temporal adaptation capacities of children with normal hearing and children with cochlear implants and/or hearing aids during verbal exchange. We also address the question of the efficiency of a rhythmic training on temporal adaptation during speech interaction in children with hearing loss. Method: We recorded electroencephalogram data in children while they named pictures delivered on a screen, in alternation with a virtual partner. We manipulated the virtual partner's speech rate (fast vs. slow) and the regularity of alternation (regular vs. irregular). The group of children with normal hearing was tested once, and the group of children with hearing loss was tested twice: once after 30 min of auditory training and once after 30 min of rhythmic training. Results: Both groups of children adjusted their speech rate to that of the virtual partner and were sensitive to the regularity of alternation with a less accurate performance following irregular turns. Moreover, irregular turns elicited a negative event-related potential in both groups, showing a detection of temporal deviancy. Notably, the amplitude of this negative component positively correlated with accuracy in the alternation task. In children with hearing loss, the effect was more pronounced and long-lasting following rhythmic training compared with auditory training. Conclusion: These results are discussed in terms of temporal adaptation abilities in speech interaction and suggest the use of rhythmic training to improve these skills of children with hearing loss.",cinahl,10924388,10.1044/2019_JSLHR-S-18-0349
validation of a frenchlanguage version of the spatial hearing questionnaire cluster analysis and comparison with the speech spatial and qualities of hearing scale,"371. Ear Hear. 2016 Jul-Aug;37(4):412-23. doi: 10.1097/AUD.0000000000000269.Validation of a French-Language Version of the Spatial Hearing Questionnaire, Cluster Analysis and Comparison with the Speech, Spatial, and Qualities of Hearing Scale.Moulin A(1), Richard C.Author information:(1)1INSERM U1028, Lyon Neuroscience Research Center, Brain Dynamics and Cognition Team, Lyon, France; 2CNRS UMR5292, Lyon Neuroscience Research Center, Brain Dynamics and Cognition Team, Lyon, France; 3University of Lyon, Lyon, France; 4Department of Oto-Rhino-Laryngology, Head and Neck Surgery, University Hospital (CHUV), Lausanne, Switzerland; and 5The Laboratory for Investigative Neurophysiology (The LINE), Department of Radiology & Department of Clinical Neurosciences, University Hospital Centre and University of Lausanne, Lausanne, Switzerland.OBJECTIVES: To validate a French-language version of the spatial hearing questionnaire (SHQ), including investigating its internal structure using cluster analysis and exploring its construct validity on a large population of hearing-impaired (HI) and normal-hearing (NH) subjects, and to compare the SHQ with the speech, spatial, and qualities of hearing scale (SSQ) in the same population.DESIGN: The SHQ was translated in accordance with the principles of the Universalist Model of cross-cultural adaptation of patient-reported outcome instruments. The SSQ and SHQ were then presented in a counterbalanced order, in a self-report mode, in a population of 230 HI subjects (mean age = 54 years and pure-tone audiometry [PTA] on the better ear = 28 dB HL) and 100 NH subjects (mean age = 21 years). The SHQ feasibility, readability, and psychometric properties were systematically investigated using reliability indices, cluster, and factor analyses and multiregression analyses. SHQ characteristics were compared both to different literature data obtained with different language versions and to the SSQ scores obtained in the same population.RESULTS: Internal validity was high and very good reproducibility of scores and intersubject variability were obtained across the 24 items between the English and French SHQ for NH subjects. Factor and cluster analyses concurred in identifying five correlated factors, corresponding to several SHQ subscales: (1) speech in noise (corresponding to SHQ subscales 7 and 8), (2) localization of voice sounds from behind, (3) speech in quiet (corresponding to SHQ subscale 1), (4) localization of everyday sounds, and (5) localization of voices and music (corresponding to parts of the SHQ localization subscale). Correlations between SSQ subscales and SHQ factors identified the greatest correlations between SHQ factors 2, 4, and 5 and SSQ spatial subscales, whereas SHQ factor 1 had the greatest correlation with SSQ_speech. SHQ and SSQ scores were similar, whether in NH subjects (8.5 versus 8.4) or in HI subjects (6.6 for both), sharing more than 80% of variance. The SHQ localization subscale gave similar scores as the SSQ spatial subscale, sharing more than 75% of variance. Construct validity identified better ear PTA and PTA asymmetry as the two main predictors of SHQ scores, to a degree similar to that seen for the SSQ. The SHQ was shorter, easier to read and less sensitive to the number of years of formal education than the SSQ, but this came at a cost of ecological validity, which was rated higher for the SSQ than for the SHQ.CONCLUSIONS: A comparison of factor analysis outcomes among the English, Dutch, and French versions of the SHQ confirmed good conceptual equivalence across languages and robustness of the SHQ for use in international settings. In addition, SHQ and SSQ scores showed remarkable similarities, suggesting the possibility of extrapolating the results from one questionnaire to the other. Although the SHQ was originally designed in a population of cochlear implant patients, the present results show that its usefulness could easily be extended to noncochlear-implanted, HI subjects.DOI: 10.1097/AUD.0000000000000269",pubmed,26808287,10.1097/AUD.0000000000000269
changes in spontaneous firing rate and neural synchrony in cat primary auditory cortex after localized toneinduced hearing loss,"408. Hear Res. 2003 Jun;180(1-2):28-38. doi: 10.1016/s0378-5955(03)00074-1.Changes in spontaneous firing rate and neural synchrony in cat primary auditory cortex after localized tone-induced hearing loss.Seki S(1), Eggermont JJ.Author information:(1)Departments of Physiology and Biophysics, and Psychology, University of Calgary, 2500 University Drive N.W., Calgary, AB, Canada T2N 1N4.Increase in spontaneous neural activity after noise-induced hearing loss has frequently been associated with the phenomenon of tinnitus. Eighteen juvenile and adult cats were exposed for 2 h to a 6 kHz tone with an intensity of 115 dB SPL at the cat's head. Seven non-exposed littermates and seven other normal hearing cats were used as age-matched controls. The trauma cats showed localized hearing losses, as assessed by ABR, ranging from less than 20 to 60 dB. The frequency representation in primary auditory cortex was mapped using an eight-electrode array. Single-unit spontaneous activity was recorded for 15 min. Peak cross-correlation coefficients (R) for unit cluster activity recorded on separate electrodes were calculated. We found elevated spontaneous firing rates in regions with reorganization of the tonotopic map compared to the neurons in the non-reorganized cortical regions in the same animals. A second finding was that in these regions the peak cross-correlation coefficients were also increased relative to the non-reorganized parts. A third finding was that exposed animals showed higher spontaneous activity compared to controls regardless of the presence of cortical reorganization. This may be a correlate of tinnitus in the presence of only minor hearing losses.DOI: 10.1016/s0378-5955(03)00074-1",pubmed,12782350,10.1016/s0378-5955(03)00074-1
perceptual effects of adjusting hearingaid gain by means of a machinelearning approach based on individual user preference,"301. Trends Hear. 2019 Jan-Dec;23:2331216519847413. doi: 10.1177/2331216519847413.Perceptual Effects of Adjusting Hearing-Aid Gain by Means of a Machine-Learning Approach Based on Individual User Preference.Søgaard Jensen N(1), Hau O(1), Bagger Nielsen JB(1), Bundgaard Nielsen T(2), Vase Legarth S(2).Author information:(1)1 Widex A/S, Lynge, Denmark.(2)2 SenseLab, FORCE Technology, Hørsholm, Denmark.This study investigated a method to adjust hearing-aid gain by use of a machine-learning algorithm that estimates the optimal setting of gain parameters based on user preference indicated in an iterative paired-comparison procedure. Twenty hearing-impaired participants completed this procedure for 12 different sound scenarios. During the adjustment procedure, their task was to indicate a preference based on one of three sound attributes: Basic Audio Quality, Listening Comfort, or Speech Clarity. In a double-blind comparison of recordings of the processed scenarios, and using the same attributes as criteria, the adjusted gain settings were subsequently compared with two prescribed settings of the same hearing aid (with and without activation of an automatic sound-classification system). The results showed that the adjustment method provided a general improvement of Basic Audio Quality, an improvement of Listening Comfort in a traffic-noise scenario but not in three scenarios with speech babble, and no significant improvement of Speech Clarity. A large variation in gain adjustments was observed across participants, both among those who did benefit and among those who did not benefit from the adjustment. There was no clear connection between the gain adjustments and the perceived benefit, which indicates that the preferred gain settings for a given sound scenario and a given listening intention are highly individual and difficult to predict.DOI: 10.1177/2331216519847413PMCID: PMC6535733",pubmed,31104581,10.1177/2331216519847413
communication systems for people with severe hearing loss,"Automatic speech recognition has long been an object of studies, as it is based on the idea of the dialog between man and computer. The first work on speech recognition was published in 1952, and it described a system created by the Bell Laboratory. Many people with severe hearing loss rely on sign language as the main means of communication, yet, in time, advanced research in the field has lead to the human communication with the computer, albeit with some limitations. Nowadays, communication with the computer may be done by using a speech recognition system and a voice synthesis system.",ieee,,10.1109/AQTR.2018.8402706
the human spiral ganglion new insights into ultrastructure survival rate and implications for cochlear implants,"660. Audiol Neurootol. 2005 Sep-Oct;10(5):258-73. doi: 10.1159/000086000. Epub 2005 May 27.The human spiral ganglion: new insights into ultrastructure, survival rate and implications for cochlear implants.Glueckert R(1), Pfaller K, Kinnefors A, Rask-Andersen H, Schrott-Fischer A.Author information:(1)Department of Otolaryngology, Institute of Anatomy and Histology, Medical University of Innsbruck, Innsbruck, Austria.This study was based on high-resolution SEM assessment of freshly fixed, normal-hearing, human inner ear tissue. In addition, semiquantitative observations were made in long-term deafened temporal bone material, focusing on the spiral ganglia and nerve projections, and a detailed study of the fine bone structure in macerated tissues was performed. Our main findings detail the presence of extensive bony fenestrae surrounding the nerve elements, permitting a relatively free flow of perilymph to modiolar structures. The clustering of the spiral ganglion cells in Rosenthal's canal and the detailed and intricate course of postganglionic axons are described. The close proximity of fibers to cell soma is demonstrated by impression in cell surfaces, and presence of small microvilli-like structures at the contact regions, anchoring nerve fibers to the cell wall. Extensive fenestrae and the presence of a fragile network of endosteal bony structures at the surfaces guiding nerve fibers are described in detail for the first time. This unique freshly prepared human material offers the opportunity for a detailed ultrastructural study not previously possible on postmortem fixed material and more accurate information to model electrostimulation of the human auditory nerve through a cochlear implant. On the basis of this study, we suggest that the concentration and high density of spiral ganglion cells, and the close physical interaction between neural elements, may explain the slow retrograde degeneration found in humans after loss of peripheral receptors. Moreover, the fragile bony columns connecting the spiral canal with the osseous spiral lamina may be a potential site for trauma in (perimodiolar) electrode positioning.Copyright (c) 2005 S. Karger AG, Basel.DOI: 10.1159/000086000",pubmed,15925863,10.1159/000086000
common audiological functional parameters cafpas for single patient cases deriving statistical models from an expertlabelled data set,"503. Int J Audiol. 2020 Jul;59(7):534-547. doi: 10.1080/14992027.2020.1728401. Epub 2020 Feb 24.Common Audiological Functional Parameters (CAFPAs) for single patient cases: deriving statistical models from an expert-labelled data set.Buhl M(1)(2), Warzybok A(1)(2), Schädler MR(1)(2), Majdani O(2)(3), Kollmeier B(1)(2)(4)(5).Author information:(1)Medizinische Physik, Universität Oldenburg, Oldenburg, Germany.(2)Cluster of Excellence Hearing4all, Universität Oldenburg, Oldenburg, Germany.(3)Clinic for otolaryngology, Städt. Klinikum Wolfsburg, Wolfsburg, Germany.(4)HörTech gGmbH, Oldenburg, Germany.(5)Hearing, Speech and Audio Technology, Fraunhofer IDMT, Oldenburg, Germany.Objective: Statistical knowledge about many patients could be exploited using machine learning to provide supporting information to otolaryngologists and other hearing health care professionals, but needs to be made accessible. The Common Audiological Functional Parameters (CAFPAs) were recently introduced for the purpose of integrating data from different databases by providing an abstract representation of audiological measurements. This paper aims at collecting expert labels for a sample database and to determine statistical models from the labelled data set.Design: By an expert survey, CAFPAs as well as labels for audiological findings and treatment recommendations were collected for patients from the database of Hörzentrum Oldenburg.Study sample: A total of 287 single patient cases were assessed by twelve highly experienced audiological experts.Results: The labelled data set was used to derive probability density functions for categories given by the expert labels. The collected data set is suitable for estimating training distributions due to realistic variability contained in data for different, distinct categories. Suitable distribution functions were determined. The derived training distributions were compared regarding different audiological questions.Conclusions: The method-expert survey, sorting data into categories, and determining training distributions - could be extended to other data sets, which could then be integrated via the CAFPAs and used in a classification task.DOI: 10.1080/14992027.2020.1728401",pubmed,32091289,10.1080/14992027.2020.1728401
g proteincoupled receptors in cochlea potential therapeutic targets for hearing loss,"The prevalence of hearing loss-related diseases caused by different factors is increasing worldwide year by year. Currently, however, the patient’s hearing loss has not been effectively improved. Therefore, there is an urgent need to adopt new treatment measures and treatment techniques to help improve the therapeutic effect of hearing loss. G protein-coupled receptors (GPCRs), as crucial cell surface receptors, can widely participate in different physiological and pathological processes, particularly play an essential role in many disease occurrences and be served as promising therapeutic targets. However, no specific drugs on the market have been found to target the GPCRs of the cochlea. Interestingly, many recent studies have demonstrated that GPCRs can participate in various pathogenic process related to hearing loss in the cochlea including heredity, noise, ototoxic drugs, cochlear structure, and so on. In this review, we comprehensively summarize the functions of 53 GPCRs known in the cochlea and their relationships with hearing loss, and highlight the recent advances of new techniques used in cochlear study including cryo-EM, AI, GPCR drug screening, gene therapy vectors, and CRISPR editing technology, as well as discuss in depth the future direction of novel GPCR-based drug development and gene therapy for cochlear hearing loss. Collectively, this review is to facilitate basic and (pre-) clinical research in this area, and provide beneficial help for emerging GPCR-based cochlear therapies. Copyright © 2022 Ma, Guo, Fu, Shen, Jiang, Zhang, Zhang, Yu, Fan and Chai.",scopus,2-s2.0-85140626949,10.3389/fnmol.2022.1028125
stemness of the organ of corti relates to the epigenetic status of sox2 enhancers,"665. PLoS One. 2012;7(5):e36066. doi: 10.1371/journal.pone.0036066. Epub 2012 May 3.Stemness of the organ of Corti relates to the epigenetic status of Sox2 enhancers.Waldhaus J(1), Cimerman J, Gohlke H, Ehrich M, Müller M, Löwenheim H.Author information:(1)Department of Otorhinolaryngology, Head and Neck Surgery, Hearing Research Center Tübingen, University of Tübingen Medical Center, Tübingen, Germany.In the adult mammalian auditory epithelium, the organ of Corti, loss of sensory hair cells results in permanent hearing loss. The underlying cause for the lack of regenerative response is the depletion of otic progenitors in the cell pool of the sensory epithelium. Here, we show that an increase in the sequence-specific methylation of the otic Sox2 enhancers NOP1 and NOP2 is correlated with a reduced self-renewal potential in vivo and in vitro; additionally, the degree of methylation of NOP1 and NOP2 is correlated with the dedifferentiation potential of postmitotic supporting cells into otic stem cells. Thus, the stemness the organ of Corti is related to the epigenetic status of the otic Sox2 enhancers. These observations validate the continued exploration of treatment strategies for dedifferentiating or reprogramming of differentiated supporting cells into progenitors to regenerate the damaged organ of Corti.DOI: 10.1371/journal.pone.0036066PMCID: PMC3343037",pubmed,22570694,10.1371/journal.pone.0036066
refinement of systemic guinea pig deafening in hearing research sensorineural hearing loss induced by coadministration of kanamycin and furosemide via the leg veins,"526. Lab Anim. 2023 Dec;57(6):631-641. doi: 10.1177/00236772231167679. Epub 2023 Apr 18.Refinement of systemic guinea pig deafening in hearing research: Sensorineural hearing loss induced by co-administration of kanamycin and furosemide via the leg veins.Behrends W(1)(2), Ahrens D(3), Bankstahl JP(3), Esser KH(2), Paasche G(1)(4), Lenarz T(1)(4), Scheper V(1)(4).Author information:(1)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.(2)Auditory Neuroethology and Neurobiology, Institute of Zoology, University of Veterinary Medicine Hannover Foundation, Germany.(3)Department of Nuclear Medicine, Hannover Medical School, Germany.(4)Hearing4all Cluster of Excellence, Hannover Medical School, Germany.Auditory disabilities have a large impact on the human population worldwide. Research into understanding and treating hearing disabilities has increased significantly in recent years. One of the most relevant animal species in this context is the guinea pig, which has to be deafened to study several of the hearing pathologies and develop novel therapies. Applying kanamycin subcutaneously and furosemide intravenously is a long-established method in hearing research, leading to permanent hearing loss without surgical intervention at the ear. The intravenous application of furosemide requires invasive surgery in the cervical area of the animals to expose the jugular vein, since a relatively large volume (1 ml per 500 g body weight) must be injected over a period of about 2.5 min. We have established a gentler alternative by applying the furosemide by puncture of the leg veins. For this, custom-made cannula-needle devices were built to allow the vein puncture and subsequent slow injection of the furosemide. This approach was tested in 11 guinea pigs through the foreleg via the cephalic antebrachial vein and through the hind leg via the saphenous vein. Frequency-specific hearing thresholds were measured before and after the procedure to verify normal hearing and successful deafening, respectively. The novel approach of systemic deafening was successfully implemented in 10 out of 11 animals. The Vena saphena was best suited to the application. Since the animals' condition, post leg vein application, was better in comparison to animals deafened by exposure of the Vena jugularis, the postulated refinement that reduced animal stress was deemed successful.DOI: 10.1177/00236772231167679",pubmed,37070340,10.1177/00236772231167679
individual hearing preservation cochlear implantation using the concept of partial insertion,"256. Otol Neurotol. 2019 Mar;40(3):e326-e335. doi: 10.1097/MAO.0000000000002127.Individual Hearing Preservation Cochlear Implantation Using the Concept of Partial Insertion.Lenarz T(1), Timm ME, Salcher R, Büchner A.Author information:(1)Cluster of Excellence Hearing4all, Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany.OBJECTIVE: Aim of this study was to evaluate the method of partial insertion of flexible lateral wall electrodes in patients with residual hearing and potential electric-acoustic stimulation (EAS) users.PATIENTS AND INTERVENTION: N = 6 patients with a high-frequency hearing loss were treated with a partial insertion using atraumatic lateral wall electrodes. In three cases, a electrode of 24 mm length was inserted with the aim to achieve a 16 mm insertion depth and in three cases a electrode of 28 mm length to achieve a 20 mm insertion depth.MAIN OUTCOME MEASURE: Differences between the pre- and postoperative unaided air-conducted pure tone thresholds in low frequencies (125 Hz-1.5 kHz) were analyzed. Freiburg monosyllables (FBM) at 65 dB and Hochmair-Desoyer sentence test in noise (10 dB SNR) were performed. The pre- and postoperative cochlea images were analyzed.RESULTS: Residual hearing could be preserved in all patients (n = 6) and is stable up to 6 months follow-up. All patients could use EAS with an average speech understanding score of 65% in monosyllables (FBM) and 76% in sentences in noise. All patients benefit significantly compared to the preoperative best aided situation.CONCLUSION: First results of patients treated with partially inserted atraumatic lateral wall electrodes show good hearing preservation rates and very good speech perception results in EAS. Partial insertion appears to be a method for an individualized cochlea implantation. In case of postoperative hearing loss the electrode can be further inserted, so the patients can benefit from deeper insertion using electric stimulation only equivalent to larger electrodes.DOI: 10.1097/MAO.0000000000002127",pubmed,30741914,10.1097/MAO.0000000000002127
hearing aid noise suppression and working memory function,"37. Int J Audiol. 2018 May;57(5):335-344. doi: 10.1080/14992027.2017.1423118. Epub 2018 Jan 9.Hearing aid noise suppression and working memory function.Neher T(1)(2), Wagener KC(3), Fischer RL(4).Author information:(1)a Medizinische Physik and Cluster of Excellence ""Hearing4all"" , Carl-von-Ossietzky University , Oldenburg , Germany.(2)b Institute of Clinical Research , University of Southern Denmark , Odense , Denmark.(3)c Hörzentrum Oldenburg GmbH , Oldenburg , Germany , and.(4)d Sivantos GmbH , Erlangen , Germany.OBJECTIVE: Research findings concerning the relation between benefit from hearing aid (HA) noise suppression and working memory function are inconsistent. The current study thus investigated the effects of three noise suppression algorithms on auditory working memory and the relation with reading span.DESIGN: Using a computer simulation of bilaterally fitted HAs, four settings were tested: (1) unprocessed, (2) directional microphones, (3) single-channel noise reduction, and (4) binaural coherence-based noise reduction. Settings 2-4 were matched in terms of the speech-weighted signal-to-noise ratio (SNR) improvement. Auditory working memory was assessed at +6 dB SNR using listening span and N-back paradigms.STUDY SAMPLE: Twenty experienced HA users aged 55-80 years with large differences in reading span.RESULTS: For the listening span measurements, there was an influence of HA setting on sentence-final word recognition and recall, with the directional microphones leading to ∼6% better performance than the single-channel noise reduction. For the N-back measurements, there was substantial test-retest variability and no influence of HA setting. No interactions with reading span were found.CONCLUSION: HA noise suppression may affect the recognition and recall of speech at positive SNRs, irrespective of individual reading span. Future work should improve the reliability of the auditory working memory measurements.DOI: 10.1080/14992027.2017.1423118",pubmed,29316819,10.1080/14992027.2017.1423118
numerical analysis of intracochlear mechanical auditory stimulation using piezoelectric bending actuators,"755. Med Biol Eng Comput. 2018 May;56(5):733-747. doi: 10.1007/s11517-017-1720-0. Epub 2017 Sep 13.Numerical analysis of intracochlear mechanical auditory stimulation using piezoelectric bending actuators.Schurzig D(1), Schwarzendahl S(2), Wallaschek J(2), van Drunen WJ(3), Rau TS(3), Lenarz T(3), Majdani O(3).Author information:(1)Cluster of Excellence Hearing4all, Department of Otolaryngology, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany. schurzig.daniel@mh-hannover.de.(2)Institute of Dynamics and Vibration Research, Leibniz Universität Hannover, Hannover, Germany.(3)Cluster of Excellence Hearing4all, Department of Otolaryngology, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany.Cochlear implantation can restore a certain degree of auditory impression of patients suffering from profound hearing loss or deafness. Furthermore, studies have shown that in case of residual hearing, patients benefit from the use of a hearing aid in addition to the cochlear implant. The presented studies aim at the improvement of this electromechanical stimulation (EMS) approach by substituting the external hearing aid by an internal stimulus provided by miniaturized piezoelectric actuators. Finite element analyses are performed in order to derive fundamental guidelines for the actuator layout aiming at maximal mechanical stimuli. Further analyses aim at investigating how the actuator position inside the cochlea influences the basilar membrane oscillation profile. While actuator layout guidelines leading to maximized acoustic stimuli could be derived, some of these guidelines are of complementary nature suggesting that further studies under realistic boundary conditions must be performed. Actuator positioning inside the cochlea is shown to have a significant influence on the resulting auditory impression of the patient. Based on the results, the main differences of external and internal stimulation of the cochlea mechanism are identified. It is shown that if the cochlea tonotopy is considered, the frequency selectivity resulting from the mechanical cochlea stimulus may be improved.DOI: 10.1007/s11517-017-1720-0",pubmed,28900873,10.1007/s11517-017-1720-0
epidural recordings in cochlear implant users,"586. J Neural Eng. 2019 Jul 30;16(5):056008. doi: 10.1088/1741-2552/ab1e80.Epidural recordings in cochlear implant users.Haumann S(1), Bauernfeind G, Teschner MJ, Schierholz I, Bleichner MG, Büchner A, Lenarz T.Author information:(1)Department of Otolaryngology, Hannover Medical School, Hannover, Germany. Cluster of Excellence 'Hearing4all', Hannover & Oldenburg, Germany.OBJECTIVE: In the long term it is desirable for CI users to control their device via brain signals. A possible strategy is the use of auditory evoked potentials (AEPs). Several studies have shown the suitability of auditory paradigms for such an approach. However, these investigations are based on non-invasive recordings. When thinking about everyday life applications, it would be more convenient to use implanted electrodes for signal acquisition. Ideally, the electrodes would be directly integrated into the CI. Further it is to be expected that invasively recorded signals have higher signal quality and are less affected by artifacts.APPROACH: In this project we investigated the feasibility of implanting epidural electrodes temporarily during CI surgery and the possibility to record AEPs in the course of several days after implantation. Intraoperatively, auditory brainstem responses were recorded, whereas various kinds of AEPs were recorded postoperatively. After a few days the epidural electrodes were removed.MAIN RESULTS: Data sets of ten subjects were obtained. Invasively recorded potentials were compared subjectively and objectively to clinical standard recordings using surface electrodes. Especially the cortical evoked response audiometry depicted clearer N1 waves for the epidural electrodes which were also visible at lower stimulation intensities compared to scalp electrodes. Furthermore the signal was less disturbed by artifacts. The objective quality measure (based on data sets of six patients) showed a significant better signal quality for the epidural compared to the scalp recordings.SIGNIFICANCE: Altogether the approach revealed to be feasible and well tolerated by the patients. The epidural recordings showed a clearly better signal quality than the scalp recordings with AEPs being clearer recognizable. The results of the present study suggest that including epidural recording electrodes in future CI systems will improve the everyday life applicability of auditory closed loop systems for CI subjects.DOI: 10.1088/1741-2552/ab1e80",pubmed,31042688,10.1088/1741-2552/ab1e80
automated machine learning based speech classification for hearing aid applications and its realtime implementation on smartphone,"606. Annu Int Conf IEEE Eng Med Biol Soc. 2020 Jul;2020:956-959. doi: 10.1109/EMBC44109.2020.9175693.Automated machine learning based speech classification for hearing aid applications and its real-time implementation on smartphone.Bhat GS, Shankar N, Panahi IMS.Deep neural networks (DNNs) have been useful in solving benchmark problems in various domains including audio. DNNs have been used to improve several speech processing algorithms that improve speech perception for hearing impaired listeners. To make use of DNNs to their full potential and to configure models easily, automated machine learning (AutoML) systems are developed, focusing on model optimization. As an application of AutoML to audio and hearing aids, this work presents an AutoML based voice activity detector (VAD) that is implemented on a smartphone as a real-time application. The developed VAD can be used to elevate the performance of speech processing applications like speech enhancement that are widely used in hearing aid devices. The classification model generated by AutoML is computationally fast and has minimal processing delay, which enables an efficient, real-time operation on a smartphone. The steps involved in real-time implementation are discussed in detail. The key contribution of this work include the utilization of AutoML platform for hearing aid applications and the realization of AutoML model on smartphone. The experimental analysis and results demonstrate the significance and importance of using the AutoML for the current approach. The evaluations also show improvements over the state of art techniques and reflect the practical usability of the developed smartphone app in different noisy environments.DOI: 10.1109/EMBC44109.2020.9175693PMCID: PMC7545263",pubmed,33018143,10.1109/EMBC44109.2020.9175693
a national survey of hearing loss in the philippines,"592. Asia Pac J Public Health. 2020 Jul;32(5):235-241. doi: 10.1177/1010539520937086. Epub 2020 Jul 1.A National Survey of Hearing Loss in the Philippines.Newall JP(1), Martinez N(2), Swanepoel W(3), McMahon CM(1).Author information:(1)Macquarie University, Sydney, New South Wales, Australia.(2)University of Santo Tomas, Manila, Philippines.(3)University of Pretoria, Pretoria, South Africa.This study aimed to estimate the prevalence of hearing loss in the Philippines using a nationally representative sample. A cross-sectional national survey was undertaken utilizing a 3-stage stratified cluster design. Participants in the present study comprised 2275 adults and children with pure tone hearing assessment results. Prevalence of moderate or worse hearing loss, defined as 4FA ≥41 dBHL, was 7.5% in children <18 years, 14.7% in adults between 18 and 65 years, and 49.1% in adults >65 years. Factors associated with greater risk of moderate hearing loss in the better ear were presence of a middle ear condition (adjusted odds ratio = 2.39, 95% confidence interval = 1.49-3.85) and socioeconomic status (household income; adjusted odds ratio = 1.64, 95% confidence interval = 1.23-2.19). Age was also associated with increased risk, with adjusted odds ratios varying with age category. Prevalence of wax occlusion and outer and middle ear disease was 12.2% and 14.2%, respectively. Prevalence of hearing loss, outer, and middle ear disease appear comparatively high in the Philippines when compared with rates reported in high-income countries. Higher proportions of severe to profound hearing loss were also identified, indicating that there is both an increased prevalence and severity of hearing loss in this population.DOI: 10.1177/1010539520937086",pubmed,32608243,10.1177/1010539520937086
learning to produce syllabic speech sounds via rewardmodulated neural plasticity,"518. PLoS One. 2016 Jan 25;11(1):e0145096. doi: 10.1371/journal.pone.0145096. eCollection 2016.Learning to Produce Syllabic Speech Sounds via Reward-Modulated Neural Plasticity.Warlaumont AS(1), Finnegan MK(2).Author information:(1)Cognitive and Information Sciences, University of California, Merced, Merced, CA, United States of America.(2)Speech & Hearing Sciences, University of Illinois at Urbana-Champaign, Champaign, IL, United States of America.At around 7 months of age, human infants begin to reliably produce well-formed syllables containing both consonants and vowels, a behavior called canonical babbling. Over subsequent months, the frequency of canonical babbling continues to increase. How the infant's nervous system supports the acquisition of this ability is unknown. Here we present a computational model that combines a spiking neural network, reinforcement-modulated spike-timing-dependent plasticity, and a human-like vocal tract to simulate the acquisition of canonical babbling. Like human infants, the model's frequency of canonical babbling gradually increases. The model is rewarded when it produces a sound that is more auditorily salient than sounds it has previously produced. This is consistent with data from human infants indicating that contingent adult responses shape infant behavior and with data from deaf and tracheostomized infants indicating that hearing, including hearing one's own vocalizations, is critical for canonical babbling development. Reward receipt increases the level of dopamine in the neural network. The neural network contains a reservoir with recurrent connections and two motor neuron groups, one agonist and one antagonist, which control the masseter and orbicularis oris muscles, promoting or inhibiting mouth closure. The model learns to increase the number of salient, syllabic sounds it produces by adjusting the base level of muscle activation and increasing their range of activity. Our results support the possibility that through dopamine-modulated spike-timing-dependent plasticity, the motor cortex learns to harness its natural oscillations in activity in order to produce syllabic sounds. It thus suggests that learning to produce rhythmic mouth movements for speech production may be supported by general cortical learning mechanisms. The model makes several testable predictions and has implications for our understanding not only of how syllabic vocalizations develop in infancy but also for our understanding of how they may have evolved.DOI: 10.1371/journal.pone.0145096PMCID: PMC4726623",pubmed,26808148,10.1371/journal.pone.0145096
relation of focal haircell lesions to noiseexposure parameters from a 4 or a 05khz octave band of noise,"519. Hear Res. 2009 Aug;254(1-2):54-63. doi: 10.1016/j.heares.2009.04.011. Epub 2009 Apr 22.Relation of focal hair-cell lesions to noise-exposure parameters from a 4- or a 0.5-kHz octave band of noise.Harding GW(1), Bohne BA.Author information:(1)Department of Otolaryngology, Box 8115, Washington University School of Medicine, 660 South Euclid Avenue, St. Louis, MO 63110, USA. hardingg@ent.wustl.eduIn a previous study, we examined the relation between total energy in a noise exposure and the percentage losses of outer (OHC) and inner (IHC) hair cells in the basal and apical halves of 607 chinchilla cochleae [Harding, G.W., Bohne, B.A., 2004a. Noise-induced hair-cell loss and total exposure energy: analysis of a large data set. J. Acoust. Soc. Am. 115, 2207-2220]. The animals had been exposed continuously to either a 4-kHz octave band of noise (OBN) at 47-108 dB SPL for 0.5h-36 d, or a 0.5-kHz OBN at 65-128 dB SPL for 3.5h-433 d. Interrupted exposures were also employed with both OBNs. Post-exposure recovery times ranged from 0 to 913 days. Cluster analysis was used to separate the data into three magnitudes of damage. The data were also separated into recovery times of 0 days (acute) and >0 days (chronic) and the apical and basal halves of the organ of Corti (OC). A substantial part of these hair-cell losses occurred in focal lesions (i.e., >or=50% loss of IHCs, OHCs or both over a distance of >or=0.03 mm). This aspect of the damage from noise was not included in the previous analysis. The present analysis describes, within the same three clusters, the apex-to-base distribution of 1820 focal lesions found in 468 of 660 (71%) noise-exposed cochleae. In these cochleae, OC length in mm was converted to percent distance from the apex. The lesion data were analyzed for location in percent distance from the apex and size (mm) of the lesions. In 55 of 140 (39%) non-noise-exposed, control OCs, there were 186 focal hair-cell lesions, the characteristics of which were also determined. Focal lesions with hair-cell loss >or=50% involved predominantly OHCs, IHCs only, or both OHCs and IHCs (i.e., combined OHC-IHC lesions). The predominantly OHC and combined lesions were pooled together for the analysis. The distributions of lesion location (in percent distance from the apex), weighted by lesion size (in percent of OC length) were tallied in 2%-distance bins. In controls, focal lesions were uniformly distributed from apex to base and 70% of them were pure IHC lesions. In cochleae exposed to the 4-kHz OBN, lesions were distributed throughout the basal half of the OC. In cochleae exposed to the 0.5-kHz OBN, lesions occurred in both halves of the OC. With continuous exposures, 74% of the lesions were predominantly OHC or combined lesions. With interrupted exposures, 52% of the lesions were OHC or combined lesions. Lesion size was generally larger in the chronic compared to acute cochleae with similar exposures. There was a minimum total energy at which focal lesions began to appear and slightly higher energies resulted in nearly all exposed cochleae having focal lesions.DOI: 10.1016/j.heares.2009.04.011",pubmed,19393307,10.1016/j.heares.2009.04.011
subjective listening effort and electrodermal activity in listening situations with reverberation and noise,"181. Trends Hear. 2016 Oct 3;20:2331216516667734. doi: 10.1177/2331216516667734.Subjective Listening Effort and Electrodermal Activity in Listening Situations with Reverberation and Noise.Holube I(1), Haeder K(2), Imbery C(2), Weber R(3).Author information:(1)Institute of Hearing Technology and Audiology, Jade University of Applied Sciences, Oldenburg, Germany Cluster of Excellence ""Hearing4All"", Oldenburg, Germany inga.holube@jade-hs.de.(2)Institute of Hearing Technology and Audiology, Jade University of Applied Sciences, Oldenburg, Germany.(3)Department of Medical Physics and Acoustics, University of Oldenburg, Oldenburg, Germany.Disturbing factors like reverberation or ambient noise can impair speech recognition and raise the listening effort needed for successful communication in daily life. Situations with high listening effort are thought to result in increased stress for the listener. The aim of this study was to explore possible measures to determine listening effort in situations with varying background noise and reverberation. For this purpose, subjective ratings of listening effort, speech recognition, and stress level, together with the electrodermal activity as a measure of the autonomic stress reaction, were investigated. It was expected that the electrodermal activity would show different stress levels in different acoustic situations and might serve as an alternative to subjective ratings. Ten young normal-hearing and 17 elderly hearing-impaired subjects listened to sentences from the Oldenburg sentence test either with stationary background noise or with reverberation. Four listening situations were generated, an easy and a hard one for each of the two disturbing factors, which were related to each other by the Speech Transmission Index. The easy situation resulted in 100% and the hard situation resulted in 30 to 80% speech recognition. The results of the subjective ratings showed significant differences between the easy and the hard listening situations in both subject groups. Two methods of analyzing the electrodermal activity values revealed similar, but nonsignificant trends. Significant correlations between subjective ratings and physiological electrodermal activity data were observed for normal-hearing subjects in the noise situation.© The Author(s) 2016.DOI: 10.1177/2331216516667734PMCID: PMC5051672",pubmed,27698257,10.1177/2331216516667734
speech perception and cortical auditory evoked potentials in cochlear implant users with auditory neuropathy spectrum disorders,"225. Int J Pediatr Otorhinolaryngol. 2012 Sep;76(9):1332-8. doi: 10.1016/j.ijporl.2012.06.001. Epub 2012 Jul 15.Speech perception and cortical auditory evoked potentials in cochlear implant users with auditory neuropathy spectrum disorders.Alvarenga KF(1), Amorim RB, Agostinho-Pesse RS, Costa OA, Nascimento LT, Bevilacqua MC.Author information:(1)Department of Audiology and Speech Pathology at the School of Dentistry, University of São Paulo, Bauru Campus, Brazil. katialv@fob.usp.brOBJECTIVE: To characterize the P(1) component of long latency auditory evoked potentials (LLAEPs) in cochlear implant users with auditory neuropathy spectrum disorder (ANSD) and determine firstly whether they correlate with speech perception performance and secondly whether they correlate with other variables related to cochlear implant use.METHODS: This study was conducted at the Center for Audiological Research at the University of São Paulo. The sample included 14 pediatric (4-11 years of age) cochlear implant users with ANSD, of both sexes, with profound prelingual hearing loss. Patients with hypoplasia or agenesis of the auditory nerve were excluded from the study. LLAEPs produced in response to speech stimuli were recorded using a Smart EP USB Jr. system. The subjects' speech perception was evaluated using tests 5 and 6 of the Glendonald Auditory Screening Procedure (GASP).RESULTS: The P(1) component was detected in 12/14 (85.7%) children with ANSD. Latency of the P(1) component correlated with duration of sensorial hearing deprivation (*p=0.007, r=0.7278), but not with duration of cochlear implant use. An analysis of groups assigned according to GASP performance (k-means clustering) revealed that aspects of prior central auditory system development reflected in the P(1) component are related to behavioral auditory skills.CONCLUSIONS: In children with ANSD using cochlear implants, the P(1) component can serve as a marker of central auditory cortical development and a predictor of the implanted child's speech perception performance.Copyright © 2012 Elsevier Ireland Ltd. All rights reserved.DOI: 10.1016/j.ijporl.2012.06.001",pubmed,22796193,10.1016/j.ijporl.2012.06.001
a novel microdeletion at chromosome 2q311312 in a threegeneration family presenting duplication of great toes with clinodactyly,"HOXD gene cluster maps to chromosome 2q31 and plays a key role in embryonic limb morphogenesis. Mutations of the HOXD13 and HOXD10 genes have been found to be associated with digital and limb malformations. In addition, dysregulation of HOXD gene cluster has been proposed to account for the limb abnormalities in patients with chromosome 2q rearrangements. In this report, we investigated a three-generation family presenting clinical phenotypes of duplication of great toes, tapering fingers, and clinodactyly of the fifth finger in both hands, which were transmitted in a dominant fashion in this family. We identified and validated an interstitial microdeletion of ∼3.4Mb at chromosome 2q31.1-31.2 by array-based comparative genomic hybridization, fluorescence in situ hybridization, and real-time quantitative polymerase chain reaction that cosegregates with the clinical phenotypes in this family. The microdeletion removes 30 labeled genes including the entire HOXD gene cluster, suggesting that the digital abnormalities of this family may be attributed to the haploinsufficiency of the HOXD gene cluster. The delineation of the microdeletion region may contribute to the genotype-phenotype correlation study in patients with genomic rearrangements of the long arm of chromosome 2 and helps to understand the pathogenesis of haploinsufficiency of the HOXD gene cluster. © 2009 John Wiley & Sons A/S.",scopus,2-s2.0-65449134777,10.1111/j.1399-0004.2008.01147.x
inference of the distortion component of hearing impairment from speech recognition by predicting the effect of the attenuation component,"139. Int J Audiol. 2022 Mar;61(3):205-219. doi: 10.1080/14992027.2021.1929515. Epub 2021 Jun 3.Inference of the distortion component of hearing impairment from speech recognition by predicting the effect of the attenuation component.Hülsmeier D(1)(2), Buhl M(1)(2), Wardenga N(2)(3), Warzybok A(1)(2), Schädler MR(1)(2), Kollmeier B(1)(2).Author information:(1)Medical Physics, CvO University Oldenburg, Oldenburg, Germany.(2)Cluster of Excellence Hearing4all, Oldenburg, Germany.(3)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.OBJECTIVE: A model-based determination of the average supra-threshold (""distortion"") component of hearing impairment which limits the benefit of hearing aid amplification.DESIGN: Published speech recognition thresholds (SRTs) were predicted with the framework for auditory discrimination experiments (FADE), which simulates recognition processes, the speech intelligibility index (SII), which exploits frequency-dependent signal-to-noise ratios (SNR), and a modified SII with a hearing-loss-dependent band importance function (PAV). Their attenuation-component-based prediction errors were interpreted as estimates of the distortion component.STUDY SAMPLE: Unaided SRTs of 315 hearing-impaired ears measured with the German matrix sentence test in stationary noise.RESULTS: Overall, the models showed root-mean-square errors (RMSEs) of 7 dB, but for steeply sloping hearing loss FADE and PAV were more accurate (RMSE = 9 dB) than the SII (RMSE = 23 dB). Prediction errors of FADE and PAV increased linearly with the average hearing loss. The consideration of the distortion component estimate significantly improved the accuracy of FADE's and PAV's predictions.CONCLUSIONS: The supra-threshold distortion component-estimated by prediction errors of FADE and PAV-seems to increase with the average hearing loss. Accounting for a distortion component improves the model predictions and implies a need for effective compensation strategies for supra-threshold processing deficits with increasing audibility loss.DOI: 10.1080/14992027.2021.1929515",pubmed,34081564,10.1080/14992027.2021.1929515
outer ear canal sound pressure and bone vibration measurement in ssd and chl patients using a transcutaneous bone conduction instrument,"405. Hear Res. 2016 Oct;340:161-168. doi: 10.1016/j.heares.2015.12.019. Epub 2015 Dec 23.Outer ear canal sound pressure and bone vibration measurement in SSD and CHL patients using a transcutaneous bone conduction instrument.Ghoncheh M(1), Lilli G(1), Lenarz T(2), Maier H(3).Author information:(1)Department of Otolaryngology and Institute of Audioneurotechnology (VIANNA), Hannover Medical School, Hannover, Germany.(2)Department of Otolaryngology and Institute of Audioneurotechnology (VIANNA), Hannover Medical School, Hannover, Germany; Cluster of Excellence Hearing4all, Germany.(3)Department of Otolaryngology and Institute of Audioneurotechnology (VIANNA), Hannover Medical School, Hannover, Germany; Cluster of Excellence Hearing4all, Germany. Electronic address: Maier.Hannes@MH-Hannover.de.The intraoperative and postoperative objective functional assessment of transcutaneous bone conduction implants is still a challenge. Here we compared intraoperative Laser-Doppler-vibrometry (LDV, Polytec Inc.) to measure vibration of the bone close to the implant to Outer Ear Canal Sound Pressure Level (OEC-SPL) measurements. Twelve single sided deafness (SSD) patients with contralateral intact ossicular chains and eight bilateral conductive hearing loss (CHL) patients were included in the study. SSD patients had a minor average air-bone-gap (ABG) of 0.4 ± 0.4 dB (0.5, 1, 2, 4 kHz mean value (MV) ± standard deviation (SD)) on the contralateral side where a normal transmission between cochlea and the tympanic membrane can be assumed. CHL patients had an impaired middle ear transmission with a mean ABG of 46.0 ± 7.9 dB (MV±SD). Vibration and OEC-SPL responses could reliably be recorded with a minimal signal-to-noise ratio of at least 12 dB. Average OEC-SPL on the contralateral side and intraoperative vibration measurements were strongly correlated in SSD (r2 = 0.75) and CHL (r2 = 0.86) patients. The correlation in individual results between OEC-SPL and vibration measurements was weak, indicating some underlying inter-individual variability. The high correlation of average responses showed that OEC-SPL are closely linked to bone vibration, although both cannot be equivalently used for intraoperative testing due to the high variability in individual results. On the other hand, OEC-SPL provides an easy and affordable measurement tool to monitor stability and functionality postoperatively using individual reference measurements. We observed no significant differences (t-test, p < 0.05) by comparing results from contralateral OEC-SPL in twelve SSD and eight CHL patients at frequencies between 0.5 and 8 kHz. This implies that the part of the measured sound pressure in the ear canal originating from the cochlea and emitted by the tympanic is not dominant and OEC-SPL is mainly due to vibration of the external ear-canal walls as the only other pathway of BC sound to reach the ear canal. In addition, the transcranial attenuation (contralateral outer ear canal sound pressure divided by ipsilateral) was compared to previous studies measuring vibration by LDV and accelerometer. The trend in the average transcranial attenuation in patients was similar to previous studies measuring the OEC-SPL with less than 5 dB difference.Copyright © 2015 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2015.12.019",pubmed,26723102,10.1016/j.heares.2015.12.019
comparative gene expression study of the vestibular organ of the igf1 deficient mouse using wholetranscript arrays,"657. Hear Res. 2015 Dec;330(Pt A):62-77. doi: 10.1016/j.heares.2015.08.016. Epub 2015 Sep 1.Comparative gene expression study of the vestibular organ of the Igf1 deficient mouse using whole-transcript arrays.Rodríguez-de la Rosa L(1), Sánchez-Calderón H(2), Contreras J(3), Murillo-Cuesta S(1), Falagan S(4), Avendaño C(5), Dopazo J(6), Varela-Nieto I(1), Milo M(7).Author information:(1)Neurobiology of Hearing, Department of Endocrine and Nervous System Pathophysiology, Alberto Sols Biomedical Research Institute (IIBM), CSIC-UAM, Madrid, Spain; Biomedical Research Networking Center on Rare Diseases (CIBERER), Institute of Health Carlos III (ISCIII), Madrid, Spain; IdiPAZ Institute for Health Research, Madrid, Spain.(2)Neurobiology of Hearing, Department of Endocrine and Nervous System Pathophysiology, Alberto Sols Biomedical Research Institute (IIBM), CSIC-UAM, Madrid, Spain.(3)Neurobiology of Hearing, Department of Endocrine and Nervous System Pathophysiology, Alberto Sols Biomedical Research Institute (IIBM), CSIC-UAM, Madrid, Spain; Biomedical Research Networking Center on Rare Diseases (CIBERER), Institute of Health Carlos III (ISCIII), Madrid, Spain; Department of Anatomy, Faculty of Veterinary, Complutense University, Madrid, Spain.(4)Department of Anatomy, Faculty of Medicine, Autonomous University, Madrid, Spain.(5)IdiPAZ Institute for Health Research, Madrid, Spain; Department of Anatomy, Faculty of Medicine, Autonomous University, Madrid, Spain.(6)Biomedical Research Networking Center on Rare Diseases (CIBERER), Institute of Health Carlos III (ISCIII), Madrid, Spain; Department of Computational Genomics, Centro de Investigación Príncipe Felipe, Valencia, Spain.(7)Department of Biomedical Science, University of Sheffield, Sheffield, UK. Electronic address: m.milo@sheffield.ac.uk.The auditory and vestibular organs form the inner ear and have a common developmental origin. Insulin like growth factor 1 (IGF-1) has a central role in the development of the cochlea and maintenance of hearing. Its deficiency causes sensorineural hearing loss in man and mice. During chicken early development, IGF-1 modulates neurogenesis of the cochleovestibular ganglion but no further studies have been conducted to explore the potential role of IGF-1 in the vestibular system. In this study we have compared the whole transcriptome of the vestibular organ from wild type and Igf1(-/-) mice at different developmental and postnatal times. RNA was prepared from E18.5, P15 and P90 vestibular organs of Igf1(-/-) and Igf1(+/+) mice and the transcriptome analysed in triplicates using Affymetrix(®) Mouse Gene 1.1 ST Array Plates. These plates are whole-transcript arrays that include probes to measure both messenger (mRNA) and long intergenic non-coding RNA transcripts (lincRNA), with a coverage of over 28 thousand coding transcripts and over 7 thousands non-coding transcripts. Given the complexity of the data we used two different methods VSN-RMA and mmBGX to analyse and compare the data. This is to better evaluate the number of false positives and to quantify uncertainty of low signals. We identified a number of differentially expressed genes that we described using functional analysis and validated using RT-qPCR. The morphology of the vestibular organ did not show differences between genotypes and no evident alterations were observed in the vestibular sensory areas of the null mice. However, well-defined cellular alterations were found in the vestibular neurons with respect their number and size. Although these mice did not show a dramatic vestibular phenotype, we conducted a functional analysis on differentially expressed genes between genotypes and across time. This was with the aim to identify new pathways that are involved in the development of the vestibular organ as well as pathways that maybe affected by the lack of IGF-1 and be associated to the morphological changes of the vestibular neurons that we observed in the Igf1(-/-) mice.Copyright © 2015 The Authors. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2015.08.016",pubmed,26341476,10.1016/j.heares.2015.08.016
a twostage sign language recognition method focusing on the semantic features of label text,"The ability to recognize sign language is an indispensable technology that plays a crucial role in facilitating communication between individuals who are deaf or hard of hearing. It is of utmost importance to comprehensively understand the nonverbal expressions employed by the hearing impaired. In order to enhance the efficacy of sign language recognition technology, it is imperative to focus on language modeling and improve the utilization of linguistic elements. At present, much attention in sign language recognition techniques that integrate language modeling is directed toward the translation of GLOSS to text in research related to Sign Language Translation (SLT). Our paper, however, proposes a creative approach that involves the linguistic modeling of the corresponding text of sign language during the process of converting signs to GLOSS. Specifically, we have implemented a text correction module that uses a front-mounted sign language recognition module to make preliminary predictions. The corrected GLOSS sequence is then used to obtain the final recognition result with higher accuracy. Our framework was tested on the RWTHPHOENIX-Weather-2014-T dataset and CSL dataset to evaluate its effectiveness in recognizing sign language on a large scale. The experimental results demonstrate that the proposed method significantly enhances the accuracy of the sign language recognition model.",ieee,2640-5768,10.1109/AISP61396.2024.10475205
using machine learning to predict sensorineural hearing loss based on perilymph micro rna expression profile,,base,aec94d0092b6b0db052420717ec1d2f34246ba2f77f955283317701e677caa44,
combination of egg tray silencer and progressive relaxation to overcome community auditory disorders in indonesian noise train environments combinacin de silenciador de bandeja de huevos y relajacin progresiva para superar los trastornos auditivos comunitarios en ambientes de ruido de tren en indonesia,"Introduction: People who live near the edge of the railroad tracks cannot avoid environmental noise. Various efforts have been made to build a barrier between railroad crossings and residential areas and install silencers on locomotives. However. the environmental noise intensity is still above the normal threshold value (NAV 55dB). Therefore, the combination of egg tray damper and progressive relaxation (KODAMSI) is expected to be an alternative solution. This study aimed to prove the effect of KODAMSI on temporary hearing loss in respondents who are exposed to train noise intensity. M ethods: This study used a randomized control trial (RCT). Research subjects were screened and determined by two-stage cluster random sampling. Three hundred people (35 %) from 3 villages living on the edge of the railway were screened using the KODAMSI, and 30 research subjects were obtained according to inclusion and exclusion criteria. An amount of 30 respondents were randomly assigned to cluster stage II. The statistical analysis used Mann-Whitney and Willcoxon test. Results: The mean difference in reduction (delta) of noise intensity before and after the intervention of the egg tray silencer in the treatment group was 33.87dB. KODAMSI proved effective in improving hearing loss (p=0.0001) from moderate (41.33dB) to mild (32.15dB) with an average delta threshold of 9.2dB. Conclusion: KODAMSI has effectively reduced the intensity of environmental noise in noise pollution from train tracks and can significantly improve temporary hearing loss. KODAMSI might become an alternative to diminish hearing loss and prevent noise in the community. © 2022 Academia Nacional de Medicina. All rights reserved.",scopus,2-s2.0-85132739062,10.47307/GMC.2022.130.S1.42
the emergence of machine learning in auditory neural impairment a systematic review,"31. Neurosci Lett. 2021 Nov 20;765:136250. doi: 10.1016/j.neulet.2021.136250. Epub 2021 Sep 15.The emergence of machine learning in auditory neural impairment: A systematic review.Abu Bakar AR(1), Lai KW(2), Hamzaid NA(3).Author information:(1)Department of Biomedical Engineering, Faculty of Engineering, Universiti Malaya, 50603 Kuala Lumpur, Malaysia. Electronic address: abdul.rauf@siswa.um.edu.my.(2)Department of Biomedical Engineering, Faculty of Engineering, Universiti Malaya, 50603 Kuala Lumpur, Malaysia. Electronic address: lai.khinwee@um.edu.my.(3)Department of Biomedical Engineering, Faculty of Engineering, Universiti Malaya, 50603 Kuala Lumpur, Malaysia.Hearing loss is a common neurodegenerative disease that can start at any stage of life. Misalignment of the auditory neural impairment may impose challenges in processing incoming auditory stimulus that can be measured using electroencephalography (EEG). The electrophysiological behaviour response emanated from EEG auditory evoked potential (AEP) requires highly trained professionals for analysis and interpretation. Reliable automated methods using techniques of machine learning would assist the auditory assessment process for informed treatment and practice. It is thus highly required to develop models that are more efficient and precise by considering the characteristics of brain signals. This study aims to provide a comprehensive review of several state-of-the-art techniques of machine learning that adopt EEG evoked response for the auditory assessment within the last 13 years. Out of 161 initially screened articles, 11 were retained for synthesis. The outcome of the review presented that the Support Vector Machine (SVM) classifier outperformed with over 80% accuracy metric and was recognized as the best suited model within the field of auditory research. This paper discussed the comprehensive iterative properties of the proposed computed algorithms and the feasible future direction in hearing impaired rehabilitation.Copyright © 2021 Elsevier B.V. All rights reserved.DOI: 10.1016/j.neulet.2021.136250",pubmed,34536511,10.1016/j.neulet.2021.136250
unmyelinated auditory type i spiral ganglion neurons in congenic ly51 mice,"With the exception of humans, the somata of type I spiral ganglion neurons (SGNs) of most mammalian species are heavily myelinated. In an earlier study, we used Ly5.1 congenic mice as transplant recipients to investigate the role of hematopoietic stem cells in the adult mouse inner ear. An unanticipated finding was that a large percentage of the SGNs in this strain were unmyelinated. Further characterization of the auditory phenotype of young adult Ly5.1 mice in the present study revealed several unusual characteristics, including 1) large aggregates of unmyelinated SGNs in the apical and middle turns, 2) symmetrical junction-like contacts between the unmyelinated neurons, 3) abnormal expression patterns for CNPase and connexin 29 in the SGN clusters, 4) reduced SGN density in the basal cochlea without a corresponding loss of sensory hair cells, 5) significantly delayed auditory brainstem response (ABR) wave I latencies at low and middle frequencies compared with control mice with similar ABR threshold, and 6) elevated ABR thresholds and deceased wave I amplitudes at high frequencies. Taken together, these data suggest a defect in Schwann cells that leads to incomplete myelinization of SGNs during cochlear development. The Ly5.1 mouse strain appears to be the only rodent model so far identified with a high degree of the ""human-like"" feature of unmyelinated SGNs that aggregate into neural clusters. Thus, this strain may provide a suitable animal platform for modeling human auditory information processing such as synchronous neural activity and other auditory response properties. © 2010 Wiley-Liss, Inc.",scopus,2-s2.0-77954613965,10.1002/cne.22398
neural correlates of agerelated declines in frequency selectivity in the auditory midbrain,"Reduced frequency selectivity is associated with an age-related decline in speech recognition in background noise and reverberant environments. To elucidate neural correlates of age-related alteration in frequency selectivity, the present study examined frequency response areas (FRAs) of multi-unit clusters in the inferior colliculus of young, middle-aged, and old CBA/CaJ mice. The FRAs in middle-aged and old mice were found to be broader and more asymmetric in shape. In addition to a decrease of closed/complex FRAs in both middle age and old groups, there was a transient decrease in V-shaped FRAs and a concomitant increase in multipeak FRAs in middle age. Intensity coding was also affected by age, as observed in an increase of monotonic responses in middle-aged and old mice. While a decline in low-level activity began in middle age, reduced driven rates at suprathreshold levels occurred later in old age. Collectively, these results support the view that aging alters frequency selectivity by widening excitatory FRAs and that these changes begin to appear in middle age. © 2009 Elsevier Inc.",scopus,2-s2.0-78449313444,10.1016/j.neurobiolaging.2009.01.006
distribution of focal lesions in the chinchilla organ of corti following exposure to a 4khz or a 05khz octave band of noise,"610. Hear Res. 2007 Mar;225(1-2):50-9. doi: 10.1016/j.heares.2006.12.012. Epub 2007 Jan 13.Distribution of focal lesions in the chinchilla organ of Corti following exposure to a 4-kHz or a 0.5-kHz octave band of noise.Harding GW(1), Bohne BA.Author information:(1)Department of Otolaryngology, Box 8115, Washington University School of Medicine, 660 South Euclid Avenue, St. Louis, MO 63110, USA. hardingg@ent.wustl.eduAn octave band of noise (OBN) delivers fairly uniform acoustic energy over a specific range of frequencies. Above and below this range, energy is at least 30 dB SPL less than that within the OBN. When the ear is exposed to an OBN, hair-cell loss often occurs outside the octave band. The frequency location of hair-cell loss is evident when the percent distance from the apex of focal lesions is analyzed. Focal lesions involve substantial loss of outer hair cells (OHCs) only, inner hair cells (IHCs) only, or both OHCs and IHCs (i.e., combined lesions) in a specific region of the organ of Corti (OC). Data sets were assembled from our permanent collection of noise-exposed chinchillas as follows: (1) the sum of exposure duration and recovery time was less than or equal to 11 d; (2) the exposure level was less than or equal to 108 dB SPL; and (3) focal lesions were less than 1.5mm in length. The data sets included a variety of exposures ranging from high-level, short duration to moderate-level, moderate duration. The center of each focal lesion was expressed as percent distance from the OC apex. Means, standard deviations and medians were calculated for focal-lesion size resulting from exposure to a 4-kHz or a 0.5-kHz OBN. Histograms were then constructed from the percent-location data using 2.0% bins. For the 4-kHz OBN, 5% of the lesions were in the apical half of the OC and 95% were in the basal half. The mean lesion size was 1.68% of total OC length for OHC and combined focal lesions and 0.42% for IHC focal lesions. Most OHC and combined lesions occurred in the 5-7-kHz region, at and just above the upper edge of the OBN. Clusters of lesions were also found around 8 and 12 kHz. A cluster was present at and just below the lower edge of the OBN, as well as another in the 1.5-kHz region. For the 0.5-kHz OBN, 34% of the lesions were in the apical half of the OC and 66% were in the basal half. The mean lesion size was 0.93% for OHC and combined focal lesions and 0.32% for IHC focal lesions. OHC and combined focal-lesion distribution showed clusters at 0.25, 0.75 and 1.5 kHz in the apical half of the OC. In the basal half, the distribution of focal lesions was similar to that seen with the 4-kHz OBN (r=0.54). With both OBNs, most IHC focal lesions occurred in the basal half of the OC. High resolution power spectrum analysis of each OBN and non-invasive tests for harmonics and distortion products in a chinchilla were performed to look for exposure energy above and below the OBN. No energy was found that could explain the OC damage.DOI: 10.1016/j.heares.2006.12.012",pubmed,17291699,10.1016/j.heares.2006.12.012
comparison of singlemicrophone noise reduction schemes can hearing impaired listeners tell the difference,"351. Int J Audiol. 2018 Jun;57(sup3):S55-S61. doi: 10.1080/14992027.2017.1279758. Epub 2017 Jan 23.Comparison of single-microphone noise reduction schemes: can hearing impaired listeners tell the difference?Huber R(1), Bisitz T(1), Gerkmann T(2), Kiessling J(3), Meister H(4), Kollmeier B(2).Author information:(1)a HörTech gGmbH and Cluster of Excellence Hearing4All , Oldenburg , Germany.(2)b Department of Medical Physics and Acoustics , University of Oldenburg, and Cluster of Excellence Hearing4All , Oldenburg , Germany.(3)c Funktionsbereich Audiologie , Justus-Liebig University Giessen , Giessen , Germany , and.(4)d Jean Uhrmacher Institute for Clinical ENT-Research , University of Cologne , Cologne , Germany.OBJECTIVE: The perceived qualities of nine different single-microphone noise reduction (SMNR) algorithms were to be evaluated and compared in subjective listening tests with normal hearing and hearing impaired (HI) listeners.DESIGN: Speech samples added with traffic noise or with party noise were processed by the SMNR algorithms. Subjects rated the amount of speech distortions, intrusiveness of background noise, listening effort and overall quality, using a simplified MUSHRA (ITU-R, 2003 ) assessment method.STUDY SAMPLE: 18 normal hearing and 18 moderately HI subjects participated in the study.RESULTS: Significant differences between the rating behaviours of the two subject groups were observed: While normal hearing subjects clearly differentiated between different SMNR algorithms, HI subjects rated all processed signals very similarly. Moreover, HI subjects rated speech distortions of the unprocessed, noisier signals as being more severe than the distortions of the processed signals, in contrast to normal hearing subjects.CONCLUSIONS: It seems harder for HI listeners to distinguish between additive noise and speech distortions or/and they might have a different understanding of the term ""speech distortion"" than normal hearing listeners have. The findings confirm that the evaluation of SMNR schemes for hearing aids should always involve HI listeners.DOI: 10.1080/14992027.2017.1279758",pubmed,28112001,10.1080/14992027.2017.1279758
the association between hearing threshold and urinary personal care and consumer product metabolites in middleaged and elderly people from the usa,"682. Environ Sci Pollut Res Int. 2022 Nov;29(53):81076-81086. doi: 10.1007/s11356-022-21459-5. Epub 2022 Jun 22.The association between hearing threshold and urinary personal care and consumer product metabolites in middle-aged and elderly people from the USA.Fu YP(#)(1), Chen WY(#)(2), Guo LQ(1), Zhu YQ(1), Yuan JS(1), Liu YH(3).Author information:(1)Department of Otorhinolaryngology Head and Neck Surgery, Second Affiliated Hospital of Nanchang University, No.1 Minde Road, Nanchang, China.(2)Interventional Cardiology Department, Second Affiliated Hospital of Nanchang University, No.1 Minde Road, Nanchang, China.(3)Department of Otorhinolaryngology Head and Neck Surgery, Second Affiliated Hospital of Nanchang University, No.1 Minde Road, Nanchang, China. Liuyuehuindefy@21cn.com.(#)Contributed equallyEndocrine disruptors have been reported to be associated with hearing ability. However, the association between personal care and consumer product chemicals, known as commonly detected endocrine disruptors, and age-related hearing loss still remains unclear. This study aimed to examine the association between exposure to 7 personal care and consumer product chemicals and hearing thresholds in middle-aged and elderly people. A nationally representative cross-sectional study was performed. Eight hundred forty-five adults aged over 45 from the National Health and Nutrition Examination Survey (NHANES) were included in this study. Bayesian kernel machine regression (BKMR) and the k-medoid cluster analysis were used to evaluate the mixture effect of exposure to 7 chemicals on pure-tone average (PTA). Exposure to these chemicals was negatively associated with PTA. 2,5-Dichlorophenol had the greatest contribution to the mixture effect. The mixture effect was stronger in women, elderly people. Four pooled clusters were identified according to 7 chemicals exposures. Cluster 4 (high TCS exposure) showed a lower HFPTA (P = 0.00258) than cluster 3 (the lowest exposure cluster, as a reference). Our study provides evidence that exposure to personal care and consumer product chemicals might be inversely associated with PTA. More studies are needed to fully understand the association of exposure to these chemicals with hearing threshold.© 2022. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.DOI: 10.1007/s11356-022-21459-5",pubmed,35731440,10.1007/s11356-022-21459-5
movement and gaze behavior in virtual audiovisual listening environments resembling everyday life,"293. Trends Hear. 2019 Jan-Dec;23:2331216519872362. doi: 10.1177/2331216519872362.Movement and Gaze Behavior in Virtual Audiovisual Listening Environments Resembling Everyday Life.Hendrikse MME(1), Llorach G(1)(2), Hohmann V(1)(2), Grimm G(1).Author information:(1)Medizinische Physik and Cluster of Excellence 'Hearing4all', Universität Oldenburg, Germany.(2)Hörzentrum Oldenburg GmbH, Germany.Recent achievements in hearing aid development, such as visually guided hearing aids, make it increasingly important to study movement behavior in everyday situations in order to develop test methods and evaluate hearing aid performance. In this work, audiovisual virtual environments (VEs) were designed for communication conditions in a living room, a lecture hall, a cafeteria, a train station, and a street environment. Movement behavior (head movement, gaze direction, and torso rotation) and electroencephalography signals were measured in these VEs in the laboratory for 22 younger normal-hearing participants and 19 older normal-hearing participants. These data establish a reference for future studies that will investigate the movement behavior of hearing-impaired listeners and hearing aid users for comparison. Questionnaires were used to evaluate the subjective experience in the VEs. A test-retest comparison showed that the measured movement behavior is reproducible and that the measures of movement behavior used in this study are reliable. Moreover, evaluation of the questionnaires indicated that the VEs are sufficiently realistic. The participants rated the experienced acoustic realism of the VEs positively, and although the rating of the experienced visual realism was lower, the participants felt to some extent present and involved in the VEs. Analysis of the movement data showed that movement behavior depends on the VE and the age of the subject and is predictable in multitalker conversations and for moving distractors. The VEs and a database of the collected data are publicly available.DOI: 10.1177/2331216519872362PMCID: PMC6732870",pubmed,32516060,10.1177/2331216519872362
influence of multimicrophone signal enhancement algorithms on the acoustics and detectability of angular and radial source movements,"449. Trends Hear. 2018 Jan-Dec;22:2331216518779719. doi: 10.1177/2331216518779719.Influence of Multi-microphone Signal Enhancement Algorithms on the Acoustics and Detectability of Angular and Radial Source Movements.Lundbeck M(1)(2), Hartog L(1)(2), Grimm G(1)(2), Hohmann V(1)(2), Bramsløw L(3), Neher T(1)(4).Author information:(1)1 Medizinische Physik and Cluster of Excellence ""Hearing4all"", Oldenburg University, Germany.(2)2 HörTech gGmbH, Oldenburg, Germany.(3)3 Eriksholm Research Centre, Oticon A/S, Snekkersten, Denmark.(4)4 Institute of Clinical Research, University of Southern Denmark, Odense, Denmark.Hearing-impaired listeners are known to have difficulties not only with understanding speech in noise but also with judging source distance and movement, and these deficits are related to perceived handicap. It is possible that the perception of spatially dynamic sounds can be improved with hearing aids (HAs), but so far this has not been investigated. In a previous study, older hearing-impaired listeners showed poorer detectability for virtual left-right (angular) and near-far (radial) source movements due to lateral interfering sounds and reverberation, respectively. In the current study, potential ways of improving these deficits with HAs were explored. Using stimuli very similar to before, detailed acoustic analyses were carried out to examine the influence of different HA algorithms for suppressing noise and reverberation on the acoustic cues previously shown to be associated with source movement detectability. For an algorithm that combined unilateral directional microphones with binaural coherence-based noise reduction and for a bilateral beamformer with binaural cue preservation, movement-induced changes in spectral coloration, signal-to-noise ratio, and direct-to-reverberant energy ratio were greater compared with no HA processing. To evaluate these two algorithms perceptually, aided measurements of angular and radial source movement detectability were performed with 20 older hearing-impaired listeners. The analyses showed that, in the presence of concurrent interfering sounds and reverberation, the bilateral beamformer could restore source movement detectability in both spatial dimensions, whereas the other algorithm only improved detectability in the near-far dimension. Together, these results provide a basis for improving the detectability of spatially dynamic sounds with HAs.DOI: 10.1177/2331216518779719PMCID: PMC6024528",pubmed,29900799,10.1177/2331216518779719
sequence variations of mitochondrial dna and individual sensitivity to the ototoxic effect of cisplatin,"563. Anticancer Res. 2003 Mar-Apr;23(2B):1249-55.Sequence variations of mitochondrial DNA and individual sensitivity to the ototoxic effect of cisplatin.Peters U(1), Preisler-Adams S, Lanvers-Kaminsky C, Jürgens H, Lamprecht-Dinnesen A.Author information:(1)Institute for Human Genetics, University of Muenster, Muenster, Germany.BACKGROUND: Since mutations in the mitochondrial genome are associated with hearing loss, we analyzed whether sequence variations of mtDNA are associated with individual sensitivity to cisplatin-induced ototoxicity.MATERIALS AND METHODS: The mtDNA of 20 patients with and 19 patients without hearing impairment under therapeutic doses of cisplatin was sequenced for mutations and characterized for haplotype by restriction analysis.RESULTS: Neither the A7445G mutation, nor the 7472insC insertion or the A1555G mutation were identified in any of the patients. Nucleotide variations in the variable D-loop region did not correlate with cisplatin-induced hearing loss. However, these patients clustered more frequently (5 out of 20) in the rare European haplogroup J, than those with normal hearing after therapy (1 out of 19).CONCLUSION: The linkage of cisplatin-induced hearing impairment to the mitochondrial haplogroup J, which is also associated with the mitochondrially-mediated Leber's Hereditary Optic Neuropathy, might act as a predisponsing genetic background for biochemical differences in mitochondria.",pubmed,12820379,
perception and production of r allophones improve with hearing from a cochlear implant,"316. J Acoust Soc Am. 2008 Nov;124(5):3191-202. doi: 10.1121/1.2987427.Perception and production of /r/ allophones improve with hearing from a cochlear implant.Matthies ML(1), Guenther FH, Denny M, Perkell JS, Burton E, Vick J, Lane H, Tiede M, Zandipour M.Author information:(1)Department of Speech Language and Hearing Sciences, Boston University, Boston, MA 02215, USA.Tongue shape can vary greatly for allophones of /r/ produced in different phonetic contexts but the primary acoustic cue used by listeners, lowered F3, remains stable. For the current study, it was hypothesized that auditory feedback maintains the speech motor control mechanisms that are constraining acoustic variability of F3 in /r/; thus the listener's percept remains /r/ despite the range of articulatory configurations employed by the speaker. Given the potential importance of auditory feedback, postlingually deafened speakers should show larger acoustic variation in /r/ allophones than hearing controls, and auditory feedback from a cochlear implant could reduce that variation over time. To test these hypotheses, measures were made of phoneme perception and of production of tokens containing /r/, stop consonants, and /r/+stop clusters in hearing controls and in eight postlingually deafened adults pre- and postimplant. Postimplant, seven of the eight implant speakers did not differ from the control mean. It was also found that implant users' production of stop and stop+/r/ blend improved with time but the measured acoustic contrast between these was still better in the control speakers than for the implant group even after the implant users had experienced a year of improved auditory feedback.DOI: 10.1121/1.2987427PMCID: PMC2677359",pubmed,19045803,10.1121/1.2987427
interleukin 8 can affect inner ear function,"426. ORL J Otorhinolaryngol Relat Spec. 1998 Jul-Aug;60(4):181-9. doi: 10.1159/000027591.Interleukin 8 can affect inner ear function.Iguchi H(1), Anniko M.Author information:(1)Department of Otorhinolaryngology and Head and Neck Surgery, Uppsala University Hospital, Sweden.The chemokine interleukin 8 (IL-8) was instilled into the round window niche of rats through a small perforation in the tympanic membrane in order to study its effect on inner ear function by electrophysiological and morphological techniques. The frequency-specific auditory brainstem response (ABR) was recorded at the frequencies 4, 8, 10, 12, 16 and 20 kHz just before and 1, 2, 5 and 14 days after instilling IL-8 to ascertain the hearing level during each interval. Morphological examination by light microscopy was performed during the same interval following the instillation of IL-8. On day 1, the rise in ABR threshold was within 5 dB SPL (non-significant elevation). However, a significant threshold elevation (above 5 dB SPL) occurred in high-frequency areas (16 and 20 kHz) on day 2, and in middle frequency areas (10 and 12 kHz) on day 5 with sensorineural hearing loss type intensity-latency curves. By day 14, the elevated thresholds had returned to pre-instillation levels. In the lowest areas (4 and 8 kHz), no significant threshold elevation was detected at any time during the observation period. By light microscopy, on day 1, clusters of inflammatory cells (predominantly neutrophils) were observed just outside the round window membrane (RWM), while only a few neutrophils were detected in the cochlea. These cells were still present outside the RWM on day 2. The neutrophils had disappeared by day 5 and only macrophages were present on the middle ear side of the RWM. However, throughout the observation period, the organ of Corti and stria vascularis appeared to be intact. These results suggest that IL-8 in the middle ear cavity is able to influence inner ear function.DOI: 10.1159/000027591",pubmed,9646304,10.1159/000027591
surface electrical stimulation of the auditory cortex preserves efferent medial olivocochlear neurons and reduces cochlear traits of agerelated hearing loss,"619. Hear Res. 2024 Apr 12;447:109008. doi: 10.1016/j.heares.2024.109008. Online ahead of print.Surface electrical stimulation of the auditory cortex preserves efferent medial olivocochlear neurons and reduces cochlear traits of age-related hearing loss.Fuentes-Santamaría V(1), Benítez-Maicán Z(1), Alvarado JC(1), Fernández Del Campo IS(2), Gabaldón-Ull MC(1), Merchán MA(2), Juiz JM(3).Author information:(1)School of Medicine, Universidad de Castilla-La Mancha (UCLM), Campus in Albacete, 02008, Albacete, Spain.(2)Lab. of Auditory Neuroplasticity, Institute for Neuroscience of Castilla y León (INCYL), University of Salamanca, Salamanca, Spain.(3)School of Medicine, Universidad de Castilla-La Mancha (UCLM), Campus in Albacete, 02008, Albacete, Spain; Hannover Medical School, Dept. of Otolaryngology and Cluster of Excellence ""H4all"" of the German Research Foundation, DFG, Carl-Neuberg-Str. 1, 30625 Hannover, Germany. Electronic address: josemanuel.juiz@uclm.es.The auditory cortex is the source of descending connections providing contextual feedback for auditory signal processing at almost all levels of the lemniscal auditory pathway. Such feedback is essential for cognitive processing. It is likely that corticofugal pathways are degraded with aging, becoming important players in age-related hearing loss and, by extension, in cognitive decline. We are testing the hypothesis that surface, epidural stimulation of the auditory cortex during aging may regulate the activity of corticofugal pathways, resulting in modulation of central and peripheral traits of auditory aging. Increased auditory thresholds during ongoing age-related hearing loss in the rat are attenuated after two weeks of epidural stimulation with direct current applied to the surface of the auditory cortex for two weeks in alternate days (Fernández del Campo et al., 2024). Here we report that the same cortical electrical stimulation protocol induces structural and cytochemical changes in the aging cochlea and auditory brainstem, which may underlie recovery of age-degraded auditory sensitivity. Specifically, we found that in 18 month-old rats after two weeks of cortical electrical stimulation there is, relative to age-matched non-stimulated rats: a) a larger number of choline acetyltransferase immunoreactive neuronal cell body profiles in the ventral nucleus of the trapezoid body, originating the medial olivocochlear system.; b) a reduction of age-related dystrophic changes in the stria vascularis; c) diminished immunoreactivity for the pro-inflammatory cytokine TNFα in the stria vascularis and spiral ligament. d) diminished immunoreactivity for Iba1 and changes in the morphology of Iba1 immunoreactive cells in the lateral wall, suggesting reduced activation of macrophage/microglia; d) Increased immunoreactivity levels for calretinin in spiral ganglion neurons, suggesting excitability modulation by corticofugal stimulation. Altogether, these findings support that non-invasive neuromodulation of the auditory cortex during aging preserves the cochlear efferent system and ameliorates cochlear aging traits, including stria vascularis dystrophy, dysregulated inflammation and altered excitability in primary auditory neurons.Copyright © 2024 The Authors. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2024.109008",pubmed,38636186,10.1016/j.heares.2024.109008
development of resnet152 unetbased segmentation algorithm for the tympanic membrane and affected areas,"Otitis media (OM) is a common disease in childhood that may have aftereffects such as hearing loss. Therefore, early diagnosis and proper treatment are important. However, the diagnostic accuracies of otolaryngology and pediatrics are low, at 73% and 50%, respectively. Therefore, clinical work that supports the early diagnosis of diseases, such as computer-aided diagnostic (CAD) systems, can be helpful. However, CAD systems for diagnosing ear diseases require an automatic tympanic membrane (TM) segmentation model to assist in diagnosis. This is because it is difficult to detect the TM and affected areas in an endoscopic image of the TM owing to irregular lighting. In this study, we propose a ResNet152 UNet++ image segmentation network. The proposed method applies the ResNet152 layer structure to the encoders in the UNet++ model to detect the location of the TM and affected area with high accuracy. Furthermore, the TM and affected regions can be segmented better than when using the previously proposed UNet and UNet++ models. To the best of our knowledge, this study is the first to use a UNet++-based segmentation model to segment TM areas in endoscopic images of the TM and evaluate its performance. The experiments revealed that ResNet152 UNet++ outperforms conventional methods in terms of segmentation of the TM and affected areas.",ieee,2169-3536,10.1109/ACCESS.2023.3281693
sensitivity to interaural time differences in the inferior colliculus of cochlear implanted rats with or without hearing experience,"770. Hear Res. 2021 Sep 1;408:108305. doi: 10.1016/j.heares.2021.108305. Epub 2021 Jul 9.Sensitivity to interaural time differences in the inferior colliculus of cochlear implanted rats with or without hearing experience.Buck AN(1), Rosskothen-Kuhl N(2), Schnupp JW(3).Author information:(1)Department of Neouroscience, City University of Hong Kong, 31 To Yuen Street, Kowloon, Hong Kong SAR, China; City University of Hong Kong Shenzhen Research Institute, Shenzhen, China. Electronic address: alexanbuck@gmail.com.(2)Department of Neouroscience, City University of Hong Kong, 31 To Yuen Street, Kowloon, Hong Kong SAR, China; Neurobiological Research Laboratory, Section for Clinical and Experimental Otology, University Medical Center Freiburg, Killianstr. 5, 79106 Freiburg i. Br., Freiburg, Germany. Electronic address: nicole.rosskothen-kuhl@uniklinik-freiburg.de.(3)Department of Neouroscience, City University of Hong Kong, 31 To Yuen Street, Kowloon, Hong Kong SAR, China; City University of Hong Kong Shenzhen Research Institute, Shenzhen, China. Electronic address: wschnupp@cityu.edu.hk.For deaf patients cochlear implants (CIs) can restore substantial amounts of functional hearing. However, binaural hearing, and in particular, the perception of interaural time differences (ITDs) with current CIs has been found to be notoriously poor, especially in the event of early hearing loss. One popular hypothesis for these deficits posits that a lack of early binaural experience may be a principal cause of poor ITD perception in pre-lingually deaf CI patients. This is supported by previous electrophysiological studies done in neonatally deafened, bilateral CI-stimulated animals showing reduced ITD sensitivity. However, we have recently demonstrated that neonatally deafened CI rats can quickly learn to discriminate microsecond ITDs under optimized stimulation conditions which suggests that the inability of human CI users to make use of ITDs is not due to lack of binaural hearing experience during development. In the study presented here, we characterized ITD sensitivity and tuning of inferior colliculus neurons under bilateral CI stimulation of neonatally deafened and hearing experienced rats. The hearing experienced rats were not deafened prior to implantation. Both cohorts were implanted bilaterally between postnatal days 64-77 and recorded immediately following surgery. Both groups showed comparably large proportions of ITD sensitive multi-units in the inferior colliculus (Deaf: 84.8%, Hearing: 82.5%), and the strength of ITD tuning, quantified as mutual information between response and stimulus ITD, was independent of hearing experience. However, the shapes of tuning curves differed substantially between both groups. We observed four main clusters of tuning curves - trough, contralateral, central, and ipsilateral tuning. Interestingly, over 90% of multi-units for hearing experienced rats showed predominantly contralateral tuning, whereas as many as 50% of multi-units in neonatally deafened rats were centrally tuned. However, when we computed neural d' scores to predict likely limits on performance in sound lateralization tasks, we did not find that these differences in tuning shapes predicted worse psychoacoustic performance for the neonatally deafened animals. We conclude that, at least in rats, substantial amounts of highly precise, ""innate"" ITD sensitivity can be found even after profound hearing loss throughout infancy. However, ITD tuning curve shapes appear to be strongly influenced by auditory experience although substantial lateralization encoding is present even in its absence.Copyright © 2021. Published by Elsevier B.V.DOI: 10.1016/j.heares.2021.108305",pubmed,34315027,10.1016/j.heares.2021.108305
a novel automatic audiometric system design based on machine learning methods using the braintextquoterights electrical activity signals,,base,a130ef7b327a2975a262bd6064ede2e76e88cfda3c677ec28bea9e493efc01cb,
the latest in tinnitus research,,cinahl,8976368,
open versus closed hearingaid fittings a literature review of both fitting approaches,"848. Trends Hear. 2016 Feb 15;20:2331216516631741. doi: 10.1177/2331216516631741.Open Versus Closed Hearing-Aid Fittings: A Literature Review of Both Fitting Approaches.Winkler A(1), Latzel M(2), Holube I(3).Author information:(1)Insitute of Hearing Technology and Audiology, Jade University of Applied Sciences and Cluster of Excellence ""Hearing4All"", Oldenburg, Germany alexandra.winkler@jade-hs.de.(2)Phonak AG, Staefa, Switzerland.(3)Insitute of Hearing Technology and Audiology, Jade University of Applied Sciences and Cluster of Excellence ""Hearing4All"", Oldenburg, Germany.One of the main issues in hearing-aid fittings is the abnormal perception of the user's own voice as too loud, ""boomy,"" or ""hollow."" This phenomenon known as the occlusion effect be reduced by large vents in the earmolds or by open-fit hearing aids. This review provides an overview of publications related to open and closed hearing-aid fittings. First, the occlusion effect and its consequences for perception while using hearing aids are described. Then, the advantages and disadvantages of open compared with closed fittings and their impact on the fitting process are addressed. The advantages include less occlusion, improved own-voice perception and sound quality, and increased localization performance. The disadvantages associated with open-fit hearing aids include reduced benefits of directional microphones and noise reduction, as well as less compression and less available gain before feedback. The final part of this review addresses the need for new approaches to combine the advantages of open and closed hearing-aid fittings.© The Author(s) 2016.DOI: 10.1177/2331216516631741PMCID: PMC4765810",pubmed,26879562,10.1177/2331216516631741
improving the performance of hearing aids in noisy environments based on deep learning technology,"168. Annu Int Conf IEEE Eng Med Biol Soc. 2018 Jul;2018:404-408. doi: 10.1109/EMBC.2018.8512277.Improving the performance of hearing aids in noisy environments based on deep learning technology.Lai YH, Zheng WZ, Tang ST, Fang SH, Liao WH, Tsao Y.The performance of a deep-learning-based speech enhancement (SE) technology for hearing aid users, called a deep denoising autoencoder (DDAE), was investigated. The hearing-aid speech perception index (HASPI) and the hearing- aid sound quality index (HASQI), which are two well-known evaluation metrics for speech intelligibility and quality, were used to evaluate the performance of the DDAE SE approach in two typical high-frequency hearing loss (HFHL) audiograms. Our experimental results show that the DDAE SE approach yields higher intelligibility and quality scores than two classical SE approaches. These results suggest that a deep-learning-based SE method could be used to improve speech intelligibility and quality for hearing aid users in noisy environments.DOI: 10.1109/EMBC.2018.8512277",pubmed,30440419,10.1109/EMBC.2018.8512277
the prevalence of hearing impairment by age and gender in a populationbased study,"818. Iran J Public Health. 2017 Sep;46(9):1237-1246.The Prevalence of Hearing Impairment by Age and Gender in a Population-based Study.Asghari A(1)(2), Farhadi M(1), Daneshi A(1), Khabazkhoob M(3), Mohazzab-Torabi S(4), Jalessi M(2), Emamjomeh H(1).Author information:(1)ENT and Head & Neck Research Center, Hazrat Rasoul Akram Hospital, Iran University of Medical Sciences, Tehran, Iran.(2)Skull Base Research Center, Iran University of Medical Sciences, Tehran, Iran.(3)Dept. of Medical Surgical Nursing, School of Nursing and Midwifery, Shahid Beheshti University of Medical Sciences, Tehran, Iran.(4)Noor Research Center for Ophthalmic Epidemiology, Noor Eye Hospital, Tehran, Iran.BACKGROUND: This study aimed to determine the prevalence of hearing impairment (HI) by age and gender in a population aged 5 yr and older residing in Tehran, Iran.METHODS: In this cross-sectional study, 140 clusters each including 10 households from Tehran, Iran were sampled between 2012 and 2013 using cluster random sampling. Trained audiologists examined the participants during face-to-face interviews. The hearing of the participants was evaluated before the removal of wax or other foreign bodies. In this study, HI was categorized as mild (grade 1, 26-40 db), moderate (grade 2, 41-60 db), severe (grade 3, 61-80 db), and deaf (grade 5, 81 db or more). All participants signed informed consent forms. The SATA software was used for data analysis.RESULTS: Of 6521 individuals, 4370 (67%) were interviewed. The prevalence of HI (auditory threshold of 0.5, 1, 2, 4 KHz and more than 25 db in the better ear) was 14.27 (11.53-17.91) of whom 9.52 (7.07-11.98) had grade 1, 4.04 (3.02-5.06) had grade 2, 0.67 (0.33-1.02) had grade 3 HI and 0.48 (0.16-0.8) were deaf. About 5.19% of the participants had disabling hearing impairment. All HI grades increased significantly with age but no significant difference was observed between men and women.CONCLUSION: The considerable prevalence of HI in Iran in comparison with other developing countries, with regards to the trend of aging in the population, seems concerning. The results of the study could be used as a treatment and research guideline for future works in the area of policymaking and plan to decrease these disorders.PMCID: PMC5632326",pubmed,29026790,
electrical cochlear stimulation in the deaf cat comparisons between psychophysical and central auditory neuronal thresholds,"353. J Neurophysiol. 2000 Apr;83(4):2145-62. doi: 10.1152/jn.2000.83.4.2145.Electrical cochlear stimulation in the deaf cat: comparisons between psychophysical and central auditory neuronal thresholds.Beitel RE(1), Snyder RL, Schreiner CE, Raggio MW, Leake PA.Author information:(1)Department of Otolaryngology, University of California, San Francisco, California 94143-0732, USA.Cochlear prostheses for electrical stimulation of the auditory nerve (""electrical hearing"") can provide auditory capacity for profoundly deaf adults and children, including in many cases a restored ability to perceive speech without visual cues. A fundamental challenge in auditory neuroscience is to understand the neural and perceptual mechanisms that make rehabilitation of hearing possible in these deaf humans. We have developed a feline behavioral model that allows us to study behavioral and physiological variables in the same deaf animals. Cats deafened by injection of ototoxic antibiotics were implanted with either a monopolar round window electrode or a multichannel scala tympani electrode array. To evaluate the effects of perceptually significant electrical stimulation of the auditory nerve on the central auditory system, an animal was trained to avoid a mild electrocutaneous shock when biphasic current pulses (0.2 ms/phase) were delivered to its implanted cochlea. Psychophysical detection thresholds and electrical auditory brain stem response (EABR) thresholds were estimated in each cat. At the conclusion of behavioral testing, acute physiological experiments were conducted, and threshold responses were recorded for single neurons and multineuronal clusters in the central nucleus of the inferior colliculus (ICC) and the primary auditory cortex (A1). Behavioral and neurophysiological thresholds were evaluated with reference to cochlear histopathology in the same deaf cats. The results of the present study include: 1) in the cats implanted with a scala tympani electrode array, the lowest ICC and A1 neural thresholds were virtually identical to the behavioral thresholds for intracochlear bipolar stimulation; 2) behavioral thresholds were lower than ICC and A1 neural thresholds in each of the cats implanted with a monopolar round window electrode; 3) EABR thresholds were higher than behavioral thresholds in all of the cats (mean difference = 6.5 dB); and 4) the cumulative number of action potentials for a sample of ICC neurons increased monotonically as a function of the amplitude and the number of stimulating biphasic pulses. This physiological result suggests that the output from the ICC may be integrated spatially across neurons and temporally integrated across pulses when the auditory nerve array is stimulated with a train of biphasic current pulses. Because behavioral thresholds were lower and reaction times were faster at a pulse rate of 30 pps compared with a pulse rate of 2 pps, spatial-temporal integration in the central auditory system was presumably reflected in psychophysical performance.DOI: 10.1152/jn.2000.83.4.2145",pubmed,10758124,10.1152/jn.2000.83.4.2145
development of speechreading supplements based on automatic speech recognition,"478. IEEE Trans Biomed Eng. 2000 Apr;47(4):487-96. doi: 10.1109/10.828148.Development of speechreading supplements based on automatic speech recognition.Duchnowski P(1), Lum DS, Krause JC, Sexton MG, Bratakos MS, Braida LD.Author information:(1)Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge 02139, USA.In manual-cued speech (MCS) a speaker produces hand gestures to resolve ambiguities among speech elements that are often confused by speechreaders. The shape of the hand distinguishes among consonants; the position of the hand relative to the face distinguishes among vowels. Experienced receivers of MCS achieve nearly perfect reception of everyday connected speech. MCS has been taught to very young deaf children and greatly facilitates language learning, communication, and general education. This manuscript describes a system that can produce a form of cued speech automatically in real time and reports on its evaluation by trained receivers of MCS. Cues are derived by a hidden markov models (HMM)-based speaker-dependent phonetic speech recognizer that uses context-dependent phone models and are presented visually by superimposing animated handshapes on the face of the talker. The benefit provided by these cues strongly depends on articulation of hand movements and on precise synchronization of the actions of the hands and the face. Using the system reported here, experienced cue receivers can recognize roughly two-thirds of the keywords in cued low-context sentences correctly, compared to roughly one-third by speechreading alone (SA). The practical significance of these improvements is to support fairly normal rates of reception of conversational speech, a task that is often difficult via SA.DOI: 10.1109/10.828148",pubmed,10763294,10.1109/10.828148
effects of agerelated hearing loss and hearing aid experience on sentence processing,"38. Sci Rep. 2021 Mar 16;11(1):5994. doi: 10.1038/s41598-021-85349-5.Effects of age-related hearing loss and hearing aid experience on sentence processing.Vogelzang M(1)(2)(3), Thiel CM(4)(5), Rosemann S(4)(5), Rieger JW(4)(6), Ruigendijk E(7)(4).Author information:(1)Institute of Dutch Studies, University of Oldenburg, Ammerländer Heerstraße 114-116, 26129, Oldenburg, Germany. mv498@cam.ac.uk.(2)Cluster of Excellence ""Hearing4all"", University of Oldenburg, Ammerländer Heerstraße 114-116, 26129, Oldenburg, Germany. mv498@cam.ac.uk.(3)Department of Theoretical and Applied Linguistics, University of Cambridge, Cambridge, UK. mv498@cam.ac.uk.(4)Cluster of Excellence ""Hearing4all"", University of Oldenburg, Ammerländer Heerstraße 114-116, 26129, Oldenburg, Germany.(5)Biological Psychology, Department of Psychology, Department for Medicine and Health Sciences, University of Oldenburg, Ammerländer Heerstraße 114-116, 26129, Oldenburg, Germany.(6)Applied Neurocognitive Psychology, Department of Psychology, University of Oldenburg, Ammerländer Heerstraße 114-116, 26129, Oldenburg, Germany.(7)Institute of Dutch Studies, University of Oldenburg, Ammerländer Heerstraße 114-116, 26129, Oldenburg, Germany.Age-related hearing loss typically affects the hearing of high frequencies in older adults. Such hearing loss influences the processing of spoken language, including higher-level processing such as that of complex sentences. Hearing aids may alleviate some of the speech processing disadvantages associated with hearing loss. However, little is known about the relation between hearing loss, hearing aid use, and their effects on higher-level language processes. This neuroimaging (fMRI) study examined these factors by measuring the comprehension and neural processing of simple and complex spoken sentences in hard-of-hearing older adults (n = 39). Neither hearing loss severity nor hearing aid experience influenced sentence comprehension at the behavioral level. In contrast, hearing loss severity was associated with increased activity in left superior frontal areas and the left anterior insula, but only when processing specific complex sentences (i.e. object-before-subject) compared to simple sentences. Longer hearing aid experience in a sub-set of participants (n = 19) was associated with recruitment of several areas outside of the core speech processing network in the right hemisphere, including the cerebellum, the precentral gyrus, and the cingulate cortex, but only when processing complex sentences. Overall, these results indicate that brain activation for language processing is affected by hearing loss as well as subsequent hearing aid use. Crucially, they show that these effects become apparent through investigation of complex but not simple sentences.DOI: 10.1038/s41598-021-85349-5PMCID: PMC7971046",pubmed,33727628,10.1038/s41598-021-85349-5
do impedance changes correlate with a delayed hearing loss after hybrid l24 implantation,"287. Ear Hear. 2021 Jan/Feb;42(1):163-172. doi: 10.1097/AUD.0000000000000914.Do Impedance Changes Correlate With a Delayed Hearing Loss After Hybrid L24 Implantation?Konrad S(1), Framke T(2), Kludt E(1), Büchner A(1)(3), Lenarz T(1)(3), Paasche G(1)(3).Author information:(1)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.(2)Institute for Biostatistics, Hannover Medical School, Hannover, Germany.(3)Hearing4all Cluster of Excellence, Hannover Medical School, Hannover, Germany.OBJECTIVES: Preservation of residual hearing is one of the main goals in present cochlear implantation surgery. Especially for this purpose, smaller and softer electrode carriers were developed that are to be inserted through the round window membrane to minimize trauma. By using these electrodes and insertion technique, residual hearing can be preserved in a large number of patients. Unfortunately, some of these patients with initially preserved residual hearing after cochlear implantation lose it later on. The reason for this is unknown but it is speculated about a correlation with an increase in impedance, since increased impedance values are linked to intracochlear inflammation and tissue reaction. Our hypothesis for this study design was that an increase in impedance predicts changes in residual hearing under clinical conditions.DESIGN: Data of all adult patients (N = 122) receiving a Hybrid-L24 cochlear implant at our center between 2005 and early 2015 were retrospectively evaluated. Impedance values in Common Ground mode as measured during clinical routine and referring audiological test data (audiometric thresholds under headphones) were collected. Changes between consecutive measurements were calculated for impedance values and hearing thresholds for each patient. Correlations between changes in impedances and acoustic hearing thresholds were calculated. Average values were compared as well as patients with largest impedance changes within the observation period were evaluated separately.RESULTS: Group mean values of impedances were between 5 and 7 kΩ and stable over time with higher values on basal electrode contacts compared with apical contacts. Average hearing thresholds at the time of initial fitting were between 40 to 50 dB (250 Hz) and 90 dB (1 kHz) with a loss of about 10 dB compared with preoperative values. Correlation between impedance changes and threshold changes was found, but too inconsistently to imply a true relationship. When evaluating the 20 patients with the largest impedance changes during the observation period (all >1 kΩ from one appointment to the next one), some patients were found where hearing loss is timely connected and highly correlated with an unusual impedance change. But large impedance changes were also observed without affecting hearing thresholds and hearing loss was found without impedance change.CONCLUSIONS: Changes in impedance as measured during clinical routine cannot be taken as an indicator for a late acoustic hearing loss.Copyright © 2020 Wolters Kluwer Health, Inc. All rights reserved.DOI: 10.1097/AUD.0000000000000914",pubmed,32769433,10.1097/AUD.0000000000000914
middle ear transducer long term stability of the latest generation t2,"336. Biomed Res Int. 2019 Jan 6;2019:4346325. doi: 10.1155/2019/4346325. eCollection 2019.Middle Ear Transducer: Long Term Stability of the Latest Generation T2.Prenzler NK(1)(2), Kludt E(2), Giere T(2), Salcher R(1)(2), Lenarz T(1)(2), Maier H(1)(2).Author information:(1)Cluster of Excellence Hearing4all, Germany.(2)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.OBJECTIVES/HYPOTHESIS: Comparing long term stability of the Middle Ear Transducers (MET) of the 1st generation T1 (Otologics LLC) with the current generation T2 (Cochlear Ltd.) in all our clinical cases with standard incus coupling.STUDY DESIGN: Retrospective chart review.METHODS: 52 ears implanted with a MET device between 2008 and 2016 were analyzed retrospectively. All patients suffered from sensorineural hearing loss and the actuator was coupled to the body of the incus (standard coupling). 23 ears were implanted with the transducer T1 (Otologics LLC) between 2008 and 2011 and 29 ears were implanted with the current transducer T2 since 2011 (Otologics LLC/Cochlear Ltd.). Latest available in situ and bone conduction (BC) thresholds were exploited for a follow-up period of up to 7 years after first fitting. Long term stability of coupling and actuator performance was evaluated by tracking differences between in situ and BC thresholds.RESULTS: In the T1 group, 9 out of 23 implants were still used by the patients at their last follow-up visit (average observation time 3.7 yrs.; min 1.0 yrs., max 7.4 yrs.). In 9 patients a technical failure identified by a decrease of in situ threshold of more than 15 dB compared to BC thresholds [Δ (in situ - BC)] lead to non-usage of the implant and 7 explantations. Five other explantations occurred due to medical reasons such as BC threshold decrease, infection, or insufficient speech intelligibility with the device. In the T2 group, 23 out of 29 implants were still used at the most current follow-up visit (average observation time 3.3 yrs.; min 1.0 yrs., max 4.8 yrs.). No technical failures were observed up to more than 4 years after implantation. Five T2 patients discontinued using the device due to insufficient benefit; two of these patients were explanted. One patient had to be explanted before the activation of the device due to disorders of wound healing. Nevertheless, a small but significant decrease of hearing loss corrected coupling efficiency [Δ (in situ - BC)] was seen in the T2 group.CONCLUSIONS: In contrast to the T1 transducers of the earlier generation of MET systems where technical failures occurred frequently, no technical failures were detected after 29 implantations with the current T2 transducers. However, a small but significant decline of transmission efficiency was observable even in the T2 implanted group.DOI: 10.1155/2019/4346325PMCID: PMC6339725",pubmed,30723738,10.1155/2019/4346325
restoring hearing with active hearing implants,"502. Biomed Tech (Berl). 2004 Apr;49(4):78-82. doi: 10.1515/BMT.2004.016.Restoring hearing with active hearing implants.Federspil PA(1), Plinkert PK.Author information:(1)University of Saarland, Department of Oto-Rhino-Laryngology, Homburg (Saar), Germany. Ph.Federspil@uniklinik-saarland.deDue to shortcomings of conventional hearing aid technology, such as unsatisfactory sound quality due to limited frequency range and undesired distortion, occlusion of the outer ear canal, and acoustic feedback with high amplification, but also psychological aspects of stigmatization, a significant of patients in need of hearing aids are actually not wearing them. Active hearing implants can be distinguished in: (1) impedance transformation implants (ITI), (2) cochlear amplifier implants (CAI), (3) cochlear implants (CI), and (4) brain stem implants (BSI). Whereas ITI are designed for patients with middle ear hearing loss, CAI are intended to restore hearing in patients with inner ear hearing loss. Advantages of CAI may be: (1) improved sound fidelity, (2) no occlusion of the outer ear canal, (3) no feedback, and (4) invisibility. However, not all features are true for every device. CI replace inner ear function in deaf or almost deaf patients. This article gives an overview on the range of active hearing implants to restore hearing and outlines the future use of computer and robot aided surgery.DOI: 10.1515/BMT.2004.016",pubmed,15171586,10.1515/BMT.2004.016
loss of inner hair cell ribbon synapses and auditory nerve fiber regression in cldn14 knockout mice,"772. Hear Res. 2020 Jun;391:107950. doi: 10.1016/j.heares.2020.107950. Epub 2020 Mar 16.Loss of inner hair cell ribbon synapses and auditory nerve fiber regression in Cldn14 knockout mice.Claußen M(1), Schulze J(2), Nothwang HG(3).Author information:(1)Division of Neurogenetics, Department of Neurosciences, Faculty for Medicine and Health Sciences, Carl von Ossietzky University Oldenburg, Oldenburg, Germany; Cluster of Excellence ""Hearing4all"", Germany. Electronic address: maike.claussen@uol.de.(2)Department of Otorhinolaryngology, Head and Neck Surgery, Hannover Medical School, Hannover, Germany; Cluster of Excellence ""Hearing4all"", Germany.(3)Division of Neurogenetics, Department of Neurosciences, Faculty for Medicine and Health Sciences, Carl von Ossietzky University Oldenburg, Oldenburg, Germany; Cluster of Excellence ""Hearing4all"", Germany.Proper functioning of the auditory nerve is of critical importance for auditory rehabilitation by cochlear implants. Here we used the Cldn14-/- mouse to study in detail the effects of Claudin 14 loss on auditory synapses and the auditory nerve. Mutations in the tight junction protein Claudin 14 cause autosomal recessive non-syndromic hearing loss (DFNB29) in humans and mice, due to extensive degeneration of outer and inner hair cells. Here we show that massive inner hair cell loss in Cldn14-/- mice starts after the third postnatal week. Immunohistochemical analysis, using presynaptic Ribeye and postsynaptic GluR2 or PSD 95 as markers, revealed the degeneration of full ribbon synapses in inner hair cells from apical cochlear regions already at postnatal day 12 (P12). At P20, significant reduction in number of ribbon synapses has been observed for all cochlear regions and the loss of synaptic ribbons becomes even more prominent in residual inner hair cells from middle and apical cochlear regions at P45, which by then lost more than 40% of all ribbon synapses. In contrast to excessive noise exposure, loss of Claudin 14 does not cause an increase in ""orphan"" ribbons with no postsynaptic counterpart due to a reduction of postsynaptic structures. Hair cell loss in Cldn14-/- mice is associated with regression of peripheral auditory nerve processes, especially of outer radial fibers, which normally innervate the outer hair cells. The number of spiral ganglion neurons per area, however, was unchanged between the genotypes. Different effects were observed in the cochlear nucleus complex (CNC), the central projection area of the auditory nerve. While the dorsal cochlear nucleus (DCN) showed a significant 19.7% volume reduction, VGLUT-1 input was reduced by 34.4% in the ventral cochlear nucleus (VCN) but not in the DCN of Cldn14-/- mice. Taken together, massive inner hair cell loss starts after the third postnatal week in Cldn14-/- mice, but is preceded by the loss of ribbon synapses, which may be a first sign of an ongoing degeneration process in otherwise morphologically inconspicuously inner hair cells. In addition to the regression of peripheral nerve processes, reduced levels of VGLUT-1 in the VCN of Cldn14-/- mice suggests that Claudin 14 loss does not only cause hair cell loss but also affects peripheral and central connectivity of the auditory nerve.Copyright © 2020 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2020.107950",pubmed,32251970,10.1016/j.heares.2020.107950
speech delay in children a functional mr imaging study,"PURPOSE: To determine if children with speech delay who have been sedated have patterns of activation to passive language paradigms that are different than those of children with normal speech. MATERIALS AND METHODS: Seventeen children with speech delay (age range, 2-7 years; mean, 4.0 years) and 35 age-matched children with normal speech (age range, 2-8 years; mean, 4.2 years) were evaluated. The subjects in the control group were selected from patients referred for conventional magnetic resonance (MR) imaging. All children had absence of auditory impairment or mental retardation, and MR findings indicated that brain structure was normal. Sedation was achieved with pentobarbital (3-5 mg/kg) or chloral hydrate (75 mg/kg). Functional MR imaging was performed with a single-shot echo-planar blood oxygen-level-dependent technique and a passive block paradigm, in which the child listened to his or her mother's prerecorded voice. Statistical postprocessing of functional MR images was performed with the t test and cluster detection methods. Comparison between groups was performed depending on the type of data with a nonparametrical Mann-Whitney test, parametrical t test, or Fisher exact test. RESULTS: Five (83%) of the six children older than 3 years with speech delay had lateralized activation of functional MR imaging signal in the right hemisphere. Ten (71%) of 14 age-matched patients with normal speech had activation in the left hemisphere when exposed to the same passive listening tasks. When these groups were compared, this difference was statistically significant. (P = .036). No statistically significant lateralization was seen across all age groups in children with activation. CONCLUSION: Children older than 3 years with speech delay have activation in the right hemisphere more frequently than children older than 3 years with normal speech, who often have the expected finding of activation in the left hemisphere. © RSNA, 2003.",scopus,2-s2.0-0344153269,10.1148/radiol.2293021746
a deep learning based wearable healthcare iot device for aienabled hearing assistance automation,"With the recent booming of artificial intelligence (AI), particularly deep learning techniques, digital healthcare is one of the prevalent areas that could gain benefits from AI-enabled functionality. This research presents a novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266 platform capable of assisting those who suffer from impairment of hearing or deafness to communicate with others in conversations. In the proposed solution, a server application is created that leverages Google’s online speech recognition service to convert the received conversations into texts, then deployed to a micro-display attached to the glasses to display the conversation contents to deaf people, to enable and assist conversation as normal with the general population. Furthermore, in order to raise alert of traffic or dangerous scenarios, an ‘urban-emergency’ classifier is developed using a deep learning model, Inception-v4, with transfer learning to detect/recognize alerting/alarming sounds, such as a horn sound or a fire alarm, with texts generated to alert the prospective user. The training of Inception-v4 was carried out on a consumer desktop PC and then implemented into the AI-based IoT application. The empirical results indicate that the developed prototype system achieves an accuracy rate of 92% for sound recognition and classification with real-time performance.",ieee,2160-1348,10.1109/ICMLC51923.2020.9469537
functionally and morphologically damaged mitochondria observed in auditory cells under senescenceinducing stress,"We aimed at determining the mitochondrial function in premature senescence model of auditory cells. Short exposure to H2O2 (1 h, 0.1 mM) induced premature cellular senescence in House Ear Institute-Organ of Corti 1 auditory cells. The transmission electron microscopy analysis revealed that damaged mitochondria and autophagosomes containing dense organelles appeared in the auditory cells after short exposure to H2O2. The branch and junction parameters of the skeletonized image of the mitochondria were found to decrease significantly in H2O2-treated cells. A branched reticulum of tubules was poorly formed, featuring coexistence of numerous tiny clusters along with few relatively large entities in the H2O2-treated cells. In terms of bioenergetics, H2O2-treatment led to the dose-dependent decrease in mitochondrial membrane potential in the auditory cells. The fragmented mitochondria (fusion < fission) were in a low potential. In addition, the potential of hyperfused mitochondria (fusion > fission) was slightly lower than the control cells. The short-time exposure of live auditory cells to H2O2 damaged the mitochondrial respiratory capacity without any effect on the baseline ATP production rates. The vulnerability of the mitochondrial membrane potential to the uncoupling reagent was increased after H2O2 treatment. Our findings indicated that the mitochondrial dysfunction due to the decline in the O2 consumption rate should be the first event of premature senescence process in the auditory cells, resulting in the imbalance of mitochondrial fusion/fission and the collapse of the mitochondrial network. Auditory system senescence: Mitochondrial dynamics and respiration The mitochondrial morphology and physiology could influence the process of age-related hearing loss. Prof. Tatsuya Yamasoba's research group at the University of Tokyo has examined the functional changes of mitochondria in terms of its respiratory function, membrane potential and morphology under premature senescence induced by oxidative stress in an auditory cell line. The morphological and functional mitochondrial damage were observed as the respiratory capacity deficiency and the fluctuation of the fusion/fission balance. Their results provide evidence of the fundamental interdependence between mitochondrial metabolic activity and its network structure in premature senescence process of auditory cells. This is a pioneer study to indicate the influence of mitochondrial dynamics and respiratory system on the premature senescence process of auditory cells. Further studies into inter cellular communication including cytoskeleton and nucleus can help us understand the etiology underlying age-related hearing loss.",cinahl,20563973,10.1038/s41514-017-0002-2
a new speech spatial and qualities of hearing scale shortform factor cluster and comparative analyses,"163. Ear Hear. 2019 Jul/Aug;40(4):938-950. doi: 10.1097/AUD.0000000000000675.A New Speech, Spatial, and Qualities of Hearing Scale Short-Form: Factor, Cluster, and Comparative Analyses.Moulin A(1)(2), Vergne J(1)(2), Gallego S(3)(4)(5), Micheyl C(6)(7)(8).Author information:(1)INSERM U1028, CNRS UMR 5292, Lyon Neuroscience Research Center, Brain Dynamics and Cognition Team, University of Lyon, Lyon, France.(2)Université C. Bernard Lyon 1, Lyon Neuroscience Research Center, Brain Dynamics and Cognition Team, University of Lyon, Lyon, France.(3)Institut des Sciences et Technologies de Réadaptation (ISTR), University of Lyon, Lyon, France.(4)Audition Conseil, Lyon, France.(5)Laboratory of Integrative and Adaptive Neurosciences (LNIA) UMR 7260, Aix-Marseille University-CNRS, Marseille, France.(6)Starkey Hearing Technologies, Créteil, France.(7)Starkey Hearing Research Center, Berkeley, California, USA.(8)INSERM U1028, CNRS UMR 5292, Cognition and Auditory Perception Team (CAP), Lyon Neuroscience Research Center, University of Lyon, Lyon, France.OBJECTIVES: The objective of this work was to build a 15-item short-form of the Speech Spatial and Qualities of Hearing Scale (SSQ) that maintains the three-factor structure of the full form, using a data-driven approach consistent with internationally recognized procedures for short-form building. This included the validation of the new short-form on an independent sample and an in-depth, comparative analysis of all existing, full and short SSQ forms.DESIGN: Data from a previous study involving 98 normal-hearing (NH) individuals and 196 people with hearing impairments (HI), non hearing aid wearers, along with results from several other published SSQ studies, were used for developing the short-form. Data from a new and independent sample of 35 NH and 88 HI hearing aid wearers were used to validate the new short-form. Factor and hierarchical cluster analyses were used to check the factor structure and internal consistency of the new short-form. In addition, the new short-form was compared with all other SSQ forms, including the full SSQ, the German SSQ15, the SSQ12, and the SSQ5. Construct validity was further assessed by testing statistical relationships between scores and audiometric factors, including pure-tone threshold averages (PTAs) and left/right PTA asymmetry. Receiver-operating characteristic analyses were used to compare the ability of different SSQ forms to discriminate between NH and HI (HI non hearing aid wearers and HI hearing aid wearers) individuals.RESULTS: Compared all other SSQ forms, including the full SSQ, the new short-form showed negligible cross-loading across the three main subscales and greater discriminatory power between NH and HI subjects (as indicated by a larger area under the receiver-operating characteristic curve), as well as between the main subscales (especially Speech and Qualities). Moreover, the new, 5-item Spatial subscale showed increased sensitivity to left/right PTA asymmetry. Very good internal consistency and homogeneity and high correlations with the SSQ were obtained for all short-forms.CONCLUSIONS: While maintaining the three-factor structure of the full SSQ, and exceeding the latter in terms of construct validity and sensitivity to audiometric variables, the new 15-item SSQ affords a substantial reduction in the number of items and, thus, in test time. Based on overall scores, Speech subscores, or Spatial subscores, but not Qualities subscores, the 15-item SSQ appears to be more sensitive to differences in self-evaluated hearing abilities between NH and HI subjects than the full SSQ.DOI: 10.1097/AUD.0000000000000675",pubmed,30461444,10.1097/AUD.0000000000000675
adjusting expectations hearing abilities in a populationbased sample using an ssq short form,"245. Trends Hear. 2018 Jan-Dec;22:2331216518784837. doi: 10.1177/2331216518784837.Adjusting Expectations: Hearing Abilities in a Population-Based Sample Using an SSQ Short Form.von Gablenz P(1)(2), Otto-Sobotka F(3), Holube I(1)(2).Author information:(1)1 Institute of Hearing Technology and Audiology, Jade University of Applied Sciences, Oldenburg, Germany.(2)2 Cluster of Excellence Hearing4all, Oldenburg, Germany.(3)3 Division of Epidemiology and Biometry, School for Medicine and Health Sciences, Carl von Ossietzky University, Oldenburg, Germany.The German short form of the Speech, Spatial, and Qualities of Hearing Scale (SSQ) was administered in a cross-sectional study based on stratified random samples complemented by audiometric tests and a general interview. Data from 1,711 unaided adults aged 18 to 97 years were analyzed in order to determine a distribution of hearing abilities considered as normal and the main factors that impact self-assessments. An innovative mathematical approach was used to overcome the constraints of statistics based on the mean. Quantile regression analysis yielded a benchmark distribution of SSQ scores that might support audiologists in setting realistic SSQ score targets and estimated how the effect of auditory and nonauditory factors changes across the distribution of SSQ scores. Regression models showed significant effects for nonauditory factors on SSQ ratings when controlled for pure-tone hearing and interaural asymmetry. Self-reporting of hearing difficulties, when asked in general terms, was substantially related to SSQ ratings. This effect was observed in both high and low scoring participants and led to a considerable score decrease in all SSQ subscales. Gender, educational level, and self-reporting of health issues also were significantly related to SSQ ratings, but the corresponding effects were regularly unbalanced across the score distribution and particularly large at lower quantiles. The estimated effects of age, however, were mostly small in size, inconsistent regarding the direction, and failed significance for all SSQ items. Overall, the results suggest that nonauditory factors and cumulative effects must be considered when evaluating rehabilitative interventions against an ideal outcome.DOI: 10.1177/2331216518784837PMCID: PMC6053860",pubmed,30022731,10.1177/2331216518784837
do you hear the noise the german matrix sentence test with a fixed noise level in subjects with normal hearing and hearing impairment,"379. Int J Audiol. 2015;54 Suppl 2:71-9. doi: 10.3109/14992027.2015.1079929. Epub 2015 Nov 10.Do you hear the noise? The German matrix sentence test with a fixed noise level in subjects with normal hearing and hearing impairment.Wardenga N(1), Batsoulis C(1)(2), Wagener KC(3)(4), Brand T(3)(5), Lenarz T(3)(1), Maier H(3)(1).Author information:(1)b Department of Otolaryngology , Hannover Medical School , Hannover , Germany.(2)e MED-EL Medical Electronics , Hannover , Germany.(3)a * Cluster of Excellence 'Hearing4all' , Hannover & Oldenburg , Germany.(4)c Hörzentrum Oldenburg GmbH , Oldenburg , Germany.(5)d Department of Medical Physics , Carl-von-Ossietzky-University Oldenburg , Oldenburg , Germany.OBJECTIVE: The aim of this study was to determine the relationship between hearing loss and speech reception threshold (SRT) in a fixed noise condition using the German Oldenburg sentence test (OLSA).DESIGN: After training with two easily-audible lists of the OLSA, SRTs were determined monaurally with headphones at a fixed noise level of 65 dB SPL using a standard adaptive procedure, converging to 50% speech intelligibility.STUDY SAMPLE: Data was obtained from 315 ears of 177 subjects with hearing losses ranging from -5 to 90 dB HL pure-tone average (PTA, 0.5, 1, 2, 3 kHz).RESULTS: Two domains were identified with a linear dependence of SRT on PTA. The SRT increased with a slope of 0.094 ± 0.006 dB SNR/dB HL (standard deviation (SD) of residuals = 1.17 dB) for PTAs < 47 dB HL and with a slope of 0.811 ± 0.049 dB SNR/dB HL (SD of residuals = 5.54 dB) for higher PTAs.CONCLUSION: The OLSA can be applied to subjects with a wide range of hearing losses. With 65 dB SPL fixed noise presentation level the SRT is determined by listening in noise for PTAs < ∼47 dB HL, and above it is determined by listening in quiet.DOI: 10.3109/14992027.2015.1079929",pubmed,26555195,10.3109/14992027.2015.1079929
psychological dimensions in patients with disabling tinnitus and craniomandibular disorders,"458. Br J Audiol. 1991 Feb;25(1):15-24. doi: 10.3109/03005369109077860.Psychological dimensions in patients with disabling tinnitus and craniomandibular disorders.Erlandsson SI(1), Rubinstein B, Axelsson A, Carlsson SG.Author information:(1)Department of Psychology, University of Göteborg, Sweden.Forty-two patients with severe tinnitus and craniomandibular disorders (CMD) are presented from an audiological and psychological point of view. During a 2-week period, the patients rated their mood and their tinnitus. Based upon mood ratings, patients were grouped into three clusters (high, medium and low mood). The three groups differed in a number of respects, audiological as well as psychological. Patients in the low mood group experienced significantly more intense and severe tinnitus and more daily stress than patients in the high mood group. Ratings of irritation and concentration difficulties seemed to be mood related, and discriminated between patients in the low mood group and patients in the moderate and the high mood groups. Difference in hearing level between the left and the right ear was more pronounced in patients with low mood. There were, however, no significant differences between the groups in the stomatognathic variables. It is concluded that the above mentioned audiological and psychological observations should be considered as potentially important for satisfactory management of individual tinnitus patients. Further studies of the effects of optimally compensated hearing on depressed mood in patients with noise-induced hearing loss (NIHL) and tinnitus are required.DOI: 10.3109/03005369109077860",pubmed,2012899,10.3109/03005369109077860
prediction of consonant recognition in quiet for listeners with normal and impaired hearing using an auditory model,"321. J Acoust Soc Am. 2014 Mar;135(3):1506-17. doi: 10.1121/1.4864293.Prediction of consonant recognition in quiet for listeners with normal and impaired hearing using an auditory model.Jürgens T(1), Ewert SD(1), Kollmeier B(1), Brand T(1).Author information:(1)Cluster of Excellence ""Hearing4all,"" Department für Medizinische Physik und Akustik, Carl-von-Ossietzky Universität Oldenburg, Carl-von Ossietzky-Strasse 9-11, D-26111 Oldenburg, Germany.Consonant recognition was assessed in normal-hearing (NH) and hearing-impaired (HI) listeners in quiet as a function of speech level using a nonsense logatome test. Average recognition scores were analyzed and compared to recognition scores of a speech recognition model. In contrast to commonly used spectral speech recognition models operating on long-term spectra, a ""microscopic"" model operating in the time domain was used. Variations of the model (accounting for hearing impairment) and different model parameters (reflecting cochlear compression) were tested. Using these model variations this study examined whether speech recognition performance in quiet is affected by changes in cochlear compression, namely, a linearization, which is often observed in HI listeners. Consonant recognition scores for HI listeners were poorer than for NH listeners. The model accurately predicted the speech reception thresholds of the NH and most HI listeners. A partial linearization of the cochlear compression in the auditory model, while keeping audibility constant, produced higher recognition scores and improved the prediction accuracy. However, including listener-specific information about the exact form of the cochlear compression did not improve the prediction further.DOI: 10.1121/1.4864293",pubmed,24606286,10.1121/1.4864293
neural changes in cat auditory cortex after a transient puretone trauma,"612. J Neurophysiol. 2003 Oct;90(4):2387-401. doi: 10.1152/jn.00139.2003. Epub 2003 May 28.Neural changes in cat auditory cortex after a transient pure-tone trauma.Noreña AJ(1), Tomita M, Eggermont JJ.Author information:(1)Department of Physiology, Neuroscience Research Group, University of Calgary, Calgary, Alberta T2N 1N4, Canada.Here we present the changes in cortical activity occurring within a few hours after a 1-h exposure to a 120-dB SPL pure tone (5 or 6 kHz). The changes in primary auditory cortex of 16 ketamine-anesthetized cats were assessed by recording, with two 8-microelectrode arrays, from the same multiunit clusters before and after the trauma. The exposure resulted in a peripheral threshold increase that stabilized after a few hours to on average 40 dB in the frequency range of 6-32 kHz, as measured by the auditory brain stem response. The trauma induced a shift in characteristic frequency toward lower frequencies, an emergence of new responses, a broadening of the tuning curve, and an increase in the maximum of driven discharges. In addition, the onset response after the trauma was of shorter duration than before the trauma. The results suggest the involvement of both a decrease and an increase in inhibition. They are discussed in terms of changes in central inhibition and its implications for tonotopic map plasticity.DOI: 10.1152/jn.00139.2003",pubmed,12773493,10.1152/jn.00139.2003
progress made in the efficacy and viability of deeplearningbased noise reduction,"7. J Acoust Soc Am. 2023 May 1;153(5):2751. doi: 10.1121/10.0019341.Progress made in the efficacy and viability of deep-learning-based noise reduction.Healy EW(1), Johnson EM(1), Pandey A(2), Wang D(2).Author information:(1)Department of Speech and Hearing Science, and Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, Ohio 43210, USA.(2)Department of Computer Science and Engineering, and Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, Ohio 43210, USA.Recent years have brought considerable advances to our ability to increase intelligibility through deep-learning-based noise reduction, especially for hearing-impaired (HI) listeners. In this study, intelligibility improvements resulting from a current algorithm are assessed. These benefits are compared to those resulting from the initial demonstration of deep-learning-based noise reduction for HI listeners ten years ago in Healy, Yoho, Wang, and Wang [(2013). J. Acoust. Soc. Am. 134, 3029-3038]. The stimuli and procedures were broadly similar across studies. However, whereas the initial study involved highly matched training and test conditions, as well as non-causal operation, preventing its ability to operate in the real world, the current attentive recurrent network employed different noise types, talkers, and speech corpora for training versus test, as required for generalization, and it was fully causal, as required for real-time operation. Significant intelligibility benefit was observed in every condition, which averaged 51% points across conditions for HI listeners. Further, benefit was comparable to that obtained in the initial demonstration, despite the considerable additional demands placed on the current algorithm. The retention of large benefit despite the systematic removal of various constraints as required for real-world operation reflects the substantial advances made to deep-learning-based noise reduction.© 2023 Acoustical Society of America.DOI: 10.1121/10.0019341PMCID: PMC10159658",pubmed,37133814,10.1121/10.0019341
the oral speech intelligibility of hearingimpaired talkersdp   aug 1983,"Recordings were made of 10 hearing-impaired adolescents (aged 11 yrs 7 mo to 15 yrs 3 mo) speaking sentences of systematically different phonologic and syntactic structure. Tapes containing these sentences were played to 19 experienced and 25 inexperienced listeners in different conditions: in the absence vs the presence of a verbal context; by auditory vs audiovisual presentation; and with 1 vs 2 presentations of each sentence token. The responses of the listeners were scored and averaged for each S, condition, and type of sentence. Significant differences were observed between (1) simple sentences and those complicated either by consonant clusters, polysyllabic words, or complex syntax; (2) experienced and inexperienced listeners; (3) sentences in and out of context; and (4) sentences heard and seen as opposed to merely heard. The results are discussed with reference to measurement of the intelligibility of the speech of hearing-impaired individuals. (15 ref) (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc2&DO=10.1044%2fjshd.4803.286
the neuroanatomical hallmarks of chronic tinnitus in comorbidity with puretone hearing loss,"63. Brain Struct Funct. 2023 Jul;228(6):1511-1534. doi: 10.1007/s00429-023-02669-0. Epub 2023 Jun 22.The neuroanatomical hallmarks of chronic tinnitus in comorbidity with pure-tone hearing loss.Elmer S(#)(1)(2), Schmitt R(#)(3), Giroud N(3)(4)(5), Meyer M(6)(7)(8)(9).Author information:(1)Department of Computational Linguistics, Computational Neuroscience of Speech & Hearing, University of Zurich, Zurich, Switzerland. stefan.elmer@uzh.ch.(2)Competence Center Language & Medicine, University of Zurich, Zurich, Switzerland. stefan.elmer@uzh.ch.(3)Department of Computational Linguistics, Computational Neuroscience of Speech & Hearing, University of Zurich, Zurich, Switzerland.(4)Center for Neuroscience Zurich, University and ETH of Zurich, Zurich, Switzerland.(5)Competence Center Language & Medicine, University of Zurich, Zurich, Switzerland.(6)Department of Comparative Language Science, University of Zurich, Zurich, Switzerland. martin.meyer@uzh.ch.(7)Center for Neuroscience Zurich, University and ETH of Zurich, Zurich, Switzerland. martin.meyer@uzh.ch.(8)Center for the Interdisciplinary Study of Language Evolution (ISLE), University of Zurich, Zurich, Switzerland. martin.meyer@uzh.ch.(9)Cognitive Psychology Unit, Alpen-Adria University, Klagenfurt, Austria. martin.meyer@uzh.ch.(#)Contributed equallyTinnitus is one of the main hearing impairments often associated with pure-tone hearing loss, and typically manifested in the perception of phantom sounds. Nevertheless, tinnitus has traditionally been studied in isolation without necessarily considering auditory ghosting and hearing loss as part of the same syndrome. Hence, in the present neuroanatomical study, we attempted to pave the way toward a better understanding of the tinnitus syndrome, and compared two groups of almost perfectly matched individuals with (TIHL) and without (NTHL) pure-tone tinnitus, but both characterized by pure-tone hearing loss. The two groups were homogenized in terms of sample size, age, gender, handedness, education, and hearing loss. Furthermore, since the assessment of pure-tone hearing thresholds alone is not sufficient to describe the full spectrum of hearing abilities, the two groups were also harmonized for supra-threshold hearing estimates which were collected using temporal compression, frequency selectivity und speech-in-noise tasks. Regions-of-interest (ROI) analyses based on key brain structures identified in previous neuroimaging studies showed that the TIHL group exhibited increased cortical volume (CV) and surface area (CSA) of the right supramarginal gyrus and posterior planum temporale (PT) as well as CSA of the left middle-anterior part of the superior temporal sulcus (STS). The TIHL group also demonstrated larger volumes of the left amygdala and of the left head and body of the hippocampus. Notably, vertex-wise multiple linear regression analyses additionally brought to light that CSA of a specific cluster, which was located in the left middle-anterior part of the STS and overlapped with the one found to be significant in the between-group analyses, was positively associated with tinnitus distress level. Furthermore, distress also positively correlated with CSA of gray matter vertices in the right dorsal prefrontal cortex and the right posterior STS, whereas tinnitus duration was positively associated with CSA and CV of the right angular gyrus (AG) and posterior part of the STS. These results provide new insights into the critical gray matter architecture of the tinnitus syndrome matrix responsible for the emergence, maintenance and distress of auditory phantom sensations.© 2023. The Author(s).DOI: 10.1007/s00429-023-02669-0PMCID: PMC10335971",pubmed,37349539,10.1007/s00429-023-02669-0
intonation contour and syntactic structure as predictors of apparent segmentation references,"Segmentation is reflected in the chunking of complex stimuli into cohesive units. In speech this segmentation may be determined by interalized grammatical rules, with boundaries of the units corresponding to major constituent boundaries. On the other hand, this segmentation may also be accomplished by physical characteristics of the speech signal, with the boundaries of the units corresponding to acoustic discontinuities such as changes in vocal pitch, duration, and/or intensity of syllables at the boundary. The present 2 experiments contrasted these 2 types of characteristics to determine their roles in the segmentation of speech. Ss in Exp I were 32 normal-hearing adults; Ss in Exp II were 16 normal-hearing and 16 profoundly deaf adults. Segmentation of spoken sentences appeared to require both prosodic and syntactic cues. The tendency for the perception of interrupting stimuli to cluster was considerably greater at boundaries that were marked by both syntactic and prosodic cues than at those marked by intonation or syntax alone. Although prosodic structure must reinforce syntactic structure to produce maximum segmental effects, it was not essential that these prosodic cues be auditory. Ss with normal hearing appeared to require some auditory cue to intonation boundaries to achieve significant perceptual migration of interrupting stimuli, but deaf Ss were able to use, through vision, durational correlates of the melodic pattern to segment the speech signal. (23 ref) (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc2&DO=10.1037%2f0096-1523.4.2.273
loss of mir18396 alters synaptic strength via presynaptic and postsynaptic mechanisms at a central synapse,"745. J Neurosci. 2021 Aug 11;41(32):6796-6811. doi: 10.1523/JNEUROSCI.0139-20.2021. Epub 2021 Jun 30.Loss of miR-183/96 Alters Synaptic Strength via Presynaptic and Postsynaptic Mechanisms at a Central Synapse.Krohs C(1), Körber C(2), Ebbers L(1), Altaf F(1), Hollje G(1), Hoppe S(2), Dörflinger Y(2), Prosser HM(3), Nothwang HG(4)(5).Author information:(1)Division of Neurogenetics, Department of Neuroscience, Carl von Ossietzky University Oldenburg, Oldenburg 26129, Germany.(2)Institute of Anatomy und Cell Biology, Department of Functional Neuroanatomy, Heidelberg University, Heidelberg 69120, Germany.(3)Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, Cambridge CB10 1SA, United Kingdom.(4)Division of Neurogenetics, Department of Neuroscience, Carl von Ossietzky University Oldenburg, Oldenburg 26129, Germany hans.g.nothwang@uni-oldenburg.de.(5)Excellence Cluster Hearing4all, Carl von Ossietzky University Oldenburg, 26129 Oldenburg, Germany.A point mutation in miR-96 causes non-syndromic progressive peripheral hearing loss and alters structure and physiology of the central auditory system. To gain further insight into the functions of microRNAs (miRNAs) within the central auditory system, we investigated constitutive Mir-183/96dko mice of both sexes. In this mouse model, the genomically clustered miR-183 and miR-96 are constitutively deleted. It shows significantly and specifically reduced volumes of auditory hindbrain nuclei, because of decreases in cell number and soma size. Electrophysiological analysis of the calyx of Held synapse in the medial nucleus of the trapezoid body (MNTB) demonstrated strongly altered synaptic transmission in young-adult mice. We observed an increase in quantal content and readily releasable vesicle pool size in the presynapse while the overall morphology of the calyx was unchanged. Detailed analysis of the active zones (AZs) revealed differences in its molecular composition and synaptic vesicle (SV) distribution. Postsynaptically, altered clustering and increased synaptic abundancy of the AMPA receptor subunit GluA1 was observed resulting in an increase in quantal amplitude. Together, these presynaptic and postsynaptic alterations led to a 2-fold increase of the evoked excitatory postsynaptic currents in MNTB neurons. None of these changes were observed in deaf Cldn14ko mice, confirming an on-site role of miR-183 and miR-96 in the auditory hindbrain. Our data suggest that the Mir-183/96 cluster plays a key role for proper synaptic transmission at the calyx of Held and for the development of the auditory hindbrain.SIGNIFICANCE STATEMENT The calyx of Held is the outstanding model system to study basic synaptic physiology. Yet, genetic factors driving its morphologic and functional maturation are largely unknown. Here, we identify the Mir-183/96 cluster as an important factor to regulate its synaptic strength. Presynaptically, Mir-183/96dko calyces show an increase in release-ready synaptic vesicles (SVs), quantal content and abundance of the proteins Bassoon and Piccolo. Postsynaptically, the quantal size as well as number and size of GluA1 puncta were increased. The two microRNAs (miRNAs) are thus attractive candidates for regulation of synaptic maturation and long-term adaptations to sound levels. Moreover, the different phenotypic outcomes of different types of mutations in the Mir-183 cluster corroborate the requirement of mutation-tailored therapies in patients with hearing loss.Copyright © 2021 the authors.DOI: 10.1523/JNEUROSCI.0139-20.2021PMCID: PMC8360680",pubmed,34193555,10.1523/JNEUROSCI.0139-20.2021
audio to sign language translation using nlp,"Communication is an essential aspect of human life, but deaf people often face challenges in communicating with hearing people due to the language barrier. According to the World Health Organization, 466 million individuals, or 5% of the global population, have severe auditory loss. In India, there are roughly 18 million people who are deaf. Sign language is the primary way of communicating information for the deaf, but it requires significant time and effort to learn. To bridge this gap, we propose an Audio to Sign Language Translation solution that converts live voice or audio recordings through text forms and shows pertinent Animated GIFs or Sign Language images. The application uses frontend Easy Gui, PyAudio for microphone input, Google Speech API, and Sphinx for speech recognition, NLP for text preprocessing, and a dictionary-based machine translation approach for sign language generation. The algorithm follows a logical sequence of steps and provides an accurate and reliable solution to facilitate communication between deaf and hearing people. The purpose of this application is to give deaf individuals access to information and services in Indian sign language and to create a scalable project that can record the whole ISL vocabulary using both manual and non-manual signs. Overall, by enabling deaf people to communicate, the Audio to Sign Language Translation has the ability to significantly improve their quality of life to communicate easily and effectively with others.",ieee,,10.1109/STCR59085.2023.10397050
where is the spike generator of the cochlear nerve voltagegated sodium channels in the mouse cochlea,"689. J Neurosci. 2005 Jul 20;25(29):6857-68. doi: 10.1523/JNEUROSCI.0123-05.2005.Where is the spike generator of the cochlear nerve? Voltage-gated sodium channels in the mouse cochlea.Hossain WA(1), Antic SD, Yang Y, Rasband MN, Morest DK.Author information:(1)Department of Neuroscience, University of Connecticut Health Center, Farmington, Connecticut 06030, USA. whossain@neuron.uchc.eduThe origin of the action potential in the cochlea has been a long-standing puzzle. Because voltage-dependent Na+ (Nav) channels are essential for action potential generation, we investigated the detailed distribution of Nav1.6 and Nav1.2 in the cochlear ganglion, cochlear nerve, and organ of Corti, including the type I and type II ganglion cells. In most type I ganglion cells, Nav1.6 was present at the first nodes flanking the myelinated bipolar cell body and at subsequent nodes of Ranvier. In the other ganglion cells, including type II, Nav1.6 clustered in the initial segments of both of the axons that flank the unmyelinated bipolar ganglion cell bodies. In the organ of Corti, Nav1.6 was localized in the short segments of the afferent axons and their sensory endings beneath each inner hair cell. Surprisingly, the outer spiral fibers and their sensory endings were well labeled beneath the outer hair cells over their entire trajectory. In contrast, Nav1.2 in the organ of Corti was localized to the unmyelinated efferent axons and their endings on the inner and outer hair cells. We present a computational model illustrating the potential role of the Nav channel distribution described here. In the deaf mutant quivering mouse, the localization of Nav1.6 was disrupted in the sensory epithelium and ganglion. Together, these results suggest that distinct Nav channels generate and regenerate action potentials at multiple sites along the cochlear ganglion cells and nerve fibers, including the afferent endings, ganglionic initial segments, and nodes of Ranvier.DOI: 10.1523/JNEUROSCI.0123-05.2005PMCID: PMC1378182",pubmed,16033895,10.1523/JNEUROSCI.0123-05.2005
americans hear as well or better today compared with 40 years ago hearing threshold levels in the unscreened adult population of the united states 19591962 and 19992004,"OBJECTIVES: (1) To present hearing threshold data from a recent nationally representative survey in the United States (National Health and Nutrition Examination Survey, 1999-2004) in a distributional format that might be appropriate to replace Annex B in international (ISO-1999) and national (ANSI S3.44) standards and (2) to compare these recent data with older survey data (National Health Examination Survey I, 1959-1962) on which the current Annex B is based. DESIGN: Better-ear threshold distributions (selected percentiles and their confidence intervals) were estimated using linear interpolation. The 95% confidence intervals for the medians for the two surveys were compared graphically for each of the four age groups and for both men and women. In addition, we calculated odds ratios comparing the prevalences of better-ear hearing impairment (thresholds > 25 dB HL) between the two surveys, for 500, 1000, 2000, and 4000 Hz, and for their four-frequency average. RESULTS: Across age and sex groups, median thresholds were lower (better) in the 1999-2004 survey at 500, 3000, 4000, and 6000 Hz (8000 Hz was not tested in the 1959-1962 survey). For both men and women, the prevalence of hearing impairment was significantly lower in 1999-2004 at 500, 2000, and 4000 Hz, but not at 1000 Hz. CONCLUSIONS: For men and women of a specific age, high-frequency hearing thresholds were lower (better) in 1999-2004 than in 1959-1962. The prevalences of hearing impairment were also lower in the recent survey. Differences seen at 500 Hz may be attributable at least in part to changes in standards for ambient noise in audiometry. The National Health and Nutrition Examination Survey 1999-2004 distributions are offered as a possible replacement for Annex B in ISO-1999 and ANSI S3.44.",cinahl,1960202,10.1097/AUD.0b013e3181e9770e
paradoxical relationship between distress and functional network topology in phantom sound perception,"Distress is a domain-general symptom that accompanies several disorders, including tinnitus. Based on previous studies, we know that distress is encoded by changes in functional connectivity between cortical and subcortical regions. However, how distress relates to large-scale brain networks is not yet clear. In the current study, we investigate the relationship between distress and the efficiency of a network by examining its topological properties using resting state fMRI collected from 90 chronic tinnitus patients. The present results indicate that distress negatively correlates with path length and positively correlates with clustering coefficient, small-worldness, and efficiency of information transfer. Specifically, path analysis showed that the relationship between distress and efficiency is significantly mediated by the resilience of the feeder connections and the centrality of the rich-club connections. In other words, the higher the network efficiency, the lower the resilience of the feeder connections and the centrality of the rich-club connections, which in turn reflects in higher distress in tinnitus patients. This indicates a reorganization of the network towards a paradoxically more efficient topology in patients with high distress, potentially explaining their increased rumination on the tinnitus percept itself. © 2021 Elsevier B.V.",scopus,2-s2.0-85094155479,10.1016/bs.pbr.2020.08.007
nextgeneration hearing prosthetics,"Neural networks and fuzzy logic are powerful tools for next-generation hearing prosthetics. A neural network, as a function fitter to map the hearing loss to desired gains requirements, provides many benefits over other approaches. The network is able to learn dynamically through experience. It is open and expandable - a physician can easily incorporate new knowledge into the system. Fuzzy logic, on the other hand, is an indispensable tool for the tuning process. It builds a direct and reasonable link between a user's subjective evaluation and the actual required modifications to the gain targets. Again, physicians are free to add new rules to the rule base in reflection of specific needs and patterns. The presented neurofuzzy approach helps hearing prosthetic devices not only in an offline fitting process, but also in online operations. Next-generation hearing prosthetics will be more intelligent than current devices. Hearing aids should be situation-dependent and capable of evolving or adapting. The neurofuzzy approach makes these features possible.",ieee,1558-223X,10.1109/MRA.2003.1191707
congenital sensorineural deafness in australian cattle dogs in the uk prevalence and association with phenotype,"85. Vet J. 2021 Aug;274:105711. doi: 10.1016/j.tvjl.2021.105711. Epub 2021 Jun 25.Congenital sensorineural deafness in Australian Cattle dogs in the UK: Prevalence and association with phenotype.Marsh O(1), Freeman J(2), Pollard D(3), De Risio L(4).Author information:(1)Southfields Veterinary Specialists, Laindon, Essex, UK. Electronic address: oliver.marsh@southfields.co.uk.(2)Davies Veterinary Specialists, Hitchen, Hertfordshire, UK.(3)British Horse Society, Kenilworth, Warwickshire, UK.(4)Linnaeus Veterinary Ltd, Friars Gate, Shirley, UK.The Australian Cattle dog (ACD) is one of many breeds predisposed to congenital sensorineural deafness (CSD). The objective of this study was to estimate CSD prevalence and investigate any association with phenotype in the ACD in the UK. The database of the authors' institution was searched for ACD puppies undergoing brainstem auditory evoked response (BAER) testing for CSD screening (1999-2019). Inclusion criteria were BAER performed at 4-10 weeks of age, testing of complete litters and available phenotypic data. The age, sex, coat and iris colour, presence and location of face and body patches, hearing status and BAER- determined parental hearing status of each puppy were recorded. A multivariable mixed-effects logistic regression model was used to calculate odds ratios and 95% confidence intervals to determine whether any of these variables were significantly associated with CSD, while adjusting for clustering at litter level. Inclusion criteria were met for 524 puppies. Hearing was bilaterally normal in 464 puppies (88.6%). The prevalence of unilateral and bilateral CSD was 9.7% and 1.7%, respectively. On the basis of multivariable analysis, the presence of a pigmented face patch was the only phenotypic variable significantly associated with CSD, and was linked to a reduced risk of the condition. The prevalence was similar to that reported in an Australian population of ACDs. The key findings from this study were that overall CSD prevalence in the ACD population in the UK was 11.4%, and puppies with a face patch were at reduced risk of the condition.Copyright © 2021 Elsevier Ltd. All rights reserved.DOI: 10.1016/j.tvjl.2021.105711",pubmed,34182072,10.1016/j.tvjl.2021.105711
on the balance of envelope and temporal fine structure in the encoding of speech in the early auditory system,"7041–0: Systems and Methods for Balance Stabilization..532. J Acoust Soc Am. 2013 May;133(5):2818-33. doi: 10.1121/1.4795783.On the balance of envelope and temporal fine structure in the encoding of speech in the early auditory system.Shamma S(1), Lorenzi C.Author information:(1)Electrical and Computer Engineering Department and Institute for Systems Research, University of Maryland, College Park, Maryland 20742, USA. sas@umd.eduThere is much debate on how the spectrotemporal modulations of speech (or its spectrogram) are encoded in the responses of the auditory nerve, and whether speech intelligibility is best conveyed via the ""envelope"" (E) or ""temporal fine-structure"" (TFS) of the neural responses. Wide use of vocoders to resolve this question has commonly assumed that manipulating the amplitude-modulation and frequency-modulation components of the vocoded signal alters the relative importance of E or TFS encoding on the nerve, thus facilitating assessment of their relative importance to intelligibility. Here we argue that this assumption is incorrect, and that the vocoder approach is ineffective in differentially altering the neural E and TFS. In fact, we demonstrate using a simplified model of early auditory processing that both neural E and TFS encode the speech spectrogram with constant and comparable relative effectiveness regardless of the vocoder manipulations. However, we also show that neural TFS cues are less vulnerable than their E counterparts under severe noisy conditions, and hence should play a more prominent role in cochlear stimulation strategies.DOI: 10.1121/1.4795783PMCID: PMC3663870",pubmed,23654388,10.1121/1.4795783
database for vertigo,"41. Otolaryngol Head Neck Surg. 1995 Mar;112(3):383-90. doi: 10.1016/S0194-59989570271-7.Database for vertigo.Kentala E(1), Pyykkö I, Auramo Y, Juhola M.Author information:(1)Department of Otolaryngology, University Hospital of Helsinki, Finland.An interactive database has been developed to assist the diagnostic procedure for vertigo and to store the data. The database offers a possibility to split and reunite the collected information when needed. It contains detailed information about a patient's history, symptoms, and findings in otoneurologic, audiologic, and imaging tests. The symptoms are classified into sets of questions on vertigo (including postural instability), hearing loss and tinnitus, and provoking factors. Confounding disorders are screened. The otoneurologic tests involve saccades, smooth pursuit, posturography, and a caloric test. In addition, findings from specific antibody tests, clinical neurotologic tests, magnetic resonance imaging, brain stem audiometry, and electrocochleography are included. The input information can be applied to workups for vertigo in an expert system called ONE. The database assists its user in that the input of information is easy. If not only can be used for diagnostic purposes but is also beneficial for research, and in combination with the expert system, it provides a tutorial guide for medical students.DOI: 10.1016/S0194-59989570271-7",pubmed,7870437,10.1016/S0194-59989570271-7
marginal structural models for multilevel clustered data,"842. Biostatistics. 2022 Oct 14;23(4):1056-1073. doi: 10.1093/biostatistics/kxac027.Marginal structural models for multilevel clustered data.Wu Y(1), Langworthy B(2), Wang M(3).Author information:(1)Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA 02215, USA.(2)Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA 02215, USA and Department of Epidemiology, Harvard T.H. Chan School of Public Health, Boston, MA 02215, USA.(3)Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA 02215, USA, Department of Epidemiology, Harvard T.H. Chan School of Public Health, Boston, MA 02215, USA, and Channing Division of Network Medicine, Department of Medicine, Brigham and Women's Hospital, Boston, MA, 02215, USA and Harvard Medical School, Boston, MA 02115, USA.Marginal structural models (MSMs), which adopt inverse probability treatment weighting in the estimating equations, are powerful tools to estimate the causal effects of time-varying exposures in the presence of time-dependent confounders. Motivated by the Conservation of Hearing Study (CHEARS) Audiology Assessment Arm (AAA) where repeated hearing measurements were clustered by study participants, time, and testing sites, we propose two methods to account for the multilevel correlation structure when fitting the MSMs. The first method directly models the covariance of the repeated outcomes when solving the weighted generalized estimating equations for MSMs, while the second two-stage analysis approach fits cluster-specific MSMs first and then combines the estimated parameters using mixed-effects meta-analysis. Finite sample simulation results suggest that our methods can obtain less biased and more efficient estimates of the parameters by accounting for the multilevel correlation. Moreover, we explore the effects of using fixed- or mixed-effects model to estimate the treatment probability on the parameter estimates of the MSMs in the presence of unmeasured cluster-level confounders. Lastly, we apply our methods to the CHEARS AAA data set, to estimate the causal effects of aspirin use on hearing loss.© The Author 2022. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.DOI: 10.1093/biostatistics/kxac027PMCID: PMC9802195",pubmed,35904119,10.1093/biostatistics/kxac027
hearing loss prediction in newborns infants and toddlers using machine learning,,base,b9a28b6c6155ae286523aaf5c6abe241dc3e2bec0a63ab17fd7c48d9ca09911f,
an analysis of current source density profiles activated by local stimulation in the mouse auditory cortex in vitro,"370. Brain Res. 2017 Mar 15;1659:96-112. doi: 10.1016/j.brainres.2017.01.021. Epub 2017 Jan 21.An analysis of current source density profiles activated by local stimulation in the mouse auditory cortex in vitro.Yamamura D(1), Sano A(2), Tateno T(3).Author information:(1)Bioengineering and Bioinformatics, Graduate School of Information Science and Technology, Hokkaido University, Kita 14, Nishi 9, Kita-ku, Sapporo 060-0814, Japan. Electronic address: Yamamura_Daiki@ist.hokudai.ac.jp.(2)Bioengineering and Bioinformatics, Graduate School of Information Science and Technology, Hokkaido University, Kita 14, Nishi 9, Kita-ku, Sapporo 060-0814, Japan.(3)Bioengineering and Bioinformatics, Graduate School of Information Science and Technology, Hokkaido University, Kita 14, Nishi 9, Kita-ku, Sapporo 060-0814, Japan. Electronic address: tateno@ist.hokudai.ac.jp.To examine local network properties of the mouse auditory cortex in vitro, we recorded extracellular spatiotemporal laminar profiles driven by short electric local stimulation on a planar multielectrode array substrate. The recorded local field potentials were subsequently evaluated using current source density (CSD) analysis to identify sources and sinks. Current sinks are thought to be an indicator of net synaptic current in the small volume of cortex surrounding the recording site. Thus, CSD analysis combined with multielectrode arrays enabled us to compare mean synaptic activity in response to small current stimuli on a layer-by-layer basis. We also used senescence-accelerated mice (SAM), some strains of which show earlier onset of age-related hearing loss, to examine the characteristic spatiotemporal CSD profiles stimulated by electrodes in specific cortical layers. Thus, the CSD patterns were classified into several clusters based on stimulation sites in the cortical layers. We also found some differences in CSD patterns between the two SAM strains in terms of aging according to principle component analysis with dimension reduction. For simultaneous two-site stimulation, we modeled the obtained CSD profiles as a linear superposition of the CSD profiles to individual single-site stimulation. The model analysis indicated the nonlinearity of spatiotemporal integration over stimulus-driven activity in a layer-specific manner. Finally, on the basis of these results, we discuss the auditory cortex local network properties and the effects of aging on these mouse strains.Copyright Â© 2017 Elsevier B.V. All rights reserved.DOI: 10.1016/j.brainres.2017.01.021",pubmed,28119054,10.1016/j.brainres.2017.01.021
of mice and men the relevance of cometin and erythropoietin origin for its effects on murine spiral ganglion neuron survival and neurite outgrowth in vitro,"730. Front Neurosci. 2023 Aug 10;17:1224463. doi: 10.3389/fnins.2023.1224463. eCollection 2023.""Of mice and men"": the relevance of Cometin and Erythropoietin origin for its effects on murine spiral ganglion neuron survival and neurite outgrowth in vitro.Schwieger J(1)(2)(3), Gao Z(1)(4), Lenarz T(1)(2)(3), Munro G(5), Petersen KA(5), Scheper V(1)(2)(3).Author information:(1)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany.(2)Lower Saxony Center for Biomedical Engineering, Implant Research and Development (NIFE), Hannover, Germany.(3)Cluster of Excellence ""Hearing4all"" EXC 1077/2, Hannover, Germany.(4)Ear Nose and Throat Institute and Department of Otorhinolaryngology, Eye & ENT Hospital, Fudan University, Shanghai, China.(5)Hoba Therapeutics ApS, Copenhagen, Denmark.Neurotrophic factors (NTF) play key roles in the survival of neurons, making them promising candidates for therapy of neurodegenerative diseases. In the case of the inner ear, sensorineural hearing loss (SNHL) is characterized over time by a degeneration of the primary auditory neurons, the spiral ganglion neurons (SGN). It is well known that selected NTF can protect SGN from degeneration, which positively influences the outcome of cochlear implants, the treatment of choice for patients with profound to severe SNHL. However, the outcome of studies investigating protective effects of NTF on auditory neurons are in some cases of high variability. We hypothesize that the factor origin may be one aspect that affects the neuroprotective potential. The aim of this study was to investigate the neuroprotective potential of human and mouse Erythropoietin (EPO) and Cometin on rat SGN. SGN were isolated from neonatal rats (P 2-5) and cultured in serum-free medium. EPO and Cometin of mouse and human origin were added in concentrations of 0.1, 1, and 10 ng/mL and 0.1, 1, and 10 μg/mL, respectively. The SGN survival rate and morphology, and the neurite outgrowth were determined and compared to negative (no additives) and positive (brain-derived neurotrophic factor, BDNF) controls. A neuroprotective effect of 10 μg/mL human Cometin comparable to that obtained with BDNF was observed in the SGN-culture. In contrast, mouse Cometin was ineffective. A similar influence of 10 μg/mL human and mouse and 1 μg/mL human Cometin on the length of regenerated neurites compared to BDNF was also detected. No other Cometin-conditions, and none of the EPO-conditions tested had neuroprotective or neuritogenic effects or influenced the neuronal morphology of the SGN. The neuroprotective effect of 10 μg/mL human Cometin on SGN indicates it is a potentially interesting protein for the supportive treatment of inner ear disorders. The finding that mouse Cometin had no effect on the SGN in the parallel-performed experiments underlines the importance of species origin of molecules being screened for therapeutic purpose.Copyright © 2023 Schwieger, Gao, Lenarz, Munro, Petersen and Scheper.DOI: 10.3389/fnins.2023.1224463PMCID: PMC10450246",pubmed,37638326,10.3389/fnins.2023.1224463
explainable machine learning prediction for the academic performance of deaf scholars,"Deaf and Hard of Hearing (DHH) students encounter obstacles in higher education due to language and communication challenges. Although research aims to improve their academic performance, the potential of Machine Learning (ML) remains underutilized in DHH education. The opacity of ML models further complicates their adoption. This study aims to fill this gap by developing a novel ML-based system with eXplainable AI (XAI), specifically utilizing Local Interpretable Model-Agnostic Explainer (LIME) and Shapley Additive Explainer (SHAP). The objective is twofold: predicting at-risk DHH students and explaining risk factors. Merging ML and XAI, this approach could positively impact DHH students' educational outcomes. A dataset of 454 records detailing DHH students is collected. To address dataset limitations, synthetic data and SMOTE are used. Students are categorized into three performance levels. The data is modeled with different ML models, transfer models, ensemble models, and combination models. Among the models, the stacked model with XGBoost, ExtraTrees, and Random Forest exhibited better performance with an accuracy of 92.99%. Results highlight the model's significance, providing insights through XAI into crucial factors affecting academic performance, including communication mode, early intervention, schooling type, and family deafness history. LIME and SHAP values were found to be effective in deriving insights into DHH student performance prediction framework. Communication mode, notably, strongly influences at-risk students. The major contribution of this study is the development of a novel ML-based system and the XAI interpretations whose value lies in its social relevance, guiding stakeholders to enhance DHH scholars' academic achievements.  © 2013 IEEE.",scopus,2-s2.0-85184809702,10.1109/ACCESS.2024.3363634
two types of afferent terminals innervate cochlear inner hair cells in c57bl6j mice,"480. Brain Res. 2004 Aug 6;1016(2):182-94. doi: 10.1016/j.brainres.2004.05.016.Two types of afferent terminals innervate cochlear inner hair cells in C57BL/6J mice.Francis HW(1), Rivas A, Lehar M, Ryugo DK.Author information:(1)Department of Otolaryngology-Head and Neck Surgery, Johns Hopkins University, 720 Rutland Avenue, Baltimore, MD 21205, USA. hfrancis@jhmi.eduAfferent synapses on inner hair cells (IHC) transfer auditory information to the central nervous system (CNS). Despite the importance of these synapses for normal hearing, their response to cochlear disease and dysfunction is not well understood. The C57BL/6J mouse is a model for presbycusis and noise-induced hearing loss because of its age-related hearing loss and susceptibility to acoustic over-exposure. In this context, we sought to establish normal synaptic structure in order to better evaluate synaptic changes due to presbycusis and noise exposure. Ultrastructural analysis of IHCs and afferent terminals was performed in a normal hearing 3-month-old C57BL/6J mouse at cochlear sites corresponding to 8, 16 and 32 kHz using semi-serial sections. A stereologic survey of random sections was conducted of IHCs in 11 additional mice. Two morphologically distinct groups of afferent terminals were identified at all 3 frequency locations in 11 out of 12 animals. ""Simple"" endings demonstrated classic features of bouton terminals, whereas ""folded"" endings were larger in size and exhibited a novel morphologic feature that consisted of a fully internalized double membrane that partially divided the terminal into two compartments. In many cases, the double membrane was continuous with the outer terminal membrane as if produced by an invagination. We still must determine the generality of these observations with respect to other mouse strains.DOI: 10.1016/j.brainres.2004.05.016",pubmed,15246854,10.1016/j.brainres.2004.05.016
auditory critical periods a review from systems perspective,"621. Neuroscience. 2013 Sep 5;247:117-33. doi: 10.1016/j.neuroscience.2013.05.021. Epub 2013 May 21.Auditory critical periods: a review from system's perspective.Kral A(1).Author information:(1)Hearing4all Cluster of Excellence, Hannover School of Medicine, Feodor-Lynen-Str. 35, D-30625 Hannover, Germany. kral.andrej@mh-hannover.deThe article reviews evidence for sensitive periods in the sensory systems and considers their neuronal mechanisms from the viewpoint of the system's neuroscience. It reviews the essential cortical developmental steps and shows its dependence on experience. It differentiates feature representation and object representation and their neuronal mechanisms. The most important developmental effect of experience is considered to be the transformation of a naive cortical neuronal network into a network capable of categorization, by that establishing auditory objects. The control mechanisms of juvenile and adult plasticity are further discussed. Total absence of hearing experience prevents the patterning of the naive auditory system with subsequent extensive consequences on the auditory function. Additional to developmental changes in synaptic plasticity, other brain functions like corticocortical interareal couplings are also influenced by deprivation. Experiments with deaf auditory systems reveal several integrative effects of deafness and their reversibility with experience. Additional to developmental molecular effects on synaptic plasticity, a combination of several integrative effects of deprivation on brain functions, including feature representation (affecting the starting point for learning), categorization function, top-down interactions and cross-modal reorganization close the sensitive periods and may contribute to their critical nature. Further, non-auditory effects of auditory deprivation are discussed. To reopen critical periods, removal of molecular breaks in synaptic plasticity and focused training therapy on the integrative effects are required.Copyright © 2013 The Author. Published by Elsevier Ltd.. All rights reserved.DOI: 10.1016/j.neuroscience.2013.05.021",pubmed,23707979,10.1016/j.neuroscience.2013.05.021
temporal coding of single auditory nerve fibers is not degraded in aging gerbils,"276. J Neurosci. 2020 Jan 8;40(2):343-354. doi: 10.1523/JNEUROSCI.2784-18.2019. Epub 2019 Nov 12.Temporal Coding of Single Auditory Nerve Fibers Is Not Degraded in Aging Gerbils.Heeringa AN(1), Zhang L(1), Ashida G(1), Beutelmann R(1), Steenken F(1), Köppl C(2).Author information:(1)Cluster of Excellence ""Hearing4all"" and Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg, 26129 Oldenburg, Germany.(2)Cluster of Excellence ""Hearing4all"" and Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg, 26129 Oldenburg, Germany christine.koeppl@uni-oldenburg.de.People suffering from age-related hearing loss typically present with deficits in temporal processing tasks. Temporal processing deficits have also been shown in single-unit studies at the level of the auditory brainstem, midbrain, and cortex of aged animals. In this study, we explored whether temporal coding is already affected at the level of the input to the central auditory system. Single-unit auditory nerve fiber recordings were obtained from 41 Mongolian gerbils of either sex, divided between young, middle-aged, and old gerbils. Temporal coding quality was evaluated as vector strength in response to tones at best frequency, and by constructing shuffled and cross-stimulus autocorrelograms, and reverse correlations, from responses to 1 s noise bursts at 10-30 dB sensation level (dB above threshold). At comparable sensation levels, all measures showed that temporal coding was not altered in auditory nerve fibers of aging gerbils. Furthermore, both temporal fine structure and envelope coding remained unaffected. However, spontaneous rates were decreased in aging gerbils. Importantly, despite elevated pure tone thresholds, the frequency tuning of auditory nerve fibers was not affected. These results suggest that age-related temporal coding deficits arise more centrally, possibly due to a loss of auditory nerve fibers (or their peripheral synapses) but not due to qualitative changes in the responses of remaining auditory nerve fibers. The reduced spontaneous rate and elevated thresholds, but normal frequency tuning, of aged auditory nerve fibers can be explained by the well known reduction of endocochlear potential due to strial dysfunction in aged gerbils.SIGNIFICANCE STATEMENT As our society ages, age-related hearing deficits become ever more prevalent. Apart from decreased hearing sensitivity, elderly people often suffer from a reduced ability to communicate in daily settings, which is thought to be caused by known age-related deficits in auditory temporal processing. The current study demonstrated, using several different stimuli and analysis techniques, that these putative temporal processing deficits are not apparent in responses of single-unit auditory nerve fibers of quiet-aged gerbils. This suggests that age-related temporal processing deficits may develop more central to the auditory nerve, possibly due to a reduced population of active auditory nerve fibers, which will be of importance for the development of treatments for age-related hearing disorders.Copyright © 2020 the authors.DOI: 10.1523/JNEUROSCI.2784-18.2019PMCID: PMC6948943",pubmed,31719164,10.1523/JNEUROSCI.2784-18.2019
a physiologicallyinspired model reproducing the speech intelligibility benefit in cochlear implant listeners with residual acoustic hearing,"298. Hear Res. 2017 Feb;344:50-61. doi: 10.1016/j.heares.2016.10.023. Epub 2016 Nov 9.A physiologically-inspired model reproducing the speech intelligibility benefit in cochlear implant listeners with residual acoustic hearing.Zamaninezhad L(1), Hohmann V(2), Büchner A(3), Schädler MR(4), Jürgens T(5).Author information:(1)Medizinische Physik, Cluster of Excellence ""Hearing4all"", and Forschungszentrum Neurosensorik, Carl-von-Ossietzky Universität Oldenburg, Germany. Electronic address: ladan.zamaninezhad@uni-oldenburg.de.(2)Medizinische Physik, Cluster of Excellence ""Hearing4all"", and Forschungszentrum Neurosensorik, Carl-von-Ossietzky Universität Oldenburg, Germany. Electronic address: volker.hohmann@uni-oldenburg.de.(3)Deutsches Hörzentrum der Medizinischen Hochschule Hannover and Cluster of Excellence ""Hearing4all"", D-30625, Germany. Electronic address: Buechner@hoerzentrum-hannover.de.(4)Medizinische Physik, Cluster of Excellence ""Hearing4all"", and Forschungszentrum Neurosensorik, Carl-von-Ossietzky Universität Oldenburg, Germany. Electronic address: marc.r.schaedler@uni-oldenburg.de.(5)Medizinische Physik, Cluster of Excellence ""Hearing4all"", and Forschungszentrum Neurosensorik, Carl-von-Ossietzky Universität Oldenburg, Germany. Electronic address: tim.juergens@uni-oldenburg.de.This study introduces a speech intelligibility model for cochlear implant users with ipsilateral preserved acoustic hearing that aims at simulating the observed speech-in-noise intelligibility benefit when receiving simultaneous electric and acoustic stimulation (EA-benefit). The model simulates the auditory nerve spiking in response to electric and/or acoustic stimulation. The temporally and spatially integrated spiking patterns were used as the final internal representation of noisy speech. Speech reception thresholds (SRTs) in stationary noise were predicted for a sentence test using an automatic speech recognition framework. The model was employed to systematically investigate the effect of three physiologically relevant model factors on simulated SRTs: (1) the spatial spread of the electric field which co-varies with the number of electrically stimulated auditory nerves, (2) the ""internal"" noise simulating the deprivation of auditory system, and (3) the upper bound frequency limit of acoustic hearing. The model results show that the simulated SRTs increase monotonically with increasing spatial spread for fixed internal noise, and also increase with increasing the internal noise strength for a fixed spatial spread. The predicted EA-benefit does not follow such a systematic trend and depends on the specific combination of the model parameters. Beyond 300 Hz, the upper bound limit for preserved acoustic hearing is less influential on speech intelligibility of EA-listeners in stationary noise. The proposed model-predicted EA-benefits are within the range of EA-benefits shown by 18 out of 21 actual cochlear implant listeners with preserved acoustic hearing.Copyright © 2016 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2016.10.023",pubmed,27838372,10.1016/j.heares.2016.10.023
research on effective methodology for hearing impairment rehabilitation based on a diary study for elderly onlineauditory training,"This study analyzes the utilization of online-based auditory training content by individuals aged 70 and above, who are the primary users of auditory training programs. Additionally, we explore effective methodologies for online-based auditory training programs by investigating the impact of differences in auditory training content on training outcomes. Two elderly individuals, one female and one male, wearing hearing aids, participated in this study. We assessed speech perception in noise at 5 dB SNR before and after auditory training. They completed a listening effort questionnaire and attended a total of 20 auditory training sessions over 7 weeks. During the training period, they maintained a “training diary” three times a week. We conducted interviews with the two audiologists who provided auditory training to objectively analyze the diary survey results. Both hearing aid users demonstrated improved speech perception in noise scores and reduced listening effort. The analysis of auditory training experiences categorized them into three types: ‘emotion-stimulating,’ ‘information-generating,’ and ‘participatory.’ These categories informed the development of strategies for online auditory training content, including: 1) personalized auditory training content using artificial intelligence, 2) integration of auditory training into senior education programs, and 3) creation of interactive online auditory rehabilitation content using bidirectional technology. The study emphasizes the need for continuous development of interactive content tailored to the demographic and lifestyle characteristics of the participants. In the context of online-based auditory training, individualized programs, such as group training and professional-led programs, are essential. Further studies are required to determine whether training outcomes can be enhanced through experience-based content. Copyright © 2023 Korean Academy of Audiology.",scopus,2-s2.0-85177775133,10.21848/asr.230126
universal neonatal hearing screening in flanders reveals sociodemographic risk factors for hearing impairment,"297. B-ENT. 2013;Suppl 21:3-8.Universal neonatal hearing screening in Flanders reveals socio-demographic risk factors for hearing impairment.Van Kerschaver E(1).Author information:(1)Kind en Gezin (Child and Family), Brussels, Belgium. erwin.vankerschaver@scarlet.beINTRODUCTION: Permanent congenital hearing impairment (CHI) occurs in approximately 1.4 per 1,000 newborns. Early treatment and rehabilitation is essential to prevent the delayed development of speech and language. This paper describes the special collaborative approach of the Flemish screening programme. It also discusses the results and the new insights into socio-demographic risk factors for CHI.METHODS: In the period 1999-2008, the entire population of 628,337 newborns in Flanders was tested using an AABR hearing screener. Positive results were referred for confirmation of the CHI diagnosis to specialised referral centres. Socio-demographic factors were investigated to study any relationship with CHI.RESULTS: The referral rate after two screenings was 2.7-7.2 per thousand of screened babies depending on the screener used. All children were referred to specialised centres and there was almost no loss to follow-up. The diagnosis of hearing loss was confirmed in 77-82% of the babies referred. The socio-demographic factors of gender, birth order and birth length, initial feeding type, level of education and origin of the mother were found to be independent predictors of CHI. Most of these risk factors can be linked to poverty. The observation that 50% of babies with CHI have no risk factors from the classic AAP list may be partly explained by the non-inclusion of socio-demographic risk factors.CONCLUSIONS: This integrated programme opens up new perspectives for hearing-impaired babies. The social impact of the screening programme is considerable. A cluster of socio-demographic risk factors for CHI can be added to the classic AAP list.",pubmed,24383217,
the 2nd clarity enhancement challenge for hearing aid speech intelligibility enhancement overview and outcomes,"This paper reports on the design and outcomes of the 2nd Clarity Enhancement Challenge (CEC2), a challenge for stimulating novel approaches to hearing-aid speech intelligibility enhancement. The challenge was for a listener attending to a target speaker in a noisy, domestic environment. The challenge extends the previous edition, CEC1, in a number of key respects: scenes have multiple interferers including speech, noise and music; ambisonics are used to model listener head movement; target speaker identity is provided to encourage speaker extraction approaches. Systems are evaluated both via the HASPI intelligibility metric and with listening tests using a panel of hearing-impaired listeners. The paper reviews the 18 systems that were submitted describing them in terms of their enhancement and amplification stages. HASPI is seen to be a good predictor of listener performance. The top system, using carefully engineered neural approaches, produces highly intelligible signals for complex scenes with SNRs down to -12 dB while obeying the challenges 5 ms latency constraint. © 2023 IEEE.",scopus,2-s2.0-85166052077,10.1109/ICASSP49357.2023.10094918
the future of otology,"721. J Laryngol Otol. 2019 Sep;133(9):747-758. doi: 10.1017/S0022215119001531. Epub 2019 Aug 29.The future of otology.Jackler RK(1), Jan TA(1).Author information:(1)Department of Otolaryngology - Head and Neck Surgery, Stanford Ear Institute, Stanford University School of Medicine, California, USA.BACKGROUND: The field of otology is increasingly at the forefront of innovation in science and medicine. The inner ear, one of the most challenging systems to study, has been rendered much more open to inquiry by recent developments in research methodology. Promising advances of potential clinical impact have occurred in recent years in biological fields such as auditory genetics, ototoxic chemoprevention and organ of Corti regeneration. The interface of the ear with digital technology to remediate hearing loss, or as a consumer device within an intelligent ecosystem of connected devices, is receiving enormous creative energy. Automation and artificial intelligence can enhance otological medical and surgical practice. Otology is poised to enter a new renaissance period, in which many previously untreatable ear diseases will yield to newly introduced therapies.OBJECTIVE: This paper speculates on the direction otology will take in the coming decades.CONCLUSION: Making predictions about the future of otology is a risky endeavour. If the predictions are found wanting, it will likely be because of unforeseen revolutionary methods.DOI: 10.1017/S0022215119001531",pubmed,31462337,10.1017/S0022215119001531
prevalence of cerumen impaction and associated factors among primary school children in mwanza city tanzania,"Background: Cerumen impaction is a worldwide problem constituting a significant proportion of health problems in many settings and its prevalence varies. There is a paucity of published data regarding this condition in Tanzania with none from Mwanza region. The aim of this study was to determine the prevalence of cerumen impaction and associated factors among primary school children in Mwanza City and to assess the effect of cerumen impaction and its removal on hearing ability Methods and Patients: This was a cross-sectional, community based study of primary school children with cerumen impaction that was carried out in randomly selected primary schools in Mwanza City between December 2016 and May 2017. Multistage cluster sampling technique was employed to obtain a required number of the study population. Results: Out of the 406 participants, ninety-five (23.4%) had cerumen impacted in their ears. Of these, 56 (58.9%) were males and 39(41.1%) were females. The mean age at presentation was 11.24±8.86 years. Ear bud abuse (83.7%) was the most common predisposing factor for cerumen impaction. Cerumen impaction was found in the right ear of 9 (9.5%) patients and in the left ear in 31 (32.6%) patients and bilateral in 55 (57.9%) of patients. The major presenting symptoms were ear itching, otalgia, hearing loss and tinnitus. Ear syringing was used to remove cerumen impaction and caused significant improvement in hearing thresholds. There were no recorded complications. Conclusion: Cerumen impaction is a common otologic presentation in our sub-region. Ignorance with the profound abuse of cotton buds is the major predisposing factor. Health education is of the essence as treatment is simple and effective. © 2019, National Institute for Medical Research. All rights reserved.",scopus,2-s2.0-85088555279,10.4314/thrb.v21i1.6
patient perspectives on the need for improved hearing rehabilitation a qualitative survey study of german cochlear implant users,"805. Front Neurosci. 2023 Jan 23;17:1105562. doi: 10.3389/fnins.2023.1105562. eCollection 2023.Patient perspectives on the need for improved hearing rehabilitation: A qualitative survey study of German cochlear implant users.Hunniford V(1)(2), Kühler R(3), Wolf B(1)(4)(5), Keppeler D(1), Strenzke N(1)(3)(6), Moser T(1)(4)(5)(6)(7).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(2)Sensory and Motor Neuroscience, Göttingen Graduate Center for Neurosciences, Biophysics, and Molecular Biosciences, Göttingen, Germany.(3)Department of Otolaryngology, University Medical Center Göttingen, Göttingen, Germany.(4)Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(5)Cluster of Excellence ""Multiscale Bioimaging: From Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, Göttingen, Germany.(6)Collaborative Research Center 889, University of Göttingen, Göttingen, Germany.(7)Auditory Neuroscience and Synaptic Nanophysiology Group, Max Planck Institute for Multidisciplinary Sciences, Göttingen, Germany.BACKGROUND: The electrical cochlear implant (eCI) partially restores hearing in individuals affected by profound hearing impairment (HI) or deafness. However, the limited resolution of sound frequency coding with eCIs limits hearing in daily situations such as group conversations. Current research promises future improvements in hearing restoration which may involve gene therapy and optical stimulation of the auditory nerve, using optogenetics. Prior to the potential clinical translation of these technologies, it is critical that patients are engaged in order to align future research agendas and technological advancements with their needs.METHODS: Here, we performed a survey study with hearing impaired, using an eCI as a means of hearing rehabilitation. We distributed a questionnaire to 180 adult patients from the University Medical Center Göttingen's Department of Otolaryngology who were actively using an eCI for 6 months or more during the time of the survey period. Questions revolved around patients needs, and willingness to accept hypothetical risks or drawbacks associated with an optical CI (oCI).RESULTS: Eighty-one participants responded to the questionnaire; 68% were greater than 60 years of age and 26% had bilateral eCIs. Participants expressed a need for improving the performance beyond that experienced with their current eCI. Primarily, they desired improved speech comprehension in background noise, greater ability to appreciate music, and more natural sound impression. They expressed a willingness for engaging with new technologies for improved hearing restoration. Notably, participants were least concerned about hypothetically receiving a gene therapy necessary for the oCI implant; but expressed greater reluctance to hypothetically receiving an implant that had yet to be evaluated in a human clinical trial.CONCLUSION: This work provides a preliminary step in engaging patients in the development of a new technology that has the potential to address the limitations of electrical hearing rehabilitation.Copyright © 2023 Hunniford, Kühler, Wolf, Keppeler, Strenzke and Moser.DOI: 10.3389/fnins.2023.1105562PMCID: PMC9899842",pubmed,36755736,10.3389/fnins.2023.1105562
positron emission tomography imaging reveals auditory and frontal cortical regions involved with speech perception and loudness adaptation,"369. PLoS One. 2015 Jun 5;10(6):e0128743. doi: 10.1371/journal.pone.0128743. eCollection 2015.Positron Emission Tomography Imaging Reveals Auditory and Frontal Cortical Regions Involved with Speech Perception and Loudness Adaptation.Berding G(1), Wilke F(2), Rode T(3), Haense C(4), Joseph G(5), Meyer GJ(4), Mamach M(1), Lenarz M(6), Geworski L(2), Bengel FM(4), Lenarz T(3), Lim HH(7).Author information:(1)Department of Nuclear Medicine, Hannover Medical School, Hannover, Germany; Cluster of Excellence Hearing4all, Hannover Medical School, Hannover, Germany.(2)Department of Medical Physics and Radiation Protection, Hannover Medical School, Hannover, Germany.(3)Cluster of Excellence Hearing4all, Hannover Medical School, Hannover, Germany; Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany.(4)Department of Nuclear Medicine, Hannover Medical School, Hannover, Germany.(5)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany.(6)Department of Otolaryngology, Charité, University Medicine Berlin, Berlin, Germany.(7)Departments of Biomedical Engineering and Otolaryngology-Head & Neck Surgery, University of Minnesota, Minneapolis, Minnesota, United States of America.Considerable progress has been made in the treatment of hearing loss with auditory implants. However, there are still many implanted patients that experience hearing deficiencies, such as limited speech understanding or vanishing perception with continuous stimulation (i.e., abnormal loudness adaptation). The present study aims to identify specific patterns of cerebral cortex activity involved with such deficiencies. We performed O-15-water positron emission tomography (PET) in patients implanted with electrodes within the cochlea, brainstem, or midbrain to investigate the pattern of cortical activation in response to speech or continuous multi-tone stimuli directly inputted into the implant processor that then delivered electrical patterns through those electrodes. Statistical parametric mapping was performed on a single subject basis. Better speech understanding was correlated with a larger extent of bilateral auditory cortex activation. In contrast to speech, the continuous multi-tone stimulus elicited mainly unilateral auditory cortical activity in which greater loudness adaptation corresponded to weaker activation and even deactivation. Interestingly, greater loudness adaptation was correlated with stronger activity within the ventral prefrontal cortex, which could be up-regulated to suppress the irrelevant or aberrant signals into the auditory cortex. The ability to detect these specific cortical patterns and differences across patients and stimuli demonstrates the potential for using PET to diagnose auditory function or dysfunction in implant patients, which in turn could guide the development of appropriate stimulation strategies for improving hearing rehabilitation. Beyond hearing restoration, our study also reveals a potential role of the frontal cortex in suppressing irrelevant or aberrant activity within the auditory cortex, and thus may be relevant for understanding and treating tinnitus.DOI: 10.1371/journal.pone.0128743PMCID: PMC4457827",pubmed,26046763,10.1371/journal.pone.0128743
transientevoked otoacoustic emission signals predicting outcomes of acute sensorineural hearing loss in patients with mnires disease,"183. Acta Otolaryngol. 2020 Mar;140(3):230-235. doi: 10.1080/00016489.2019.1704865. Epub 2020 Jan 31.Transient-evoked otoacoustic emission signals predicting outcomes of acute sensorineural hearing loss in patients with Ménière's disease.Liu YW(1), Kao SL(1), Wu HT(2), Liu TC(1), Fang TY(3)(4), Wang PC(3)(4).Author information:(1)Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan.(2)Department of Mathematics and Department of Statistical Science, Duke University, Durham, NC, USA.(3)Department of Otolaryngology, Cathay General Hospital, Taipei, Taiwan.(4)School of Medicine, Fu Jen Catholic University, Taipei, Taiwan.Background: Fluctuating hearing loss is characteristic of Ménière's disease (MD) during acute episodes. However, no reliable audiometric hallmarks are available for counselling the hearing recovery possibility.Aims/objectives: To find parameters for predicting MD hearing outcomes.Material and methods: We applied machine learning techniques to analyse transient-evoked otoacoustic emission (TEOAE) signals recorded from patients with MD. Thirty unilateral MD patients were recruited prospectively after onset of acute cochleo-vestibular symptoms. Serial TEOAE and pure-tone audiogram (PTA) data were recorded longitudinally. Denoised TEOAE signals were projected onto the three most prominent principal directions through a linear transformation. Binary classification was performed using a support vector machine (SVM). TEOAE signal parameters, including signal energy and group delay, were compared between improved (PTA improvement: ≥15 dB) and nonimproved groups using Welch's t-test.Results: Signal energy did not differ (p = .64) but a significant difference in 1-kHz (p = .045) group delay was recorded between improved and nonimproved groups. The SVM achieved a cross-validated accuracy of >80% in predicting hearing outcomes.Conclusions and significance: This study revealed that baseline TEOAE parameters obtained during acute MD episodes, when processed through machine learning technology, may provide information on outer hair cell function to predict hearing recovery.DOI: 10.1080/00016489.2019.1704865",pubmed,32003266,10.1080/00016489.2019.1704865
changed crossmodal functional connectivity in older adults with hearing loss,"501. Cortex. 2017 Jan;86:109-122. doi: 10.1016/j.cortex.2016.10.014. Epub 2016 Oct 31.Changed crossmodal functional connectivity in older adults with hearing loss.Puschmann S(1), Thiel CM(2).Author information:(1)Biological Psychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all"", European Medical School, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany. Electronic address: sebastian.puschmann@uni-oldenburg.de.(2)Biological Psychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all"", European Medical School, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany; Research Center Neurosensory Science, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany.Previous work compellingly demonstrates a crossmodal plastic reorganization of auditory cortex in deaf individuals, leading to increased neural responses to non-auditory sensory input. Recent data indicate that crossmodal adaptive plasticity is not restricted to severe hearing impairments, but may also occur as a result of high-frequency hearing loss in older adults and affect audiovisual processing in these subjects. We here used functional magnetic resonance imaging (fMRI) to study the effect of hearing loss in older adults on auditory cortex response patterns as well as on functional connectivity between auditory and visual cortex during audiovisual processing. Older participants with a varying degree of high frequency hearing loss performed an auditory stimulus categorization task, in which they had to categorize frequency-modulated (FM) tones presented alone or in the context of matching or non-matching visual motion. A motion only condition served as control for a visual take-over of auditory cortex. While the individual hearing status did not affect auditory cortex responses to auditory, visual, or audiovisual stimuli, we observed a significant hearing loss-related increase in functional connectivity between auditory cortex and the right motion-sensitive visual area MT+ when processing matching audiovisual input. Hearing loss also modulated resting state connectivity between right area MT+ and parts of the left auditory cortex, suggesting the existence of permanent, task-independent changes in coupling between visual and auditory sensory areas with an increasing degree of hearing loss. Our data thus indicate that hearing loss impacts on functional connectivity between sensory cortices in older adults.Copyright © 2016 Elsevier Ltd. All rights reserved.DOI: 10.1016/j.cortex.2016.10.014",pubmed,27930898,10.1016/j.cortex.2016.10.014
improving sign language recognition with machine learning and artificial intelligence,"The use of machine learning (ML) and artificial intelligence (AI) has shown great potential in improving sign language recognition for the hearing impaired community. By leveraging large datasets of sign language videos, these technologies can help to develop more accurate and efficient recognition systems that can greatly enhance communication and accessibility. However, there are still significant challenges that need to be addressed, such as the lack of standardized sign language and the need for real-time recognition. Despite these challenges, by conducting more study and development in this area, it will be possible to create a society where everyone has equal access to knowledge and communication, regardless of language or aptitude. This paper reviews the advances in Artificial Intelligence and Machine Learning in sign language recognition, focusing on Russian, and Bengali sign languages, highlighting the potential benefits and challenges of these technologies. Both static and dynamic signs are used to improve the sign language recognition methods. While the result demonstrates approximately 94% accuracy for the static signs with convolutional neural network models, the dynamic sign recognition not only shows lower accuracy but also highlights the significance of using hybrid methods to overcome issues related to frame rate, alignment, and other aspects of video datasets.",ieee,2831-7262,10.1109/REEPE60449.2024.10479844
what brain connectivity patterns from eeg tell us about hearing loss a graph theoretic approach,"We investigated brain connectivity patterns from the electroencephalogram (EEG) to classify and understand hearing loss using a graph theoretic approach. In particular, we investigated global and nodal graph features of normal hearing (NH) and hearing impaired (HI) participants' brain networks via functional connectivity while they responded to clear and noise-degraded speech stimuli. We found that HI listeners had higher eccentricity, diameter and characteristic path length than the NH listeners for clear speech sounds, and larger radii for noisy speech signal. Moreover, we classified groups based on these graph theoretic features using support vector machine (SVM). Maximum classifier accuracy was 85.71 % for clear speech (Fl-score=86.00%) and 71.42% (Fl=67%) for degraded speech signals, respectively. Group classification based on nodal connectivity measures was more accurate in the left brain hemisphere, consistent with the leftward laterality of auditory-linguistic processing. Our data suggest HI listeners have more extended communication pathways and less efficient information exchange among brain regions than NH, establishing new biomarkers of hearing loss based on full-brain connectivity. © 2018 IEEE.",scopus,2-s2.0-85062892402,10.1109/ICECE.2018.8636698
identification of disease biomarkers from brain fmri data using machine learning techniques applications in sensorineural hearing loss and attention deficit hyperactivity disorder,,base,92c82eb555c359371e098e8c085f27dbf01504bf953744d9ed86f35a25c2a232,
early stages of sensorimotor map acquisition neurochemical signature in primary motor cortex and its relation to functional connectivity,"The earliest stages of sensorimotor learning involve learning the correspondence between movements and sensory results-a sensorimotor map. The present exploratory study investigated the neurochemical underpinnings of map acquisition by monitoring 25 participants as they acquired a new association between movements and sounds. Functional magnetic resonance spectroscopy was used to measure neurochemical concentrations in the left primary motor cortex during learning. Resting-state functional magnetic resonance imaging data were also collected before and after training to assess learning-related changes in functional connectivity. There were monotonic increases in c-aminobutyric acid (GABA) and decreases in glucose during training, which extended into the subsequent rest period and, importantly, in the case of GABA correlated with the amount of learning: participants who showed greater behavioral learning showed greater GABA increase. The GABA change was furthermore correlated with changes in functional connectivity between the primary motor cortex and a cluster of voxels in the right intraparietal sulcus: greater increases in GABA were associated with greater strengthening of connectivity. Transiently, there were increases in lactate and reductions in aspartate, which returned to baseline at the end of training, but only lactate showed a statistical trend to correlate with the amount of learning. In summary, during the earliest stages of sensorimotor learning, GABA levels are linked on a subject-level basis to both behavioral learning and a strengthening of functional connections that persists beyond the training period. The findings are consistent with the idea that GABA-mediated inhibition is linked to maintenance of newly learned information. Copyright © 2020 the American Physiological Society",scopus,2-s2.0-85097749357,10.1152/jn.00285.2020
clustering upper airway physicals otitis media with effusion and auditory functions in children,"105. Auris Nasus Larynx. 2022 Apr;49(2):195-201. doi: 10.1016/j.anl.2021.07.001. Epub 2021 Jul 23.Clustering upper airway physicals, otitis media with effusion and auditory functions in children.Aslıer M(1), Aslıer NGY(2), Ercan İ(3), Keskin S(2).Author information:(1)Department of Otorhinolaryngology, Sancaktepe Education and Research Hospital, Istanbul, Turkey. Electronic address: mustafaaslier@uludag.edu.tr.(2)Department of Otorhinolaryngology, Sancaktepe Education and Research Hospital, Istanbul, Turkey.(3)Department of Biostatistics, Uludağ University School of Medicine, Bursa, Turkey.OBJECTIVE: Adenoid hypertrophy (AH) has been identified as a cause of otitis media with effusion (OME), which is the most common cause of childhood hearing loss. Indeed, there may be other upper airway-related predisposing factors such as, location of the adenoid, accompanying tonsillar hypertrophy (TH) and nasal septal deviation (NSD) for the development of OME. In this study, we aimed to evaluate the associations between the upper airway physicals and OME with auditory functions.METHODS: Eighty-six ears of 43 children, aged 3-11 years were included in this prospective clinical study. Findings of otolaryngologic examinations were noted. Data of pure tone audiometry (PTA), traditional tympanometry (TT) and wideband tympanometry (WBT) parameters were collected. Cluster analysis was performed to the following variables: age, sex; the adenoid choana percentage (ACP), the presences of adenoid around torus tubarius (AATT), TH, NSD and OME; peak pressure (PP) values on TT, resonance frequencies (RF) on WBT, ambient pressure absorbance ratios (APAR) and PTA hearing thresholds.RESULTS: Two groups of ears revealed by clustering; cluster-1 (n = 46) and cluster-2 (n = 40), at the similarity level of 0.662. The presences of AH, AATT, OME and the medians of ACP, PP, RF, WBT APARs at all frequencies except 5656 Hz and 8000 Hz, all PTA thresholds were significantly different between two clusters (p < 0.05). The lower WBT APARs and higher PTA thresholds were associated with higher levels of ACP and higher frequencies of the presence of AATT and OME in cluster-1.CONCLUSION: There are associations between AH, AATT and OME together with decline in hearing and SEA. Whereas, TH and NSD are not related to the formation of clusters and they are insignificant factors.Copyright © 2021. Published by Elsevier B.V.DOI: 10.1016/j.anl.2021.07.001",pubmed,34304942,10.1016/j.anl.2021.07.001
effects of chronic cochlear damage on threshold and frequency tuning of neurons in ai auditory cortex,"551. Acta Otolaryngol Suppl. 1995;519:30-5. doi: 10.3109/00016489509121867.Effects of chronic cochlear damage on threshold and frequency tuning of neurons in AI auditory cortex.Harrison RV(1), Stanton SG, Mount RJ.Author information:(1)Research Institute, Hospital for Sick Children, Toronto, Canada.We describe the effects of long-term cochlear lesions on the frequency response properties of AI cortical neurons in the cat. Young animals were treated with amikacin to produce bilateral, basal to mid-turn cochlear lesions. After 12-24 months the response properties of single neurons or small unit clusters in primary auditory cortex were recorded in anesthetized animals. Responses to stimulus frequency and intensity were mapped in detail and frequency threshold curves (FTCs) and Q10dB values were derived. Subsequent to recording experiments, scanning electron microscopy of the sensory epithelium was used to characterize the degree and extent of the cochlear damage. In normal control animals, Q10dB values were, on average, lower than those derived by others from cochlear nerve fibre recordings in the same species. In amikacin-treated animals, deterioration was evident in the threshold and tuning properties of cortical neurons, particularly in those cells whose input originated in damaged cochlear regions. Often, neurons associated with 'normal' cochlear areas (as assessed by scanning microscopy) also had poor frequency tuning compared with controls. As an animal model of sensorineural hearing loss, we consider the cat with long-term cochlear lesions to be more appropriate than animals with acute or short-term pathology. We also suggest that in making physiological-psychophysical correlations, neural responses from the central auditory system (e.g. cortex) should perhaps be given more consideration than data derived at the cochlear level.DOI: 10.3109/00016489509121867",pubmed,7610889,10.3109/00016489509121867
optical microphonebased speech reconstruction system with deep learning for individuals with hearing loss,"74. IEEE Trans Biomed Eng. 2023 Dec;70(12):3330-3341. doi: 10.1109/TBME.2023.3285437. Epub 2023 Nov 21.Optical Microphone-Based Speech Reconstruction System With Deep Learning for Individuals With Hearing Loss.Lin YM, Han JY, Lin CH, Lai YH.OBJECTIVE: Although many speech enhancement (SE) algorithms have been proposed to promote speech perception in hearing-impaired patients, the conventional SE approaches that perform well under quiet and/or stationary noises fail under nonstationary noises and/or when the speaker is at a considerable distance. Therefore, the objective of this study is to overcome the limitations of the conventional speech enhancement approaches.METHOD: This study proposes a speaker-closed deep learning-based SE method together with an optical microphone to acquire and enhance the speech of a target speaker.RESULTS: The objective evaluation scores achieved by the proposed method outperformed the baseline methods by a margin of 0.21-0.27 and 0.34-0.64 in speech quality (HASQI) and speech comprehension/intelligibility (HASPI), respectively, for seven typical hearing loss types.CONCLUSION: The results suggest that the proposed method can enhance speech perception by cutting off noise from speech signals and mitigating interference caused by distance.SIGNIFICANCE: The results of this study show a potential way that can help improve the listening experience in enhancing speech quality and speech comprehension/intelligibility for hearing-impaired people.DOI: 10.1109/TBME.2023.3285437",pubmed,37327105,10.1109/TBME.2023.3285437
electrical compound action potentials recorded with automated neural response telemetry threshold changes as a function of time and electrode position,"385. Ear Hear. 2011 Feb;32(1):104-13. doi: 10.1097/AUD.0b013e3181ec5d95.Electrical compound action potentials recorded with automated neural response telemetry: threshold changes as a function of time and electrode position.Spivak L(1), Auerbach C, Vambutas A, Geshkovich S, Wexler L, Popecki B.Author information:(1)Apelian Cochlear Implant Center, Long Island Jewish Medical Center, New Hyde Park, New York, USA. spivak@lij.eduOBJECTIVE: Since the introduction of neural response telemetry (NRT) for the Nucleus 24 cochlear implant (CI24), researchers and clinicians have investigated the feasibility of using the electrically evoked compound action potential (ECAP) threshold to objectively predict psychophysical measurements that are used in the programming of the speech processor. The ability to substitute objective for behavioral measurements, particularly measurements made at the time of surgery, would greatly facilitate programming the MAP for young children and other individuals who are not able to provide reliable behavioral data required for MAP programming. There have been a number of studies that have examined characteristics of the ECAP measured at the time of surgery and postoperatively; however, all the available published data are based on the CI24. With the introduction of the Nucleus Freedom device, an automated NRT (AutoNRT) program became available, which was capable of measuring ECAP thresholds at lower levels than was previously possible with NRT software associated with the CI24 device. It was hypothesized that the enhancements to the NRT program may improve the predictability of postoperative measurements from intraoperatively recorded ECAP thresholds. The purpose of this study was to track ECAP thresholds obtained using AutoNRT as a function of time and electrode position.DESIGN: ECAP thresholds were recorded from 71 children and adults implanted with the Nucleus Freedom device using the AutoNRT test protocol. ECAP thresholds were obtained at the time of surgery, at initial stimulation, and 3 mos poststimulation. Five electrodes located at basal, middle, and apical positions in the cochlea were tested at each time interval and thresholds were compared.RESULTS: Significant differences were found in ECAP thresholds measured with AutoNRT as a function of both time and electrode position. Basal electrodes had higher ECAP thresholds than apical electrodes and that relationship was consistent for each time period. Thresholds for all electrodes decreased between surgery and initial stimulation and remained relatively stable at 3 mos poststimulation. ECAP thresholds were consistently lower for children compared with adults at each time point. Mid-array electrodes (11 and 16) showed the least amount of change over time.CONCLUSIONS: AutoNRT thresholds demonstrated significant change over time, limiting the ability to use intraoperatively recorded ECAP thresholds to predict postoperative measurements. In this study, electrodes 11 and 16 showed the least amount of change in ECAP threshold over time and therefore would be the best choices for estimating postoperative ECAP thresholds. Although not an ideal solution, mid-array ECAP thresholds obtained intraoperatively may prove to be helpful in creating a first MAP when no other behavioral or electrophysiological data are available.DOI: 10.1097/AUD.0b013e3181ec5d95",pubmed,20686409,10.1097/AUD.0b013e3181ec5d95
3d printed biomimetic cochleae and machine learning comodelling provides clinical informatics for cochlear implant patients,"Cochlear implants restore hearing in patients with severe to profound deafness by delivering electrical stimuli inside the cochlea. Understanding stimulus current spread, and how it correlates to patient-dependent factors, is hampered by the poor accessibility of the inner ear and by the lack of clinically-relevant in vitro, in vivo or in silico models. Here, we present 3D printing-neural network co-modelling for interpreting electric field imaging profiles of cochlear implant patients. With tuneable electro-anatomy, the 3D printed cochleae can replicate clinical scenarios of electric field imaging profiles at the off-stimuli positions. The co-modelling framework demonstrated autonomous and robust predictions of patient profiles or cochlear geometry, unfolded the electro-anatomical factors causing current spread, assisted on-demand printing for implant testing, and inferred patients’ in vivo cochlear tissue resistivity (estimated mean = 6.6 kΩcm). We anticipate our framework will facilitate physical modelling and digital twin innovations for neuromodulation implants. © 2021, The Author(s).",scopus,2-s2.0-85118472042,10.1038/s41467-021-26491-6
systematic review on technological devices for the communication of hearingimpaired children from 2002 to 2023,"Currently, the World Health Organization reports that more than 430 million people worldwide have hearing impairment, with a projection of 900 million by 2050. In particular, approximately 35 million children worldwide are affected by this condition. Early cortical auditory development plays a crucial role in a child's life, and any deficiency in auditory function during these formative years can hinder proper synaptic development. This systematic review aims to explore the landscape of technological devices designed to improve communication for children with hearing disabilities. This study analyzed existing literature through rigorous methodology, employing bibliometric network analysis to uncover key insights and utilizing the Scopus database in conjunction with Vosviewer and Visual Basic Applications. As a result, 8,827 scientific documents were obtained, wherein authors, countries, affiliations, document types, publication years, and keywords were analyzed. Articles constituted the majority of these documents, comprising over 64.1%, and the peak in document production occurred in 2022 with 620 documents. In conclusion, the development of technological devices has yielded significant results over the last 20 years, incorporating recent trends such as machine learning and artificial intelligence for creating new devices. However, there was limited consideration for children or the caregivers and family members of individuals with hearing disabilities. © 2024 Seventh Sense Research Group®",scopus,2-s2.0-85187293615,10.14445/22315381/IJETT-V72I2P124
fmri evidence for activation of multiple cortical regions in the primary auditory cortex of deaf subjects users of multichannel cochlear implants,"616. Cereb Cortex. 2005 Jan;15(1):40-8. doi: 10.1093/cercor/bhh106. Epub 2004 Jul 6.FMRI evidence for activation of multiple cortical regions in the primary auditory cortex of deaf subjects users of multichannel cochlear implants.Seghier ML(1), Boëx C, Lazeyras F, Sigrist A, Pelizzone M.Author information:(1)Department of Radiology, University Hospital of Geneva, Micheli-du-Crest 24, 1211 Geneva, Switzerland. mohamed.seghier@medecine.unige.chTo investigate the activation of the auditory cortex by fMRI, three deaf subjects users of the Ineraid cochlear implant participated in our study. Possible interference between fMRI acquisition and the implanted electrodes was controlled and safe experimental conditions were obtained. For each subject, electrical stimuli were applied on different intracochlear electrodes, in monopolar mode. Stimulation of each electrode was actually producing auditory sensations of different pitches, as demonstrated by psychophysical pitch-ranking measurements in the same subjects. Because deaf subjects did not hear scanner noise, the data were collected in 'silent background' conditions, i.e. as a result of pure auditory sensations. Functional maps showed activation of the primary auditory cortex, predominantly in the left hemisphere. Stimulation of each different intracochlear electrode revealed different clusters of activation. After cluster grouping, at least three regions have been identified in the auditory cortex of each subject, and comparisons with previous architectonic and functional studies are proposed. However, a tonotopic organization could not be clearly identified within each region. These arguments, obtained without interference with unwanted scanner noise, plead in favor of a functional subdivision of the primary auditory cortex into multiple cortical regions in cochlear implant users.DOI: 10.1093/cercor/bhh106",pubmed,15238446,10.1093/cercor/bhh106
hearing recovery prediction for patients with chronic otitis media who underwent canalwalldown mastoidectomy,"Background: Chronic otitis media affects approximately 2% of the global population, causing significant hearing loss and diminishing the quality of life. However, there is a lack of studies focusing on outcome prediction for otitis media patients undergoing canal-wall-down mastoidectomy. Methods: This study proposes a recovery prediction model for chronic otitis media patients undergoing canal-wall-down mastoidectomy, utilizing data from 298 patients treated at Korea University Ansan Hospital between March 2007 and August 2020. Various machine learning techniques, including logistic regression, decision tree, random forest, support vector machine (SVM), extreme gradient boosting (XGBoost), and light gradient boosting machine (light GBM), were employed. Results: The light GBM model achieved a predictive value (PPV) of 0.6945, the decision tree algorithm showed a sensitivity of 0.7574 and an F1 score of 0.6751, and the light GBM algorithm demonstrated the highest AUC-ROC values of 0.7749 for each model. XGBoost had the most efficient PR-AUC curve, with a value of 0.7196. Conclusions: This study presents the first predictive model for chronic otitis media patients undergoing canal-wall-down mastoidectomy. The findings underscore the potential of machine learning techniques in predicting hearing recovery outcomes in this population, offering valuable insights for personalized treatment strategies and improving patient care. © 2024 by the authors.",scopus,2-s2.0-85188997545,10.3390/jcm13061557
altered restingstate eeg microstate in idiopathic sudden sensorineural hearing loss patients with tinnitus,"In order to clarify the central reorganization in acute period of hearing loss, this study explored the aberrant dynamics of electroencephalogram (EEG) microstates and the correlations with the features of idiopathic sudden sensorineural hearing loss (ISSNHL) and tinnitus. We used high-density EEG with 128 channels to investigate alterations in microstate parameters between 25 ISSNHL patients with tinnitus and 27 healthy subjects. This study also explored the associations between microstate characteristics and tinnitus features. Microstates were clustered into four categories. There was a reduced presence of microstate A in amplitude, coverage, lifespan, frequency and an increased presence of microstate B in frequency in ISSNHL patients with tinnitus. According to the syntax analysis, a reduced transition from microstate C to microstate A and an increased transition from microstate C to microstate B were found in ISSNHL subjects. In addition, the significant negative correlations were found between Tinnitus Handicap Inventory (THI) scores and frequency of microstate A as well as between THI scores and the probability of transition from microstate D to microstate A. While THI was positively correlated with the transition probability from microstate D to microstate B. To sum up, the significant differences in the characteristics of resting-state EEG microstates were found between ISSNHL subjects with tinnitus and healthy controls. This study suggests that the alterations of central neural networks occur in acute stage of hearing loss and tinnitus. And EEG microstate may be considered as a useful tool to study the whole brain network in ISSNHL patients. © 2019 Cai, Chen, Chen, Li, Wang, Zhao, Dang, Liang, He, Liang and Zheng.",scopus,2-s2.0-85068576258,10.3389/fnins.2019.00443
investigating tinnitus subgroups based on hearingrelated difficulties,"121. Int J Clin Pract. 2021 Oct;75(10):e14684. doi: 10.1111/ijcp.14684. Epub 2021 Aug 6.Investigating tinnitus subgroups based on hearing-related difficulties.Beukes EW(1)(2), Baguley DM(3)(4)(5), Manchaiah V(1)(6), Andersson G(7)(8), Allen PM(2), Kaldo V(7)(9), Jacquemin L(10)(11), Lourenco MPCG(12)(13), Onozuka J(14), Stockdale D(15), Maidment DW(16).Author information:(1)Department of Speech and Hearing Sciences, Lamar University, Beaumont, TX, USA.(2)Vision and Hearing Sciences Research Group, School of Psychology and Sports Sciences, Anglia Ruskin University, Cambridge, UK.(3)National Institute for Health Research, Nottingham Biomedical Research Centre, Ropewalk House, Nottingham, UK.(4)Hearing Sciences, Division of Clinical Neuroscience, School of Medicine, University of Nottingham, Nottingham, UK.(5)Nottingham Audiology Services, Nottingham University Hospitals, Nottingham, UK.(6)Department of Speech and Hearing, School of Allied Health Sciences, Manipal University, Karnataka, India.(7)Department of Behavioral Sciences and Learning, Linköping University, Linköping, Sweden.(8)Centre for Psychiatry Research, Department of Clinical Neuroscience, Karolinska Institutet, & Stockholm Health Care Services, Region Stockholm, Sweden.(9)Department of Psychology, Faculty of Health and Life Sciences, Linnaeus University, Växjö, Sweden.(10)Department of Otorhinolaryngology and Head and Neck Surgery, Antwerp University Hospital, Edegem, Belgium.(11)Department of Translational Neurosciences, Faculty of Medicine and Health Sciences, University of Antwerp, Wilrijk, Belgium.(12)Experimental Health Psychology, Maastricht University, Maastricht, The Netherlands.(13)Research Group Health Psychology, KU Leuven University, Leuven, Belgium.(14)American Tinnitus Association, Washington, DC, USA.(15)British Tinnitus Association, Sheffield, UK.(16)School of Sport, Exercise and Health Sciences, Loughborough University, Loughborough, UK.PURPOSE: Meaningfully grouping individuals with tinnitus who share a common characteristics (ie, subgrouping, phenotyping) may help tailor interventions to certain tinnitus subgroups and hence reduce outcome variability. The purpose of this study was to test if the presence of tinnitus subgroups are discernible based on hearing-related comorbidities, and to identify predictors of tinnitus severity for each subgroup identified.METHODS: An exploratory cross-sectional study was used. The study was nested within an online survey distributed worldwide to investigate tinnitus experiences during the COVID-19 pandemic. The main outcome measure was the tinnitus Handicap Inventory- Screening Version.RESULTS: From the 3400 respondents, 2980 were eligible adults with tinnitus with an average age of 58 years (SD = 14.7) and 49% (n = 1457) being female. A three-cluster solution identified distinct subgroups, namely, those with tinnitus-only (n = 1306; 44%), those presenting with tinnitus, hyperacusis, hearing loss and/or misophonia (n = 795; 27%), and those with tinnitus and hearing loss (n = 879; 29%). Those with tinnitus and hyperacusis reported the highest tinnitus severity (M = 20.3; SD = 10.5) and those with tinnitus and no hearing loss had the lowest tinnitus severity (M = 15.7; SD = 10.4). Younger age and the presence of mental health problems predicted greater tinnitus severity for all groups (β ≤ -0.1, P ≤ .016).CONCLUSION: Further exploration of these potential subtypes are needed in both further research and clinical practice by initially triaging tinnitus patients prior to their clinical appointments based on the presence of hearing-related comorbidities. Unique management pathways and interventions could be tailored for each tinnitus subgroup.© 2021 John Wiley & Sons Ltd.DOI: 10.1111/ijcp.14684",pubmed,34331723,10.1111/ijcp.14684
dorsal and ventral streams a framework for understanding aspects of the functional anatomy of language,"Despite intensive work on language-brain relations, and a fairly impressive accumulation of knowledge over the last several decades, there has been little progress in developing large-scale models of the functional anatomy of language that integrate neuropsychological, neuroimaging, and psycholinguistic data. Drawing on relatively recent developments in the cortical organization of vision, and on data from a variety of sources, we propose a new framework for understanding aspects of the functional anatomy of language which moves towards remedying this situation. The framework posits that early cortical stages of speech perception involve auditory fields in the superior temporal gyrus bilaterally (although asymmetrically). This cortical processing system then diverges into two broad processing streams, a ventral stream, which is involved in mapping sound onto meaning, and a dorsal stream, which is involved in mapping sound onto articulatory-based representations. The ventral stream projects ventro-laterally toward inferior posterior temporal cortex (posterior middle temporal gyrus) which serves as an interface between sound-based representations of speech in the superior temporal gyrus (again bilaterally) and widely distributed conceptual representations. The dorsal stream projects dorso-posteriorly involving a region in the posterior Sylvian fissure at the parietal-temporal boundary (area Spt), and ultimately projecting to frontal regions. This network provides a mechanism for the development and maintenance of ""parity"" between auditory and motor representations of speech. Although the proposed dorsal stream represents a very tight connection between processes involved in speech perception and speech production, it does not appear to be a critical component of the speech perception process under normal (ecologically natural) listening conditions, that is, when speech input is mapped onto a conceptual representation. We also propose some degree of bi-directionality in both the dorsal and ventral pathways. We discuss some recent empirical tests of this framework that utilize a range of methods. We also show how damage to different components of this framework can account for the major symptom clusters of the fluent aphasias, and discuss some recent evidence concerning how sentence-level processing might be integrated into the framework. © 2004 Elsevier B.V. All rights reserved.",scopus,2-s2.0-1442351125,10.1016/j.cognition.2003.10.011
spectral contrast enhancement improves speech intelligibility in noise for cochlear implants,"120. J Acoust Soc Am. 2016 Feb;139(2):728-39. doi: 10.1121/1.4939896.Spectral contrast enhancement improves speech intelligibility in noise for cochlear implants.Nogueira W(1), Rode T(2), Büchner A(1).Author information:(1)Department of Otolaryngology, Medical University Hannover, Cluster of Excellence Hearing4all, Hannover, Germany.(2)HörSys GmbH, Hannover, Germany.Spectral smearing causes, at least partially, that cochlear implant (CI) users require a higher signal-to-noise ratio to obtain the same speech intelligibility as normal hearing listeners. A spectral contrast enhancement (SCE) algorithm has been designed and evaluated as an additional feature for a standard CI strategy. The algorithm keeps the most prominent peaks within a speech signal constant while attenuating valleys in the spectrum. The goal is to partly compensate for the spectral smearing produced by the limited number of stimulation electrodes and the overlap of electrical fields produced in CIs. Twelve CI users were tested for their speech reception threshold (SRT) using the standard CI coding strategy with and without SCE. No significant differences in SRT were observed between conditions. However, an analysis of the electrical stimulation patterns shows a reduction in stimulation current when using SCE. In a second evaluation, 12 CI users were tested in a similar configuration of the SCE strategy with the stimulation being balanced between the SCE and the non-SCE variants such that the loudness perception delivered by the strategies was the same. Results show a significant improvement in SRT of 0.57 dB (p < 0.0005) for the SCE algorithm.DOI: 10.1121/1.4939896",pubmed,26936556,10.1121/1.4939896
prevalence and causes of hearing handicap in ardabil province western iran,"Background and Aim: Hearing impairment is the most prevalent sensorineural defect in human. Epidemiological studies and the following preventive programs are the first steps to save many individuals from being handicapped and non-productive. Hereby, we aimed to study the prevalence of hearing impairment in Ardabil province and to assess the prevalent causes of hearing impairment in Ardabil. Methods: In this study, 10718 cases were selected by random cluster sampling from rural and urban population in Ardabil province. Data was gathered using screening questionnaire, normal- and impaired-hearing individual questionnaire, and clinical audiometry. Statistical indices were calculated and data was analyzed using chi-square test. Results: 7.1 per thousand individuals suffer hearing impairment and 4.3 per thousand are deaf. Hearing impairment was significantly more among villagers and aged individuals (p=0.46). However, there was no significant deference between two genders (p>0.05). Our results also showed significant deference in consanguineous marriage (p=0.031), accidents in pregnancy (p=0.007), older age of mother (p=0.007), parents hearing loss, severe illness during childhood (p=0.001), low family income (p=0.004), rural housing and educational level of parents (p=0.001) with hearing impairment. Conclusion: Health status, economical, cultural and educational level of society are the most important factors associated with hearing impairment in Ardabil province. Consequently, extensive preventive programs are required to limit such factors.",cinahl,17351936,
lowfrequency fluctuation amplitude changes in restingstate brain functional magnetic resonance imaging and its correlation with clinical hearing levels in patients with unilateral hearing impairment ,"Objective To investigate low‑frequency fluctuation amplitude changes in resting‑state brain fMRI and its correlation with clinical hearing levels in patients with clinical hearing level in patients with unilateral hearing impairment. Methods Forty‑five patients with unilateral hearing impairment[12 males and 33 females, aged 36‑67 (46.0±9.7) years], and 31 controls with normal hearing[9 males and 22 females, aged 36‑67 (46.0±10.1) years], were retrospectively included. All subjects underwent blood oxygen level‑dependent (BOLD) resting‑state functional magnetic resonance imaging and high‑resolution T1‑weighted imaging. The patients were divided into the left‑sided hearing impaired group(24 cases), and the right‑sided hearing impaired group(21 cases). After data being preprocessed, differences in low frequency amplitude (ALFF) metrics between the evaluated patients and controls were calculated and analyzed, and the statistics were corrected for Gaussian random field (GFR). Results Overall comparative analysis of patients with hearing impairment showed that one‑way ANOVA among the three groups showed abnormal ALFF values only in the right anterior cuneiform lobe (GRF adjusted P=0.002). The ALFF value of the hearing impaired group was higher than that of the control group in one cluster (peak coordinates: X=9, Y=-72, Z=48, T=5.82), involving the left occipital gyrus, right anterior cuneiform lobe, left superior cuneiform lobe, left superior parietal gyrus, and left angular gyrus (GRF adjusted P=0.031). The ALFF value of the hearing impaired group was lower than that of the control group in three clusters (peak coordinates: X=57, Y=-48, Z=-24; T=-4.99; X=45, Y=-66, Z=0, T=-4.06; X=42, Y=-12, Z=36, T=-4.03), involving the right inferior temporal gyrus, the right middle temporal gyrus, and the right precentral gyrus (GRF adjusted P=0.009). Compared with the control group, the ALFF value of the left hearing impairment group was significantly higher than that of the control group in one cluster (peak coordinates: X=-12, Y=-75, Z=45, T=5.78), involving the left anterior cuneiform lobe, right anterior cuneiform lobe, left middle occipital gyrus, left superior parietal gyrus, left superior occipital gyrus, left cuneiform lobe, and right cuneiform lobe (P=0.023 after GRF correction). Compared with the control group, the right hearing impairment group had a significantly higher ALFF value in one cluster (peak coordinates: X=9, Y=-46, Z=22, T=6.06), involving the left middle occipital gyrus, right anterior cuneiform lobe, left cuneiform lobe, right cuneiform lobe, left superior occipital gyrus, and right superior occipital gyrus (GRF adjusted P=0.022); The brain area with reduced ALFF values is located in the right inferior temporal gyrus (GRF adjusted P=0.029). Spearman′s two‑tailed correlation analysis between ALFF values and pure tone average in the abnormal brain regions showed that ALFF values in the abnormal brain regions correlated to some extent with the pure tone average (PTA) only in the left‑sided hearing impaired group(PTA=2 000 Hz, r=0.318,P= 0.033; PTA=4 000 Hz, r=0.386, P=0.009). Conclusion The abnormal neural activity within the brain are different in patients with left‑sided and right‑sided hearing impairment, and the severity of hearing impairment is related to the difference in functional integration of brain regions. © 2023 Chinese Medical Association. All rights reserved.",scopus,2-s2.0-85164246507,10.3760/cma.j.cn112137-20221107-02337
sensorineural hearing loss enhances auditory sensitivity and temporal integration for amplitude modulation,"693. J Acoust Soc Am. 2017 Feb;141(2):971. doi: 10.1121/1.4976080.Sensorineural hearing loss enhances auditory sensitivity and temporal integration for amplitude modulation.Wallaert N(1), Moore BC(2), Ewert SD(3), Lorenzi C(1).Author information:(1)UMR CNRS LSP 8248, Institut d'Etude de la Cognition, Ecole normale supérieure, Paris Sciences et Lettres Research University, 29 rue d'Ulm, 75005 Paris, France.(2)Department of Experimental Psychology, University of Cambridge, Downing Street, Cambridge CB2 3EB, United Kingdom.(3)Medizinische Physik and Cluster of Excellence Hearing4All, Universität Oldenburg, 26111 Oldenburg, Germany.Amplitude-modulation detection thresholds (AMDTs) were measured at 40 dB sensation level for listeners with mild-to-moderate sensorineural hearing loss (age: 50-64 yr) for a carrier frequency of 500 Hz and rates of 2 and 20 Hz. The number of modulation cycles, N, varied between two and nine. The data were compared with AMDTs measured for young and older normal-hearing listeners [Wallaert, Moore, and Lorenzi (2016). J. Acoust. Soc. Am. 139, 3088-3096]. As for normal-hearing listeners, AMDTs were lower for the 2-Hz than for the 20-Hz rate, and AMDTs decreased with increasing N. AMDTs were lower for hearing-impaired listeners than for normal-hearing listeners, and the effect of increasing N was greater for hearing-impaired listeners. A computational model based on the modulation-filterbank concept and a template-matching decision strategy was developed to account for the data. The psychophysical and simulation data suggest that the loss of amplitude compression in the impaired cochlea is mainly responsible for the enhanced sensitivity and temporal integration of temporal envelope cues found for hearing-impaired listeners. The data also suggest that, for AM detection, cochlear damage is associated with increased internal noise, but preserved short-term memory and decision mechanisms.DOI: 10.1121/1.4976080",pubmed,28253641,10.1121/1.4976080
assessing the relationship between neural health measures and speech performance with simultaneous electric stimulation in cochlear implant listeners,"205. PLoS One. 2021 Dec 13;16(12):e0261295. doi: 10.1371/journal.pone.0261295. eCollection 2021.Assessing the relationship between neural health measures and speech performance with simultaneous electric stimulation in cochlear implant listeners.Langner F(1), Arenberg JG(2), Büchner A(1), Nogueira W(1).Author information:(1)Department of Otorhinolaryngology, Hannover Medical School and Cluster of Excellence Hearing4all, Hanover, Germany.(2)Department of Otolaryngology, Massachusetts Eye and Ear, Harvard Medical School, Boston, MA, United States of America.OBJECTIVES: The relationship between electrode-nerve interface (ENI) estimates and inter-subject differences in speech performance with sequential and simultaneous channel stimulation in adult cochlear implant listeners were explored. We investigated the hypothesis that individuals with good ENIs would perform better with simultaneous compared to sequential channel stimulation speech processing strategies than those estimated to have poor ENIs.METHODS: Fourteen postlingually deaf implanted cochlear implant users participated in the study. Speech understanding was assessed with a sentence test at signal-to-noise ratios that resulted in 50% performance for each user with the baseline strategy F120 Sequential. Two simultaneous stimulation strategies with either two (Paired) or three sets of virtual channels (Triplet) were tested at the same signal-to-noise ratio. ENI measures were estimated through: (I) voltage spread with electrical field imaging, (II) behavioral detection thresholds with focused stimulation, and (III) slope (IPG slope effect) and 50%-point differences (dB offset effect) of amplitude growth functions from electrically evoked compound action potentials with two interphase gaps.RESULTS: A significant effect of strategy on speech understanding performance was found, with Triplets showing a trend towards worse speech understanding performance than sequential stimulation. Focused thresholds correlated positively with the difference required to reach most comfortable level (MCL) between Sequential and Triplet strategies, an indirect measure of channel interaction. A significant offset effect (difference in dB between 50%-point for higher eCAP growth function slopes with two IPGs) was observed. No significant correlation was observed between the slopes for the two IPGs tested. None of the measures used in this study correlated with the differences in speech understanding scores between strategies.CONCLUSIONS: The ENI measure based on behavioral focused thresholds could explain some of the difference in MCLs, but none of the ENI measures could explain the decrease in speech understanding with increasing pairs of simultaneously stimulated electrodes in processing strategies.DOI: 10.1371/journal.pone.0261295PMCID: PMC8668108",pubmed,34898654,10.1371/journal.pone.0261295
the development and evaluation of the finnish digit triplet test,"176. Acta Otolaryngol. 2016 Oct;136(10):1035-40. doi: 10.1080/00016489.2016.1175662. Epub 2016 Apr 28.The development and evaluation of the Finnish digit triplet test.Willberg T(1)(2), Buschermöhle M(3)(4), Sivonen V(5), Aarnisalo AA(5), Löppönen H(1)(2), Kollmeier B(3)(4)(6), Dietz A(1)(2).Author information:(1)a Department of Otorhinolaryngology , Kuopio University Hospital , Kuopio , Finland ;(2)b Institute of Clinical Medicine, University of Eastern Finland , Kuopio , Finland ;(3)c HörTech gGmbH , Oldenburg , Germany ;(4)d Cluster of Excellence, 'Hearing4all' , Oldenburg , Germany ;(5)e Department of Otorhinolaryngology , Helsinki University Central Hospital , Helsinki , Finland ;(6)f Department for Medical Physics and Acoustics , Carl Von Ossietzky Universität Oldenburg , Oldenburg , Germany.OBJECTIVES: The aim of the study was to develop a reliable and easily accessible screening test for primary detection of hearing impairment.METHODS: Digits 0-9 were used to form quasirandom digit triplets. First, digit specific intelligibility functions and speech recognition thresholds (SRTs) were determined. To homogenize the test material digits with steep intelligibility function slopes were chosen and level correction up to ±2 dB were applied to the digits as needed. Evaluation measurements were performed to check for systematic differences in intelligibility between the test lists and to obtain normative reference function for normal-hearing listeners.RESULTS: The mean SRT and the final slope of the test lists were -10.8 ± 0.1 dB signal-to-noise ratio (SNR) and 21.7 ± 1.8%/dB, respectively (measurements at constant level; inter-list variability). The mean SRT and slope of the test subjects were -10.8 ± 0.5 dB SNR and 23.4 ± 5.2%/dB (measurements at constant level; inter-subject variability). The mean SRT for normal-hearing young adults for a single adaptive measurement is -9.8 ± 0.9 dB SNR.CONCLUSION: The Finnish digit triplet test is the first self-screening hearing test in the Finnish language. It was developed according to current standards, and it provides reliable and internationally comparable speech intelligibility measurements.DOI: 10.1080/00016489.2016.1175662",pubmed,27121373,10.1080/00016489.2016.1175662
targeting regional pediatric congenital hearing loss using a spatial scan statistic,"387. Ear Hear. 2015 Mar-Apr;36(2):212-6. doi: 10.1097/AUD.0000000000000101.Targeting regional pediatric congenital hearing loss using a spatial scan statistic.Bush ML(1), Christian WJ, Bianchi K, Lester C, Schoenberg N.Author information:(1)1Department of Otolaryngology-Head and Neck Surgery, University of Kentucky, Lexington, Kentucky, USA; 2Department of Epidemiology, College of Public Health, Lexington, Kentucky, USA; 3College of Medicine, University of Kentucky, Lexington, Kentucky, USA; 4Cabinet for Health and Family Services, Commission for Children with Special Health Care Needs, Louisville, Kentucky, USA; and 5Department of Behavioral Science, University of Kentucky, Lexington, Kentucky, USA.OBJECTIVES: Congenital hearing loss is a common problem, and timely identification and intervention are paramount for language development. Patients from rural regions may have many barriers to timely diagnosis and intervention. The purpose of this study was to examine the spatial and hospital-based distribution of failed infant hearing screening testing and pediatric congenital hearing loss throughout Kentucky.DESIGN: Data on live births and audiological reporting of infant hearing loss results in Kentucky from 2009 to 2011 were analyzed. The authors used spatial scan statistics to identify high-rate clusters of failed newborn screening tests and permanent congenital hearing loss (PCHL), based on the total number of live births per county. The authors conducted further analyses on PCHL and failed newborn hearing screening tests, based on birth hospital data and method of screening.RESULTS: The authors observed four statistically significant (p < 0.05) high-rate clusters with failed newborn hearing screenings in Kentucky, including two in the Appalachian region. Hospitals using two-stage otoacoustic emission testing demonstrated higher rates of failed screening (p = 0.009) than those using two-stage automated auditory brainstem response testing. A significant cluster of high rate of PCHL was observed in Western Kentucky. Five of the 54 birthing hospitals were found to have higher relative risk of PCHL, and two of those hospitals are located in a very rural region of Western Kentucky within the cluster.CONCLUSIONS: This spatial analysis in children in Kentucky has identified specific regions throughout the state with high rates of congenital hearing loss and failed newborn hearing screening tests. Further investigation regarding causative factors is warranted. This method of analysis can be useful in the setting of hearing health disparities to focus efforts on regions facing high incidence of congenital hearing loss.DOI: 10.1097/AUD.0000000000000101PMCID: PMC4336591",pubmed,25225918,10.1097/AUD.0000000000000101
occupational noise smoking and a high body mass index are risk factors for agerelated hearing impairment and moderate alcohol consumption is protective a european populationbased multicenter study,"420. J Assoc Res Otolaryngol. 2008 Sep;9(3):264-76; discussion 261-3. doi: 10.1007/s10162-008-0123-1. Epub 2008 Jun 10.Occupational noise, smoking, and a high body mass index are risk factors for age-related hearing impairment and moderate alcohol consumption is protective: a European population-based multicenter study.Fransen E(1), Topsakal V, Hendrickx JJ, Van Laer L, Huyghe JR, Van Eyken E, Lemkens N, Hannula S, Mäki-Torkko E, Jensen M, Demeester K, Tropitzsch A, Bonaconsa A, Mazzoli M, Espeso A, Verbruggen K, Huyghe J, Huygen PL, Kunst S, Manninen M, Diaz-Lacava A, Steffens M, Wienker TF, Pyykkö I, Cremers CW, Kremer H, Dhooge I, Stephens D, Orzan E, Pfister M, Bille M, Parving A, Sorri M, Van de Heyning P, Van Camp G.Author information:(1)Department of Medical Genetics, University of Antwerp, Universiteitsplein, 2610 Antwerp, Belgium.A multicenter study was set up to elucidate the environmental and medical risk factors contributing to age-related hearing impairment (ARHI). Nine subsamples, collected by nine audiological centers across Europe, added up to a total of 4,083 subjects between 53 and 67 years. Audiometric data (pure-tone average [PTA]) were collected and the participants filled out a questionnaire on environmental risk factors and medical history. People with a history of disease that could affect hearing were excluded. PTAs were adjusted for age and sex and tested for association with exposure to risk factors. Noise exposure was associated with a significant loss of hearing at high sound frequencies (>1 kHz). Smoking significantly increased high-frequency hearing loss, and the effect was dose-dependent. The effect of smoking remained significant when accounting for cardiovascular disease events. Taller people had better hearing on average with a more pronounced effect at low sound frequencies (<2 kHz). A high body mass index (BMI) correlated with hearing loss across the frequency range tested. Moderate alcohol consumption was inversely correlated with hearing loss. Significant associations were found in the high as well as in the low frequencies. The results suggest that a healthy lifestyle can protect against age-related hearing impairment.DOI: 10.1007/s10162-008-0123-1PMCID: PMC2492985",pubmed,18543032,10.1007/s10162-008-0123-1
noisevocoded sentence recognition and the use of context in older and younger adult listeners,"Purpose: When listening to speech under adverse conditions, older adults, even with “age-normal” hearing, face challenges that may lead to poorer speech recognition than their younger peers. Older listeners generally demonstrate poorer suprathreshold auditory processing along with aging-related declines in neurocognitive functioning that may impair their ability to compensate using “top-down” cognitive–linguistic functions. This study explored top-down processing in older and younger adult listeners, specifically the use of semantic context during noise-vocoded sentence recognition. Method: Eighty-four adults with age-normal hearing (45 young normal-hearing [YNH] and 39 older normal-hearing [ONH] adults) participated. Participants were tested for recognition accuracy for two sets of noise-vocoded sentence materials: one that was semantically meaningful and the other that was syntactically appropriate but semantically anomalous. Participants were also tested for hearing ability and for neurocognitive functioning to assess working memory capacity, speed of lexical access, inhibitory control, and nonverbal fluid reasoning, as well as vocabulary knowledge. Results: The ONH and YNH listeners made use of semantic context to a similar extent. Nonverbal reasoning predicted recognition of both meaningful and anomalous sentences, whereas pure-tone average contributed additionally to anomalous sentence recognition. None of the hearing, neurocognitive, or language measures significantly predicted the amount of context gain, computed as the difference score between meaningful and anomalous sentence recognition. However, exploratory cluster analyses demonstrated four listener profiles and suggested that individuals may vary in the strategies used to recognize speech under adverse listening conditions. Conclusions: Older and younger listeners made use of sentence context to similar degrees. Nonverbal reasoning was found to be a contributor to noise-vocoded sentence recognition. However, different listeners may approach the problem of recognizing meaningful speech under adverse conditions using different strategies based on their hearing, neurocognitive, and language profiles. These findings provide support for the complexity of bottom-up and top-down interactions during speech recognition under adverse listening conditions.",cinahl,10924388,10.1044/2022_JSLHR-22-00184
cochlear implant effectiveness in postlingual singlesided deaf individuals whats the point,"242. Int J Audiol. 2017 Jun;56(6):417-423. doi: 10.1080/14992027.2017.1296595. Epub 2017 Mar 5.Cochlear implant effectiveness in postlingual single-sided deaf individuals: what's the point?Finke M(1)(2), Bönitz H(1), Lyxell B(3)(4)(5), Illg A(1).Author information:(1)a Department of Otolaryngology , Hannover Medical School , Hannover , Germany.(2)b Cluster of Excellence ""Hearing4all"" , Oldenburg & Hannover , Germany.(3)c Linnaeus Centre HEAD , Linköping University , Linköping , Sweden.(4)d Department of Behavioral Sciences and Learning , Linköping University , Linköping , Sweden , and.(5)e The Swedish Institute for Disability Research , Linköping University , Linköping , Sweden.OBJECTIVES: By extending the indication criteria for cochlear implants (CI), the population of CI candidates increased in age, as well as range and type of hearing loss. This qualitative study identified factors that contributed to seek CI treatment in single-sided deaf individuals and gained insights how single-sided deafness (SSD) and hearing with a CI affect their lives.DESIGN: An open-ended questionnaire and a standardised inventory (IOI-HA) were used. Qualitative data reflecting the reasons to seek CI treatment and the individual experiences after CI switch-on were collected.STUDY SAMPLE: A total of 19 postlingually deafened single-sided deaf CI users.RESULTS: Participants use their CI daily and stated that their life satisfaction increased since CI activation. The analysis of the qualitative data revealed four core categories: sound localisation, tinnitus and noise sensitivity, fear to lose the second ear and quality of life.CONCLUSIONS: Our results show how strongly and diversely quality of hearing and quality of life is affected by acquired SSD and improved after CI activation. Our data suggest that the fear of hearing loss (HL) on the normal hearing (NH) ear is an important but so far neglected reason to seek treatment with a CI in individuals with postlingual SSD.DOI: 10.1080/14992027.2017.1296595",pubmed,28635507,10.1080/14992027.2017.1296595
experiences of the use of fox an intelligent agent for programming cochlear implant sound processors in new users,"488. Int J Audiol. 2011 Jan;50(1):50-8. doi: 10.3109/14992027.2010.531294. Epub 2010 Nov 22.Experiences of the use of FOX, an intelligent agent, for programming cochlear implant sound processors in new users.Vaerenberg B(1), Govaerts PJ, de Ceulaer G, Daemers K, Schauwers K.Author information:(1)The Eargroup, Antwerp-Deurne, Belgium.OBJECTIVE: This report describes the application of the software tool ""Fitting to Outcomes eXpert"" (FOX) in programming the cochlear implant (CI) processor in new users. FOX is an intelligent agent to assist in the programming of CI processors. The concept of FOX is to modify maps on the basis of specific outcome measures, achieved using heuristic logic and based on a set of deterministic ""rules"".DESIGN: A prospective study was conducted on eight consecutive CI-users with a follow-up of three months.STUDY SAMPLE: Eight adult subjects with postlingual deafness were implanted with the Advanced Bionics HiRes90k device. The implants were programmed using FOX, running a set of rules known as Eargroup's EG0910 advice, which features a set of ""automaps"". The protocol employed for the initial 3 months is presented, with description of the map modifications generated by FOX and the corresponding psychoacoustic test results.RESULTS: The 3 month median results show 25 dBHL as PTA, 77% (55 dBSPL) and 71% (70 dBSPL) phoneme score at speech audiometry and loudness scaling in or near to the normal zone at different frequencies.CONCLUSIONS: It is concluded that this approach is feasible to start up CI fitting and yields good outcome.DOI: 10.3109/14992027.2010.531294",pubmed,21091083,10.3109/14992027.2010.531294
headlocnet deep convolutional neural networks for accurate classification and multilandmark localization of head cts,"505. Med Image Anal. 2020 Apr;61:101659. doi: 10.1016/j.media.2020.101659. Epub 2020 Jan 28.HeadLocNet: Deep convolutional neural networks for accurate classification and multi-landmark localization of head CTs.Zhang D(1), Wang J(2), Noble JH(2), Dawant BM(3).Author information:(1)Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, 37235, USA. Electronic address: zhangdq20@gmail.com.(2)Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, 37235, USA.(3)Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, 37235, USA. Electronic address: benoit.dawant@vanderbilt.edu.Cochlear implants (CIs) are used to treat subjects with hearing loss. In a CI surgery, an electrode array is inserted into the cochlea to stimulate auditory nerves. After surgery, CIs need to be programmed. Studies have shown that the cochlea-electrode spatial relationship derived from medical images can guide CI programming and lead to significant improvement in hearing outcomes. We have developed a series of algorithms to segment the inner ear anatomy and localize the electrodes. But, because clinical head CT images are acquired with different protocols, the field of view and orientation of the image volumes vary greatly. As a consequence, visual inspection and manual image registration to an atlas image are needed to document their content and to initialize intensity-based registration algorithms used in our processing pipeline. For large-scale evaluation and deployment of our methods these steps need to be automated. In this article we propose to achieve this with a deep convolutional neural network (CNN) that can be trained end-to-end to classify a head CT image in terms of its content and to localize landmarks. The detected landmarks can then be used to estimate a point-based registration with the atlas image in which the same landmark set's positions are known. We achieve 99.5% classification accuracy and an average localization error of 3.45 mm for 7 landmarks located around each inner ear. This is better than what was achieved with earlier methods we have proposed for the same tasks.Copyright © 2020 Elsevier B.V. All rights reserved.DOI: 10.1016/j.media.2020.101659PMCID: PMC7959656",pubmed,32062157,10.1016/j.media.2020.101659
audiological results in ssd with an active transcutaneous bone conduction implant at a retrosigmoidal position,"374. Otol Neurotol. 2017 Jun;38(5):642-647. doi: 10.1097/MAO.0000000000001394.Audiological Results in SSD With an Active Transcutaneous Bone Conduction Implant at a Retrosigmoidal Position.Salcher R(1), Zimmermann D, Giere T, Lenarz T, Maier H.Author information:(1)*Cluster of Excellence Hearing4All †Department of Otolaryngology, Medical University Hannover, Hannover, Germany.OBJECTIVE: One option for patients with single sided deafness (SSD) who experience problems with insufficient hearing in different surroundings is the treatment with percutaneous bone-anchored hearing aids. Common medical problems associated to a skin penetrating abutment can be avoided by active transcutaneous bone conduction hearing implants. The purpose of our study was to evaluate the benefit of an active transcutaneous bone conduction hearing implant in patients with SSD.PATIENTS AND METHODS: Patients suffering from SSD who are implanted with an active transcutaneous bone conduction hearing implant in retrosigmoidal position were audiologically analyzed. The audiological test battery included air and bone conduction thresholds, word recognition score (WRS) in quiet and speech intelligibility (Oldenburg Sentence Test [OLSA]) in noise. Patient satisfaction was evaluated with the Abbreviated Profile of Hearing Aid Benefit (APHAB) and the Bern-Benefit in Single-Sided Deafness (BBSS) questionnaire.RESULTS: The monosyllable WRS and the signal-to-noise ratio (SNR) assessed by the OLSA was significantly better in all aided conditions. Also, the APHAB categories ease of communication and reverberation and the average benefit in the BBSS improved significantly if using the device.CONCLUSION: The Bonebridge is a transcutaneous alternative to the well-established percutaneous bone conducting devices in patients with single sided deafness. An improvement in hearing in noise and quiet as well as a decrease of the head shadow effect can be expected.DOI: 10.1097/MAO.0000000000001394",pubmed,28375939,10.1097/MAO.0000000000001394
a personalizable mobile sound detector app design for deaf and hardofhearing users,"Sounds provide informative signals about the world around us. In situations where non-Auditory cues are inaccessible, it can be useful for deaf and hard-of-hearing people to be notified about sounds. Through a survey, we explored which sounds are of interest to deaf and hard-of-hearing people, and which means of notification are appropriate. Motivated by these findings, we designed a mobile phone app that alerts deaf and hard-of-hearing people to sounds they care about. The app uses training examples of personally relevant sounds recorded by the user to learn a model of those sounds. It then screens the incoming audio stream from the phone's microphone for those sounds. When it detects a sound, it alerts the user by vibrating and providing a pop-up notification. To evaluate the interface design independent of sound detection errors, we ran a Wizard-of-Oz user study, and found that the app design successfully facilitated deaf and hard-of-hearing users recording training examples. We also explored the viability of a basic machine learning algorithm for sound detection.",scopus,2-s2.0-85006784988,10.1145/2982142.2982171
on the relationship between auditory cognition and speech intelligibility in cochlear implant users an erp study,"191. Neuropsychologia. 2016 Jul 1;87:169-181. doi: 10.1016/j.neuropsychologia.2016.05.019. Epub 2016 May 19.On the relationship between auditory cognition and speech intelligibility in cochlear implant users: An ERP study.Finke M(1), Büchner A(2), Ruigendijk E(3), Meyer M(4), Sandmann P(5).Author information:(1)Department of Otolaryngology, Hannover Medical School, Germany; Cluster of Excellence ""Hearing4all"", Germany. Electronic address: Finke.Mareike@mh-hannover.de.(2)Department of Otolaryngology, Hannover Medical School, Germany; Cluster of Excellence ""Hearing4all"", Germany.(3)Cluster of Excellence ""Hearing4all"", Germany; Department of Dutch, University of Oldenburg, Germany.(4)Neuroplasticity and Learning in the Healthy Aging Brain, Psychological Institute, University of Zurich, Switzerland; Cognitive Psychology Unit (CPU), University of Klagenfurt, Austria.(5)Cluster of Excellence ""Hearing4all"", Germany; Department of Neurology, Hannover Medical School, Germany; Department of Otorhinolaryngology, University of Cologne, Germany.There is a high degree of variability in speech intelligibility outcomes across cochlear-implant (CI) users. To better understand how auditory cognition affects speech intelligibility with the CI, we performed an electroencephalography study in which we examined the relationship between central auditory processing, cognitive abilities, and speech intelligibility. Postlingually deafened CI users (N=13) and matched normal-hearing (NH) listeners (N=13) performed an oddball task with words presented in different background conditions (quiet, stationary noise, modulated noise). Participants had to categorize words as living (targets) or non-living entities (standards). We also assessed participants' working memory (WM) capacity and verbal abilities. For the oddball task, we found lower hit rates and prolonged response times in CI users when compared with NH listeners. Noise-related prolongation of the N1 amplitude was found for all participants. Further, we observed group-specific modulation effects of event-related potentials (ERPs) as a function of background noise. While NH listeners showed stronger noise-related modulation of the N1 latency, CI users revealed enhanced modulation effects of the N2/N4 latency. In general, higher-order processing (N2/N4, P3) was prolonged in CI users in all background conditions when compared with NH listeners. Longer N2/N4 latency in CI users suggests that these individuals have difficulties to map acoustic-phonetic features to lexical representations. These difficulties seem to be increased for speech-in-noise conditions when compared with speech in quiet background. Correlation analyses showed that shorter ERP latencies were related to enhanced speech intelligibility (N1, N2/N4), better lexical fluency (N1), and lower ratings of listening effort (N2/N4) in CI users. In sum, our findings suggest that CI users and NH listeners differ with regards to both the sensory and the higher-order processing of speech in quiet as well as in noisy background conditions. Our results also revealed that verbal abilities are related to speech processing and speech intelligibility in CI users, confirming the view that auditory cognition plays an important role for CI outcome. We conclude that differences in auditory-cognitive processing contribute to the variability in speech performance outcomes observed in CI users.Copyright © 2016 Elsevier Ltd. All rights reserved.DOI: 10.1016/j.neuropsychologia.2016.05.019",pubmed,27212057,10.1016/j.neuropsychologia.2016.05.019
stabilitycontrolled hybrid adaptive feedback cancellation scheme for hearing aids,"611. J Acoust Soc Am. 2018 Jan;143(1):150. doi: 10.1121/1.5020269.Stability-controlled hybrid adaptive feedback cancellation scheme for hearing aids.Nordholm S(1), Schepker H(2), Tran LTT(1), Doclo S(2).Author information:(1)Department of Electrical and Computer Engineering, Curtin University, Perth, WA, 6102, Australia.(2)Signal Processing Group, Department of Medical Physics and Acoustics and Cluster of Excellence ""Hearing4All,"" University of Oldenburg, Oldenburg, Germany.Adaptive feedback cancellation (AFC) techniques are common in modern hearing aid devices (HADs) since these techniques have been successful in increasing the stable gain. Accordingly, there has been a significant effort to improve AFC technology, especially for open-fitting and in-ear HADs, for which howling is more prevalent due to the large acoustic coupling between the loudspeaker and the microphone. In this paper, the authors propose a hybrid AFC (H-AFC) scheme that is able to shorten the time it takes to recover from howling. The proposed H-AFC scheme consists of a switched combination adaptive filter, which is controlled by a soft-clipping-based stability detector to select either the standard normalized least mean squares (NLMS) algorithm or the prediction-error-method (PEM) NLMS algorithm to update the adaptive filter. The standard NLMS algorithm is used to obtain fast convergence, while the PEM-NLMS algorithm is used to provide a low bias solution. This stability-controlled adaptation is hence the means to improve performance in terms of both convergence rate as well as misalignment, while only slightly increasing computational complexity. The proposed H-AFC scheme has been evaluated for both speech and music signals, resulting in a significantly improved convergence and re-convergence rate, i.e., a shorter howling period, as well as a lower average misalignment and a larger added stable gain compared to using either the NLMS or the PEM-NLMS algorithm alone. An objective evaluation using the perceptual evaluation of speech quality and the perceptual evaluation of audio quality measures shows that the proposed H-AFC scheme provides very high-quality speech and music signals. This has also been verified through a subjective listening experiment with N = 15 normal-hearing subjects using a multi-stimulus test with hidden reference and anchor, showing that the proposed H-AFC scheme results in a better perceptual quality than the state-of-the-art PEM-NLMS algorithm.DOI: 10.1121/1.5020269",pubmed,29390746,10.1121/1.5020269
performance of machinelearning algorithms to pattern recognition and classification of hearing impairment in brazilian farmers exposed to pesticide andor cigarette smoke,"306. Environ Sci Pollut Res Int. 2019 Mar;26(7):6481-6491. doi: 10.1007/s11356-018-04106-w. Epub 2019 Jan 8.Performance of machine-learning algorithms to pattern recognition and classification of hearing impairment in Brazilian farmers exposed to pesticide and/or cigarette smoke.Tomiazzi JS(1), Pereira DR(1), Judai MA(2), Antunes PA(1), Favareto APA(3).Author information:(1)Graduate Program in Environment and Regional Development, University of Western São Paulo - UNOESTE, Presidente Prudente, SP, Brazil.(2)Faculty of Health Sciences, University of Western São Paulo - UNOESTE, Presidente Prudente, SP, Brazil.(3)Graduate Program in Environment and Regional Development, University of Western São Paulo - UNOESTE, Presidente Prudente, SP, Brazil. anafavareto@unoeste.br.The use of pesticides has been increasing in agriculture, leading to a public health problem. The aim of this study was to evaluate ototoxic effects in farmers who were exposed to cigarette smoke and/or pesticides and to identify possible classification patterns in the exposure groups. The sample included 127 participants of both sexes aged between 18 and 39, who were divided into the following four groups: control group (CG), smoking group (SG), pesticide group (PG), and smoking + pesticide group (SPG). Meatoscopy, pure tone audiometry, logoaudiometry, high-frequency thresholds, and immittance testing were performed. Data were evaluated by artificial neural network (ANN), K-nearest neighbors (K-NN), and support vector machine (SVM). There was symmetry between the right and left ears, an increase in the incidence of hearing loss at high frequency and of downward sloping audiometric curve configuration, and alteration of stapedial reflex in the three exposed groups. The machine-learning classifiers achieved good classification performance (control and exposed). The best classification results occur in high type (I and II) datasets (about 90% accuracy) in k-NN test. It is concluded that both xenobiotic substances have ototoxic potential; however, their combined use does not present additive or potentiating effects recognizable by the algorithms.DOI: 10.1007/s11356-018-04106-w",pubmed,30623325,10.1007/s11356-018-04106-w
structure of perceived handicap in middleaged males with noiseinduced hearing loss with and without tinnitus,"446. Audiology. 1993;32(2):137-52. doi: 10.3109/00206099309071863.Structure of perceived handicap in middle-aged males with noise-induced hearing loss, with and without tinnitus.Hallberg LR(1), Johnsson T, Axelsson A.Author information:(1)Department of Psychology, University of Göteborg, Sweden.By using a modified stepwise regression analysis technique, the structure of self-perceived handicap and tinnitus annoyance in 89 males with noise-induced hearing loss was described. Handicap was related to three clusters of variables, reflecting individual, environmental, and socioeconomic aspects, and 60% of the variance in self-perceived handicap was explained by the representatives of these clusters: i.e. 'acceptance of hearing problems', 'social support related to tinnitus' and 'years of education'. Tinnitus had no impact of its own on self-perceived handicap and only a modest portion (36%) of the variance in tinnitus annoyance was explained by 'sleep disturbance' and 'auditory perceptual difficulties'.DOI: 10.3109/00206099309071863",pubmed,8476352,10.3109/00206099309071863
hearing aid acquisition and ownership what can we learn from online consumer reviews,"194. Int J Audiol. 2021 Nov;60(11):917-926. doi: 10.1080/14992027.2021.1931487. Epub 2021 Jun 13.Hearing aid acquisition and ownership: what can we learn from online consumer reviews?Bennett RJ(1)(2), Swanepoel W(1)(2)(3), Ratinaud P(4), Bailey A(5), Pennebaker JW(6), Manchaiah V(7)(8).Author information:(1)Ear Science Institute Australia, Subiaco, Australia.(2)Ear Sciences Centre, School of Surgery, The University of Western Australia, Nedlands, Australia.(3)Department of Speech-Language Pathology and Audiology, University of Pretoria, Gauteng, South Africa.(4)LERASS Laboratory, University of Toulouse, Toulouse, France.(5)Hearing Tracker Inc., Austin, TX, USA.(6)Department of Psychology, University of Texas at Austin, Austin, TX, USA.(7)Department of Speech and Hearing Sciences, Lamar University, Beaumont, TX, USA.(8)Department of Speech and Hearing, School of Allied Health Sciences, Manipal University, Manipal, India.OBJECTIVE: To explore the publicised opinions of consumers actively participating in online hearing aid reviews.DESIGN: A retrospective design examining data generated from an online consumer review website (www.HearingTracker.com). Qualitative data (open text responses) were analysed using the open source automated topic modelling software IRaMuTeQ (http://www.iramuteq.org/) to identify themes. Outputs were compared with quantitative data from the consumer reviews (short response questions exploring hearing aid performance and benefit, and some meta-data such as hearing aid brand and years of hearing aid ownership).STUDY SAMPLE: 1378 online consumer hearing aid reviews.RESULTS: Six clusters within two domains were identified. The domain Device Acquisition included three clusters: Finding the right provider, device and price-point; Selecting a hearing aid to suit the hearing loss; Attaining physical fit and device management skills. The domain Device Use included three clusters: Smartphone streaming to hearing aids; Hearing aid adjustment using smartphone; and Hearing in noise.CONCLUSIONS: Although online hearing aid consumers indicate positive performance on multiple-choice questions relating to hearing aid performance and benefit, their online reviews describe a number of barriers limiting their success. Hearing healthcare clinicians must employ a personalised approach to audiological rehabilitation to ensure individual clients' needs are met.DOI: 10.1080/14992027.2021.1931487",pubmed,34120557,10.1080/14992027.2021.1931487
role of natural language processing in improving lives of disabled,"Natural Language Processing is a branch of AI that aims to assist the machine in deciphering natural language. We can implement this Artificial Intelligence to assist Real Intelligence where people of determination need it. A vast amount of potential can be reached in NLP to help people with impairments. NLP has both a negative and positive role in the lives of the disabled, but correct implementation can advance recent work to improve the future. This paper talks about the role of natural language processing in the lives of people of determination. It suggests a model that could most likely be the tool that hearing, and visually impaired people need, to keep up with abled people. It also mentions the negative side of NLP in the lives of people of determination and social biases. Possible faults can arise during implementation that has been addressed along with other work from authors. A system must understand natural language to proceed and implement it, which is a difficult task due to the many complexities of natural language. However, once implemented, NLP can be used as a tool in many areas of life, from aiding the disabled to allowing diverse people to communicate easily.",ieee,,10.1109/ICRITO56286.2022.9964626
synergistic integration of multiview brain networks and advanced machine learning techniques for auditory disorders diagnostics,"723. Brain Inform. 2024 Jan 14;11(1):3. doi: 10.1186/s40708-023-00214-7.Synergistic integration of Multi-View Brain Networks and advanced machine learning techniques for auditory disorders diagnostics.Ahmed MAO(1), Satar YA(2), Darwish EM(3)(4), Zanaty EA(5).Author information:(1)Department of Computer Science, Faculty of Computers and Information, Luxor University, 85951, Luxor, Egypt. mao.khfagy@fci.luxor.edu.eg.(2)Mathematics Department, Faculty of Science, Sohag University, 82511, Sohag, Egypt.(3)Physics Department, College of Science, Taibah University, Medina, 41411, Saudi Arabia.(4)Physics Department, Faculty of Science, Sohag University, 82524, Sohag, Egypt.(5)Department of Computer Science, Faculty of Computers and Artificial Intelligence, Sohag University, 82511, Sohag, Egypt.In the field of audiology, achieving accurate discrimination of auditory impairments remains a formidable challenge. Conditions such as deafness and tinnitus exert a substantial impact on patients' overall quality of life, emphasizing the urgent need for precise and efficient classification methods. This study introduces an innovative approach, utilizing Multi-View Brain Network data acquired from three distinct cohorts: 51 deaf patients, 54 with tinnitus, and 42 normal controls. Electroencephalogram (EEG) recording data were meticulously collected, focusing on 70 electrodes attached to an end-to-end key with 10 regions of interest (ROI). This data is synergistically integrated with machine learning algorithms. To tackle the inherently high-dimensional nature of brain connectivity data, principal component analysis (PCA) is employed for feature reduction, enhancing interpretability. The proposed approach undergoes evaluation using ensemble learning techniques, including Random Forest, Extra Trees, Gradient Boosting, and CatBoost. The performance of the proposed models is scrutinized across a comprehensive set of metrics, encompassing cross-validation accuracy (CVA), precision, recall, F1-score, Kappa, and Matthews correlation coefficient (MCC). The proposed models demonstrate statistical significance and effectively diagnose auditory disorders, contributing to early detection and personalized treatment, thereby enhancing patient outcomes and quality of life. Notably, they exhibit reliability and robustness, characterized by high Kappa and MCC values. This research represents a significant advancement in the intersection of audiology, neuroimaging, and machine learning, with transformative implications for clinical practice and care.© 2023. The Author(s).DOI: 10.1186/s40708-023-00214-7PMCID: PMC10788326",pubmed,38219249,10.1186/s40708-023-00214-7
prevalence of scarpas ganglion enhancement on highresolution mri imaging,"764. Neuroradiol J. 2024 Jan 16:19714009231224415. doi: 10.1177/19714009231224415. Online ahead of print.Prevalence of Scarpa's ganglion enhancement on high-resolution MRI imaging.Siminski C(1), Benson JC(1), Carlson ML(2), Lane JI(1).Author information:(1)Department of Radiology, Mayo Clinic, Rochester, MN, USA.(2)Department of Otolaryngology-Head and Neck Surgery, Mayo Clinic, Rochester, MN, USA.BACKGROUND AND PURPOSE: The vestibular ganglion, or Scarpa's ganglion, is a cluster of afferent vestibular neurons within the internal auditory canal (IAC). There is minimal literature describing enhancement of this region on magnetic resonance imaging (MRI) and its correlation to clinical symptoms. Here, we sought to find the prevalence of enhancement at Scarpa's ganglion, and determine whether such enhancement correlates with demographics or clinical symptoms.MATERIALS AND METHODS: A retrospective review was performed of consecutive patients with an MRI of the IAC between 3/1/2021 and 5/20/2021. Two neuroradiologists independently reviewed for T1 and FLAIR enhancement of the Scarpa's ganglion on post-contrast fat-saturated T1 and post-contrast FLAIR images. Discrepancies were agreed upon by consensus. Clinical variables (hearing loss, vestibular symptoms, tinnitus, and MRI indication) were gathered from a retrospective chart review.RESULTS: Eighty-nine patients were included (51 female); the mean age was 58 (range 19-85). The most common MRI indication was hearing loss (n = 53). FLAIR enhancement was present on the right in 7 patients, on the left in 7 patients, and bilaterally in 6 patients. No enhancement was seen on post-contrast T1 images. There was no statistically significant correlation between consensus FLAIR on at least one side and age (p = .74), gender (p = .29), hearing loss (p = .32), hearing loss side (p = .39), type of hearing loss (p = .87), vestibular symptoms (p = .71), or tinnitus (p = .81).CONCLUSIONS: Enhancement is present in the minority of patients on post-contrast FLAIR images. If seen, it should be considered an uncommon but not unexpected finding with no clinical significance.DOI: 10.1177/19714009231224415",pubmed,38226489,10.1177/19714009231224415
genetic mapping of xlinked albinismdeafness syndrome adfn to xq263q271,"X-linked albinism-deafness syndrome (ADFN) was described in one Israeli Jewish family and is characterized by congenital nerve deafness and piebaldness. The ADFN mutation probably affects the migration of neural crest-derived precursors of the melanocytes. As a first step toward identifying the ADFN gene, a linkage study was performed to localize the disease locus on the X chromosome. The family was found to be informative for 11 of 107 RFLPs along the X, and two-point analysis showed four of them - factor 9 (F9), DXS91, DXS37, and DNF1-to have definite or suggestive linkage with ADFN. Multipoint linkage analysis indicated two possible orders within this cluster of loci, neither of which was preferable. In both orders F9 was the most distal, and the best estimate for the location of ADFN was between F9 and the next proximal marker (8.6 cM from F9 [Z = 8.1] or 8.3 cM from F9 [Z = 7.9]). These results suggest that the ADFN is at Xq26.3-q27.1. Disagreement between our data and previous localization of DXS91 at Xq11-q13 was resolved by hybridization of the probe pXG-17, which detects the DXS91 locus, to a panel of somatic cell hybrids containing different portions of the X chromosome. This experiment showed that this locus is definitely at Xq24-q26. Together with the linkage data, our results place DXS91 at Xq26 and underscore the importance of using more than one mapping method for the localization of molecular probes.",scopus,2-s2.0-0025375331,
clerc an intelligent sign language translator for improved accessibility using machine learning,"Clerc's system is a modern smart language interpreter using advanced methods like artificial intelligence, imagery recognition, and natural languages translation. The Clergs System includes these are just some of tools used. The system is able to do this by turning gestures in sign language into verbal and written outputs smoothly thus helping people with hearing impairment to communicate easily in real time. In this research, an object recognition SSD MobileNets and FDN feature extraction models are used for accurate gesture detection. A machine-learning model can also be used to predict accurate translations. The efficacy of the Clerc system has been tested and validated to the utmost extent in different settings such as public places, schools, hospitals amongst others. This research seeks to explore possibilities of improving the system's precision, efficacy, scalability, and use with regards to the practical application.",ieee,,10.1109/ICCSAI59793.2023.10421422
cochlear implantation in postlingually deaf adults is timesensitive towards positive outcome prediction using advanced machine learning techniques,"299. Sci Rep. 2018 Dec 20;8(1):18004. doi: 10.1038/s41598-018-36404-1.Cochlear Implantation in Postlingually Deaf Adults is Time-sensitive Towards Positive Outcome: Prediction using Advanced Machine Learning Techniques.Kim H(1), Kang WS(2), Park HJ(3), Lee JY(2), Park JW(2), Kim Y(2), Seo JW(2), Kwak MY(2), Kang BC(4), Yang CJ(5), Duffy BA(1), Cho YS(6), Lee SY(7), Suh MW(7), Moon IJ(6), Ahn JH(2), Cho YS(6), Oh SH(7), Chung JW(2).Author information:(1)Department of Neurology, USC Stevens Neuroimaging and Informatics Institute, Keck School of Medicine, University of Southern California, Los Angeles, USA.(2)Department of Otolaryngology, Asan Medical Center, University of Ulsan College of Medicine, Seoul, South Korea.(3)Department of Otolaryngology, Asan Medical Center, University of Ulsan College of Medicine, Seoul, South Korea. dzness@amc.seoul.kr.(4)Department of Otorhinolaryngology-Head and Neck Surgery, Ulsan University Hospital, University of Ulsan College of Medicine, Ulsan, Korea.(5)Department of Otolaryngology, Hanil General Hospital, Seoul, South Korea.(6)Department of Otorhinolaryngology-Head and Neck Surgery, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul, South Korea.(7)Department of Otorhinolaryngology-Head and Neck Surgery, Seoul National University Hospital, Seoul National University College of Medicine, Seoul, South Korea.Given our aging society and the prevalence of age-related hearing loss that often develops during adulthood, hearing loss is a common public health issue affecting almost all older adults. Moderate-to-moderately severe hearing loss can usually be corrected with hearing aids; however, severe-to-profound hearing loss often requires a cochlear implant (CI). However, post-operative CI results vary, and the performance of the previous prediction models is limited, indicating that a new approach is needed. For postlingually deaf adults (n de120) who received CI with full insertion, we predicted CI outcomes using a Random-Forest Regression (RFR) model and investigated the effect of preoperative factors on CI outcomes. Postoperative word recognition scores (WRS) served as the dependent variable to predict. Predictors included duration of deafness (DoD), age at CI operation (ageCI), duration of hearing-aid use (DoHA), preoperative hearing threshold and sentence recognition score. Prediction accuracy was evaluated using mean absolute error (MAE) and Pearson's correlation coefficient r between the true WRS and predicted WRS. The fitting using a linear model resulted in prediction of WRS with r = 0.7 and MAE = 15.6 ± 9. RFR outperformed the linear model (r = 0.96, MAE = 6.1 ± 4.7, p < 0.00001). Cross-hospital data validation showed reliable performance using RFR (r = 0.91, MAE = 9.6 ± 5.2). The contribution of DoD to prediction was the highest (MAE increase when omitted: 14.8), followed by ageCI (8.9) and DoHA (7.5). After CI, patients with DoD < 10 years presented better WRSs and smaller variations (p < 0.01) than those with longer DoD. Better WRS was also explained by younger age at CI and longer-term DoHA. Machine learning demonstrated a robust prediction performance for CI outcomes in postlingually deaf adults across different institutes, providing a reference value for counseling patients considering CI. Health care providers should be aware that the patients with severe-to-profound hearing loss who cannot have benefit from hearing aids need to proceed with CI as soon as possible and should continue using hearing aids until after CI operation.DOI: 10.1038/s41598-018-36404-1PMCID: PMC6301958",pubmed,30573747,10.1038/s41598-018-36404-1
a step forward in measuring tinnitus objectively in the brain,,cinahl,8976368,
brain plasticity and auditory spatial adaptation in patients with unilateral hearing loss,"21. Cereb Cortex. 2023 May 24;33(11):7221-7236. doi: 10.1093/cercor/bhad033.Brain plasticity and auditory spatial adaptation in patients with unilateral hearing loss.Alzaher M(1)(2), Strelnikov K(2), Marx M(1)(2), Barone P(1).Author information:(1)Centre de Recherche Cerveau et Cognition, CNRS, Toulouse, 31300, France.(2)Service ORL, CHU Toulouse, Toulouse, 31300, France.Erratum in    Cereb Cortex. 2023 Apr 25;33(9):5760.The ability to localize sounds in patients with Unilateral Hearing Loss (UHL) is usually disrupted due to alteration in the integration of binaural cues. Nonetheless, some patients are able to compensate deficit using adaptive strategies. In this study, we explored the neural correlates underlying this adaptation. Twenty-one patients with UHL were separated into 3 groups using cluster analysis based on their binaural performance. The resulting clusters were referred to as better, moderate, and poorer performers cluster (BPC, MPC, and PPC). We measured the mismatch negativity (MMN) elicited by deviant sounds located at 10°, 20°, and 100° from a standard positioned at 50° ipsilateral to the deaf ear. The BPC exhibited significant MMN for all 3 deviants, similar to normal hearing (NH) subjects. In contrast, there was no significant MMN for 10° and 20° deviants for the PPC and for NH when one ear was plugged and muffed. Scalp distribution was maximal over central regions in BPC, while PPC showed more frontal MMN distribution. Thus, the BPC exhibited a contralateral activation pattern, similar to NH, while the PPC exhibited more symmetrical hemispheric activation. MMN can be used as a neural marker to reflect spatial adaptation in patients with UHL.© The Author(s) 2023. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.DOI: 10.1093/cercor/bhad033",pubmed,36806394,10.1093/cercor/bhad033
comodulation masking release induced by controlled electrical stimulation of auditory nerve fibers,"415. Hear Res. 2013 Feb;296:60-6. doi: 10.1016/j.heares.2012.11.023. Epub 2012 Dec 5.Comodulation masking release induced by controlled electrical stimulation of auditory nerve fibers.Zirn S(1), Hempel JM, Schuster M, Hemmert W.Author information:(1)Department of Otolaryngology (ENT)/ Head & Neck Surgery, University Medical Center of the Ludwig-Maximilians-University Munich, Marchioninistr. 15, 81377 München, Germany. Stefan.Zirn@med.uni-muenchen.deNormal-hearing listeners can perceptually segregate concurrent sound sources, but listeners with significant hearing loss or who wear a cochlear implant (CI) lag behind in this ability. Perceptual grouping mechanisms are essential to segregate concurrent sound sources and affect comodulation masking release (CMR). Thus, CMR measurements in CI users could shed light on segregation cues needed for forming and grouping of auditory objects. CMR illustrates the fact that detection of a target sound embedded in a fluctuating masker is improved by the addition of masker energy remote from the target frequency, provided the envelope fluctuations across masker components are coherent. We modified such a CMR experiment to electrically-induced hearing using direct stimulation and measured the effect in 21 CI users. Cluster analysis of our data revealed two groups: one showed no or only small CMR of 0.1 dB ± 2.7 (N = 14) and a second group achieved a CMR of 10.7 dB ± 3.2 (N = 7), a value that is close to the enhancement observed in a comparable acoustic experiment in normal-hearing listeners (12.9 dB ± 2.6, N = 6). Interestingly, we observed that CMR in CI users may relate to hearing etiology and duration of hearing loss pre-implantation. Our study demonstrates for the first time that a substantial minority of cochlear-implant listeners (about a third) can show significant CMR. This outcome motivates the development of physiologically inspired multi-band gain control and/or different coding strategies for these groups in order to better preserve coherent modulation and thus to take advantage of the individual remaining capabilities to analyze spectro-temporal patterns.Copyright © 2012 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2012.11.023",pubmed,23220120,10.1016/j.heares.2012.11.023
the application of bionic wavelet transform to speech signal processing in cochlear implants using neural network simulations,"452. IEEE Trans Biomed Eng. 2002 Nov;49(11):1299-309. doi: 10.1109/TBME.2002.804590.The application of bionic wavelet transform to speech signal processing in cochlear implants using neural network simulations.Yao J(1), Zhang YT.Author information:(1)Department of Electronic Engineering, Chinese University of Hong Kong, N. T., Shatin, Hong Kong.Cochlear implants (CIs) restore partial hearing to people with severe to profound sensorineural deafness; but there is still a marked performance gap in speech recognition between those who have received cochlear implant and people with a normal hearing capability. One of the factors that may lead to this performance gap is the inadequate signal processing method used in CIs. This paper investigates the application of an improved signal-processing method called bionic wavelet transform (BWT). This method is based upon the auditory model and allows for signal processing. Comparing the neural network simulations on the same experimental materials processed by wavelet transform (WT) and BWT, the application of BWT to speech signal processing in CI has a number of advantages, including: improvement in recognition rates for both consonants and vowels, reduction of the number of required channels, reduction of the average stimulation duration for words, and high noise tolerance. Consonant recognition results in 15 normal hearing subjects show that the BWT produces significantly better performance than the WT (t = -4.36276, p = 0.00065). The BWT has great potential to reduce the performance gap between CI listeners and people with a normal hearing capability in the future.DOI: 10.1109/TBME.2002.804590",pubmed,12450360,10.1109/TBME.2002.804590
detecting hearing loss in highrisk neonates using machine learning,,base,fb961a501a1ad53fc0ff41c44c4037706d411d7fa522533bcb30c8b0f49189b4,
prevalence and causes of hearing impairment in fundong health district northwest cameroon,"107. Trop Med Int Health. 2017 Apr;22(4):485-492. doi: 10.1111/tmi.12840. Epub 2017 Feb 7.Prevalence and causes of hearing impairment in Fundong Health District, North-West Cameroon.Ferrite S(1)(2), Mactaggart I(1), Kuper H(1), Oye J(3), Polack S(1).Author information:(1)International Centre for Evidence in Disability, London School of Hygiene & Tropical Medicine, London, UK.(2)Department of Hearing and Speech Sciences, Federal University of Bahia, Salvador, Brazil.(3)Sightsavers International, Yaounde, Cameroon.OBJECTIVE: To estimate the prevalence and causes of hearing impairment in Fundong Health District, North-West Cameroon.METHODS: We selected 51 clusters of 80 people (all ages) through probability proportionate to size sampling. Initial hearing screening was undertaken through an otoacoustic emission (OAE) test. Participants aged 4+ years who failed this test in both ears or for whom an OAE reading could not be taken underwent a manual pure-tone audiometry (PTA) screening. Cases of hearing impairment were defined as those with pure-tone average ≥41 dBHL in adults and ≥35 dBHL in children in the better ear, or children under age 4 who failed the OAE test in both ears. Each case with hearing loss was examined by an ear, nose and throat nurse who indicated the main likely cause.RESULTS: We examined 3567 (86.9%) of 4104 eligible people. The overall prevalence of hearing impairment was 3.6% (95% confidence interval [CI]: 2.8-4.6). The prevalence was low in people aged 0-17 (1.1%, 0.7-1.8%) and 18-49 (1.1%, 0.5-2.6%) and then rose sharply in people aged 50+ (14.8%, 11.7-19.1%). Among cases, the majority were classified as moderate (76%), followed by severe (15%) and profound (9%). More than one-third of cases of hearing impairment were classified as unknown (37%) or conductive (37%) causes, while sensorineural causes were less common (26%).CONCLUSIONS: Prevalence of hearing impairment in North-West Cameroon is in line with the WHO estimate for sub-Saharan Africa. The majority of cases with known causes are treatable, with impacted wax playing a major role.© 2017 John Wiley & Sons Ltd.DOI: 10.1111/tmi.12840",pubmed,28102004,10.1111/tmi.12840
spatial speechinnoise performance in simulated singlesided deaf and bimodal cochlear implant users in comparison with real patients,"98. Int J Audiol. 2023 Jan;62(1):30-43. doi: 10.1080/14992027.2021.2015633. Epub 2021 Dec 28.Spatial speech-in-noise performance in simulated single-sided deaf and bimodal cochlear implant users in comparison with real patients.Jürgens T(1)(2), Wesarg T(3), Oetting D(4), Jung L(3), Williges B(2)(5).Author information:(1)Institute of Acoustics, University of Applied Sciences Lübeck, Lübeck, Germany.(2)Medical Physics and Cluster of Excellence ""Hearing4all"", Carl-von-Ossietzky University, Oldenburg, Germany.(3)Faculty of Medicine, Department of Otorhinolaryngology - Head and Neck Surgery, Medical Center, University of Freiburg, Freiburg, Germany.(4)Hörzentrum Oldenburg gGmbH, Oldenburg, Germany.(5)SOUND Lab, Cambridge Hearing Group, Department of Clinical Neurosciences, University of Cambridge, Cambridge, UK.OBJECTIVE: Speech reception thresholds (SRTs) in spatial scenarios were measured in simulated cochlear implant (CI) listeners with either contralateral normal hearing, or aided hearing impairment (bimodal), and compared to SRTs of real patients, who were measured using the exact same paradigm, to assess goodness of simulation.DESIGN: CI listening was simulated using a vocoder incorporating actual CI signal processing and physiologic details of electric stimulation on one side. Unprocessed signals or simulation of aided moderate or profound hearing impairment was used contralaterally. Three spatial speech-in-noise scenarios were tested using virtual acoustics to assess spatial release from masking (SRM) and combined benefit.STUDY SAMPLE: Eleven normal-hearing listeners participated in the experiment.RESULTS: For contralateral normal and aided moderately impaired hearing, bilaterally assessed SRTs were not statistically different from unilateral SRTs of the better ear, indicating ""better-ear-listening"". Combined benefit was only found for contralateral profound impaired hearing. As in patients, SRM was highest for contralateral normal hearing and decreased systematically with more severe simulated impairment. Comparison to actual patients showed good reproduction of SRTs, SRM, and better-ear-listening.CONCLUSIONS: The simulations reproduced better-ear-listening as in patients and suggest that combined benefit in spatial scenes predominantly occurs when both ears show poor speech-in-noise performance.DOI: 10.1080/14992027.2021.2015633",pubmed,34962428,10.1080/14992027.2021.2015633
audiological results with the samba audio processor in comparison to the amad for the vibrant soundbridge,"224. Audiol Neurootol. 2020;25(3):164-172. doi: 10.1159/000506067. Epub 2020 Feb 25.Audiological Results with the SAMBA Audio Processor in Comparison to the Amadé for the Vibrant Soundbridge.Zimmermann D(1), Busch S(1)(2), Lenarz T(1)(2), Maier H(3)(4).Author information:(1)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.(2)Cluster of Excellence EXC 1077/1 ""Hearing4all"", Hannover, Germany.(3)Department of Otolaryngology, Hannover Medical School, Hannover, Germany, Maier.Hannes@MH-Hannover.de.(4)Cluster of Excellence EXC 1077/1 ""Hearing4all"", Hannover, Germany, Maier.Hannes@MH-Hannover.de.BACKGROUND: Since its introduction in 1996, the Vibrant Soundbridge (VSB) has been upgraded with several improved generations of processors. As all systems are compatible, implanted patients can benefit from new technologies by upgrading to the newest processor type available.OBJECTIVES: The aim of this study was to compare the performance of the new (current) SAMBA processor with the previous Amadé processor.METHODS: Twenty subjects monaurally implanted with a VSB and the Amadé processor tested the new SAMBA processor for a trial period of 4 weeks. We measured air conduction and bone conduction thresholds, unaided thresholds, and aided free field thresholds with both devices. Speech performance in quiet using the Freiburg monosyllabic test at 65 dB SPL (S0) was compared. The speech intelligibility in noise was determined using the Oldenburg sentence test measured in different listening conditions (S0NVSB/S0Ncontra) and microphone settings (omni/directional vs. adaptive directivity).RESULTS: Word recognition scores in quiet with the SAMBA were still significantly lower than with the Amadé after the 4 weeks trial period but improved over the following year. Speech intelligibility with the SAMBA was significantly better than with the Amadé in omnidirectional mode and comparable with the Amadé in directional mode. Hence, the adaptive directionality provides an advantage in difficult hearing situations such as noisy environments. The subjective benefit was evaluated using the Abbreviated Profile of Hearing Aid Benefit and the Speech, Spatial and Qualities-C questionnaire. Results of the questionnaires demonstrate an overall higher level of satisfaction with the new SAMBA speech processor than with the older processor.CONCLUSION: The SAMBA enables similar speech perception in quiet but more flexible adaptation in acoustically challenging environments compared to the previous Amadé processor.© 2020 S. Karger AG, Basel.DOI: 10.1159/000506067",pubmed,32097930,10.1159/000506067
higher peripheral inflammation is associated with lower orbitofrontal gamma power in chronic tinnitus,"Chronic tinnitus, the continuous perception of a phantom sound, is a highly prevalent audiological symptom, for which the underlying pathology has not yet been fully understood. It is associated with neurophysiological alterations in the central nervous system and chronic stress, which can be related with a disinhibition of the inflammatory system. We here investigated the association between resting-state oscillatory activity assessed with Magnetoencephalography (MEG), and peripheral inflammation assessed by C-reactive protein (CRP) in a group of patients with chronic tinnitus (N = 21, nine males, mean age: 40.6 ± 14.6 years). Additionally, CRP was assessed in an age- and sex-matched healthy control group (N = 21, nine males, mean age: 40.9 ± 15.2 years). No MEG data was available for the control group. We found a significant negative correlation between CRP and gamma power in the orbitofrontal cortex in tinnitus patients (p < 0.001), pointing to a deactivation of the orbitofrontal cortex when CRP was high. No significant clusters were found for other frequency bands. Moreover, CRP levels were significantly higher in the tinnitus group than in the healthy controls (p = 0.045). Our results can be interpreted based on findings from previous studies having disclosed the orbitofrontal cortex as part of the tinnitus distress network. We suggest that higher CRP levels and the associated deactivation of the orbitofrontal cortex in chronic tinnitus patients is maintaining the tinnitus percept through disinhibition of the auditory cortex and attentional or emotional top-down processes. Although the direction of the association (i.e., causation) between CRP levels and orbitofrontal gamma power in chronic tinnitus is not yet known, inflammation reducing interventions are promising candidates when developing treatments for tinnitus patients. Overall, our study highlights the importance of considering immune-brain communication in tinnitus research. Copyright © 2022 Becker, Keck, Rohleder and Müller-Voggel.",scopus,2-s2.0-85128817949,10.3389/fnbeh.2022.883926
metaanalysis on effectiveness of prelingually deaf patients at different ages following cochlear implantation,"479. Lin Chuang Er Bi Yan Hou Tou Jing Wai Ke Za Zhi. 2015 Feb;29(4):310-4.[Meta-analysis on effectiveness of prelingually deaf patients at different ages following cochlear implantation].[Article in Chinese]Xu Q, Zhai S, Han D, Yang S, Shen W.OBJECTIVE: To assess the clinical effeetiveness of prelingually deaf children after cochlear implantation at different ages so as to provide reasonable expectations for the patients and guidance for the clinical treatment.METHOD: Electronic databases PubMed, YZ365. COM, WANFANG DATA, CMJD, CHKD, CNKI were searched using relevant keywords. Extracted data included author, year of publication, diagnosis, et al. Reported treatment outcomes were clustered into speech discrimination and hearing abilities. Meta-analyses were performed on studies with numerical results using random or fixed effects model.RESULT: There were eight randomized control studies including 442 patients. Comparing speech perception of prelingually deaf children after cochlear implantation younger than three years old (experimental group) and 3-6 years old (control group), three and six months after operation showed that experimental group performed significantly worse than control group; 12 months after operation showed that experimental group performed significantly better than control group. Comparing hearing abilities, three and six months after operation showed that experimental group performed significantly worse than control group; 12 months after operation showed showed that experimental group performed significantly better than control group. Comparing speech perception of younger or older than 4. 5 years old children showed that after 1.5-2 years of operation children implanted younger than 4.5 years of age performed significantly better than children implanted older than 4.5 years old. Comparing speech perception of 7-12 years old children showed that after 3, 6, 12 months of operation patients of 7-12 years old performed significantly better than those children older than 12 years old. Comparing speech perception of implantation younger or older than 18 years old (7-14 yeas old was group A, > 14-18 yeas old was group B, older than 18 yeas old was group C) showed that after one and four years of operation A > B > C, and there were significant differences among them. Comparing warble tone threshold average (WTA) showed that after one year of operation A < B < C, and there were significant differences among them. However, after four years of operation, there was no significant difference among them.CONCLUSION: Prelinguistically deafened patients younger than three years old with cochlear implantation, insisting on scienctific rehabilitation training for a long period of time can receive the optimal recovery effect. The older patients are suggested as early as possible receiving cochlear implantation. The longer they are implanted, the better results they will receive. Moreover, the younger age they are implanted, the faster postoperative language progress they will receive. Further controlled studies with longer follow-up periods and more person included may make the effectiveness of cochlear implantaion more reliable.",pubmed,26121827,
gaussian processes for hearing threshold estimation using auditory brainstem responses,"The Auditory Brainstem Response (ABR) plays an important role in diagnosing and managing hearing loss, but can be challenging and time-consuming to measure. Test times are especially long when multiple ABR measurements are needed, e.g., when estimating hearing threshold at a range of frequencies. While many detection methods have been developed to reduce ABR test times, the majority were designed to detect the ABR at a single stimulus level and do not consider correlations in ABR waveforms across levels. These correlations hold valuable information, and can be exploited for more efficient hearing threshold estimation. This was achieved in the current work using a Gaussian Process (GP), i.e., a Bayesian approach for non-linear regression. The function to estimate with the GP was the ABR's amplitude across stimulus levels, from which hearing threshold was ultimately inferred. Active learning rules were also designed to automatically adjust the stimulus level and efficiently locate hearing threshold. Simulation results show test time reductions of up to ∼50% for the GP compared to a sequentially applied Hotelling's T2 test, which does not consider correlations across ABR waveforms. A case study was also included to briefly assess the GP approach in ABR data from an adult volunteer. © 1964-2012 IEEE.",scopus,2-s2.0-85173057207,10.1109/TBME.2023.3318729
enabling twoway communication of deaf using saudi sign language,"Disabled people are facing many difficulties communicating with others and involving in society. Modern societies have dedicated significant efforts to promote the integration of disabled individuals into their societies and services. Currently, smart healthcare systems are used to facilitate disabled people. The objective of this paper is to enable two-way communication of deaf individuals with the rest of society, thus enabling their migration from marginal elements of society to mainstream contributing elements. In the proposed system, we developed three modules; the sign recognition module (SRM) that recognizes the signs of a deaf individual, the speech recognition and synthesis module (SRSM) that processes the speech of a non-deaf individual and converts it to text, and an Avatar module (AM) to generate and perform the corresponding sign of the non-deaf speech, which were integrated into the sign translation companion system called Saudi deaf companion system (SDCS) to facilitate the communication from the deaf to the hearing and vice versa. This paper also contributes to the literature by utilizing our self-developed database, the largest Saudi Sign Language (SSL) database—the King Saud University Saudi-SSL (KSU-SSL). The proposed SDCS system performs 293 Saudi signs that are recommended by the Saudi Association for Hearing Impairment (SAHI) from 10 domains (healthcare, common, alphabets, verbs, pronouns and adverbs, numbers, days, kings, family, and regions).",ieee,2169-3536,10.1109/ACCESS.2023.3337514
evaluation of combined dynamic compression and single channel noise reduction for hearing aid applications,"189. Int J Audiol. 2018 Jun;57(sup3):S43-S54. doi: 10.1080/14992027.2017.1300695. Epub 2017 Mar 30.Evaluation of combined dynamic compression and single channel noise reduction for hearing aid applications.Kortlang S(1), Chen Z(1), Gerkmann T(2), Kollmeier B(1), Hohmann V(1), Ewert SD(1).Author information:(1)a Medizinische Physik and Cluster of Excellence Hearing4all , Universität Oldenburg , Oldenburg , Germany and.(2)b Speech Signal Processing and Cluster of Excellence Hearing4all , Universität Oldenburg , Oldenburg , Germany.OBJECTIVE: Single-channel noise reduction (SCNR) and dynamic range compression (DRC) are important elements in hearing aids. Only relatively few studies have addressed interaction effects and typically used real hearing aids with limited knowledge about the integrated algorithms. Here the potential benefit of different combinations and integration of SCNR and DRC was systematically assessed.DESIGN: Ten different systems combining SCNR and DRC were implemented, including five serial arrangements, a parallel and two multiplicative approaches. In an instrumental evaluation, signal-to-noise ratio (SNR) improvement and spectral contrast enhancement (SCE) were assessed. Quality ratings at 0 and +6 dB SNR, and speech reception thresholds (SRTs) in noise were measured using stationary and babble noise.STUDY SAMPLE: Thirteen young normal-hearing (NH) listeners and 12 hearing-impaired (HI) listeners participated.RESULTS: In line with an increased segmental SNR and spectral contrast compared to a serial concatenation, the parallel approach significantly reduced the perceived noise annoyance for both subject groups. The proposed multiplicative approaches could partly counteract increased speech distortions introduced by DRC and achieved the best overall quality for the HI listeners.CONCLUSIONS: For high SNRs well above the individual SRT, the specific combination of SCNR and DRC is perceptually relevant and the integrative approaches were preferred.DOI: 10.1080/14992027.2017.1300695",pubmed,28355947,10.1080/14992027.2017.1300695
crossmodal cortical activity in the brain can predict cochlear implantation outcome in adults a machine learning study,"233. J Int Adv Otol. 2021 Sep;17(5):380-386. doi: 10.5152/iao.2021.9337.Cross-Modal Cortical Activity in the Brain Can Predict Cochlear Implantation Outcome in Adults: A Machine Learning Study.Kyong JS(1), Suh MW(2), Han JJ(3), Park MK(2), Noh TS(2), Oh SH(2), Lee JH(2).Author information:(1)Department of Otorhinolaryngology-Head and Neck Surgery, Seoul National University, Seoul, Korea; Audiology Institute, Hallym University of Graduate Studies, Seoul, Korea.(2)Department of Otorhinolaryngology-Head and Neck Surgery, Seoul National University, Seoul, Korea.(3)Department of Otorhinolaryngology-Head and Neck Surgery, Seoul National University, Seoul, Korea; Department of Otorhinolaryngology-Head and Neck Surgery, Soonchunhyang University Hospital, Seoul, Korea.OBJECTIVES: Prediction of cochlear implantation (CI) outcome is often difficult because outcomes vary among patients. Though the brain plasticity across modalities during deafness is associated with individual CI outcomes, longitudinal observations in multiple patients are scarce. Therefore, we sought a prediction system based on cross-modal plasticity in a longitudinal study with multiple patients.METHODS: Classification of CI outcomes between excellent or poor was tested based on the features of brain cross-modal plasticity, measured using event-related responses and their corresponding electromagnetic sources. A machine learning estimation model was applied to 13 datasets from 3 patients based on linear supervised training. Classification efficiency was evaluated comparing prediction accuracy, sensitivity/specificity, total mis-classification cost, and training time among feature set conditions.RESULTS: Combined feature sets with the sensor and source levels dramatically improved classification accuracy between excellent and poor outcomes. Specifically, the tactile feature set best explained CI outcome (accuracy, 98.83 ± 2.57%; sensitivity, 98.00 ± 0.01%; specificity, 98.15 ± 4.26%; total misclassification cost, 0.17 ± 0.38; training time, 0.51 ± 0.09 sec), followed by the visual feature (accuracy, 93.50 ± 4.89%; sensitivity, 89.17 ± 8.16%; specificity, 98.00 ± 0.01%; total misclassification cost, 0.65 ± 0.49; training time, 0.38 ± 0.50 sec).CONCLUSION: Individual tactile and visual processing in the brain best classified the current status when classified by combined sensor-source level features. Our results suggest that cross-modal brain plasticity due to deafness may provide a basis for classifying the status. We expect this novel method to contribute to the evaluation and prediction of CI outcomes.DOI: 10.5152/iao.2021.9337PMCID: PMC8975390",pubmed,34617886,10.5152/iao.2021.9337
concept and considerations of a medical device the active noise cancelling incubator,"702. Front Pediatr. 2023 Jul 3;11:1187815. doi: 10.3389/fped.2023.1187815. eCollection 2023.Concept and considerations of a medical device: the active noise cancelling incubator.Jaschke AC(1)(2)(3), Bos AF(1).Author information:(1)Department of Paediatrics, Division of Neonatology, Beatrix Children's Hospital, University Medical Center Groningen, University of Groningen, Groningen, Netherlands.(2)Department of Music Therapy, ArtEZ University of the Arts, Enschede, Netherlands.(3)Cambridge Institute for Music Therapy Research, Anglia Ruskin University, Cambridge, United Kingdom.BACKGROUND: An increasingly 24/7 connected and urbanised world has created a silent pandemic of noise-induced hearing loss. Ensuring survival to children born (extremely) preterm is crucial. The incubator is a closed medical device, modifying the internal climate, and thus providing an environment for the child, as safe, warm, and comfortable as possible. While sound outside the incubator is managed and has decreased over the years, managing the noise inside the incubator is still a challenge.METHOD: Using active noise cancelling in an incubator will eliminate unwanted sounds (i.e., from the respirator and heating) inside the incubator, and by adding sophisticated algorithms, normal human speech, neonatal intensive care unit music-based therapeutic interventions, and natural sounds will be sustained for the child in the pod. Applying different methods such as active noise cancelling, motion capture, sonological engineering. and sophisticated machine learning algorithms will be implemented in the development of the incubator.PROJECTED RESULTS: A controlled and active sound environment in and around the incubator can in turn promote the wellbeing, neural development, and speech development of the child and minimise distress caused by unwanted noises. While developing the hardware and software pose individual challenges, it is about the system design and aspects contributing to it. On the one hand, it is crucial to measure the auditory range and frequencies in the incubator, as well as the predictable sounds that will have to be played back into the environment. On the other, there are many technical issues that have to be addressed when it comes to algorithms, datasets, delay, microphone technology, transducers, convergence, tracking, impulse control and noise rejection, noise mitigation stability, detection, polarity, and performance.CONCLUSION: Solving a complex problem like this, however, requires a de-disciplinary approach, where each discipline will realise its own shortcomings and boundaries, and in turn will allow for innovations and new avenues. Technical developments used for building the active noise cancellation-incubator have the potential to contribute to improved care solutions for patients, both infants and adults.Code available at: 10.3389/fped.2023.1187815.© 2023 Jaschke and Bos.DOI: 10.3389/fped.2023.1187815PMCID: PMC10350684",pubmed,37465419,10.3389/fped.2023.1187815
intrinsic brain activity of inferior temporal region increased in prodromal alzheimers disease with hearing loss,"6-month period on other research work, received a (cofunding) research grant from Sonova AG (mother company of AudioNova International BV) for other research work, and has been paid for delivering a one-off scientific presentation for Sonova AG. VJ is an employee at Schoonenberg HoorSupport and SEK has been paid for delivering a presentation for Sonova AG; no other relationships or activities that could appear to have influenced the submitted work can be reported.750. Front Aging Neurosci. 2022 Jan 28;13:772136. doi: 10.3389/fnagi.2021.772136. eCollection 2021.Intrinsic Brain Activity of Inferior Temporal Region Increased in Prodromal Alzheimer's Disease With Hearing Loss.Hong L(1), Zeng Q(1), Li K(1), Luo X(1), Xu X(1), Liu X(1), Li Z(2), Fu Y(2), Wang Y(2), Zhang T(3), Chen Y(2), Liu Z(2), Huang P(1), Zhang M(1).Author information:(1)Department of Radiology, The 2nd Affiliated Hospital of Zhejiang University School of Medicine, Hangzhou, China.(2)Department of Neurology, The 2nd Affiliated Hospital of Zhejiang University School of Medicine, Hangzhou, China.(3)Department of Neurology, Tongde Hospital of Zhejiang Province, Hangzhou, China.BACKGROUND AND OBJECTIVE: Hearing loss (HL) is one of the modifiable risk factors for Alzheimer's disease (AD). However, the underlying mechanism behind HL in AD remains elusive. A possible mechanism is cognitive load hypothesis, which postulates that over-processing of degraded auditory signals in the auditory cortex leads to deficits in other cognitive functions. Given mild cognitive impairment (MCI) is a prodromal stage of AD, untangling the association between HL and MCI might provide insights for potential mechanism behind HL.METHODS: We included 85 cognitively normal (CN) subjects with no hearing loss (NHL), 24 CN with HL, 103 mild cognitive impairment (MCI) patients with NHL, and 23 MCI with HL from the ADNI database. All subjects underwent resting-state functional MRI and neuropsychological scale assessments. Fractional amplitude of low-frequency fluctuation (fALFF) was used to reflect spontaneous brain activity. The mixed-effects analysis was applied to explore the interactive effects between HL and cognitive status (GRF corrected, voxel p-value <0.005, cluster p-value < 0.05, two-tailed). Then, the FDG data was included to further reflect the regional neuronal abnormalities. Finally, Pearson correlation analysis was performed between imaging metrics and cognitive scores to explore the clinical significance (Bonferroni corrected, p < 0.05).RESULTS: The interactive effects primarily located in the left superior temporal gyrus (STG) and bilateral inferior temporal gyrus (ITG). Post-hoc analysis showed that NC with HL had lower fALFF in bilateral ITG compared to NC with NHL. NC with HL had higher fALFF in the left STG and decreased fALFF in bilateral ITG compared to MCI with HL. In addition, NC with HL had lower fALFF in the right ITG compared to MCI with NHL. Correlation analysis revealed that fALFF was associated with MMSE and ADNI-VS, while SUVR was associated with MMSE, MoCA, ADNI-EF and ADNI-Lan.CONCLUSION: HL showed different effects on NC and MCI stages. NC had increased spontaneous brain activity in auditory cortex while decreased activity in the ITG. Such pattern altered with disease stage changing and manifested as decreased activity in auditory cortex along with increased activity in ITG in MCI. This suggested that the cognitive load hypothesis may be the underlying mechanism behind HL.Copyright © 2022 Hong, Zeng, Li, Luo, Xu, Liu, Li, Fu, Wang, Zhang, Chen, Liu, Huang and Zhang.DOI: 10.3389/fnagi.2021.772136PMCID: PMC8831745",pubmed,35153717,10.3389/fnagi.2021.772136
potential of gene and cell therapy for inner ear hair cells,"542. Biomed Res Int. 2018 Jun 13;2018:8137614. doi: 10.1155/2018/8137614. eCollection 2018.Potential of Gene and Cell Therapy for Inner Ear Hair Cells.Lee MY(1), Park YH(2)(3).Author information:(1)Department of Otorhinolaryngology and Head & Neck Surgery, Dankook University Hospital, Cheonan, Chungnam, Republic of Korea.(2)Department of Otolaryngology-Head and Neck Surgery, College of Medicine, Chungnam National University, Daejeon, Republic of Korea.(3)Brain Research Institute, College of Medicine, Chungnam National University, Daejeon, Republic of Korea.Erratum in    Biomed Res Int. 2019 Mar 7;2019:9601260.Sensorineural hearing loss is caused by the loss of sensory hair cells (HCs) or a damaged afferent nerve pathway to the auditory cortex. The most common option for the treatment of sensorineural hearing loss is hearing rehabilitation using hearing devices. Various kinds of hearing devices are available but, despite recent advancements, their perceived sound quality does not mimic that of the ""naïve"" cochlea. Damage to crucial cochlear structures is mostly irreversible and results in permanent hearing loss. Cochlear HC regeneration has long been an important goal in the field of hearing research. However, it remains challenging because, thus far, no medical treatment has successfully regenerated cochlear HCs. Recent advances in genetic modulation and developmental techniques have led to novel approaches to generating HCs or protecting against HC loss, to preserve hearing. In this review, we present and review the current status of two different approaches to restoring or protecting hearing, gene therapy, including the newly introduced CRISPR/Cas9 genome editing, and stem cell therapy, and suggest the future direction.DOI: 10.1155/2018/8137614PMCID: PMC6020521",pubmed,30009175,10.1155/2018/8137614
personality and psychopathology in mnires disease personalidad y psicopatologa en la enfermedad de mnire,"Introduction and Objectives: Psychological factors in vertigo patients have been extensively studied but the role of anxiety and personality traits in the clinical course of Ménière's disease (MD) is unknown. The objectives of this study are to identify and characterize psychopathology in MD and to find risk factors for an increased rate and intensity of crisis and chronic symptoms. Materials and Methods: We performed a transversal study in all patients diagnosed with definite MD in our department during a 5-year period. Sample subjects were interviewed in 3 steps: first, an otorhinolaryngologist collected information about clinical and pharmacological background of MD; second, a psychiatrist screened for mood, anxiety and personality disorders; in a third stage, the patient completed the DHI (Dizziness Handicap Inventory), STAI-Y (State Trait Anxiety Inventory), NEO-PI-R (Neo Personality Inventory Reviewed) and VAS (Visual Analogue Scale) for vertigo and dizziness. Statistical analysis was performed to search for risk factors for multiple and intense crisis and chronic symptoms. Results: Thirty-four patients completed all 3 phases of the study. A predominant dysfunctional personality trait was identified in 80% of patients (predominantly cluster C type), 35% were being treated with psychiatric medication and 34.4% had a considerable mood or anxiety disorder. All patients scored high (>7) in VAS during crisis. There was a statistically significant positive correlation between crisis rate and STAI, anxiety-subscale (N1) in NEO-PI-R, VAS and DHI scores (p<.044). Crises were more common in bilateral MD (p=.041). DHI scores were higher with higher STAI and N1 (p=.001). Disease duration and pure tone average were found to have a positive moderate correlation (p=.017). Conclusions: The positive correlations between crisis rate, chronic dizziness and anxiety-related personality traits reveal a bidirectional and intimate relationship between personality, anxiety and MD, affecting these patients’ quality of life. These results support the relevance of prospecting adjuvant psychological and psychiatric approaches to these patients. © 2020 Sociedad Española de Otorrinolaringología y Cirugía de Cabeza y Cuello",scopus,2-s2.0-85092542100,10.1016/j.otorri.2020.06.007
recent advances in sign language recognition using deep learning techniques,"Communication is essential to express and receive information, knowledge, ideas, and views among people, but it has been quite a while to be an obstruction for people with hearing and mute disabilities. Though there is sign language to communicate with non-sign people it is difficult for everyone to interpret and understand. As reported by World Health Organization (WHO), five percent of the world population of over 300 million people suffers from hearing disability. Technology has grown rapidly in the last few decades with the presence of deep learning and artificial intelligence, but somehow physically impaired people are not able to get the maximum benefit of it due to lack of awareness, accessibility problems, cost, and other reasons. This survey paper discusses the recent development and assistance provided for signers to communicate with non-signers.",ieee,,10.1109/ICOEI53556.2022.9777104
now i understand an application to bridge the gap in communication with hearing impaired people,"""Now I Understand"" is a mobile application that enables bilateral communication between hearing and non-hearing people using artificial intelligence techniques. The application uses deep learning models to recognize sign language and translate it into text in real-time, while also translating text into sign language. The details of the application implementation are discussed, including data acquisition and preprocessing, selection and training of deep learning models, and the user interface. This application has the potential to revolutionize communication and accessibility for people with hearing impairments by providing a reliable and accurate tool for bilateral communication, as well as improving the general public's awareness and understanding of sign language.",ieee,2473-5728,10.1109/GHTC56179.2023.10354884
hearing loss classification via alexnet and extreme learning machine,,base,05984d21c23b887737b5e56f97d91e138d6ae4ada91a0a17709496fb4aa247ba,
curtailing insomnia in a nonintrusive hardware less approach with machine learning,"The significant challenges nowadays with the expanded utilisation of cell phones are restlessness and a risk to mental health. Rest time is implied for the cerebrum to revive. If the rest time is disturbed because of a non-stop outer aggravation, it upsets the profound rest. Most of us prefer music as the option to induce sleep and relax. Headphones or earphones are used for the same. It is shrewd to turn off the music after an individual rests, which the majority of us do not do, as we by at that point, are rested. This causes damage. Excessive usage of earphones or headphones is one part of it and unnecessary feed to the ears while sleeping shall trigger noise-induced hearing loss. Here, we propose a framework built with machine learning as the key. This will guarantee that the music player stops once the individual using it has dozed off. This ensures proper rest and forestalls sleep deprivation/NIHL. Copyright © 2022 Inderscience Enterprises Ltd.",scopus,2-s2.0-85142481025,10.1504/ijmei.2022.126524
a screening platform for hearing loss and cognitive decline whisper widespread hearing impairment screening and prevention of risk,"134. Stud Health Technol Inform. 2023 Oct 20;309:170-174. doi: 10.3233/SHTI230768.A Screening Platform for Hearing Loss and Cognitive Decline: WHISPER (Widespread Hearing Impairment Screening and PrEvention of Risk).Paglialonga A(1), Polo EM(2)(3), Lenatti M(1), Mollura M(3), Barbieri R(3).Author information:(1)Cnr-Istituto di Elettronica e di Ingegneria dell'Informazione e delle Telecomunicazioni (CNR-IEIIT), 20133 Milan, Italy.(2)Sapienza University of Rome, DIAG, 00185 Rome, Italy.(3)Politecnico di Milano, DEIB, 20133 Milan, Italy.The WHISPER (Widespread Hearing Impairment Screening and PrEvention of Risk) platform was recently developed for screening for hearing loss (HL) and cognitive decline in adults. It includes a battery of tests (a risk factors (RF) questionnaire, a language-independent speech-in-noise test, and cognitive tests) and provides a pass/fail outcome based on the analysis of several features. Earlier studies demonstrated high accuracy of the speech-in-noise test for predicting HL in 350 participants. In this study, preliminary results from the RF questionnaire (137 participants) and from the visual digit span test (DST) (78 participants) are presented. Despite the relatively small sample size, these findings indicate that the RF and DST may provide additional features that could be useful to characterize the overall individual profile, providing additional knowledge related to short-term memory performance and overall risk of HL and cognitive decline. Future research is needed to expand number of subjects tested, number of features analyzed, and the range of algorithms (including supervised and unsupervised machine learning) used to identify novel measures able to predict the individual hearing and cognitive abilities, also including components related to the individual risk.DOI: 10.3233/SHTI230768",pubmed,37869833,10.3233/SHTI230768
deep learning based speaker separation and dereverberation can generalize across different languages to improve intelligibility,"328. J Acoust Soc Am. 2021 Oct;150(4):2526. doi: 10.1121/10.0006565.Deep learning based speaker separation and dereverberation can generalize across different languages to improve intelligibility.Healy EW(1), Johnson EM(1), Delfarah M(2), Krishnagiri DS(1), Sevich VA(1), Taherian H(2), Wang D(2).Author information:(1)Department of Speech and Hearing Science, The Ohio State University, Columbus, Ohio 43210, USA.(2)Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio 43210, USA.The practical efficacy of deep learning based speaker separation and/or dereverberation hinges on its ability to generalize to conditions not employed during neural network training. The current study was designed to assess the ability to generalize across extremely different training versus test environments. Training and testing were performed using different languages having no known common ancestry and correspondingly large linguistic differences-English for training and Mandarin for testing. Additional generalizations included untrained speech corpus/recording channel, target-to-interferer energy ratios, reverberation room impulse responses, and test talkers. A deep computational auditory scene analysis algorithm, employing complex time-frequency masking to estimate both magnitude and phase, was used to segregate two concurrent talkers and simultaneously remove large amounts of room reverberation to increase the intelligibility of a target talker. Significant intelligibility improvements were observed for the normal-hearing listeners in every condition. Benefit averaged 43.5% points across conditions and was comparable to that obtained when training and testing were performed both in English. Benefit is projected to be considerably larger for individuals with hearing impairment. It is concluded that a properly designed and trained deep speaker separation/dereverberation network can be capable of generalization across vastly different acoustic environments that include different languages.DOI: 10.1121/10.0006565PMCID: PMC8637753",pubmed,34717521,10.1121/10.0006565
voicemitra  a speech to visual translation approach for disabled,"Nowadays, Internet’s expansion has multiplied greatly with a variety of information in all contexts available. Although it has been beneficial for the majority group of people, those with special needs, such as the deaf, have few resources at their disposal. The visual graphics and hand gestures can be used to communicate in such cases. The objective of this research is to develop a system that transforms speech and text input into a sequence of visuals representing sign language. This system basically consists of three modules- 1. Speech to Text, which converts the spoken audio into text format, 2. Language translator, wherein different standard languages can also be given as input to get the output and 3. Sign Language Converter, through which the given audio/text is converted to either hand gestures or related gif. These graphics are understood by them, and so a well understood communication can be established. The language translation modules give an average confidence level of 0.916823557. In addition to these three modules, the feature of saving the history in multiple languages is also added, making this system a user-friendly system useful in future as well.",ieee,,10.1109/ICACTA58201.2023.10393103
disability of hearing impairment is positively associated with urine albumincreatinine ratio in korean adults the 20112012 korea national health and nutrition examination survey,"698. Clin Exp Otorhinolaryngol. 2016 Sep;9(3):212-9. doi: 10.21053/ceo.2015.01368. Epub 2016 Jul 2.Disability of Hearing Impairment Is Positively Associated With Urine Albumin/Creatinine Ratio in Korean Adults: The 2011-2012 Korea National Health and Nutrition Examination Survey.Kim YS(1), Lee DH(1), Chae HS(1), Lee TK(1), Sohn TS(1), Jeong SC(1), Kim HY(1), Lee JI(1), Song JY(1), Yeo CD(1), Lee YB(1), Ahn HS(1), Hong M(1), Han K(2).Author information:(1)Epidemiology Study Cluster of Uijeongbu St. Mary's Hospital, Uijeongbu St. Mary's Hospital, College of Medicine, The Catholic University of Korea, Uijeongbu, Korea.(2)Department of Biostatistics, College of Medicine, The Catholic University of Korea, Seoul, Korea.OBJECTIVES: The aim of this study was to determine whether chronic kidney disease (CKD) is associated with hearing thresholds in the nationwide, large-scaled Korean population.METHODS: This study analyzed the data of 9,798 subjects of 19 years and older (4,387 males and 5,411 females). Urine albumin-to-creatinine ratio (ACR) was measured from first-voided spot urine samples. The air-conduction hearing threshold was measured at 0.5, 1, 2, 3, 4, and 6 kHz and pure tone audiogram (PTA) average was calculated as the four-frequency average of 0.5, 1, 2, and 4 kHz.RESULTS: Urine ACR was significantly correlated with the PTA average of better ear in both genders, especially at 3 and 6 kHz in males and at 1, 3, 4, and 6 kHz in females. After adjusting, urine ACR also increased the risk of hearing loss in female, especially if urine ACR was 30 mg/g and more (odds ratio, 1.636-2.229. This study showed that the degree of hearing loss was significantly different according to categories of urine ACR in both genders. Hearing loss without disability was found less but that with bilateral hearing disability was found more as urine ACR increased. In generally, prevalence of hearing loss with disability was higher in males than females.CONCLUSION: This study demonstrated that urine ACR was significantly correlated with the PTA average of better ear in Korean adults of both genders. This study suggests that clinicians should carefully monitor the hearing level for subjects with elevated urine ACR, even though high urine ACR within the normal range.DOI: 10.21053/ceo.2015.01368PMCID: PMC4996098",pubmed,27416740,10.21053/ceo.2015.01368
probing auditory scene analysis,[No abstract available],scopus,2-s2.0-84907930527,10.3389/fnins.2014.00293
action detection for sign language using machine learning,"This research intends to build an effective and quick algorithm for identifying the alphabets in American Sign Language (ASL) using natural hand movements, increasing communication accessibility for people with hearing impaired limitations. The system's ultimate goal is to act as a translator between spoken language and sign language, enabling more effective and efficient communication between those with hearing loss and others who don't have any hearing loss. The research uses image processing, machine learning, and CNN-based artificial intelligence to recognize ASL movements and generate outputs that are simple to interpret. The potential impact of this work on communication accessibility for people who have hearing loss is significant.",ieee,,10.1109/NMITCON58196.2023.10275950
comparison of effects on subjective intelligibility and quality of speech in babble for two algorithms a deep recurrent neural network and spectral subtraction,"307. J Acoust Soc Am. 2019 Mar;145(3):1493. doi: 10.1121/1.5094765.Comparison of effects on subjective intelligibility and quality of speech in babble for two algorithms: A deep recurrent neural network and spectral subtraction.Keshavarzi M(1), Goehring T(2), Turner RE(3), Moore BCJ(1).Author information:(1)Department of Psychology, University of Cambridge, Cambridge, United Kingdom.(2)MRC Cognition and Brain Sciences Unit, University of Cambridge, Cambridge, United Kingdom.(3)Department of Engineering, University of Cambridge, Cambridge, United Kingdom.The effects on speech intelligibility and sound quality of two noise-reduction algorithms were compared: a deep recurrent neural network (RNN) and spectral subtraction (SS). The RNN was trained using sentences spoken by a large number of talkers with a variety of accents, presented in babble. Different talkers were used for testing. Participants with mild-to-moderate hearing loss were tested. Stimuli were given frequency-dependent linear amplification to compensate for the individual hearing losses. A paired-comparison procedure was used to compare all possible combinations of three conditions. The conditions were: speech in babble with no processing (NP) or processed using the RNN or SS. In each trial, the same sentence was played twice using two different conditions. The participants indicated which one was better and by how much in terms of speech intelligibility and (in separate blocks) sound quality. Processing using the RNN was significantly preferred over NP and over SS processing for both subjective intelligibility and sound quality, although the magnitude of the preferences was small. SS processing was not significantly preferred over NP for either subjective intelligibility or sound quality. Objective computational measures of speech intelligibility predicted better intelligibility for RNN than for SS or NP.DOI: 10.1121/1.5094765",pubmed,31067946,10.1121/1.5094765
tracking musical voices in bachs the art of the fugue timbral heterogeneity differentially affects younger normalhearing listeners and older hearingaid users,"826. Front Psychol. 2021 Apr 14;12:608684. doi: 10.3389/fpsyg.2021.608684. eCollection 2021.Tracking Musical Voices in Bach's The Art of the Fugue: Timbral Heterogeneity Differentially Affects Younger Normal-Hearing Listeners and Older Hearing-Aid Users.Siedenburg K(1), Goldmann K(1), van de Par S(1).Author information:(1)Department of Medical Physics and Acoustics and Cluster of Excellence Hearing4all, Carl von Ossietzky University of Oldenburg, Oldenburg, Germany.Auditory scene analysis is an elementary aspect of music perception, yet only little research has scrutinized auditory scene analysis under realistic musical conditions with diverse samples of listeners. This study probed the ability of younger normal-hearing listeners and older hearing-aid users in tracking individual musical voices or lines in JS Bach's The Art of the Fugue. Five-second excerpts with homogeneous or heterogenous instrumentation of 2-4 musical voices were presented from spatially separated loudspeakers and preceded by a short cue for signaling the target voice. Listeners tracked the cued voice and detected whether an amplitude modulation was imposed on the cued voice or a distractor voice. Results indicated superior performance of young normal-hearing listeners compared to older hearing-aid users. Performance was generally better in conditions with fewer voices. For young normal-hearing listeners, there was interaction between the number of voices and the instrumentation: performance degraded less drastically with an increase in the number of voices for timbrally heterogeneous mixtures compared to homogeneous mixtures. Older hearing-aid users generally showed smaller effects of the number of voices and instrumentation, but no interaction between the two factors. Moreover, tracking performance of older hearing aid users did not differ when these participants did or did not wear hearing aids. These results shed light on the role of timbral differentiation in musical scene analysis and suggest reduced musical scene analysis abilities of older hearing-impaired listeners in a realistic musical scenario.Copyright © 2021 Siedenburg, Goldmann and van de Par.DOI: 10.3389/fpsyg.2021.608684PMCID: PMC8079728",pubmed,33935864,10.3389/fpsyg.2021.608684
identification of neonatal hearing impairment distortion product otoacoustic emissions during the perinatal period,"Objectives: 1) To describe distortion product otoacoustic emission (DPOAE) levels, noise levels and signal to noise ratios (SNRs) for a wide range of frequencies and two stimulus levels in neonates and infants. 2) To describe the relations between these DPOAE measurements and age, test environment, baby state, and test time.Design: DPOAEs were measured in 2348 well babies without risk indicators, 353 well babies with at least one risk indicator, and 4478 graduates of neonatal intensive care units (NICUs). DPOAE and noise levels were measured at f2 frequencies of 1.0, 1.5, 2.0, 3.0, and 4.0 kHz, and for primary levels (L1/L2) of 65/50 dB SPL and 75/75 dB SPL. Measurement-based stopping rules were used such that a test did not terminate unless the response was at least 3 dB above the mean noise floor + 2 SDs (SNR) for at least four of five test frequencies. The test would terminate, however, if these criteria were not met after 360 sec. Baby state, test environment, and other test factors were captured at the time of each test.Results: DPOAE levels, noise levels and SNRs were similar for well babies without risk indicators, well babies with risk indicators, and NICU graduates. There was a tendency for larger responses at f2 frequencies of 1.5 and 2.0 Hz, compared with 3.0 and 4.0 kHz; however, the noise levels systematically decreased as frequency increased, resulting in the most favorable SNRs at 3.0 and 4.0 kHz. Response levels were least and noise levels highest for an f2 frequency of 1.0 kHz. In addition, test time to achieve automatic stopping criteria was greatest for 1.0 kHz. With the exception of ""active/alert"" and ""crying"" babies, baby state had little influence on DPOAE measurements. Additionally, test environment had little impact on these measurements, at least for the environments in which babies were tested in this study. However, the lowest SNRs were observed for infants who were tested in functioning isolettes. Finally, there were some subtle age affects on DPOAE levels, with the infants born most prematurely producing the smallest responses, regardless of age at the time of test.Conclusions: DPOAE measurements in neonates and infants result in robust responses in the vast majority of ears for f2 frequencies of at least 2.0, 3.0 and 4.0 kHz. SNRs decrease as frequency decreases, making the measurements less reliable at 1.0 kHz. When considered along with test time, there may be little justification for including an f2 frequency at 1.0 kHz in newborn screening programs. It would appear that DPOAEs result in reliable measurements when tests are conducted in the environments in which babies typically are found. Finally, these data suggest that babies can be tested in those states of arousal that are most commonly encountered in the perinatal period.",cinahl,1960202,10.1097/00003446-200010000-00007
adaptive dynamic range optimization adro a digital amplification strategy for hearing aids and cochlear implants,"395. Trends Amplif. 2005;9(2):77-98. doi: 10.1177/108471380500900203.Adaptive dynamic range optimization (ADRO): a digital amplification strategy for hearing aids and cochlear implants.Blamey PJ(1).Author information:(1)Dynamic Hearing Pty Ltd, Richmond, Victoria, Australia. pblamey@dynamichearing.com.auAdaptive dynamic range optimization (ADRO) is an amplification strategy that uses digital signal processing techniques to improve the audibility, comfort, and intelligibility of sounds for people who use cochlear implants and/or hearing aids. The strategy uses statistical analysis to select the most information-rich section of the input dynamic range in multiple-frequency channels. Fuzzy logic rules control the gain in each frequency channel so that the selected section of the dynamic range is presented at an audible and comfortable level. The ADRO processing thus adaptively optimizes the dynamic range of the signal in multiple-frequency channels. Clinical studies show that ADRO can be fitted easily to all degrees of hearing loss for hearing aids and cochlear implants in a direct and intuitive manner, taking the preferences of the listener into account. The result is high acceptance by new and experienced hearing aid users and strong preferences for ADRO compared with alternative amplification strategies. The ADRO processing is particularly well suited to bimodal and hybrid stimulation which combine electric and acoustic stimulation in opposite ears or in the same ear, respectively.DOI: 10.1177/108471380500900203PMCID: PMC4111489",pubmed,16012705,10.1177/108471380500900203
the production of sstop clusters by preschoolers with hearing loss,"Producing word-initial /s/-stop clusters can be a challenge for English-speaking pre-schoolers. For children with hearing loss (HL), fricatives can be also difficult to perceive, raising questions about their production and representation of /s/-stop clusters. The goal of this study was therefore to determine if pre-schoolers with HL can produce and represent the /s/ in word-initial /s/-stop clusters, and to compare this to their normal hearing (NH) peers. Based on both acoustic and perceptual analysis, we found that children with HL had little /s/-omission, suggesting that their phonological representation of these clusters closely aligns with that of their NH peers.",cinahl,3050009,10.1017/S0305000922000228
pseudohypacusis in childhood and adolescence is associated with increased gray matter volume in the medial frontal gyrus and superior temporal gyrus references,"Pseudohypacusis is a somatoform disorder characterized by hearing loss with discrepancies between pure-tone audiometry and auditory brainstem response (ABR), but the underlying neuronal mechanisms remain unclear. Using voxel-based morphometry (VBM) with magnetic resonance (MR) imaging for 14 unmedicated, right-handed patients and 35 healthy control subjects, we investigated whether functional hearing loss was associated with discernible changes of brain morphology. Group differences in gray matter volume (GMV) were assessed using high-resolution, T1-weighted, volumetric MR imaging datasets (3T Trio scanner; Siemens AG) and analyzed with covariant factors of age, sex, socioeconomic status (SES), and total GMV, which was increased by 27.9% in the left medial frontal gyrus (MFG) (Brodmann area 10) (p =.001, corrected cluster level) and by 14.4% in the right superior temporal gyrus (STG) and the adjacent middle temporal gyrus (MTG) (BA42 to 21) (p =.009, corrected cluster level) in patients with pseudohypacusis. The GMV in the right STG (BA42) and verbal intelligence quotient (IQ) were correlated significantly with the Wechsler Intelligence Scale for Children-Third Edition (WISC-III) (s =-.57, p <.0001) and level of SES (s =-.55, p <.0001). The present findings suggest that the development of the auditory association cortex involved in language processing is affected, causing insufficient pruning during brain development. We therefore assert that differences in the neuroanatomical substrate of pseudohypacusis subjects result from a developmental disorder in auditory processing. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc11&DO=10.1016%2fj.cortex.2010.10.001
detecting cochlear synaptopathy through curvature quantification of the auditory brainstem response,"The sound-evoked electrical compound potential known as auditory brainstem response (ABR) represents the firing of a heterogenous population of auditory neurons in response to sound stimuli, and is often used for clinical diagnosis based on wave amplitude and latency. However, recent ABR applications to detect human cochlear synaptopathy have led to inconsistent results, mainly due to the high variability of ABR wave-1 amplitude. Here, rather than focusing on the amplitude of ABR wave 1, we evaluated the use of ABR wave curvature to detect cochlear synaptic loss. We first compared four curvature quantification methods using simulated ABR waves, and identified that the cubic spline method using five data points produced the most accurate quantification. We next evaluated this quantification method with ABR data from an established mouse model with cochlear synaptopathy. The data clearly demonstrated that curvature measurement is more sensitive and consistent in identifying cochlear synaptic loss in mice compared to the amplitude and latency measurements. We further tested this curvature method in a different mouse model presenting with otitis media. The change in curvature profile due to middle ear infection in otitis media is different from the profile of mice with cochlear synaptopathy. Thus, our study suggests that curvature quantification can be used to address the current ABR variability issue, and may lead to additional applications in the clinic diagnosis of hearing disorders. Copyright © 2022 Bao, Jegede, Hawks, Dade, Guan, Middaugh, Qiu, Levina and Tsai.",scopus,2-s2.0-85127391194,10.3389/fncel.2022.851500
understanding and treating paediatric hearing impairment,"8. EBioMedicine. 2021 Jan;63:103171. doi: 10.1016/j.ebiom.2020.103171. Epub 2021 Jan 7.Understanding and treating paediatric hearing impairment.Wrobel C(1), Zafeiriou MP(2), Moser T(3).Author information:(1)Department of Otolaryngology and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany; Multiscale Bioimaging Cluster of Excellence (MBExC), University of Göttingen, Germany.(2)Multiscale Bioimaging Cluster of Excellence (MBExC), University of Göttingen, Germany; Institute of Pharmacology and Toxicology, University Medical Center, 37075 Göttingen, Germany.(3)Multiscale Bioimaging Cluster of Excellence (MBExC), University of Göttingen, Germany; Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany. Electronic address: tmoser@gwdg.de.Sensorineural hearing impairment is the most frequent form of hearing impairment affecting 1-2 in 1000 newborns and another 1 in 1000 adolescents. More than 50% of congenital hearing impairment is of genetic origin and some forms of monogenic deafness are likely targets for future gene therapy. Good progress has been made in clinical phenotyping, genetic diagnostics, and counselling. Disease modelling, e.g. in transgenic mice, has helped elucidate disease mechanisms underlying genetic hearing impairment and informed clinical phenotyping in recent years. Clinical management of paediatric hearing impairment involves hearing aids, cochlear or brainstem implants, signal-to-noise improvement in educational settings, speech therapy, and sign language. Cochlear implants, for example, have much improved the situation of profoundly hearing impaired and deaf children. Nonetheless there remains a major unmet clinical need for improving hearing restoration. Preclinical studies promise that we will witness clinical trials on gene therapy and a next generation of cochlear implants during the coming decade. Moreover, progress in generating sensory hair cells and neurons from stem cells spurs disease modelling, drug screening, and regenerative approaches. This review briefly summarizes the pathophysiology of paediatric hearing impairment and provides an update on the current preclinical development of innovative approaches toward improved hearing restoration.Copyright © 2020 The Author(s). Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.ebiom.2020.103171PMCID: PMC7808910",pubmed,33422987,10.1016/j.ebiom.2020.103171
lineage analysis of the late otocyst stage mouse inner ear by transuterine microinjection of a retroviral vector encoding alkaline phosphatase and an oligonucleotide library,"782. PLoS One. 2013 Jul 25;8(7):e69314. doi: 10.1371/journal.pone.0069314. Print 2013.Lineage analysis of the late otocyst stage mouse inner ear by transuterine microinjection of a retroviral vector encoding alkaline phosphatase and an oligonucleotide library.Jiang H(1), Wang L, Beier KT, Cepko CL, Fekete DM, Brigande JV.Author information:(1)Department of Otolaryngology, Oregon Hearing Research Center, Oregon Health & Science University, Portland, Oregon, United States of America.The mammalian inner ear subserves the special senses of hearing and balance. The auditory and vestibular sensory epithelia consist of mechanically sensitive hair cells and associated supporting cells. Hearing loss and balance dysfunction are most frequently caused by compromise of hair cells and/or their innervating neurons. The development of gene- and cell-based therapeutics will benefit from a thorough understanding of the molecular basis of patterning and cell fate specification in the mammalian inner ear. This includes analyses of cell lineages and cell dispersals across anatomical boundaries (such as sensory versus nonsensory territories). The goal of this study was to conduct retroviral lineage analysis of the embryonic day 11.5(E11.5) mouse otic vesicle. A replication-defective retrovirus encoding human placental alkaline phosphatase (PLAP) and a variable 24-bp oligonucleotide tag was microinjected into the E11.5 mouse otocyst. PLAP-positive cells were microdissected from cryostat sections of the postnatal inner ear and subjected to nested PCR. PLAP-positive cells sharing the same sequence tag were assumed to have arisen from a common progenitor and are clonally related. Thirty five multicellular clones consisting of an average of 3.4 cells per clone were identified in the auditory and vestibular sensory epithelia, ganglia, spiral limbus, and stria vascularis. Vestibular hair cells in the posterior crista were related to one another, their supporting cells, and nonsensory epithelial cells lining the ampulla. In the organ of Corti, outer hair cells were related to a supporting cell type and were tightly clustered. By contrast, spiral ganglion neurons, interdental cells, and Claudius' cells were related to cells of the same type and could be dispersed over hundreds of microns. These data contribute new information about the developmental potential of mammalian otic precursors in vivo.DOI: 10.1371/journal.pone.0069314PMCID: PMC3723842",pubmed,23935981,10.1371/journal.pone.0069314
acoustic and perceptual effects of magnifying interaural difference cues in a simulated binaural hearing aid,"270. Int J Audiol. 2018 Jun;57(sup3):S81-S91. doi: 10.1080/14992027.2017.1308564. Epub 2017 Apr 10.Acoustic and perceptual effects of magnifying interaural difference cues in a simulated ""binaural"" hearing aid.de Taillez T(1), Grimm G(1), Kollmeier B(1), Neher T(1).Author information:(1)a Medizinische Physik and Cluster of Excellence Hearing4all , Universität Oldenburg , Oldenburg , Germany.OBJECTIVE: To investigate the influence of an algorithm designed to enhance or magnify interaural difference cues on speech signals in noisy, spatially complex conditions using both technical and perceptual measurements. To also investigate the combination of interaural magnification (IM), monaural microphone directionality (DIR), and binaural coherence-based noise reduction (BC).DESIGN: Speech-in-noise stimuli were generated using virtual acoustics. A computational model of binaural hearing was used to analyse the spatial effects of IM. Predicted speech quality changes and signal-to-noise-ratio (SNR) improvements were also considered. Additionally, a listening test was carried out to assess speech intelligibility and quality.STUDY SAMPLE: Listeners aged 65-79 years with and without sensorineural hearing loss (N = 10 each).RESULTS: IM increased the horizontal separation of concurrent directional sound sources without introducing any major artefacts. In situations with diffuse noise, however, the interaural difference cues were distorted. Preprocessing the binaural input signals with DIR reduced distortion. IM influenced neither speech intelligibility nor speech quality.CONCLUSIONS: The IM algorithm tested here failed to improve speech perception in noise, probably because of the dispersion and inconsistent magnification of interaural difference cues in complex environments.DOI: 10.1080/14992027.2017.1308564",pubmed,28395561,10.1080/14992027.2017.1308564
global characteristics and trends of presbycusis research from 2002 to 2021 a bibliometric study,"605. Am J Transl Res. 2023 Apr 15;15(4):2407-2425. eCollection 2023.Global characteristics and trends of presbycusis research from 2002 to 2021: a bibliometric study.Lv H(1), Gao Z(1), Wang Y(1), Xie Y(1), Guan M(1), Liao H(1)(2), Xu Y(1)(2).Author information:(1)Department of Otolaryngology-Head and Neck Surgery, Renmin Hospital of Wuhan University Wuhan 430060, Hubei, China.(2)Research Institute of Otolaryngology-Head and Neck Surgery, Renmin Hospital of Wuhan University Wuhan 430060, Hubei, China.BACKGROUND AND OBJECTIVE: Age-related hearing loss, also termed presbycusis, is the most prevalent sensory impairment in older adults. Presbycusis research has considerably advanced over the past few decades, however, comprehensive and objective reports on the current state of presbycusis research are lacking. We used bibliometric methods to objectively analyzed the progress of presbycusis research over the past 20 years and to identify the research hotspots and emerging trends in this field.METHODS: Eligible literature metadata published between 2002 and 2021 were obtained from the Web of Science Core Collection on September 1, 2022. Bibliometric tools including CiteSpace, VOSviewer, Bibliometrix R Package, Microsoft Excel 2019, and an online bibliometric platform were used to conduct bibliometric and visualized analyses.RESULTS: A total of 1,693 publications related to presbycusis were retrieved. The number of publications increased continuously from 2002 to 2021, and the USA occupied the lead position in the field, with the highest research output. The most productive and influential institution, author, and journal were the University of California, Frisina DR of the University of South Florida, and Hearing Research, respectively. Co-citation cluster and trend topics analyses revealed that ""cochlear synaptopathy"", ""oxidative stress"", and ""dementia"" were the predominant foci of presbycusis research. Burst detection of keywords indicated that ""auditory cortex"" and ""Alzheimer's disease"" were the newly-emerged aspects.CONCLUSION: During the past two decades, presbycusis research has been flourishing. The current research foci are ""cochlear synaptopathy"", ""oxidative stress"", and ""dementia"". ""Auditory cortex"" and ""Alzheimer's disease"" may be potential future directions in this field. This bibliometric analysis represents the first quantitative overview of presbycusis research, thus providing valuable references and insights for scholars, medical practitioners, and policymakers concerned with this field.AJTR Copyright © 2023.PMCID: PMC10182487",pubmed,37193136,
can realear insertion gain deviations from generic fitting prescriptions predict selfreported outcomes,"The aim of this study was to determine whether the differences in insertion gains from the first fit to generic prescriptions of hearing aids can predict the self-reported hearing aid (HA) outcomes for first-time and experienced HA users. This was a prospective observational study. The study included 885 first-time and 330 experienced HA users with a valid real-ear measurement on both ears and answers to the abbreviated version of the Speech, Spatial, and Quality of Hearing (SSQ12) and the International Outcome Inventory for Hearing Aids (IOI-HA) questionnaires. K-means clustering of gain differences between individual real-ear insertion gain to three generic gain prescriptions (NAL-NL2, NAL-RP, and one-third gain rules) was performed. The gain difference at higher frequencies generally differentiated the clusters. The experienced users in the cluster with fittings closest to NAL-NL2 and NAL-RP prescription were found to exhibit a higher IOI-HA Factor 1 score (representing the overall benefit of the hearing aid use). The gain differences to generic prescription did not affect other self-reported outcomes for first-time and experienced HA users. The experienced HA users with minimal gain deviations from generic prescriptions reported better self-perceived benefits than users with larger deviations. However, this was not apparent in first-time users.",cinahl,14992027,10.1080/14992027.2022.2053594
white matter integrity associated with clinical symptoms in tinnitus patients a tractbased spatial statistics study,"Objective: To assess the relationship between white matter (WM) integrity and clinical variables in tinnitus patients using diffusion tensor imaging (DTI).Methods: Sixty-seven tinnitus patients and 39 healthy controls were enrolled in this study. The tinnitus duration, laterality, pitch and characteristics, and two psychological self-rating tests were used as independent variables. Differences between patients and controls in diffusion indices were evaluated using tract-based spatial statistics (TBSS), and multiple regression between DTI values in significant clusters and clinical variables was investigated. TBSS correlation analysis between the clinical variables and DTI indices was performed in tinnitus patients.Results: The tinnitus group had higher mean diffusivity (MD) and axial diffusivity in WM under the auditory cortex and limbic system compared with control group. Depression symptom score (BDI) was the only significant variable affecting MD and axial diffusivity value in these clusters. TBSS correlation analysis with BDI in tinnitus patients showed BDI was associated with diffusion indices in widespread regions of WM.Conclusions: WM integrity in tinnitus was associated with depression symptoms in both inter- and intragroup analyses. Our results support the hypothesized implication of altered WM integrity in the physiopathology of emotional symptoms of tinnitus.Key Points: • WM integrity of left auditory-limbic circuit in tinnitus is different in controls. • Depression symptoms are a significant clinical variable affecting DTI values. • DTI value is correlated with depression symptoms in tinnitus patients.",cinahl,9387994,10.1007/s00330-015-4034-3
generalizable sampleefficient siamese autoencoder for tinnitus diagnosis in listeners with subjective tinnitus,"Electroencephalogram (EEG)-based neurofeedback has been widely studied for tinnitus therapy in recent years. Most existing research relies on experts' cognitive prediction, and studies based on machine learning and deep learning are either data-hungry or not well generalizable to new subjects. In this paper, we propose a robust, data-efficient model for distinguishing tinnitus from the healthy state based on EEG-based tinnitus neurofeedback. We propose trend descriptor, a feature extractor with lower fineness, to reduce the effect of electrode noises on EEG signals, and a siamese encoder-decoder network boosted in a supervised manner to learn accurate alignment and to acquire high-quality transferable mappings across subjects and EEG signal channels. Our experiments show the proposed method significantly outperforms state-of-the-art algorithms when analyzing subjects' EEG neurofeedback to 90dB and 100dB sound, achieving an accuracy of 91.67%-94.44% in predicting tinnitus and control subjects in a subject-independent setting. Our ablation studies on mixed subjects and parameters show the method's stability in performance. © 2001-2011 IEEE.",scopus,2-s2.0-85111742420,10.1109/TNSRE.2021.3095298
cochlear ribbon synapses in aged gerbils,"688. Int J Mol Sci. 2024 Feb 27;25(5):2738. doi: 10.3390/ijms25052738.Cochlear Ribbon Synapses in Aged Gerbils.Bovee S(1), Klump GM(1)(2)(3), Pyott SJ(4), Sielaff C(1)(5), Köppl C(1)(2)(3).Author information:(1)Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky Universität Oldenburg, 26129 Oldenburg, Germany.(2)Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, 26129 Oldenburg, Germany.(3)Research Centre Neurosensory Science, Carl von Ossietzky Universität Oldenburg, 26129 Oldenburg, Germany.(4)Department of Otorhinolaryngology/Head and Neck Surgery, University Medical Center Groningen, University of Groningen, P.O. Box 30.001, 9700 RB Groningen, The Netherlands.(5)Fraunhofer Institute for Toxicology and Experimental Medicine (ITEM), 30625 Hannover, Germany.In mammalian hearing, type-I afferent auditory nerve fibers comprise the basis of the afferent auditory pathway. They are connected to inner hair cells of the cochlea via specialized ribbon synapses. Auditory nerve fibers of different physiological types differ subtly in their synaptic location and morphology. Low-spontaneous-rate auditory nerve fibers typically connect on the modiolar side of the inner hair cell, while high-spontaneous-rate fibers are typically found on the pillar side. In aging and noise-damaged ears, this fine-tuned balance between auditory nerve fiber populations can be disrupted and the functional consequences are currently unclear. Here, using immunofluorescent labeling of presynaptic ribbons and postsynaptic glutamate receptor patches, we investigated changes in synaptic morphology at three different tonotopic locations along the cochlea of aging gerbils compared to those of young adults. Quiet-aged gerbils showed about 20% loss of afferent ribbon synapses. While the loss was random at apical, low-frequency cochlear locations, at the basal, high-frequency location it almost exclusively affected the modiolar-located synapses. The subtle differences in volumes of pre- and postsynaptic elements located on the inner hair cell's modiolar versus pillar side were unaffected by age. This is consistent with known physiology and suggests a predominant, age-related loss in the low-spontaneous-rate auditory nerve population in the cochlear base, but not the apex.DOI: 10.3390/ijms25052738PMCID: PMC10931817",pubmed,38473985,10.3390/ijms25052738
towards a joint reflectiondistortion otoacoustic emission profile results in normal and impaired ears,"296. J Acoust Soc Am. 2017 Aug;142(2):812. doi: 10.1121/1.4996859.Towards a joint reflection-distortion otoacoustic emission profile: Results in normal and impaired ears.Abdala C(1), Kalluri R(1).Author information:(1)Caruso Department of Otolaryngology, Auditory Research Center, University of Southern California, 1640 Marengo Street, Suite 326, Los Angeles, California 90033, USA.Otoacoustic emissions (OAEs) provide salient information about cochlear function and dysfunction. Two broad classes of emissions, linear reflection and nonlinear distortion, arise via distinct cochlear processes and hence, appear to provide independent information about cochlear health and hearing. Considered in combination, these two OAE types may characterize sensory hearing loss most effectively. In this study, the level-dependent growth of stimulus-frequency OAEs (a reflection-type emission) and distortion-product OAEs (a distortion-type emission) were measured in ten normal-hearing ears and eight ears with slight-to-moderate sensorineural hearing loss. Metrics of OAE strength and compression were derived from OAE input/output functions and then considered in a combined fashion. Results indicate that SFOAEs and DPOAEs differ significantly in their strength and compression features. When SFOAE and DPOAE metrics are displayed together on a two-dimensional plot, relatively well-defined data clusters describe their normative relationship. In hearing-impaired ears, this relationship is disrupted but not in a uniform way across ears; ears with similar audiograms showed differently altered joint-OAE profiles. Hearing loss sometimes affected only one OAE or one more than the other. Results suggest a joint-OAE profile is promising and warrants study in a large group of subjects with sensory hearing loss of varied etiologies.DOI: 10.1121/1.4996859PMCID: PMC5552396",pubmed,28863614,10.1121/1.4996859
sign and machine language recognition for physically impaired individuals,"Sign language is a type of communication used by people who are deaf or hard of hearing. Disabled People use sign language gestures as a non-verbal communication tool to express their feelings and thoughts to other people. It's tough to converse with those who are deaf or hard of hearing. Deaf and mute persons communicate via hand gesture sign language, which makes it difficult for non-deaf and mute people to understand their language. As a result, technologies that recognise numerous signs and communicate the data to regular people are needed. However, because these ordinary people have a hard time understanding their expressions, experienced sign language experts are required for medical and legal appointments, as well as educational and training sessions. There has been an upsurge in demand for these services in recent years. Other types of services, such as video remote human interpreter using a high-speed Internet connection, have been established, providing an easy-to-use sign language interpreter service that may be utilised and benefited, but with significant limits. In order to address this issue, the artificial intelligence technology can be implemented to analyze user’s hand with finger detection. A novel system has been suggested to design the vision-based system in real time environments. Then the Convolutional Neural Network (CNN) algorithm is used to classify the sign language and provide the label about recognized sign with voice alert.",ieee,,10.1109/ICESC54411.2022.9885433
generation method of hierarchical pronunciation font for hearingimpaired children based on artificial intelligence,"Setting up personalized hierarchical pronunciation font for hearing-impaired children is one of the most important segments when designing individualized rehabilitation teaching. Creating an individual Chinese pronunciation training word stock is a big challenge in speech rehabilitation training for children with hearing impairment. This research applies the speech recognition technology and Chinese syllable classification theory, and proposes a convenient method of automatic generation of personalized hierarchical word base. The method includes first analyzing the initials and vowel of children's test pronunciation by calling the API of artificial intelligence speech recognition. Subsequently, on the basis of the established multi-dimensional information table of pronunciation classification of Mandarin initials and vowel, the multi-level pronunciation ability of the initials and vowel tested by children is rated. As a result, the datasets of initials and vowel of different levels are obtained. and according to the combination of initials and vowel of different levels, it is convenient to generate a personalized graded Chinese character library. Finaly, The effectiveness of this method is proved by a typical example.",ieee,,10.1109/AEECA49918.2020.9213534
personalization of hearing aid compression by humanintheloop deep reinforcement learning,Existing prescriptive compression strategies used in hearing aid fitting are designed based on gain averages from a group of users which may not be necessarily optimal for a specific user. Nearly half of hearing aid users prefer settings that differ from the commonly prescribed settings. This paper presents a human-in-the-loop deep reinforcement learning approach that personalizes hearing aid compression to achieve improved hearing perception. The developed approach is designed to learn a specific user's hearing preferences in order to optimize compression based on the user's feedbacks. Both simulation and subject testing results are reported. These results demonstrate the proof-of-concept of achieving personalized compression via human-in-the-loop deep reinforcement learning.,ieee,2169-3536,10.1109/ACCESS.2020.3035728
brain volume differences associated with hearing impairment in adults,"580. Trends Hear. 2018 Jan-Dec;22:2331216518763689. doi: 10.1177/2331216518763689.Brain Volume Differences Associated With Hearing Impairment in Adults.Alfandari D(1)(2), Vriend C(3)(4)(5), Heslenfeld DJ(6), Versfeld NJ(1)(2), Kramer SE(1)(2), Zekveld AA(1)(2)(7).Author information:(1)1 Department of Otolaryngology-Head and Neck Surgery, Section Ear & Hearing, VU University Medical Center, Amsterdam, the Netherlands.(2)2 Amsterdam Public Health Research Institute, VU University Medical Center, the Netherlands.(3)3 Department of Anatomy & Neurosciences, VU University Medical Center, Amsterdam, the Netherlands.(4)4 Department of Psychiatry, VU University Medical Center, Amsterdam, the Netherlands.(5)5 Amsterdam Neuroscience, Amsterdam, the Netherlands.(6)6 Department of Psychology, VU University, Amsterdam, the Netherlands.(7)7 Department of Behavioural Sciences and Learning, Linnaeus Centre HEAD, The Swedish Institute for Disability Research, Linköping University, Sweden.Speech comprehension depends on the successful operation of a network of brain regions. Processing of degraded speech is associated with different patterns of brain activity in comparison with that of high-quality speech. In this exploratory study, we studied whether processing degraded auditory input in daily life because of hearing impairment is associated with differences in brain volume. We compared T1-weighted structural magnetic resonance images of 17 hearing-impaired (HI) adults with those of 17 normal-hearing (NH) controls using a voxel-based morphometry analysis. HI adults were individually matched with NH adults based on age and educational level. Gray and white matter brain volumes were compared between the groups by region-of-interest analyses in structures associated with speech processing, and by whole-brain analyses. The results suggest increased gray matter volume in the right angular gyrus and decreased white matter volume in the left fusiform gyrus in HI listeners as compared with NH ones. In the HI group, there was a significant correlation between hearing acuity and cluster volume of the gray matter cluster in the right angular gyrus. This correlation supports the link between partial hearing loss and altered brain volume. The alterations in volume may reflect the operation of compensatory mechanisms that are related to decoding meaning from degraded auditory input.DOI: 10.1177/2331216518763689PMCID: PMC5863860",pubmed,29557274,10.1177/2331216518763689
glucose hypometabolism in the auditory pathway in age related hearing loss in the adni cohort,"710. Neuroimage Clin. 2021;32:102823. doi: 10.1016/j.nicl.2021.102823. Epub 2021 Sep 21.Glucose hypometabolism in the Auditory Pathway in Age Related Hearing Loss in the ADNI cohort.Zainul Abidin FN(1), Scelsi MA(2), Dawson SJ(3), Altmann A(4); Alzheimer's Disease Neuroimaging Initiative.Author information:(1)UCL Ear Institute, University College London, London, UK; Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, UK.(2)Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, UK.(3)UCL Ear Institute, University College London, London, UK.(4)Centre for Medical Image Computing, Department of Medical Physics and Biomedical Engineering, University College London, London, UK. Electronic address: a.altmann@ucl.ac.uk.PURPOSE: Hearing loss (HL) is one of the most common age-related diseases. Here, we investigate the central auditory correlates of HL in people with normal cognition and mild cognitive impairment (MCI) and test their association with genetic markers with the aim of revealing pathogenic mechanisms.METHODS: Brain glucose metabolism based on FDG-PET, self-reported HL status, and genetic data were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. FDG-PET data was analysed from 742 control subjects (non-HL with normal cognition or MCI) and 162 cases (HL with normal cognition or MCI) with age ranges of 72.2 ± 7.1 and 77.4 ± 6.4, respectively. Voxel-wise statistics of FDG uptake differences between cases and controls were computed using the generalised linear model in SPM12. An additional 1515 FDG-PET scans of 618 participants were analysed using linear mixed effect models to assess longitudinal HL effects. Furthermore, a quantitative trait genome-wide association study (GWAS) was conducted on the glucose uptake within regions of interest (ROIs), which were defined by the voxel-wise comparison, using genotyping data with 5,082,878 variants available for HL cases and HL controls (N = 817).RESULTS: The HL group exhibited hypometabolism in the bilateral Heschl's gyrus (kleft = 323; kright = 151; Tleft = 4.55; Tright = 4.14; peak Puncorr < 0.001), the inferior colliculus (k = 219;T = 3.53; peak Puncorr < 0.001) and cochlear nucleus (k = 18;T = 3.55; peak Puncorr < 0.001) after age correction and using a cluster forming height threshold P < 0.005 (FWE-uncorrected). Moreover, in an age-matched subset, the cluster comprising the left Heschl's gyrus survived the FWE-correction (kleft = 1903; Tleft = 4.39; cluster PFWE-corr = 0.001). The quantitative trait GWAS identified no genome-wide significant locus in the three HL ROIs. However, various loci were associated at the suggestive threshold (p < 1e-05).CONCLUSION: Compared to the non-HL group, glucose metabolism in the HL group was lower in the auditory cortex, the inferior colliculus, and the cochlear nucleus although the effect sizes were small. The GWAS identified candidate genes that might influence FDG uptake in these regions. However, the specific biological pathway(s) underlying the role of these genes in FDG-hypometabolism in the auditory pathway requires further investigation.Copyright © 2021 The Author(s). Published by Elsevier Inc. All rights reserved.DOI: 10.1016/j.nicl.2021.102823PMCID: PMC8503577",pubmed,34624637,10.1016/j.nicl.2021.102823
speech audiometry at home automated listening tests via smart speakers with normalhearing and hearingimpaired listeners,"128. Trends Hear. 2020 Jan-Dec;24:2331216520970011. doi: 10.1177/2331216520970011.Speech Audiometry at Home: Automated Listening Tests via Smart Speakers With Normal-Hearing and Hearing-Impaired Listeners.Ooster J(1)(2), Krueger M(2)(3), Bach JH(2)(3)(4), Wagener KC(2)(3)(4), Kollmeier B(2)(3)(4)(5), Meyer BT(1)(2)(3).Author information:(1)Communication Acoustics, Carl von Ossietzky Universität, Oldenburg, Germany.(2)Cluster of Excellence Hearing4all, Germany.(3)HörTech gGmbH, Oldenburg, Germany.(4)Hörzentrum GmbH, Oldenburg, Germany.(5)Medizinische Physik, Carl von Ossietzky Universität, Oldenburg, Germany.Speech audiometry in noise based on sentence tests is an important diagnostic tool to assess listeners' speech recognition threshold (SRT), i.e., the signal-to-noise ratio corresponding to 50% intelligibility. The clinical standard measurement procedure requires a professional experimenter to record and evaluate the response (expert-conducted speech audiometry). The use of automatic speech recognition enables self-conducted measurements with an easy-to-use speech-based interface. This article compares self-conducted SRT measurements using smart speakers with expert-conducted laboratory measurements. With smart speakers, there is no control over the absolute presentation level, potential errors from the automated response logging, and room acoustics. We investigate the differences between highly controlled measurements in the laboratory and smart speaker-based tests for young normal-hearing (NH) listeners as well as for elderly NH, mildly and moderately hearing-impaired listeners in low, medium, and highly reverberant room acoustics. For the smart speaker setup, we observe an overall bias in the SRT result that depends on the hearing loss. The bias ranges from +0.7 dB for elderly moderately hearing-impaired listeners to +2.2 dB for young NH listeners. The intrasubject standard deviation is close to the clinical standard deviation (0.57/0.69 dB for the young/elderly NH compared with 0.5 dB observed for clinical tests and 0.93/1.09 dB for the mild/moderate hearing-impaired listeners compared with 0.9 dB). For detecting a clinically elevated SRT, the speech-based test achieves an area under the curve value of 0.95 and therefore seems promising for complementing clinical measurements.DOI: 10.1177/2331216520970011PMCID: PMC7720343",pubmed,33272109,10.1177/2331216520970011
vowel perception experiments with a singleelectrode cochlear implantdp   jun 1986,"15 postlingually deaf adults (aged 19-79 yrs) with histories of unsuccessful hearing aid use and a minimum of 6-22 mo experience with a single-electrode cochlear implant were presented with audiorecordings of 11 natural and loudness-matched vowels. Ss rated the dissimilarity of both the naturally spoken and the loudness-matched vowels and performed identification of the latter. Two normal-hearing Ss served as controls for the dissimilarity tasks. Multidimensional scaling, hierarchical clustering, and percent correct identification analyses were used to help determine the perceptual features used by the Ss in their judgments. Data indicate that, in general, normal-hearing Ss took advantage of 2nd formant frequency information. Cochlear-implant Ss relied primarily on fundamental and 1st formant frequency information and demonstrated difficulty in vowel identification. Appendices include audiometric data and descriptive data for the implant Ss, analog unprocessed waveforms of the stimuli, and averaged dissimilarity ratings of the loudness-matched vowels by the Ss. (23 ref) (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc2&DO=10.1044%2fjshr.2902.179
auditory feedback modulates development of kitten vocalizations,"569. Cell Tissue Res. 2015 Jul;361(1):279-94. doi: 10.1007/s00441-014-2059-6. Epub 2014 Dec 19.Auditory feedback modulates development of kitten vocalizations.Hubka P(1), Konerding W, Kral A.Author information:(1)Institute of AudioNeuroTechnology and Department of Experimental Otology, ENT Clinics, Cluster of Excellence 'Hearing4all', Hannover Medical School, Feodor-Lynen-Str. 35, 30175, Hannover, Germany.Effects of hearing loss on vocal behavior are species-specific. To study the impact of auditory feedback on feline vocal behavior, vocalizations of normal-hearing, hearing-impaired (white) and congenitally deaf (white) cats were analyzed at around weaning age. Eleven animals were placed in a soundproof booth for 30 min at different ages, from the first to the beginning of the fourth postnatal month, every 2 weeks of life. In total, 13,874 vocalizations were analyzed using an automated procedure. Firstly, vocalizations were detected and segmented, with voiced and unvoiced vocalizations being differentiated. The voiced isolation calls ('meow') were further analyzed. These vocalizations showed developmental changes affecting several parameters in hearing controls, whereas the developmental sequence was delayed in congenitally deaf cats. In hearing-impaired and deaf animals, we observed differences both in vocal behavior (loudness and duration) and in the calls' acoustic structure (fundamental frequency and higher harmonics). The fundamental frequency decreased with age in all groups, most likely due to maturation of the vocal apparatus. In deaf cats, however, other aspects of the acoustic structure of the vocalizations did not fully mature. The harmonic ratio (i.e., frequency of first harmonic divided by fundamental frequency) was higher and more variable in deaf cats than in the other study groups. Auditory feedback thus affects the acoustic structure of vocalizations and their ontogenetic development. The study suggests that both the vocal apparatus and its neuronal motor control are subject to maturational processes, whereas the latter is additionally dependent on auditory feedback in cats.DOI: 10.1007/s00441-014-2059-6PMCID: PMC4487352",pubmed,25519045,10.1007/s00441-014-2059-6
audiologistpatient communication profiles in hearing rehabilitation appointments,"576. Patient Educ Couns. 2017 Aug;100(8):1490-1498. doi: 10.1016/j.pec.2017.03.022. Epub 2017 Mar 18.Audiologist-patient communication profiles in hearing rehabilitation appointments.Meyer C(1), Barr C(2), Khan A(3), Hickson L(4).Author information:(1)HEARing CRC, Australia; School of Health and Rehabilitation Sciences, University of Queensland, Brisbane, Australia. Electronic address: carlyjmeyer@outlook.com.(2)HEARing CRC, Australia; Department of Audiology and Speech Pathology, University of Melbourne, Melbourne, Australia. Electronic address: barrcm@unimelb.edu.au.(3)HEARing CRC, Australia; School of Health and Rehabilitation Sciences, University of Queensland, Brisbane, Australia. Electronic address: a.khan2@uq.edu.au.(4)HEARing CRC, Australia; School of Health and Rehabilitation Sciences, University of Queensland, Brisbane, Australia. Electronic address: l.hickson@uq.edu.au.OBJECTIVE: To profile the communication between audiologists and patients in initial appointments on a biomedical-psychosocial continuum; and explore the associations between these profiles and 1) characteristics of the appointment and 2) patients' decisions to pursue hearing aids.METHODS: Sixty-three initial hearing assessment appointments were filmed and audiologist-patient communication was coded using the Roter Interaction Analysis System. A hierarchical cluster analysis was conducted to profile audiologist-patient communication, after which regression modelling and Chi-squared analyses were conducted.RESULTS: Two distinct audiologist-patient communication profiles were identified during both the history taking phase (46=biopsychosocial profile, 15=psychosocial profile) and diagnosis and management planning phase (45=expanded biomedical profile, 11=narrowly biomedical profile). Longer appointments were significantly more likely to be associated with an expanded biomedical interaction during the diagnosis and management planning phase. No significant associations were found between audiologist-patient communication profile and patients' decisions to pursue hearing aids.CONCLUSION: Initial audiology consultations appear to remain clinician-centred. Three quarters of appointments began with a biopsychosocial interaction; however, 80% ended with an expanded biomedical interaction.PRACTICE IMPLICATIONS: Findings suggest that audiologists could consider modifying their communication in initial appointments to more holistically address the needs of patients.Copyright © 2017 Elsevier B.V. All rights reserved.DOI: 10.1016/j.pec.2017.03.022",pubmed,28372897,10.1016/j.pec.2017.03.022
inferring hearing loss from learned speech kernels,"Does a hearing-impaired individual's speech reflect his hearing loss, and if it does, can the nature of hearing loss be inferred from his speech? To investigate these questions, at least four hours of speech data were recorded from each of 37 adult individuals, both male and female, belonging to four classes: 7 normal, and 30 severely-to-profoundly hearing impaired with high, medium or low speech intelligibility. Acoustic kernels were learned for each individual by capturing the distribution of his speech data points represented as 20 ms duration windows. These kernels were evaluated using a set of neurophysiological metrics, namely, distribution of characteristic frequencies, equal loudness contour, bandwidth and Q10 value of tuning curve. Our experimental results reveal that a hearing-impaired individual's speech does reflect his hearing loss provided his loss of hearing has considerably affected the intelligibility of his speech. For such individuals, the lack of tuning in any frequency range can be inferred from his learned speech kernels.",ieee,,10.1109/ICMLA.2016.0014
monosyllable speech perception of japanese hearing aid users with prelingual hearing loss implications for surgical indication of cochlear implant,"383. Int J Pediatr Otorhinolaryngol. 2003 Oct;67(10):1061-7. doi: 10.1016/s0165-5876(03)00187-3.Monosyllable speech perception of Japanese hearing aid users with prelingual hearing loss: implications for surgical indication of cochlear implant.Fukuda S(1), Fukushima K, Toida N, Tsukamura K, Maeda Y, Kibayashi N, Nagayasu R, Orita Y, Kasai N, Kataoka Y, Nishizaki K.Author information:(1)Auditory Center for Hearing Impaired Children, Kanariya Gakuen, Okayama, Japan.OBJECTIVE: The monosyllable speech perception ability after years of educational intervention was compared between prelingually deafened pediatric hearing aid users and their cochlear implant counterparts.DESIGN: An open-set monosyllabic speech perception test was conducted on all subjects. The test required subjects to indicate a corresponding Japanese character to that spoken by the examiner. Fifty-two subjects with prelingual hearing impairment (47 hearing aid users and 5 cochlear implant users) were examined.RESULTS: Hearing aid users with average pure-tone thresholds less than 90 dB HL demonstrated generally better monosyllable perception than 70%, which was equivalent or better performance than that of the cochlear implant group. Widely dispersed speech perception was observed within the 90-99 dB HL hearing-aid user group with most subjects demonstrating less than 50% speech perception. In the cluster of >100 dB HL, few cases demonstrated more than 50% in speech perception. The perception ability of the vowel part of each mora within the cochlear implant group was 100% and corresponding to that of hearing aid users with moderate and severe hearing loss.CONCLUSION: Hearing ability among cochlear implant users can be comparable with that of hearing aid users with average unaided pure-tone thresholds of 90 dB HL, after monosyllabic speech perception testing was performed.DOI: 10.1016/s0165-5876(03)00187-3",pubmed,14550959,10.1016/s0165-5876(03)00187-3
conception and design of an automated insertion tool for cochlear implants,"630. Annu Int Conf IEEE Eng Med Biol Soc. 2008;2008:5593-6. doi: 10.1109/IEMBS.2008.4650482.Conception and design of an automated insertion tool for cochlear implants.Hussong A(1), Rau T, Eilers H, Baron S, Heimann B, Leinung M, Lenarz T, Majdani O.Author information:(1)Institute of Robotics, Leibniz Universität Hannover, Germany.Cochlear implants (CI) are electronic devices incorporating an electrode inserted into the human cochlea for direct electric stimulation of the auditory nerve. The implantation has become the standard treatment for patients with severe-to-profound sensorineural loss not aidable with conventional hearing aids. The state of the art operative technique is a facial recess approach to the middle ear, following the opening of the scala tympani (cochleostomy) and insertion of the electrode array. The facial recess approach is applicable only by experienced surgeons and optimal CI results primarily depend on optimal electrode placement and minimal traumatic insertion. This also requires a certain amount of experience. Additionally several groups work on minimally-invasive approaches to the cochlea, resulting in the necessity to insert the implant via a keyhole access, which is not applicable with current techniques. This paper presents a mechatronic device for an automated insertion of the electrode array of a cochlear implant system. Being designed especially for minimally-invasive approaches, the tool is also applicable for regular facial recess approaches. Moreover the device allows reliable and repeatable insertion studies at synthetic models or cadaver specimen. The functionality of the tool is proofed with first experiments on a synthetic model.DOI: 10.1109/IEMBS.2008.4650482",pubmed,19163985,10.1109/IEMBS.2008.4650482
salicylateinduced cochlear impairments cortical hyperactivity and retuning and tinnitus references,"High doses of sodium salicylate (SS) have long been known to induce temporary hearing loss and tinnitus, effects attributed to cochlear dysfunction. However, our recent publications reviewed here show that SS can induce profound, permanent, and unexpected changes in the cochlea and central nervous system. Prolonged treatment with SS permanently decreased the cochlear compound action potential (CAP) amplitude in vivo. In vitro, high dose SS resulted in a permanent loss of spiral ganglion neurons and nerve fibers, but did not damage hair cells. Acute treatment with high-dose SS produced a frequency-dependent decrease in the amplitude of distortion product otoacoustic emissions and CAP. Losses were greatest at low and high frequencies, but least at the mid-frequencies (10-20 kHz), the mid-frequency band that corresponds to the tinnitus pitch measured behaviorally. In the auditory cortex, medial geniculate body and amygdala, high-dose SS enhanced sound-evoked neural responses at high stimulus levels, but it suppressed activity at low intensities and elevated response threshold. When SS was applied directly to the auditory cortex or amygdala, it only enhanced sound evoked activity, but did not elevate response threshold. Current source density analysis revealed enhanced current flow into the supragranular layer of auditory cortex following systemic SS treatment. Systemic SS treatment also altered tuning in auditory cortex and amygdala; low frequency and high frequency multiunit clusters up-shifted or down-shifted their characteristic frequency into the 10-20 kHz range thereby altering auditory cortex tonotopy and enhancing neural activity at mid-frequencies corresponding to the tinnitus pitch. These results suggest that SS-induced hyperactivity in auditory cortex originates in the central nervous system, that the amygdala potentiates these effects and that the SS-induced tonotopic shifts in auditory cortex, the putative neural correlate of tinnitus, arises from the interaction between the frequency-dependent losses in the cochlea and hyperactivity in the central nervous system. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc12&DO=10.1016%2fj.heares.2012.11.016
the effect of agerelated hearing loss and listening effort on resting state connectivity,"64. Sci Rep. 2019 Feb 20;9(1):2337. doi: 10.1038/s41598-019-38816-z.The effect of age-related hearing loss and listening effort on resting state connectivity.Rosemann S(1)(2), Thiel CM(3)(4).Author information:(1)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl-von-Ossietzky Universität Oldenburg, Oldenburg, Germany. Stephanie.rosemann@uni-oldenburg.de.(2)Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany. Stephanie.rosemann@uni-oldenburg.de.(3)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl-von-Ossietzky Universität Oldenburg, Oldenburg, Germany.(4)Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany.Age-related hearing loss is associated with a decrease in hearing abilities for high frequencies. This increases not only the difficulty to understand speech but also the experienced listening effort. Task based neuroimaging studies in normal-hearing and hearing-impaired participants show an increased frontal activation during effortful speech perception in the hearing-impaired. Whether the increased effort in everyday listening in hearing-impaired even impacts functional brain connectivity at rest is unknown. Nineteen normal-hearing and nineteen hearing-impaired participants with mild to moderate hearing loss participated in the study. Hearing abilities, listening effort and resting state functional connectivity were assessed. Our results indicate no differences in functional connectivity between hearing-impaired and normal-hearing participants. Increased listening effort, however, was related to significantly decreased functional connectivity between the dorsal attention network and the precuneus and superior parietal lobule as well as between the auditory and the inferior frontal cortex. We conclude that already mild to moderate age-related hearing loss can impact resting state functional connectivity. It is however not the hearing loss itself but the individually perceived listening effort that relates to functional connectivity changes.DOI: 10.1038/s41598-019-38816-zPMCID: PMC6382886",pubmed,30787339,10.1038/s41598-019-38816-z
ptsd is associated with selfperceived hearing handicap an evaluation of comorbidities in veterans with normal audiometric thresholds,"728. J Am Acad Audiol. 2023 Jan 19. doi: 10.1055/a-2015-8524. Online ahead of print.PTSD is associated with self-perceived hearing handicap: An evaluation of comorbidities in Veterans with normal audiometric thresholds.Jedlicka D(1)(2), Zhen L(3)(4).Author information:(1)Audiology, VA Pittsburgh Healthcare System, Pittsburgh, United States.(2)University of Pittsburgh, Pittsburgh, United States.(3)Communication Science and Disorders, University of Pittsburgh, Pittsburgh, United States.(4)VA Pittsburgh Healthcare System, Pittsburgh, United States.Background Cases of self-reported hearing difficulty despite normal audiometric results have risen with the return of Veterans from recent conflicts in Operation Enduring Freedom, Operation Iraqi Freedom, and Operation New Dawn. Auditory outcomes improved despite low compliance among those receiving treatment. Medical chart data appeared more comprehensive for Veterans with, rather than without, auditory complaints. One possibility is that self-reported hearing problems are associated with a subset of these comorbidities, the treatment of which improved auditory outcomes. Purpose This study examined the relationships between Veterans' self-reported auditory problems and other diagnosed medical conditions. Research Design A retrospective chart review was used. Study Sample Participants were 286 Veterans, aged 21 - 52 with normal hearing. Veterans were dichotomized into a group with either self-reported hearing complaints (n = 143) or an aged-matched control group with no auditory complaints (n = 143). Data Collection and Analysis A query of the Computerized Patient Record System was performed with the date range restricted to 2009 to 2018. Metrics of self-perceived hearing handicap, APD testing, and hearing aid use were collected. All diagnoses and related symptoms were recorded. A best subsets regression with principled model selection was performed to investigate the role of these comorbidities on self-perceived hearing loss. Results The Self-Report group had 16 comorbidities that were classified as prevalent, having occurred in ≥33.3% of the group, compared to the age-matched control group, which had 2 comorbidities. The number of diagnosed medical conditions was associated with self-perceived hearing impairment. Specifically, posttraumatic stress disorder (PTSD) and related symptom clusters constituted the largest group of comorbidities that were significantly associated with self-reported hearing problems. Conclusions The significant association between PTSD and self-perceived hearing impairment warrants investigations on whether treatment of PTSD would reduce perceived hearing handicap severity. Further, PTSD assessments could be useful for audiologists to identify potential candidates for auditory complaints with normal audiometric thresholds. Keywords: Auditory processing disorder, hidden hearing loss, comorbidities, Veterans, posttraumatic stress disorder, traumatic brain injury Abbreviations: APD, auditory processing disorder; CAP, central auditory processing; HHIA, Hearing Handicap Inventory for Adults; mTBI, mild traumatic brain injury; PTSD, posttraumatic stress disorder; TBI, traumatic brain injury; U.S., United States.American Academy of Audiology. This article is published by Thieme.DOI: 10.1055/a-2015-8524",pubmed,36657469,10.1055/a-2015-8524
cellular cartography of the organ of corti based on optical tissue clearing and machine learning,"239. Elife. 2019 Jan 18;8:e40946. doi: 10.7554/eLife.40946.Cellular cartography of the organ of Corti based on optical tissue clearing and machine learning.Urata S(1)(2), Iida T(1), Yamamoto M(3), Mizushima Y(2), Fujimoto C(2), Matsumoto Y(2), Yamasoba T(2), Okabe S(1).Author information:(1)Department of Cellular Neurobiology, Graduate School of Medicine, The University of Tokyo, Tokyo, Japan.(2)Department of Otolaryngology, Graduate School of Medicine, The University of Tokyo, Tokyo, Japan.(3)Department of Nephrology, Graduate School of Medicine, Kyoto University, Kyoto, Japan.The highly organized spatial arrangement of sensory hair cells in the organ of Corti is essential for inner ear function. Here, we report a new analytical pipeline, based on optical clearing of tissue, for the construction of a single-cell resolution map of the organ of Corti. A sorbitol-based optical clearing method enabled imaging of the entire cochlea at subcellular resolution. High-fidelity detection and analysis of all hair cell positions along the entire longitudinal axis of the organ of Corti were performed automatically by machine learning-based pattern recognition. Application of this method to samples from young, adult, and noise-exposed mice extracted essential information regarding cellular pathology, including longitudinal and radial spatial characteristics of cell loss, implying that multiple mechanisms underlie clustered cell loss. Our method of cellular mapping is effective for system-level phenotyping of the organ of Corti under both physiological and pathological conditions.© 2019, Urata et al.DOI: 10.7554/eLife.40946PMCID: PMC6338463",pubmed,30657453,10.7554/eLife.40946
closing the gap exploring the untapped potential of machine learning in deaf students and hearing students academic performance,"Assessments and critical feedback play a crucial role in helping students not only master a skill but also apply it effectively. Educational data mining (EDM) and machine learning (ML) tools are aiding educators in tailoring teaching strategies to individual student needs. While predictive analytics are widely used for hearing students, there is a notable gap in research on deaf students. Assessing deaf students necessitates the expertise of trained specialists, and their feedback is particularly critical in assisting these students in skill mastery. Various strategies have been developed to analyze the academic performance of deaf children, but there is a lack of integration of data to create a model categorizing different methods of early classification based on student academic performance. As part of a broader effort to address challenges faced by students struggling with speech perception and language development, there is an opportunity to conduct a systematic study of early academic interventions for deaf students. Failure to address these issues can result in an increased risk of delays in social-emotional development. The findings from our review highlight several key aspects, including (i) ML and EDM-based applications for student performance analysis, (ii) factors influencing academic performance among deaf students, (iii) potential EDM methods useful for assessing deaf children, (iv) the absence of benchmark data and the need for interpretability in existing methods, (v) the necessity for ML approaches in predicting the performance of deaf students, and (vi) the anticipated major assessment trend in the future through deep learning models. Our findings have implications for various stakeholders in education, including teachers, students, administrators, and researchers. © 2023 Raji N R et al.",scopus,2-s2.0-85178402240,10.19101/IJATEE.2023.10101685
speech reception with different bilateral directional processing schemes influence of binaural hearing audiometric asymmetry and acoustic scenario,"335. Hear Res. 2017 Sep;353:36-48. doi: 10.1016/j.heares.2017.07.014. Epub 2017 Jul 29.Speech reception with different bilateral directional processing schemes: Influence of binaural hearing, audiometric asymmetry, and acoustic scenario.Neher T(1), Wagener KC(2), Latzel M(3).Author information:(1)Medizinische Physik and Cluster of Excellence ""Hearing4all"", Dept. of Medical Physics and Acoustics, Carl-von-Ossietzky University, D-26111 Oldenburg, Germany. Electronic address: tneher@health.sdu.dk.(2)Hörzentrum Oldenburg GmbH, Marie-Curie-Str. 2, D-26129 Oldenburg, Germany. Electronic address: k.wagener@hoerzentrum-oldenburg.de.(3)Phonak AG, Laubisrütistrasse 28, CH-8712 Stäfa, Switzerland. Electronic address: matthias.latzel@phonak.com.Hearing aid (HA) users can differ markedly in their benefit from directional processing (or beamforming) algorithms. The current study therefore investigated candidacy for different bilateral directional processing schemes. Groups of elderly listeners with symmetric (N = 20) or asymmetric (N = 19) hearing thresholds for frequencies below 2 kHz, a large spread in the binaural intelligibility level difference (BILD), and no difference in age, overall degree of hearing loss, or performance on a measure of selective attention took part. Aided speech reception was measured using virtual acoustics together with a simulation of a linked pair of completely occluding behind-the-ear HAs. Five processing schemes and three acoustic scenarios were used. The processing schemes differed in the tradeoff between signal-to-noise ratio (SNR) improvement and binaural cue preservation. The acoustic scenarios consisted of a frontal target talker presented against two speech maskers from ±60° azimuth or spatially diffuse cafeteria noise. For both groups, a significant interaction between BILD, processing scheme, and acoustic scenario was found. This interaction implied that, in situations with lateral speech maskers, HA users with BILDs larger than about 2 dB profited more from preserved low-frequency binaural cues than from greater SNR improvement, whereas for smaller BILDs the opposite was true. Audiometric asymmetry reduced the influence of binaural hearing. In spatially diffuse noise, the maximal SNR improvement was generally beneficial. N0Sπ detection performance at 500 Hz predicted the benefit from low-frequency binaural cues. Together, these findings provide a basis for adapting bilateral directional processing to individual and situational influences. Further research is needed to investigate their generalizability to more realistic HA conditions (e.g., with low-frequency vent-transmitted sound).Copyright © 2017 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2017.07.014",pubmed,28783570,10.1016/j.heares.2017.07.014
application of the mathematical model for prognosis in the rehabilitation of children after cochlear implantation,"460. Vestn Otorinolaringol. 2016;81(6):47-50. doi: 10.17116/otorino201681647-50.[Application of the mathematical model for prognosis in the rehabilitation of children after cochlear implantation].[Article in Russian; Abstract available in Russian from the publisher]Petrova IP(1), Balashova EA(2), Goykhburg MV(3), Mashchenko AI(1), Markova TG(4), Bakhshinyan VV(4), Tavartkiladze GA(4).Author information:(1)National Research Center for Audiology and Hearing Rehabilitation, Russian Medico-Biological Agency, Moscow, Russia, 117513; Voronezh Children's Clinical Hospital No 1, Voronezh, Russia, 394024.(2)Federal State Budgetary Educational Institution of Higher Professional Education 'Voronezh State University of Engineering Technologies', Voronezh, Russia, 394036.(3)National Research Center for Audiology and Hearing Rehabilitation, Russian Medico-Biological Agency, Moscow, Russia, 117513.(4)National Research Center for Audiology and Hearing Rehabilitation, Russian Medico-Biological Agency, Moscow, Russia, 117513; Russian Medical Academy for Post-Graduate Education, Moscow, Russia, 125993.Despite the variety of etiological factors, cochlear implantation (CI) remains the only effective method for the rehabilitation of the patients presenting with total deafness. The aim of this study was the enhancement of the efficiency of selection of the candidates for CI, the improvement of the quality of rehabilitation of the patients with cochlear implants, and the determination of the prognostic criteria for clinical trials.PATIENTS AND METHODS: (CI). The decision-making support system (DMSS) based on the artificial neural networks (ANNs) has been created to enhance the efficiency of rehabilitation of the patients with cochlear implants and increase the effectiveness of the selection of candidates for cochlear implantation. The results of the children's rehabilitation after CI have been analyzed by using a mathematical model of artificial neural networks (Kohonen layer). The basis for the assessment of ANNs was formed by the results of the observations of audioverbal perception in 110 patients aged from 6 months to 17 years. The initial data were the average values obtained with the use of the Russian-language version of the Nottingham children's implant profile's test T1 - T3. The testing was performed before CI and 3, 6, 12, 18, and 24 months after it.MAIN RESULTS: The work yielded the four-cluster data structure. It made it possible to estimate the effectiveness of the clinical trials in selected classes depending on the etiology of the disease, the age of the patients, and their experience with the application of hearing aids. The reliable estimation of the dynamics of auditory perception at the stage of rehabilitation and prognosis of the outcomes of CI made it possible to take additional preventive and therapeutic measures in the combination with complementary psychological and educational procedures.Publisher: Цель исследования - повышение эффективности отбора кандидатов на кохлеарную имплантацию (КИ), улучшение качества реабилитации пациентов с имплантами, определение прогностических критериев КИ. Проведен анализ результатов реабилитации детей после КИ с использованием математической модели ИНС (слоя Кохонена). В основу построения ИНС легли результаты оценки развития слухоречевого восприятия у 110 пациентов в возрасте от 6 мес до 17 лет. Тестирование пациентов проводилось до КИ и через 3, 6, 12, 18 и 24 мес после проведенной КИ с помощью русскоязычной версии батареи тестов EARS. Проведенная работа позволила получить четырехкластерную структуру данных, оценить эффективность КИ в выделенных классах в зависимости от этиологии заболевания, возраста, наличия опыта слухопротезирования. На этапе реабилитации произведены достоверная оценка динамики развития слухового восприятия и прогноз результатов КИ, что дало возможность своевременно применить дополнительные методы лечения в сложных случаях.Publisher: Цель исследования - повышение эффективности отбора кандидатов на кохлеарную имплантацию (КИ), улучшение качества реабилитации пациентов с имплантами, определение прогностических критериев КИ. Проведен анализ результатов реабилитации детей после КИ с использованием математической модели ИНС (слоя Кохонена). В основу построения ИНС легли результаты оценки развития слухоречевого восприятия у 110 пациентов в возрасте от 6 мес до 17 лет. Тестирование пациентов проводилось до КИ и через 3, 6, 12, 18 и 24 мес после проведенной КИ с помощью русскоязычной версии батареи тестов EARS. Проведенная работа позволила получить четырехкластерную структуру данных, оценить эффективность КИ в выделенных классах в зависимости от этиологии заболевания, возраста, наличия опыта слухопротезирования. На этапе реабилитации произведены достоверная оценка динамики развития слухового восприятия и прогноз результатов КИ, что дало возможность своевременно применить дополнительные методы лечения в сложных случаях.DOI: 10.17116/otorino201681647-50",pubmed,28091476,10.17116/otorino201681647-50
association of leukocyte telomere length and the risk of agerelated hearing impairment in chinese hans,"248. Sci Rep. 2017 Aug 31;7(1):10106. doi: 10.1038/s41598-017-10680-9.Association of leukocyte telomere length and the risk of age-related hearing impairment in Chinese Hans.Liu H(1), Luo H(2), Yang T(3)(4)(5), Wu H(6)(7)(8), Chen D(9).Author information:(1)Ministry of Education and Shanghai Key Laboratory of Children's Environmental Health, Xin Hua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, China.(2)Department of Otolaryngology, Renji Hospital Affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, China.(3)Department of Otolaryngology-Head and Neck Surgery, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, China. yangtfxl@sina.com.(4)Ear Institute, Shanghai Jiaotong University School of Medicine, Shanghai, China. yangtfxl@sina.com.(5)Shanghai Key Laboratory of Translational Medicine on Ear and Nose Diseases, Shanghai, China. yangtfxl@sina.com.(6)Department of Otorhinolaryngology-Head and Neck Surgery, Shanghai Ninth People's Hospital Affiliated to Shanghai Jiaotong University School of Medicine, Shanghai, China. wuhao622@sina.cn.(7)Ear Institute, Shanghai Jiaotong University School of Medicine, Shanghai, China. wuhao622@sina.cn.(8)Shanghai Key Laboratory of Translational Medicine on Ear and Nose Diseases, Shanghai, China. wuhao622@sina.cn.(9)Ministry of Education and Shanghai Key Laboratory of Children's Environmental Health, Xin Hua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, China. simpledandan1981@163.com.Age-related hearing loss (ARHI) is the most common sensory disorder in the elderly. Although telomere attrition has been shown as a determinant in the pathobiology of various age-related diseases, it remains unknown whether telomere length is associated with ARHI. We hypothesized that decreased leukocyte telomere length (LTL) increased the risk of ARHI. Thus, we measured LTL of 666 ARHI and 43 controls by an established quantitative PCR technique. Four audiogram shape subtypes of ARHI, including ""flat shape (FL)"", ""2-4 kHz abrupt loss (AL) shape"", ""8 kHz dip (8D) shape"" and ""sloping shape (SL)"" could be identified among the cases using K-means cluster analysis. Longer LTL was associated with the reduced incidence of ARHI (adjusted OR = 0.550, 95% CI: 0.420-0.721, P < 0.0001 for all the ARHI; 0.498, 0.318-0.780, P = 0.0023 for FL subgroup; 0.428, 0.292-0.628, P < 0.0001 for AL subgroup; 0.552, 0.399-0.764, P = 0.0003 for mSL subgroup). Subjects in the highest tertile of LTL were at less risk for ARHI than those in the lowest and middle tertiles (OR for ARHI: 0.327, 95% CI 0.170-0.629, P = 0.0008). There was a descending trend of LTL as the degree of pure tone threshold average (PTA) aggravated. These results suggest that telomere attrition may be involved in the progression of ARHI.DOI: 10.1038/s41598-017-10680-9PMCID: PMC5578975",pubmed,28860610,10.1038/s41598-017-10680-9
deep learning models to remix music for cochlear implant users,"295. J Acoust Soc Am. 2018 Jun;143(6):3602. doi: 10.1121/1.5042056.Deep learning models to remix music for cochlear implant users.Gajęcki T(1), Nogueira W(1).Author information:(1)Department of Otolaryngology, Medical University Hannover and Cluster of Excellence Hearing4all, Hannover, 30625, Germany.The severe hearing loss problems that some people suffer can be treated by providing them with a surgically implanted electrical device called cochlear implant (CI). CI users struggle to perceive complex audio signals such as music; however, previous studies show that CI recipients find music more enjoyable when the vocals are enhanced with respect to the background music. In this manuscript source separation (SS) algorithms are used to remix pop songs by applying gain to the lead singing voice. This work uses deep convolutional auto-encoders, a deep recurrent neural network, a multilayer perceptron (MLP), and non-negative matrix factorization to be evaluated objectively and subjectively through two different perceptual experiments which involve normal hearing subjects and CI recipients. The evaluation assesses the relevance of the artifacts introduced by the SS algorithms considering their computation time, as this study aims at proposing one of the algorithms for real-time implementation. Results show that the MLP performs in a robust way throughout the tested data while providing levels of distortions and artifacts which are not perceived by CI users. Thus, an MLP is proposed to be implemented for real-time monaural audio SS to remix music for CI users.DOI: 10.1121/1.5042056",pubmed,29960485,10.1121/1.5042056
translational applications of machine learning in auditory electrophysiology,"Machine learning (ML) is transforming nearly every aspect of modern life including medicine and its subfields, such as hearing science. This article presents a brief conceptual overview of selected ML approaches and describes how these techniques are being applied to outstanding problems in hearing science, with a particular focus on auditory evoked potentials (AEPs). Two vignettes are presented in which ML is used to analyze subcortical AEP data. The first vignette demonstrates how ML can be used to determine if auditory learning has influenced auditory neurophysiologic function. The second vignette demonstrates how ML analysis of AEPs may be useful in determining whether hearing devices are optimized for discriminating speech sounds.",cinahl,7340451,10.1055/s-0042-1756166
neurocognitive testing and cochlear implantation insights into performance in older adults,"304. Clin Interv Aging. 2016 May 12;11:603-13. doi: 10.2147/CIA.S100255. eCollection 2016.Neurocognitive testing and cochlear implantation: insights into performance in older adults.Cosetti MK(1), Pinkston JB(2), Flores JM(3), Friedmann DR(4), Jones CB(2), Roland JT Jr(5), Waltzman SB(4).Author information:(1)Department of Otolaryngology - Head and Neck Surgery, Louisiana State University Health Sciences Center, Shreveport, LA, USA; Department of Neurosurgery, Louisiana State University Health Sciences Center, Shreveport, LA, USA.(2)Department of Neurology, Louisiana State University Health Sciences Center, Shreveport, LA, USA.(3)Department of Epidemiology, Johns Hopkins Bloomberg School of Public Health, Baltimore, MA, USA.(4)Department of Otolaryngology, New York University School of Medicine, New York, NY, USA.(5)Department of Otolaryngology, New York University School of Medicine, New York, NY, USA; Department of Neurosurgery, New York University School of Medicine, New York, NY, USA.OBJECTIVE: The aim of this case series was to assess the impact of auditory rehabilitation with cochlear implantation on the cognitive function of elderly patients over time.DESIGN: This is a longitudinal case series of prospective data assessing neurocognitive function and speech perception in an elderly cohort pre- and post-implantation.SETTING: University cochlear implant center.PARTICIPANTS: The patients were post-lingually deafened elderly female (mean, 73.6 years; SD, 5.82; range, 67-81 years) cochlear implant recipients (n=7).MEASUREMENTS: A neurocognitive battery of 20 tests assessing intellectual function, learning, short- and long-term memory, verbal fluency, attention, mental flexibility, and processing speed was performed prior to and 2-4.1 years (mean, 3.7) after cochlear implant (CI). Speech perception testing using Consonant-Nucleus-Consonant words was performed prior to implantation and at regular intervals postoperatively. Individual and aggregate differences in cognitive function pre- and post-CI were estimated. Logistic regression with cluster adjustment was used to estimate the association (%improvement or %decline) between speech understanding and years from implantation at 1 year, 2 years, and 3 years post-CI.RESULTS: Improvements after CI were observed in 14 (70%) of all subtests administered. Declines occurred in five (25%) subtests. In 55 individual tests (43%), post-CI performance improved compared to a patient's own performance before implantation. Of these, nine (45%) showed moderate or pronounced improvement. Overall, improvements were largest in the verbal and memory domains. Logistic regression demonstrated a significant relationship between speech perception and cognitive function over time. Five neurocognitive tests were predictive of improved speech perception following implantation.CONCLUSION: Comprehensive neurocognitive testing of elderly women demonstrated areas of improvement in cognitive function and auditory perception following cochlear implantation. Multiple neurocognitive tests were strongly associated with current speech perception measures. While these data shed light on the complex relationship between hearing and cognition by showing that CI may slow the expected age-related cognitive decline, further research is needed to examine the impact of hearing rehabilitation on cognitive decline.DOI: 10.2147/CIA.S100255PMCID: PMC4869653",pubmed,27274210,10.2147/CIA.S100255
does experience with hearing aid amplification influence electrophysiological measures of speech comprehension,"695. Int J Audiol. 2023 Nov 27:1-10. doi: 10.1080/14992027.2023.2284675. Online ahead of print.Does experience with hearing aid amplification influence electrophysiological measures of speech comprehension?Deshpande P(1)(2), Brandt C(1)(2), Debener S(3)(4)(5), Neher T(1)(2).Author information:(1)Institute of Clinical Research, University of Southern Denmark, Odense, Denmark.(2)Research Unit for ORL - Head & Neck Surgery and Audiology, Odense University Hospital & University of Southern Denmark, Odense, Denmark.(3)Neuropsychology Lab, Department of Psychology, University of Oldenburg, Germany.(4)Cluster of Excellence Hearing4all, University of Oldenburg, Oldenburg, Germany.(5)Branch for Hearing, Speech and Audio Technology HSA, Fraunhofer Institute for Digital Media Technology IDMT, Oldenburg, Germany.OBJECTIVE: To explore if experience with hearing aid (HA) amplification affects speech-evoked cortical potentials reflecting comprehension abilities.DESIGN: N400 and late positive complex (LPC) responses as well as behavioural response times to congruent and incongruent digit triplets were measured. The digits were presented against stationary speech-shaped noise 10 dB above individually measured speech recognition thresholds. Stimulus presentation was either acoustic (digits 1-3) or first visual (digits 1-2) and then acoustic (digit 3).STUDY SAMPLE: Three groups of older participants (N = 3 × 15) with (1) pure-tone average hearing thresholds <25 dB HL from 500-4000 Hz, (2) mild-to-moderate sensorineural hearing loss (SNHL) but no prior HA experience, and (3) mild-to-moderate SNHL and >2 years of HA experience. Groups 2-3 were fitted with test devices in accordance with clinical gain targets.RESULTS: No group differences were found in the electrophysiological data. N400 amplitudes were larger and LPC latencies shorter with acoustic presentation. For group 1, behavioural response times were shorter with visual-then-acoustic presentation.CONCLUSION: When speech audibility is ensured, comprehension-related electrophysiological responses appear intact in individuals with mild-to-moderate SNHL, regardless of prior experience with amplified sound. Further research into the effects of audibility versus acclimatisation-related neurophysiological changes is warranted.DOI: 10.1080/14992027.2023.2284675",pubmed,38010629,10.1080/14992027.2023.2284675
exploring practical metrics to support automatic speech recognition evaluations,"123. Stud Health Technol Inform. 2023 Aug 23;306:305-310. doi: 10.3233/SHTI230636.Exploring Practical Metrics to Support Automatic Speech Recognition Evaluations.Draffan EA(1), Wald M(1), Ding C(1), Li Y(2).Author information:(1)ECS, University of Southampton, UK.(2)Yunjia Li, Habitat Learn, UK.Recent studies into the evaluation of automatic speech recognition for its quality of output in the form of text have shown that using word error rate to see how many mistakes exist in English does not necessarily help the developer of automatic transcriptions or captions. Confidence levels as to the type of errors being made remain low because mistranslations from speech to text are not always captured with a note that details the reason for the error. There have been situations in higher education where students requiring captions and transcriptions have found that some academic lecture results are littered with word errors which means that comprehension levels drop and those with cognitive, physical and sensory disabilities are particularly affected. Despite the incredible improvements in general understanding of conversational automatic speech recognition, academic situations tend to include numerous domain specific terms and the lecturers may be non-native speakers, coping with recording technology in noisy situations. This paper aims to discuss the way additional metrics are used to capture issues and feedback into the machine learning process to enable enhanced quality of output and more inclusive practices for those using virtual conferencing systems. The process goes beyond what is expressed and examines paralinguistic aspects such as timing, intonation, voice quality and speech understanding.DOI: 10.3233/SHTI230636",pubmed,37638929,10.3233/SHTI230636
health hazards and hearing loss risk assessment of workers exposed to noise in an automobile manufacturing enterprise,"545. Zhonghua Lao Dong Wei Sheng Zhi Ye Bing Za Zhi. 2022 Jun 20;40(6):434-438. doi: 10.3760/cma.j.cn121094-20210615-00286.[Health hazards and hearing loss risk assessment of workers exposed to noise in an automobile manufacturing enterprise].[Article in Chinese; Abstract available in Chinese from the publisher]Liu T(1), Liu J(2), Han C(2), Liu YT(2), Zeng Q(3), Gu Q(4).Author information:(1)School of Public Health, Tianjin Medical University, Tianjin 300070, China.(2)Institute for Occupational Health, Tianjin Centers for Disease Control and Prevention, Tianjin 300011, China.(3)School of Public Health, Tianjin Medical University, Tianjin 300070, China Institute for Occupational Health, Tianjin Centers for Disease Control and Prevention, Tianjin 300011, China.(4)School of Public Health, Tianjin Medical University, Tianjin 300070, China Tianjin Municipal Health Commission, Tianjin 300070, China.Objective: To investigate the current situation of occupational exposure to noise among noise workers in an automobile manufacturing enterprise in Tianjin, understand the impact of noise on workers＇ nervous system and hearing, and assess the risk of hearing loss among noise workers. Methods: In May 2021, 3516 workers in an automobile manufacturing enterprise were investigated by using a self-made questionnaire＂Noise Workers Questionnaire＂ and cluster sampling method. The occupational noise hygiene survey and occupational hazards detection were carried out in their workplaces. They were divided into noise exposure group and non-noise exposure group according to whether they were exposed to noise or not. The general characteristics, hearing and nervous system symptoms of the two groups of workers were compared, and the risk of hearing loss was assessed. Results: There were 758 workers in the noise exposure group, aged (26±5) years old, with a working age of 3.0 (2.0, 6.0) years exposed to noise. 2758 workers in the non-noise exposure group, aged (25±6) years old, with a working age of 2.0 (1.0, 4.0) years. There were statistically significant differences in the distribution of workers＇education level, working age and memory loss between the two groups (χ(2)=37.98, 38.70, 5.20, P<0.05). The workers in the noise exposure group showed a decreasing trend of insomnia, dreaminess, sweating and fatigue with the increase of working age (χ(2trend)=6.16, 7.99, P<0.05). The risk classification of binaural high-frequency hearing loss for workers in all noise positions until the age of 50 and 60 was negligible, the risk of occupational noise deafness was low for workers in stamping and welding noise positions until the age of 60. Conclusion: The occupational noise exposed to automobile manufacturing workers may cause certain harm to their nervous and auditory systems. Noise protection measures should be taken to reduce the risk of hearing loss and occupational noise deafness.Publisher: 目的： 调查天津市某汽车制造企业噪声岗位工人职业接触噪声现状，了解噪声对作业工人神经系统和听力的影响，并对噪声岗位工人进行听力损失风险评估。 方法： 于2021年5月，以整群抽样方法，对某汽车制造企业3 516名工人进行《噪声作业工人调查表》调查，对其所在工作场所进行职业噪声检测，按是否接触噪声作业分为接噪组和非接噪组。比较两组工人之间的一般特征、听力情况、神经系统症状，并对接触噪声工人进行听力损失风险评估。 结果： 接噪组工人758人，年龄（26±5）岁，接触噪声工龄3.0（2.0，6.0）年；非接噪组工人2 758人，年龄（25±6）岁，工龄2.0（1.0，4.0）年；两组作业工人文化程度、工龄和记忆力减退分布差异均有统计学意义（χ(2)=37.98、38.70、5.20，P<0.05）；接噪组工人随着工龄增加，失眠多梦、多汗乏力呈下降趋势（χ(2)(趋势)=6.16、7.99，P<0.05）。各噪声岗位工人工作至50、60岁时发生双耳高频听力损失的风险分级均为可忽略风险，冲压、焊装噪声岗位工人工作至60岁时发生职业性噪声聋的风险均为低风险。 结论： 汽车制造业工人接触的职业噪声会对其神经、听觉系统产生一定危害，应采取噪声防护措施降低其听力损失和职业性噪声聋的风险。.DOI: 10.3760/cma.j.cn121094-20210615-00286",pubmed,35785897,10.3760/cma.j.cn121094-20210615-00286
a precision driver device for intraoperative stimulation of a bone conduction implant,"536. Sci Rep. 2020 Feb 4;10(1):1797. doi: 10.1038/s41598-020-58512-7.A Precision Driver Device for Intraoperative Stimulation of a Bone Conduction Implant.Ghoncheh M(1)(2), Lenarz T(3)(4), Maier H(3)(4).Author information:(1)Cluster of Excellence Hearing4all, Hannover, Germany. Ghoncheh.Mohammad@MH-Hannover.de.(2)Department of Otolaryngology and Institute of Audioneurotechnology (VIANNA), Hannover Medical School, Hannover, Germany. Ghoncheh.Mohammad@MH-Hannover.de.(3)Cluster of Excellence Hearing4all, Hannover, Germany.(4)Department of Otolaryngology and Institute of Audioneurotechnology (VIANNA), Hannover Medical School, Hannover, Germany.Semi-implantable bone conduction implants (BCI) and active middle ear implants (AMEI) for patients with sensorineural, conductive or mixed hearing loss commonly use an amplitude modulation technology to transmit power and sound signals from an external audio processor to the implant. In patients, the distance dependence of the signal amplitude is of minor importance as the skin thickness is constant and only varies between 3-7 mm. In this range, critical coupling transmission technique sufficiently reduces the variability in amplitude, but fails to provide well-defined amplitudes in many research and clinical applications such as intraoperative integrity tests where the distance range is exceeded by using sterile covers. Here we used the BCI Bonebridge (BB, Med-El, Austria) as an example to develop and demonstrate a system that synthesizes the transmission signal, determines the distance between the transmitter and the receiver implant coil and compensates transmission losses. When compared to an external audio processor (AP304) on an artificial mastoid, our system mainly decreased amplitude variability from over 11 dB to less than 3 dB for audio frequencies (0.1-10 kHz) at distances up to 15 mm, making it adequate for intraoperative and audiometric tests.DOI: 10.1038/s41598-020-58512-7PMCID: PMC7000405",pubmed,32019957,10.1038/s41598-020-58512-7
fmri as a preimplant objective tool to predict childrens postimplant auditory and language outcomes as measured by parental observations,"437. J Am Acad Audiol. 2018 May;29(5):389-404. doi: 10.3766/jaaa.16149.fMRI as a Preimplant Objective Tool to Predict Children's Postimplant Auditory and Language Outcomes as Measured by Parental Observations.Deshpande AK(1), Tan L(2)(3), Lu LJ(2)(4), Altaye M(5), Holland SK(6)(7).Author information:(1)Department of Speech-Language-Hearing Sciences, Hofstra University, Hempstead, NY.(2)Division of Biomedical Informatics, Cincinnati Children's Hospital Research Foundation, Cincinnati, OH.(3)School of Computing Sciences and Informatics, University of Cincinnati, Cincinnati, OH.(4)Department of Environmental Health, College of Medicine, University of Cincinnati, Cincinnati, OH.(5)Division of Biostatistics and Epidemiology, Cincinnati Children's Hospital Medical Center, Cincinnati, OH.(6)Pediatric Neuroimaging Research Consortium, Cincinnati Children's Hospital Medical Center, Cincinnati, OH.(7)Department of Pediatric Radiology, Cincinnati Children's Hospital Medical Center, Cincinnati, OH.BACKGROUND: The trends in cochlear implantation candidacy and benefit have changed rapidly in the last two decades. It is now widely accepted that early implantation leads to better postimplant outcomes. Although some generalizations can be made about postimplant auditory and language performance, neural mechanisms need to be studied to predict individual prognosis.PURPOSE: The aim of this study was to use functional magnetic resonance imaging (fMRI) to identify preimplant neuroimaging biomarkers that predict children's postimplant auditory and language outcomes as measured by parental observation/reports.RESEARCH DESIGN: This is a pre-post correlational measures study.STUDY SAMPLE: Twelve possible cochlear implant candidates with bilateral severe to profound hearing loss were recruited via referrals for a clinical magnetic resonance imaging to ensure structural integrity of the auditory nerve for implantation.INTERVENTION: Participants underwent cochlear implantation at a mean age of 19.4 mo. All children used the advanced combination encoder strategy (ACE, Cochlear Corporation™, Nucleus® Freedom cochlear implants). Three participants received an implant in the right ear; one in the left ear whereas eight participants received bilateral implants. Participants' preimplant neuronal activation in response to two auditory stimuli was studied using an event-related fMRI method.DATA COLLECTION AND ANALYSIS: Blood oxygen level dependent contrast maps were calculated for speech and noise stimuli. The general linear model was used to create z-maps. The Auditory Skills Checklist (ASC) and the SKI-HI Language Development Scale (SKI-HI LDS) were administered to the parents 2 yr after implantation. A nonparametric correlation analysis was implemented between preimplant fMRI activation and postimplant auditory and language outcomes based on ASC and SKI-HI LDS. Statistical Parametric Mapping software was used to create regression maps between fMRI activation and scores on the aforementioned tests. Regression maps were overlaid on the Imaging Research Center infant template and visualized in MRIcro.RESULTS: Regression maps revealed two clusters of brain activation for the speech versus silence contrast and five clusters for the noise versus silence contrast that were significantly correlated with the parental reports. These clusters included auditory and extra-auditory regions such as the middle temporal gyrus, supramarginal gyrus, precuneus, cingulate gyrus, middle frontal gyrus, subgyral, and middle occipital gyrus. Both positive and negative correlations were observed. Correlation values for the different clusters ranged from -0.90 to 0.95 and were significant at a corrected p value of <0.05. Correlations suggest that postimplant performance may be predicted by activation in specific brain regions.CONCLUSIONS: The results of the present study suggest that (1) fMRI can be used to identify neuroimaging biomarkers of auditory and language performance before implantation and (2) activation in certain brain regions may be predictive of postimplant auditory and language performance as measured by parental observation/reports.American Academy of Audiology.DOI: 10.3766/jaaa.16149",pubmed,29708489,10.3766/jaaa.16149
neuroinflammation in a mouse model of alzheimers disease versus auditory dysfunction machine learning interpretation and analysis,"626. Res Sq [Preprint]. 2023 Sep 27:rs.3.rs-3370200. doi: 10.21203/rs.3.rs-3370200/v1.Neuroinflammation in a Mouse Model of Alzheimer's Disease versus Auditory Dysfunction: Machine Learning Interpretation and Analysis.Na D(1), Yang Y(2), Xie L(1), Piekna-Przybylska D(1), Bunn D(1), Shamambo M(1), White P(1).Author information:(1)University of Rochester Medical Center.(2)Rochester Institute of Technology.BACKGROUND: Auditory dysfunction, including central auditory hyperactivity, hearing loss and hearing in noise deficits, has been reported in 5xFAD Alzheimer's disease (AD) mice, suggesting a causal relationship between amyloidosis and auditory dysfunction. Central auditory hyperactivity correlated in time with small amounts of plaque deposition in the inferior colliculus and medial geniculate body, which are the auditory midbrain and thalamus, respectively. Neuroinflammation has been associated with excitation to inhibition imbalance in the central nervous system, and therefore has been proposed as a link between central auditory hyperactivity and AD in our previous report. However, neuroinflammation in the auditory pathway has not been investigated in mouse amyloidosis models.METHODS: Machine learning was used to classify the previously obtained auditory brainstem responses (ABRs) from 5xFAD mice and their wild type (WT) littermates. Neuroinflammation was assessed in six auditory-related regions of the cortex, thalamus, and brainstem. Cochlear pathology was assessed in cryosection and whole mount. Behavioral changes were assessed with fear conditioning, open field testing and novel objection recognition.RESULTS: Reliable machine learning classification of 5xFAD and WT littermate ABRs were achieved for 6M and 12M, but not 3M. The top features for accurate classification at 6 months of age were characteristics of Waves IV and V. Microglial and astrocytic activation were pronounced in 5xFAD inferior colliculus and medial geniculate body at 6 months, two neural centers that are thought to contribute to these waves. Lower regions of the brainstem were unaffected, and cortical auditory centers also displayed inflammation beginning at 6 months. No losses were seen in numbers of spiral ganglion neurons (SGNs), auditory synapses, or efferent synapses in the cochlea. 5xFAD mice had reduced responses to tones in fear conditioning compared to WT littermates beginning at 6 months.CONCLUSIONS: Serial use of ABR in early AD patients represents a promising approach for early and inexpensive detection of neuroinflammation in higher auditory brainstem processing centers. As changes in auditory processing are strongly linked to AD progression, central auditory hyperactivity may serve as a biomarker for AD progression and/or stratify AD patients into distinct populations.DOI: 10.21203/rs.3.rs-3370200/v1PMCID: PMC10571613",pubmed,37841847,10.21203/rs.3.rs-3370200/v1
making connections in the inner ear recent insights into the development of spiral ganglion neurons and their connectivity with sensory hair cells,"814. Semin Cell Dev Biol. 2013 May;24(5):460-9. doi: 10.1016/j.semcdb.2013.04.003. Epub 2013 May 6.Making connections in the inner ear: recent insights into the development of spiral ganglion neurons and their connectivity with sensory hair cells.Coate TM(1), Kelley MW.Author information:(1)Laboratory of Cochlear Development, National Institute on Deafness and Other Communication Disorders, National Institutes of Health, Bethesda, MD 20892, USA. coatet@nidcd.nih.govIn mammals, auditory information is processed by the hair cells (HCs) located in the cochlea and then rapidly transmitted to the CNS via a specialized cluster of bipolar afferent connections known as the spiral ganglion neurons (SGNs). Although many anatomical aspects of SGNs are well described, the molecular and cellular mechanisms underlying their genesis, how they are precisely arranged along the cochlear duct, and the guidance mechanisms that promote the innervation of their hair cell targets are only now being understood. Building upon foundational studies of neurogenesis and neurotrophins, we review here new concepts and technologies that are helping to enrich our understanding of the development of the nervous system within the inner ear.Published by Elsevier Ltd.DOI: 10.1016/j.semcdb.2013.04.003PMCID: PMC3690159",pubmed,23660234,10.1016/j.semcdb.2013.04.003
streptococcus pneumoniaeinduced ototoxicity in organ of corti explant cultures,"599. Hear Res. 2017 Jul;350:100-109. doi: 10.1016/j.heares.2017.04.012. Epub 2017 Apr 25.Streptococcus pneumoniae-induced ototoxicity in organ of Corti explant cultures.Perny M(1), Solyga M(2), Grandgirard D(3), Roccio M(2), Leib SL(4), Senn P(5).Author information:(1)Neuroinfection Laboratory, Institute for Infectious Diseases, University of Bern, Switzerland; Inner Ear Research Laboratory, Department of Otorhinolaryngology, Head& Neck Surgery, Inselspital Bern and Department of Clinical Research, University of Bern, Switzerland; Cluster for Regenerative Neuroscience, Department of Clinical Research, University of Bern, Switzerland.(2)Inner Ear Research Laboratory, Department of Otorhinolaryngology, Head& Neck Surgery, Inselspital Bern and Department of Clinical Research, University of Bern, Switzerland; Cluster for Regenerative Neuroscience, Department of Clinical Research, University of Bern, Switzerland.(3)Neuroinfection Laboratory, Institute for Infectious Diseases, University of Bern, Switzerland; Cluster for Regenerative Neuroscience, Department of Clinical Research, University of Bern, Switzerland.(4)Neuroinfection Laboratory, Institute for Infectious Diseases, University of Bern, Switzerland; Cluster for Regenerative Neuroscience, Department of Clinical Research, University of Bern, Switzerland. Electronic address: Stephen.leib@ifik.unibe.ch.(5)Inner Ear Research Laboratory, Department of Otorhinolaryngology, Head& Neck Surgery, Inselspital Bern and Department of Clinical Research, University of Bern, Switzerland; Department of Otorhinolaryngology, Head & Neck Surgery, University Hospital Geneva (HUG), Genève, Switzerland; Cluster for Regenerative Neuroscience, Department of Clinical Research, University of Bern, Switzerland. Electronic address: Pascal.Senn@hcuge.ch.Hearing loss remains the most common long-term complication of pneumococcal meningitis (PM) reported in up to 30% of survivors. Streptococcus pneumoniae have been shown to possess different ototoxic properties. Here we present a novel ex vivo experimental setup to examine in detail the pattern of hair cell loss upon exposure to different S. pneumoniae strains, therefore recapitulating pathogen derived aspects of PM-induced hearing loss. Our results show a higher susceptibility towards S. pneumoniae-induced cochlear damage for outer hair cells (OHC) compared to inner hair cells (IHC), which is consistent with in vivo data. S. pneumoniae-induced hair cell loss was both time and dose-dependent. Moreover, we have found significant differences in the level of cell damage between tissue from the basal and the apical turns. This shows that the higher vulnerability of hair cells located at high frequency regions observed in vivo cannot be explained solely by the spatial organisation and bacterial infiltration from the basal portion of the cochlea. Using a wild type D39 strain and a mutant defective for the pneumolysin (PLY) gene, we also have shown that the toxin PLY is an important factor involved in ototoxic damages. The obtained results indicate that PLY can cause both IHC and OHC loss. Finally, we are reporting here for the first time a higher vulnerability of HC located at the basal and middle cochlear region to pneumolysin-induced damage. The detailed description of the susceptibility of hair cells to Streptococcus pneumoniae provided in this report can in the future determine the choice and the development of novel otoprotective therapies during pneumococcal meningitis.Copyright © 2017 The Authors. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2017.04.012",pubmed,28460251,10.1016/j.heares.2017.04.012
agerelated hearing loss increases crossmodal distractibility,"66. Hear Res. 2014 Oct;316:28-36. doi: 10.1016/j.heares.2014.07.005. Epub 2014 Jul 28.Age-related hearing loss increases cross-modal distractibility.Puschmann S(1), Sandmann P(2), Bendixen A(3), Thiel CM(4).Author information:(1)Biological Psychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all"", European Medical School, Carl von Ossietzky Universität Oldenburg, 26111 Oldenburg, Germany. Electronic address: sebastian.puschmann@uni-oldenburg.de.(2)Central Auditory Diagnostics Lab, Department of Neurology, Cluster of Excellence ""Hearing4all"", Hannover Medical School, Hannover, Germany.(3)Auditory Psychophysiology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all"", European Medical School, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany; Research Center Neurosensory Science, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany.(4)Biological Psychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all"", European Medical School, Carl von Ossietzky Universität Oldenburg, 26111 Oldenburg, Germany; Research Center Neurosensory Science, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany.Recent electrophysiological studies have provided evidence that changes in multisensory processing in auditory cortex cannot only be observed following extensive hearing loss, but also in moderately hearing-impaired subjects. How the reduced auditory input affects audio-visual interactions is however largely unknown. Here we used a cross-modal distraction paradigm to investigate multisensory processing in elderly participants with an age-related high-frequency hearing loss as compared to young and elderly subjects with normal hearing. During the experiment, participants were simultaneously presented with independent streams of auditory and visual input and were asked to categorize either the auditory or visual information while ignoring the other modality. Unisensory sequences without any cross-modal input served as control conditions to assure that all participants were able to perform the task. While all groups performed similarly in these unisensory conditions, hearing-impaired participants showed significantly increased error rates when confronted with distracting cross-modal stimulation. This effect could be observed in both the auditory and the visual task. Supporting these findings, an additional regression analysis indicted that the degree of high-frequency hearing loss significantly modulates cross-modal visual distractibility in the auditory task. These findings provide new evidence that already a moderate sub-clinical hearing loss, a common phenomenon in the elderly population, affects the processing of audio-visual information.Copyright © 2014 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2014.07.005",pubmed,25080386,10.1016/j.heares.2014.07.005
tinnituslike hallucinations elicited by sensory deprivation in an entropy maximization recurrent neural network,"291. PLoS Comput Biol. 2021 Dec 8;17(12):e1008664. doi: 10.1371/journal.pcbi.1008664. eCollection 2021 Dec.Tinnitus-like ""hallucinations"" elicited by sensory deprivation in an entropy maximization recurrent neural network.Dotan A(1)(2), Shriki O(1)(3)(2).Author information:(1)Department of Cognitive and Brain Sciences, Ben-Gurion University of the Negev, Beer-Sheva, Israel.(2)Zlotowski Center for Neuroscience, Ben-Gurion University of the Negev, Beer-Sheva, Israel.(3)Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva, Israel.Sensory deprivation has long been known to cause hallucinations or ""phantom"" sensations, the most common of which is tinnitus induced by hearing loss, affecting 10-20% of the population. An observable hearing loss, causing auditory sensory deprivation over a band of frequencies, is present in over 90% of people with tinnitus. Existing plasticity-based computational models for tinnitus are usually driven by homeostatic mechanisms, modeled to fit phenomenological findings. Here, we use an objective-driven learning algorithm to model an early auditory processing neuronal network, e.g., in the dorsal cochlear nucleus. The learning algorithm maximizes the network's output entropy by learning the feed-forward and recurrent interactions in the model. We show that the connectivity patterns and responses learned by the model display several hallmarks of early auditory neuronal networks. We further demonstrate that attenuation of peripheral inputs drives the recurrent network towards its critical point and transition into a tinnitus-like state. In this state, the network activity resembles responses to genuine inputs even in the absence of external stimulation, namely, it ""hallucinates"" auditory responses. These findings demonstrate how objective-driven plasticity mechanisms that normally act to optimize the network's input representation can also elicit pathologies such as tinnitus as a result of sensory deprivation.DOI: 10.1371/journal.pcbi.1008664PMCID: PMC8687580",pubmed,34879061,10.1371/journal.pcbi.1008664
a deep neuralnetwork classifier for photographbased estimation of hearing protection attenuation and fit,"170. J Acoust Soc Am. 2021 Aug;150(2):1067. doi: 10.1121/10.0005820.A deep neural-network classifier for photograph-based estimation of hearing protection attenuation and fit.Smalt CJ(1), Ciccarelli GA(1), Rodriguez AR(1), Murphy WJ(2).Author information:(1)Human Health & Performance Systems Group, MIT Lincoln Laboratory, Lexington, Massachusetts 02421, USA.(2)DFSE/EPHB/Noise & Bioacoustics Team, National Institute for Occupational Safety and Health, Cincinnati, Ohio 45226, USA.Occupational and recreational acoustic noise exposure is known to cause permanent hearing damage and reduced quality of life, which indicates the importance of noise controls including hearing protection devices (HPDs) in situations where high noise levels exist. While HPDs can provide adequate protection for many noise exposures, it is often a challenge to properly train HPD users and maintain compliance with usage guidelines. HPD fit-testing systems are commercially available to ensure proper attenuation is achieved, but they often require specific facilities designed for hearing testing (e.g., a quiet room or an audiometric booth) or special equipment (e.g., modified HPDs designed specifically for fit testing). In this study, we explored using visual information from a photograph of an HPD inserted into the ear to estimate hearing protector attenuation. Our dataset consists of 960 unique photographs from four types of hearing protectors across 160 individuals. We achieved 73% classification accuracy in predicting if the fit was greater or less than the median measured attenuation (29 dB at 1 kHz) using a deep neural network. Ultimately, the fit-test technique developed in this research could be used for training as well as for automated compliance monitoring in noisy environments to prevent hearing loss.DOI: 10.1121/10.0005820PMCID: PMC8689361",pubmed,34470332,10.1121/10.0005820
predicting threemonth and 12month postfitting realworld hearingaid outcome using prefitting acceptable noise level anl,"278. Int J Audiol. 2016;55(5):285-94. doi: 10.3109/14992027.2015.1120892. Epub 2016 Feb 15.Predicting three-month and 12-month post-fitting real-world hearing-aid outcome using pre-fitting acceptable noise level (ANL).Wu YH(1), Ho HC(2)(3), Hsiao SH(2)(3), Brummet RB(4), Chipara O(4).Author information:(1)a Department of Communication Sciences and Disorders , The University of Iowa , Iowa City , USA .(2)b Department of Otolaryngology , Buddhist Dalin Tzu-Chi General Hospital , Chiayi , Taiwan .(3)c School of Medicine, Tzu-Chi University , Hualien , Taiwan , and.(4)d Department of Computer Science , The University of Iowa , Iowa City , USA.OBJECTIVE: Determine the extent to which pre-fitting acceptable noise level (ANL), with or without other predictors such as hearing-aid experience, can predict real-world hearing-aid outcomes at three and 12 months post-fitting.DESIGN: ANLs were measured before hearing-aid fitting. Post-fitting outcome was assessed using the international outcome inventory for hearing aids (IOI-HA) and a hearing-aid use questionnaire. Models that predicted outcomes (successful vs. unsuccessful) were built using logistic regression and several machine learning algorithms, and were evaluated using the cross-validation technique.STUDY SAMPLE: A total of 132 adults with hearing impairment.RESULTS: The prediction accuracy of the models ranged from 61% to 68% (IOI-HA) and from 55% to 61% (hearing-aid use questionnaire). The models performed more poorly in predicting 12-month than three-month outcomes. The ANL cutoff between successful and unsuccessful users was higher for experienced (∼18 dB) than first-time hearing-aid users (∼10 dB), indicating that most experienced users will be predicted as successful users regardless of their ANLs.CONCLUSIONS: Pre-fitting ANL is more useful in predicting short-term (three months) hearing-aid outcomes for first-time users, as measured by the IOI-HA. The prediction accuracy was lower than the accuracy reported by some previous research that used a cross-sectional design.DOI: 10.3109/14992027.2015.1120892PMCID: PMC4823154",pubmed,26878163,10.3109/14992027.2015.1120892
lightemitting device for supporting auditory awareness of hearingimpaired people during group conversations,"This study proposes a novel wearable device that augments the auditory awareness of hearing impaired people to help them identify the speaker during group conversations. A number of hearing impaired people are able to understand speech by using auditory-oral methods such as lip-reading, however they always need to watch the speaker closely. The proposed device estimates the direction of the sound source and indicates the estimated direction in real time with light-emitting diodes (LEDs), thus aiding hearing impaired people during group conversations. The device has 4 unidirectional microphones and estimates the sound source direction by comparing the signal amplitudes in each microphone. Hearing assistance devices are required to be small and wearable in order to assist anytime in daily life. The proposed device is small, wearable, and can easily be used in various situations.",ieee,1062-922X,10.1109/SMC.2013.608
eegbased emotion identification using 1d deep residual shrinkage network with microstate features,"Previous studies on emotion identification from electroencephalogram (EEG) mostly focused on normal and depressed people. However, hearing-impaired subjects may require emotional identification due to their chronic lack of perception of auditory information. In this article, we designed an experiment to collect EEG signals from 15 hearing-impaired subjects when they are watching the four kinds of emotional movie clips (happiness, calmness, sadness, and fear). The novel K -means method is used to extract the ten kinds of microstates (as A, B, C, D, E, F, G, H, I, and J) from the raw EEG signal, and then the new EEG single will be retrofitted by those ten microstates. For feature extraction, six kinds of microstate features (global explained variance (GEV), GEV total (GEVT), global field power (GFP), coverage, duration, and occurrence) are calculated. To classify the microstate features, a 1-D deep residual shrinkage network (1-D-DRSN) is utilized, which can filter the emotional irrelevant noise information, and capture emotional representational information. Experimental results show that the proposed model can significantly improve performance compared with other machine learning methods, with an average accuracy of 87.48%. Moreover, we explore different combinations of microstate features to reduce redundant information, and the combination of occurrence, GEV, and coverage reaches 90.15%. From the exploration of each microstate, we find that microstate C has the advantage with an average accuracy of 49.07%.  © 2001-2012 IEEE.",scopus,2-s2.0-85148445529,10.1109/JSEN.2023.3239507
occupational hearing loss for platinum miners in south africa a case study of data sharing practices and ethical challenges in the mining industry,"589. Int J Environ Res Public Health. 2021 Dec 21;19(1):1. doi: 10.3390/ijerph19010001.Occupational Hearing Loss for Platinum Miners in South Africa: A Case Study of Data Sharing Practices and Ethical Challenges in the Mining Industry.Ntlhakana L(1)(2), Nelson G(1), Khoza-Shangase K(2), Dorkin E(3).Author information:(1)Faculty of Health Sciences, School of Public Health, University of the Witwatersrand, Johannesburg 2000, South Africa.(2)Department of Speech Pathology and Audiology, Faculty of Humanities, School of Human and Community Development, University of the Witwatersrand, Johannesburg 2050, South Africa.(3)Anglo American, Johannesburg 2091, South Africa.BACKGROUND: The relevant legislation ensures confidentiality and has paved the way for data handling and sharing. However, the industry remains uncertain regarding big data handling and sharing practices for improved healthcare delivery and medical research.METHODS: A semi-qualitative cross-sectional study was used which entailed analysing miners' personal health records from 2014 to 2018. Data were accessed from the audiometry medical surveillance database (n = 480), the hearing screening database (n = 24,321), and the occupational hygiene database (n = 15,769). Ethical principles were applied to demonstrate big data protection and sharing.RESULTS: Some audiometry screening and occupational hygiene records were incomplete and/or inaccurate (N = 4675). The database containing medical disease and treatment records could not be accessed. Ethical challenges included a lack of clarity regarding permission rights when sharing big data, and no policy governing the divulgence of miners' personal and medical records for research.CONCLUSION: This case study illustrates how research can be effectively, although not maliciously, obstructed by the strict protection of employee medical data. Clearly communicated company policies should be developed for the sharing of workers' records in the mining industry to improve HCPs.DOI: 10.3390/ijerph19010001PMCID: PMC8750121",pubmed,35010261,10.3390/ijerph19010001
psychoacoustic and electrophysiological electricacoustic interaction effects in cochlear implant users with ipsilateral residual hearing,"143. Hear Res. 2020 Feb;386:107873. doi: 10.1016/j.heares.2019.107873. Epub 2019 Dec 18.Psychoacoustic and electrophysiological electric-acoustic interaction effects in cochlear implant users with ipsilateral residual hearing.Imsiecke M(1), Büchner A(2), Lenarz T(3), Nogueira W(4).Author information:(1)Department of Otorhinolaryngology, Hannover Medical School, Hanover, Germany. Electronic address: imsiecke.marina@mh-hannover.de.(2)Department of Otorhinolaryngology, Hannover Medical School, Hanover, Germany; Cluster of Excellence 'Hearing4All', Hanover, Germany. Electronic address: buechner.andreas@mh-hannover.de.(3)Department of Otorhinolaryngology, Hannover Medical School, Hanover, Germany; Cluster of Excellence 'Hearing4All', Hanover, Germany. Electronic address: lenarz.thomas@mh-hannover.de.(4)Department of Otorhinolaryngology, Hannover Medical School, Hanover, Germany; Cluster of Excellence 'Hearing4All', Hanover, Germany. Electronic address: nogueiravazquez.waldo@mh-hannover.de.Cochlear implant users with ipsilateral residual hearing combine acoustic and electric hearing in one ear, this is called electric-acoustic stimulation (EAS). In EAS users, masking can be shown for electric probes in the presence of acoustic maskers and vice versa. Masking effects in acoustic hearing are generally attributed to nonlinearities of the basilar membrane and hair cell adaptation effects. However, similar masking patterns are observed more centrally in electric hearing. Consequently, there is no consensus so far on the level of interaction between the two modalities. Animal studies have shown that electric-acoustic interaction effects can result in reduced physiological responses in the cochlear nerve and the inferior colliculus. In CI users with residual hearing, it has recently become feasible to record intracochlear potentials with a high spatial resolution via the implanted electrode array. An investigation of the electrophysiological effects during combined electric-acoustic stimulation in humans might be used to assess peripheral mechanisms of masking. Seventeen MED-EL Flex electrode users with ipsilateral residual hearing participated in both a behavioral and a physiological electric-acoustic masking experiment. Psychoacoustic methods were used to measure the changes in behavioral thresholds due to the presence of a masker of the opposing modality. Subjects were stimulated electrically with unmodulated pulse trains using a research interface and acoustically with pure tones delivered via headphones. Auditory response telemetry was used to obtain objective electrophysiological changes of electrically evoked compound action potential and electrocochleography for electric, acoustic and combined electric-acoustic presentation in the same subjects. Behavioral thresholds of probe tones, either electric or acoustic, were significantly elevated in the presence of acoustic or electric maskers, respectively. 15 subjects showed significant electric threshold elevation with acoustic masking that did not depend on the electric-acoustic frequency difference (EAFD), a measure for the proximity of stimulation sites in the cochlea. Electric masking showed significant threshold elevation in eleven subjects, which depended significantly on EAFD. In the electrophysiological masking experiment, reduced responses to electric and acoustic stimulation with additional stimulation of the opposing modality were observed. Results showed a similar asymmetry as the psychoacoustic masking experiment. Response reduction was smaller than threshold elevation, especially for electric masking. Some subjects showed reduced responses to acoustic stimulation with electric masking, especially for small EAFD. The reduction of electrically evoked responses was significant in some subjects. No correlation was observed between psychoacoustic and electrophysiological masking results. From present study, it can be concluded that both electric and acoustic stimulation mask each other when presented simultaneously. Electrophysiological measurements indicate that masking effects are already to some extent present in the periphery.Copyright © 2019 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2019.107873",pubmed,31884220,10.1016/j.heares.2019.107873
a twopath model of auditory modulation detection using temporal fine structure and envelope cues,"349. Eur J Neurosci. 2020 Mar;51(5):1265-1278. doi: 10.1111/ejn.13846. Epub 2018 Feb 12.A two-path model of auditory modulation detection using temporal fine structure and envelope cues.Ewert SD(1), Paraouty N(2), Lorenzi C(2).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4All, Universität Oldenburg, 26111, Oldenburg, Germany.(2)Laboratoire des systèmes perceptifs, Département d'études cognitives, École normale supérieure, CNRS, PSL Research University, Paris, France.A model using temporal-envelope cues was previously developed to explain perceptual interference effects between amplitude modulation and frequency modulation (FM). As that model could not accurately predict FM sensitivity and the interference effects, temporal fine structure (TFS) cues were added to the model. Thus, following the initial stage of the model consisting of a linear filter bank simulating cochlear filtering, processing was split into an 'envelope path' based on envelope power cues and a 'TFS path' based on a measure of the distribution of time intervals between successive zero-crossings. This yielded independent detectability indices for envelope and TFS cues, which were optimally combined to produce a single decision statistic. Independent internal noises in the envelope and TFS paths were adjusted to match the data. Simulations indicate that TFS cues are required to account for FM data for young normal-hearing listeners and that TFS processing is impaired for both older normal-hearing and hearing-impaired listeners. The role of TFS was further assessed by relating the monaural FM sensitivity to measures of interaural phase difference, commonly assumed to rely on binaural TFS sensitivity. The model demonstrates that binaural TFS sensitivity is considerably lower than monaural TFS sensitivity. Similar to FM thresholds, interaural phase difference sensitivity declined with age and hearing loss, although higher degradations were observed in binaural temporal processing compared to monaural processing. Overall, this model provides a novel tool to explore the mechanisms involved in FM processing in the normal auditory system and the degradations in FM sensitivity with ageing and hearing loss.© 2018 Federation of European Neuroscience Societies and John Wiley & Sons Ltd.DOI: 10.1111/ejn.13846",pubmed,29368797,10.1111/ejn.13846
sex difference in the efferent inner hair cell synapses of the aging murine cochlea,"570. Hear Res. 2021 May;404:108215. doi: 10.1016/j.heares.2021.108215. Epub 2021 Feb 21.Sex difference in the efferent inner hair cell synapses of the aging murine cochlea.Dondzillo A(1), Takeda H(2), Gubbels SP(3).Author information:(1)Department of Otolaryngology-Head and Neck Surgery, School of Medicine, University of Colorado Anschutz Medical Campus, Aurora, CO, USA. Electronic address: anna.dondzillo@ucdenver.edu.(2)Department of Otolaryngology-Head and Neck Surgery, School of Medicine, University of Colorado Anschutz Medical Campus, Aurora, CO, USA; Departments of Otolaryngology-Head and Neck Surgery, Kumamoto University Graduate School of Medicine, Kumamoto city, Japan.(3)Department of Otolaryngology-Head and Neck Surgery, School of Medicine, University of Colorado Anschutz Medical Campus, Aurora, CO, USA.Efferent innervation of the inner hair cells changes over time. At an early age in mice, inner hair cells receive efferent feedback, which helps fine-tune tonotopic maps in the brainstem. In adulthood, inner hair cell efferent innervation wanes but increases again in older animals. It is not clear, however, whether age-related inner hair cell efferents increase along the entire range of the cochlear frequencies, or if this increase is restricted to a particular frequency-region, and whether this phenomenon occurs in both sexes. Age-related hearing loss, presbycusis, affects men and women differently. In mice, this difference is also strain specific. In aging black six mice, the auditory brainstem response thresholds increase in females earlier than in males. Here, we study age-related increase of the inner hair cell efferent innervation throughout the cochlea before hearing onset, in one month old and in ten months old and older male and female black six mice. We collected confocal images of immunostained inner hair cell efferents and quantified the labeled terminals in the entire cochlea using a machine learning algorithm. The overall number of the inner hair cell efferents in both sexes did not change significantly between age-groups. The distribution of the inner hair cell efferent innervation did not differ across frequencies in the cochlea. However, in females, inner hair cells received on average up to four times more efferent innervation than in males per each of the frequency regions tested. Sex differences were also found in the oldest age-group tested (≥ 10 months) where on average inner hair cells received six times more efferents in females than in males of matching age. Our findings emphasize the importance of including both sexes in sensorineural hearing loss research.Copyright © 2021 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2021.108215PMCID: PMC8143057",pubmed,33677192,10.1016/j.heares.2021.108215
supernumerary human hair cellssigns of regeneration or impaired development a field emission scanning electron microscopy study,"676. Ups J Med Sci. 2017 Mar;122(1):11-19. doi: 10.1080/03009734.2016.1271843. Epub 2017 Feb 1.Supernumerary human hair cells-signs of regeneration or impaired development? A field emission scanning electron microscopy study.Rask-Andersen H(1)(2), Li H(1)(2), Löwenheim H(3)(4)(5), Müller M(3)(4)(5), Pfaller K(6), Schrott-Fischer A(7), Glueckert R(7).Author information:(1)a Department of Surgical Sciences, Head and Neck Surgery , Section of Otolaryngology, Uppsala University Hospital , Uppsala , Sweden.(2)b Department of Otolaryngology , Uppsala University Hospital , Uppsala , Sweden.(3)c Department of Otolaryngology , Medical University of Innsbruck , Innsbruck , Austria.(4)d Medical Campus University of Oldenburg School of Medicine and Health Sciences, European Medical School , Oldenburg , Germany.(5)e Research Center of Neurosensory Science, University of Oldenburg , Oldenburg , Germany.(6)f Cluster of Excellence Hearing4all , University of Oldenburg , Oldenburg , Germany.(7)g Department of Histology and Molecular Cell Biology , Institute of Anatomy and Histology, Medical University of Innsbruck , Innsbruck , Austria.BACKGROUND: Current attempts to regenerate cochlear sensorineural structures motivate further inspection of the human organ of hearing. Here, we analyzed the supernumerary inner hair cell (sIHC), a possible sign of regeneration and cell replacement.METHODS: Human cochleae were studied using field emission scanning electron microscopy (FESEM; maximum resolution 2 nm) obtained from individuals aged 44, 48, and 58 years with normal sensorineural pure-tone average (PTA) thresholds (PTA <20 dB). The wasted tissue was harvested during trans-cochlear approaches and immediately fixed for ultrastructural analysis.RESULTS: All specimens exhibited sIHCs at all turns except at the extreme lower basal turn. In one specimen, it was possible to image and count the inner hair cells (IHCs) along the cochlea representing the 0.2 kHz-8 kHz region according to the Greenwood place/frequency scale. In a region with 2,321 IHCs, there were 120 scattered one-cell losses or 'gaps' (5%). Forty-two sIHCs were present facing the modiolus. Thirty-eight percent of the sIHCs were located near a 'gap' in the IHC row (±6 IHCs).CONCLUSIONS: The prevalence of ectopic inner hair cells was higher than expected. The morphology and placement could reflect a certain ongoing regeneration. Further molecular studies are needed to verify if the regenerative capacity of the human auditory periphery might have been underestimated.DOI: 10.1080/03009734.2016.1271843PMCID: PMC5361427",pubmed,28145795,10.1080/03009734.2016.1271843
towards the optical cochlear implant optogenetic approaches for hearing restoration,"20. EMBO Mol Med. 2020 Apr 7;12(4):e11618. doi: 10.15252/emmm.201911618. Epub 2020 Mar 30.Towards the optical cochlear implant: optogenetic approaches for hearing restoration.Dieter A(1)(2), Keppeler D(1), Moser T(1)(3)(4)(5).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(2)Göttingen Graduate School for Neurosciences, Biophysics and Molecular Biosciences, University of Göttingen, Göttingen, Germany.(3)Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(4)Auditory Neuroscience Group, Max Planck Institute of Experimental Medicine, Göttingen, Germany.(5)Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, Göttingen, Germany.Cochlear implants (CIs) are considered the most successful neuroprosthesis as they enable speech comprehension in the majority of half a million CI users suffering from sensorineural hearing loss. By electrically stimulating the auditory nerve, CIs constitute an interface re-connecting the brain and the auditory scene, providing the patient with information regarding the latter. However, since electric current is hard to focus in conductive environments such as the cochlea, the precision of electrical sound encoding-and thus quality of artificial hearing-is limited. Recently, optogenetic stimulation of the cochlea has been suggested as an alternative approach for hearing restoration. Cochlear optogenetics promises increased spectral selectivity of artificial sound encoding, hence improved hearing, as light can conveniently be confined in space to activate the auditory nerve within smaller tonotopic ranges. In this review, we discuss the latest experimental and technological developments of cochlear optogenetics and outline the remaining challenges on the way to clinical translation.© 2020 The Authors. Published under the terms of the CC BY 4.0 license.DOI: 10.15252/emmm.201911618PMCID: PMC7136966",pubmed,32227585,10.15252/emmm.201911618
potential of augmented reality platforms to improve individual hearing aids and to support more ecologically valid research,"389. Ear Hear. 2020 Nov/Dec;41 Suppl 1(Suppl 1):140S-146S. doi: 10.1097/AUD.0000000000000961.Potential of Augmented Reality Platforms to Improve Individual Hearing Aids and to Support More Ecologically Valid Research.Mehra R(1), Brimijoin O, Robinson P, Lunner T.Author information:(1)Facebook Reality Labs Research, Redmond, Washington, USA.An augmented reality (AR) platform combines several technologies in a system that can render individual ""digital objects"" that can be manipulated for a given purpose. In the audio domain, these may, for example, be generated by speaker separation, noise suppression, and signal enhancement. Access to the ""digital objects"" could be used to augment auditory objects that the user wants to hear better. Such AR platforms in conjunction with traditional hearing aids may contribute to closing the gap for people with hearing loss through multimodal sensor integration, leveraging extensive current artificial intelligence research, and machine-learning frameworks. This could take the form of an attention-driven signal enhancement and noise suppression platform, together with context awareness, which would improve the interpersonal communication experience in complex real-life situations. In that sense, an AR platform could serve as a frontend to current and future hearing solutions. The AR device would enhance the signals to be attended, but the hearing amplification would still be handled by hearing aids. In this article, suggestions are made about why AR platforms may offer ideal affordances to compensate for hearing loss, and how research-focused AR platforms could help toward better understanding of the role of hearing in everyday life.DOI: 10.1097/AUD.0000000000000961PMCID: PMC7676615",pubmed,33105268,10.1097/AUD.0000000000000961
audiovisual speech processing in agerelated hearing loss stronger integration and increased frontal lobe recruitment,"108. Neuroimage. 2018 Jul 15;175:425-437. doi: 10.1016/j.neuroimage.2018.04.023. Epub 2018 Apr 12.Audio-visual speech processing in age-related hearing loss: Stronger integration and increased frontal lobe recruitment.Rosemann S(1), Thiel CM(2).Author information:(1)Biological Psychology, Department of Psychology, Department for Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany; Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany. Electronic address: Stephanie.rosemann@uni-oldenburg.de.(2)Biological Psychology, Department of Psychology, Department for Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany; Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany.Hearing loss is associated with difficulties in understanding speech, especially under adverse listening conditions. In these situations, seeing the speaker improves speech intelligibility in hearing-impaired participants. On the neuronal level, previous research has shown cross-modal plastic reorganization in the auditory cortex following hearing loss leading to altered processing of auditory, visual and audio-visual information. However, how reduced auditory input effects audio-visual speech perception in hearing-impaired subjects is largely unknown. We here investigated the impact of mild to moderate age-related hearing loss on processing audio-visual speech using functional magnetic resonance imaging. Normal-hearing and hearing-impaired participants performed two audio-visual speech integration tasks: a sentence detection task inside the scanner and the McGurk illusion outside the scanner. Both tasks consisted of congruent and incongruent audio-visual conditions, as well as auditory-only and visual-only conditions. We found a significantly stronger McGurk illusion in the hearing-impaired participants, which indicates stronger audio-visual integration. Neurally, hearing loss was associated with an increased recruitment of frontal brain areas when processing incongruent audio-visual, auditory and also visual speech stimuli, which may reflect the increased effort to perform the task. Hearing loss modulated both the audio-visual integration strength measured with the McGurk illusion and brain activation in frontal areas in the sentence task, showing stronger integration and higher brain activation with increasing hearing loss. Incongruent compared to congruent audio-visual speech revealed an opposite brain activation pattern in left ventral postcentral gyrus in both groups, with higher activation in hearing-impaired participants in the incongruent condition. Our results indicate that already mild to moderate hearing loss impacts audio-visual speech processing accompanied by changes in brain activation particularly involving frontal areas. These changes are modulated by the extent of hearing loss.Copyright © 2018 Elsevier Inc. All rights reserved.DOI: 10.1016/j.neuroimage.2018.04.023",pubmed,29655940,10.1016/j.neuroimage.2018.04.023
deletion of clusterin protects cochlear hair cells against hair cell aging and ototoxicity,"302. Neural Plast. 2021 May 29;2021:9979157. doi: 10.1155/2021/9979157. eCollection 2021.Deletion of Clusterin Protects Cochlear Hair Cells against Hair Cell Aging and Ototoxicity.Zhao X(1), Henderson HJ(2), Wang T(1), Liu B(1)(3), Li Y(1).Author information:(1)Department of Otorhinolaryngology Head and Neck Surgery, Beijing Tongren Hospital, Capital Medical University, Beijing, China.(2)Department of Biomedical Sciences, Creighton University School of Medicine, Omaha, Nebraska 68178, USA.(3)Beijing Institute of Otolaryngology, Key Laboratory of Otolaryngology Head and Neck Surgery (Capital Medical University), Ministry of Education, Beijing, China.Hearing loss is a debilitating disease that affects 10% of adults worldwide. Most sensorineural hearing loss is caused by the loss of mechanosensitive hair cells in the cochlea, often due to aging, noise, and ototoxic drugs. The identification of genes that can be targeted to slow aging and reduce the vulnerability of hair cells to insults is critical for the prevention of sensorineural hearing loss. Our previous cell-specific transcriptome analysis of adult cochlear hair cells and supporting cells showed that Clu, encoding a secreted chaperone that is involved in several basic biological events, such as cell death, tumor progression, and neurodegenerative disorders, is expressed in hair cells and supporting cells. We generated Clu-null mice (C57BL/6) to investigate its role in the organ of Corti, the sensory epithelium responsible for hearing in the mammalian cochlea. We showed that the deletion of Clu did not affect the development of hair cells and supporting cells; hair cells and supporting cells appeared normal at 1 month of age. Auditory function tests showed that Clu-null mice had hearing thresholds comparable to those of wild-type littermates before 3 months of age. Interestingly, Clu-null mice displayed less hair cell and hearing loss compared to their wildtype littermates after 3 months. Furthermore, the deletion of Clu is protected against aminoglycoside-induced hair cell loss in both in vivo and in vitro models. Our findings suggested that the inhibition of Clu expression could represent a potential therapeutic strategy for the alleviation of age-related and ototoxic drug-induced hearing loss.Copyright © 2021 Xiaochang Zhao et al.DOI: 10.1155/2021/9979157PMCID: PMC8181089",pubmed,34194490,10.1155/2021/9979157
hochfrequentes training der auditiven analyse bei aphasie einzelfallstudie bei einem 16jhrigen patienten mit wernickeaphasie,"Aim of this treatment study was to improve speech comprehension of a 16-year-old patient with Wernicke's aphasia and a partial disorder of auditory analysis (""word sound deafness""). To this end, tasks addressing phoneme-grapheme correspondence with syllables were used as well as speech discrimination tasks with syllables, consonant clusters and neologisms. The patient showed significant improvement with trained items and generalization effects to untrained items. Furthermore, secondary improvements could be observed in tasks which are based on the auditory analysis (e.g. lexical decision). However, performance in an untrained control task (rime generation) did not change, indicating that the effects of therapy were specific. Moreover, the patient showed an enhanced self-monitoring, evidenced by an increased rate of self-corrections.",cinahl,9320547,0.2443/skv-s-2012-53020120601
consequences of stimulus type on higherorder processing in singlesided deaf cochlear implant users,"409. Audiol Neurootol. 2016;21(5):305-315. doi: 10.1159/000452123. Epub 2016 Nov 19.Consequences of Stimulus Type on Higher-Order Processing in Single-Sided Deaf Cochlear Implant Users.Finke M(1), Sandmann P, Bönitz H, Kral A, Büchner A.Author information:(1)Cluster of Excellence ''Hearing4all'', Hannover Medical School, Hannover, Germany.Single-sided deaf subjects with a cochlear implant (CI) provide the unique opportunity to compare central auditory processing of the electrical input (CI ear) and the acoustic input (normal-hearing, NH, ear) within the same individual. In these individuals, sensory processing differs between their two ears, while cognitive abilities are the same irrespectively of the sensory input. To better understand perceptual-cognitive factors modulating speech intelligibility with a CI, this electroencephalography study examined the central-auditory processing of words, the cognitive abilities, and the speech intelligibility in 10 postlingually single-sided deaf CI users. We found lower hit rates and prolonged response times for word classification during an oddball task for the CI ear when compared with the NH ear. Also, event-related potentials reflecting sensory (N1) and higher-order processing (N2/N4) were prolonged for word classification (targets versus nontargets) with the CI ear compared with the NH ear. Our results suggest that speech processing via the CI ear and the NH ear differs both at sensory (N1) and cognitive (N2/N4) processing stages, thereby affecting the behavioral performance for speech discrimination. These results provide objective evidence for cognition to be a key factor for speech perception under adverse listening conditions, such as the degraded speech signal provided from the CI.© 2016 S. Karger AG, Basel.DOI: 10.1159/000452123",pubmed,27866186,10.1159/000452123
on the interplay between cochlear gain loss and temporal envelope coding deficits,"294. Adv Exp Med Biol. 2016;894:467-475. doi: 10.1007/978-3-319-25474-6_49.On the Interplay Between Cochlear Gain Loss and Temporal Envelope Coding Deficits.Verhulst S(1), Piktel P(2), Jagadeesh A(2), Mauermann M(2).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4all, Department of Medical Physics and Acoustics, Oldenburg University, Carl-von-Ossietzky Strasse 9-11, 26129, Oldenburg, Germany. Sarah.verhulst@uni-oldenburg.de.(2)Medizinische Physik and Cluster of Excellence Hearing4all, Department of Medical Physics and Acoustics, Oldenburg University, Carl-von-Ossietzky Strasse 9-11, 26129, Oldenburg, Germany.Hearing impairment is characterized by two potentially coexisting sensorineural components: (i) cochlear gain loss that yields wider auditory filters, elevated hearing thresholds and compression loss, and (ii) cochlear neuropathy, a noise-induced component of hearing loss that may impact temporal coding fidelity of supra-threshold sound. This study uses a psychoacoustic amplitude modulation (AM) detection task in quiet and multiple noise backgrounds to test whether these aspects of hearing loss can be isolated in listeners with normal to mildly impaired hearing ability. Psychoacoustic results were compared to distortion-product otoacoustic emission (DPOAE) thresholds and envelope-following response (EFR) measures. AM thresholds to pure-tone carriers (4 kHz) in normal-hearing listeners depended on temporal coding fidelity. AM thresholds in hearing-impaired listeners were normal, indicating that reduced cochlear gain may counteract how reduced temporal coding fidelity degrades AM thresholds. The amount with which a 1-octave wide masking noise worsened AM detection was inversely correlated to DPOAE thresholds. The narrowband noise masker was shown to impact the hearing-impaired listeners more so than the normal hearing listeners, suggesting that this masker may be targeting a temporal coding deficit. This study offers a window into how psychoacoustic difference measures can be adopted in the differential diagnostics of hearing deficits in listeners with mixed forms of sensorineural hearing loss.DOI: 10.1007/978-3-319-25474-6_49",pubmed,27080688,10.1007/978-3-319-25474-6_49
deaf helper mobile application for interaction of hearing disorders communities,"People with hearing loss in this world have not received much serious attention from the authorities. This makes these sufferers confused in choosing learning media to interact with and isolated from their social environment. This application was created to help people with hearing loss to be noticed and understood from the way they communicate using sign language through the mobile application called Assistant for the Deaf, which has many features such as registration, interactive videos, sign language translator, forums, customer service, library, information, history, events, donations, and shops. The application is designed using use case diagrams and class diagrams modeling the database, and the implementation used Android Studio and MySQL database.",ieee,,10.1109/ICAIS53314.2022.9742988
intracochlear near infrared stimulation feasibility of optoacoustic stimulation in vivo,"627. Hear Res. 2019 Jan;371:40-52. doi: 10.1016/j.heares.2018.11.003. Epub 2018 Nov 12.Intracochlear near infrared stimulation: Feasibility of optoacoustic stimulation in vivo.Baumhoff P(1), Kallweit N(2), Kral A(3).Author information:(1)Institute of AudioNeuroTechnology and Department of Experimental Otology, ENT Clinics, Hannover Medical School, Stadtfelddamm 34, 30625, Hannover, Germany. Electronic address: baumhoff.peter@mh-hannover.de.(2)Laser Zentrum Hannover e.V. (LZH), Hollerithallee 8, 30419, Hannover, Germany; DFG Cluster of Excellence, Hearing 4 All, Germany. Electronic address: nicolekallweit@gmx.de.(3)Institute of AudioNeuroTechnology and Department of Experimental Otology, ENT Clinics, Hannover Medical School, Stadtfelddamm 34, 30625, Hannover, Germany; DFG Cluster of Excellence, Hearing 4 All, Germany. Electronic address: kral.andrej@mh-hannover.de.Intracochlear optical stimulation has been suggested as an alternative approach to hearing prosthetics in recent years. This study investigated the properties of a near infrared laser (NIR) induced optoacoustic effect. Pressure recordings were performed at the external meatus of anaesthetized guinea pigs during intracochlear NIR stimulation. The sound pressure and power spectra were determined. The results were compared to multi unit responses in the inferior colliculus (IC). Additionally, the responses to NIR stimulation were compared to IC responses induced by intracochlear electric stimulation at the same cochlear position to investigate a potentially confounding contribution of direct neural NIR stimulation. The power spectra of the sound recorded at the external meatus (n = 7) had most power at frequencies below 10 kHz and showed little variation for different stimulation sites. The mean spike rates of IC units responding to intracochlear NIR stimulation (n = 222) of 17 animals were significantly correlated with the power of the externally recorded signal at frequencies corresponding to the best frequencies of the IC units. The response strength as well as the sound pressure at the external meatus depended on the pulse peak power of the optical stimulus. The sound pressure recorded at the external meatus reached levels above 70 dB SPL peak equivalent. In hearing animals a cochlear activation apical to the location of the fiber was found. The absence of any NIR responses after pharmacologically deafening and the comparison to electric stimulation at the NIR stimulation site revealed no indication of a confounding direct neural NIR stimulation. Intracochlear optoacoustic stimulation might become useful in combined electro-acoustic stimulation devices in the future.Copyright © 2018 The Authors. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2018.11.003",pubmed,30458383,10.1016/j.heares.2018.11.003
mismatched response predicts behavioral speech discrimination outcomes in infants with hearing loss and normal hearing references,"Children with hearing loss (HL) remain at risk for poorer language abilities than normal hearing (NH) children despite targeted interventions; reasons for these differences remain unclear. In NH children, research suggests speech discrimination is related to language outcomes, yet we know little about it in children with HL under the age of 2 years. We utilized a vowel contrast, /a-i/, and a consonant-vowel contrast, /ba-da/, to examine speech discrimination in 47 NH infants and 40 infants with HL. At Mean age = 3 months, EEG recorded from 11 scalp electrodes was used to compute the time-frequency mismatched response (TF-MMRSE) to the contrasts; at Mean age =9 months, behavioral discrimination was assessed using a head turn task. A machine learning (ML) classifier was used to predict behavioral discrimination when given an arbitrary TF-MMRSE as input, achieving accuracies of 73% for exact classification and 92% for classification within a distance of one class. Linear fits revealed a robust relationship regardless of hearing status or speech contrast. TF-MMRSE responses in the delta (1-3.5 Hz), theta (3.5-8 Hz), and alpha (8-12 Hz) bands explained the most variance in behavioral task performance. Our findings demonstrate the feasibility of using TF-MMRSE to predict later behavioral speech discrimination. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc20&DO=10.1111%2finfa.12386
altered topological patterns of gray matter networks in tinnitus a graphtheoreticalbased study,"830. Front Neurosci. 2020 May 27;14:541. doi: 10.3389/fnins.2020.00541. eCollection 2020.Altered Topological Patterns of Gray Matter Networks in Tinnitus: A Graph-Theoretical-Based Study.Lin X(1)(2), Chen Y(3), Wang M(4), Song C(5), Lin B(6), Yuan X(5), Liu Q(5), Yang H(7), Jiang N(1)(2).Author information:(1)Department of Nuclear Medicine, Sun Yat-sen Memorial Hospital, Sun Yat-sen University, Guangzhou, China.(2)Department of Nuclear Medicine, The Seventh Affiliated Hospital, Sun Yat-sen University, Shenzhen, China.(3)Department of Radiology, Shenzhen Traditional Chinese Medicine Hospital, Shenzhen, China.(4)Department of Hearing and Speech Sciences, Xinhua College of Sun Yat-sen University, Guangzhou, China.(5)Department of Radiology, Sun Yat-sen Memorial Hospital, Sun Yat-sen University, Guangzhou, China.(6)Department of Radiology, Peking University Shen Zhen Hospital, Shenzhen, China.(7)Department of Otolaryngology, Sun Yat-sen Memorial Hospital, Sun Yat-sen University, Guangzhou, China.OBJECTIVE: Tinnitus is a prevalent hearing disorder, which could have a devastating impact on a patient's life. Functional studies have revealed connectivity pattern changes in the tinnitus brains that suggested a change of network dynamics as well as topological organization. However, no studies have yet provided evidence for the topological network changes in the gray matter. In this research, we aim to use the graph-theoretical approach to investigate the changes of topology in the tinnitus brain using structural MRI data, which could provide insights into the underlying anatomical basis for the neural mechanism in generating phantom sounds.METHODS: We collected 3D MRI images on 46 bilateral tinnitus patients and 46 age and gender-matched healthy controls. Brain networks were constructed with correlation matrices of the cortical thickness and subcortical volumes of 80 cortical/subcortical regions of interests. Global network properties were analyzed using local and global efficiency, clustering coefficient, and small-world coefficient, and regional network properties were evaluated using the betweenness coefficient for hub connectivity, and interregional correlations for edge properties. Between-group differences in cortical thickness and subcortical volumes were assessed using independent sample t-tests, and local efficiency, global efficiency, clustering coefficient, sigma, and interregional correlation were compared using non-parametric permutation tests.RESULTS: Tinnitus was found to have increased global efficiency, local efficiency, and cluster coefficient, indicating generally heightened connectivity of the network. The small-world coefficient remained normal for tinnitus, indicating intact small-worldness. Betweenness centrality analysis showed that hubs in the amygdala and parahippocampus were only found for tinnitus but not controls. In contrast, hubs in the auditory cortex, insula, and thalamus were only found for controls but not tinnitus. Interregional correlation analysis further found in tinnitus enhanced connectivity between the auditory cortex and prefrontal lobe, and decreased connectivity of the insula with anterior cingulate gyrus and parahippocampus.CONCLUSION: These findings provided the first morphological evidence of altered topological organization of the brain networks in tinnitus. These alterations suggest that heightened efficiency of the brain network and altered auditory-limbic connection for tinnitus, which could be developed in compensation for the auditory deafferentation, leading to overcompensation and, ultimately, an emotional and cognitive burden.Copyright © 2020 Lin, Chen, Wang, Song, Lin, Yuan, Liu, Yang and Jiang.DOI: 10.3389/fnins.2020.00541PMCID: PMC7267018",pubmed,32536854,10.3389/fnins.2020.00541
hearing norton sound a community randomised trial protocol to address childhood hearing loss in rural alaska,"216. BMJ Open. 2019 Jan 15;9(1):e023078. doi: 10.1136/bmjopen-2018-023078.Hearing Norton Sound: a community randomised trial protocol to address childhood hearing loss in rural Alaska.Emmett SD(1)(2), Robler SK(3), Wang NY(4)(5), Labrique A(6), Gallo JJ(7), Hofstetter P(8).Author information:(1)Department of Surgery, Duke University School of Medicine, Durham, North Carolina, USA.(2)Duke Global Health Institute, Durham, North Carolina, USA.(3)Department of Audiology, Norton Sound Health Corporation, Nome, Alaska, USA.(4)Department of Medicine, Johns Hopkins University School of Medicine, Baltimore, Maryland, USA.(5)Departments of Biostatistics and Epidemiology, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, USA.(6)Departments of International Health and Epidemiology, Johns Hopkins University Bloomberg School of Public Health, Baltimore, Maryland, USA.(7)Department of Mental Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, USA.(8)Norton Sound Health Corporation, Nome, Alaska, USA.INTRODUCTION: The population in rural Alaska experiences a disproprionately high burden of infection-mediated hearing loss. While the state mandates school hearing screening, many children with hearing loss are not identified or are lost to follow-up before ever receiving treatment. A robust, tribally owned healthcare system exists in Alaska, but children with hearing loss must first be identified and referred for existing infrastructure to be used. This trial will evaluate a new school hearing screening and referral process in rural Alaska, with the goal of improving timely identification and treatment of childhood hearing loss.METHODS AND ANALYSIS: Comparative effectiveness community randomised trial testing digital innovations to improve school hearing screening and referral in 15 communities in the Norton Sound region of northwest Alaska, with data collection from October 2017 to February 2020. All children (K-12) attending school in Bering Strait School District with parental informed consent and child assent will be eligible (target recruitment n=1500). Participating children will undergo both the current school hearing screen and new mobile health (mHealth) screen, with screening test validity evaluated against an audiometric assessment. Communities will be cluster randomised to continue the current primary care referral process or receive telemedicine referral for follow-up diagnosis and treatment. The primary outcome will be time to International Statistical Classification of Diseases, 10th Revision, ear/hearing diagnosis from screening date, measured in days. Secondary outcomes will include: sensitivity and specificity of current school and mHealth screening protocols measured against a benchmark audiometric assessment (air and bone conduction audiometry, tympanometry and digital otoscopy); hearing loss prevalence; hearing-related quality of life; and school performance (AIMSweb). Intention-to-treat analysis will be used.ETHICS AND DISSEMINATION: This study has been approved by the Institutional Review Boards of Alaska Area, Norton Sound and Duke University and is registered on clinicaltrials.gov. Results will be distributed with equal emphasis on scientific and community dissemination.TRIAL REGISTRATION NUMBER: NCT03309553; Pre-results.© Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.DOI: 10.1136/bmjopen-2018-023078PMCID: PMC6340015",pubmed,30782695,10.1136/bmjopen-2018-023078
indiesign a learning module for indian sign language using supervised machine learning techniques,"Globalization is moving at a rapid pace with virtually no borders amongst various cultures and societies, the hearing-impaired face a monumental challenge to be included in the multi-faceted society. There has been a recent push towards introducing inclusiveness policies so that the hearing impaired shall have an easier way to be included in the society. However, as hearing-impaired people are stigmatized as being “diseased”, people do not interact with them. This affects their mental well-being and discourages future interactions. With a recent push towards digitization, it is believed that providing an online learning module can promote inclusiveness between the two parties. The issue of not being taught the concept of a word makes the learning incomplete. This is not only restricted to day-to-day concepts but also niche categories like technology, agriculture etc. To bridge this gap, this paper presents a unique model through which any person would be able to learn the words along with its concepts and their signage. As people with no hearing impairment tend to not learn sign language, this paper discusses methods through which learning sign language can be made unexacting.",ieee,,10.1109/ICAAIC53929.2022.9793170
evaluation of a semisupervised selfadjustment finetuning procedure for hearing aids,"180. Int J Audiol. 2023 Feb;62(2):159-171. doi: 10.1080/14992027.2022.2028022. Epub 2022 Jan 25.Evaluation of a semi-supervised self-adjustment fine-tuning procedure for hearing aids.Gößwein JA(1), Rennies J(1), Huber R(1), Bruns T(1), Hildebrandt A(2), Kollmeier B(1)(3).Author information:(1)Fraunhofer Institute for Digital Media Technology (IDMT), Oldenburg Branch for Hearing, Speech and Audio Technology (HSA) and Cluster of Excellence ""Hearing4all"", Oldenburg, Germany.(2)Department of Psychology, Carl von Ossietzky University of Oldenburg, and Cluster of Excellence ""Hearing4all"", Oldenburg, Germany.(3)Department of Medical Physics and Acoustics, Carl von Ossietzky University of Oldenburg, Oldenburg, Germany.OBJECTIVE: This study investigated the effects of different adjustment criteria and sound scenes on self-adjusted hearing-aid gain settings. Self-adjusted settings were evaluated for speech recognition in noise, perceived listening effort, and preference.DESIGN: This study evaluated a semi-supervised self-adjustment fine-tuning procedure that presents realistic everyday sound scenes in a laboratory environment, using a two-dimensional user interface, and enabling simultaneous changes in amplitude and spectral slope. While exploring the two-dimensional space of parameter settings, the hearing-aid users were instructed to optimise either listening comfort or speech understanding.STUDY SAMPLE: Twenty experienced hearing aid users (median age 69.5 years) were invited to participate in this study.RESULTS: Adjustment criterion and sound scenes had a significant effect on preferred gain settings. No differences in signal-to-noise ratios required for 50% speech intelligibility or in the perceived listening effort were observed between the adjusted settings of the two adjustment criteria. There was a preference for the self-adjusted settings over the prescriptive first fit.CONCLUSIONS: Listeners could reliably select their preferred gains to the two adjustment criteria and for different speech stimuli.DOI: 10.1080/14992027.2022.2028022",pubmed,35076330,10.1080/14992027.2022.2028022
mental health problems in adolescents with cochlear implants peer problems persist after controlling for additional handicaps references,"The aims of the present multi-center study were to investigate the extent of mental health problems in adolescents with a hearing loss and cochlear implants (CIs) in comparison to normal hearing (NH) peers and to investigate possible relations between the extent of mental health problems of young CI users and hearing variables, such as age at implantation, or functional gain of CI. The survey included 140 adolescents with CI (mean age = 14.7, SD = 1.5 years) and 140 NH adolescents (mean age = 14.8, SD = 1.4 years), their parents and teachers. Participants were matched by age, gender and social background. Within the CI group, 35 adolescents were identified as ""risk cases"" due to possible and manifest additional handicaps, and 11 adolescents were non-classifiable. Mental health problems were assessed with the Strengths and Difficulties Questionnaire (SDQ) in the versions ""Self,"" ""Parent,"" and ""Teacher."" The CI group showed significantly more ""Peer Problems"" than the NH group. When the CI group was split into a ""risk-group"" (35 ""risk cases"" and 11 non-classifiable persons) and a ""non-risk group"" (n = 94), increased peer problems were perceived in both CI subgroups by adolescents themselves. However, no further differences between the CI non-risk group and the NH group were observed in any rater. The CI risk-group showed significantly more hyperactivity compared to the NH group and more hyperactivity and conduct problems compared to the CI non-risk group. Cluster analyses confirmed that there were significantly more adolescents with high problems in the CI risk-group compared to the CI non-risk group and the NH group. Adolescents with CI, who were able to understand speech in noise had significantly less difficulties compared to constricted CI users. Parents, teachers, and clinicians should be aware that CI users with additionally special needs may have mental health problems. However, peer problems were also experienced by CI adolescents without additional handicaps. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc14&DO=10.3389%2ffpsyg.2015.00953
the phonological abilities of cantonesespeaking children with hearing loss,"477. J Speech Hear Res. 1994 Jun;37(3):671-9. doi: 10.1044/jshr.3703.671.The phonological abilities of Cantonese-speaking children with hearing loss.Dodd BJ(1), So LK.Author information:(1)Department of Speech and Hearing, University of Queensland, St. Lucia, Australia.Little is known about the acquisition of phonology by children with hearing loss who learn languages other than English. In this study, the phonological abilities of 12 Cantonese-speaking children (ages 4:2 to 6:11) with prelingual hearing impairment are described. All but 3 children had almost complete syllable-initial consonant repertoires; all but 2 had complete syllable-final consonant and vowel repertoires; and only 1 child failed to produce all nine tones. Children's perception of single words was assessed using sets of words that included tone, consonant, and semantic distractors. Although the performance of the subjects was not age appropriate, they nevertheless most often chose the target, with most errors observed for the tone distractor. The phonological rules used included those that characterize the speech of younger hearing children acquiring Cantonese (e.g., cluster reduction, stopping, and deaspiration). However, most children also used at least one unusual phonological rule (e.g., frication, addition, initial consonant deletion, and/or backing). These rules are common in the speech of Cantonese-speaking children diagnosed as phonologically disordered. The influence of the ambient language on children's patterns of phonological errors is discussed.DOI: 10.1044/jshr.3703.671",pubmed,8084197,10.1044/jshr.3703.671
proteomic analysis of the organ of corti using nanoscale liquid chromatography coupled with tandem mass spectrometry,"411. Int J Mol Sci. 2012;13(7):8171-8188. doi: 10.3390/ijms13078171. Epub 2012 Jul 2.Proteomic analysis of the organ of corti using nanoscale liquid chromatography coupled with tandem mass spectrometry.Peng H(1)(2), Liu M(1), Pecka J(3), Beisel KW(3), Ding SJ(1)(4).Author information:(1)Department of Pathology and Microbiology, University of Nebraska Medical Center, Omaha, NE 68198, USA.(2)Department of Environmental, Agricultural & Occupational Health, University of Nebraska Medical Center, Omaha, NE 68198, USA.(3)Department of Biomedical Sciences, Creighton University, Omaha, NE 68178, USA.(4)Mass Spectrometry and Proteomics Core Facility, University of Nebraska Medical Center, Omaha, NE 68198, USA.The organ of Corti (OC) in the cochlea plays an essential role in auditory signal transduction in the inner ear. For its minute size and trace amount of proteins, the identification of the molecules in pathophysiologic processes in the bone-encapsulated OC requires both delicate separation and a highly sensitive analytical tool. Previously, we reported the development of a high resolution metal-free nanoscale liquid chromatography system for highly sensitive phosphoproteomic analysis. Here this system was coupled with a LTQ-Orbitrap XL mass spectrometer to investigate the OC proteome from normal hearing FVB/N male mice. A total of 628 proteins were identified from six replicates of single LC-MS/MS analysis, with a false discovery rate of 1% using the decoy database approach by the OMSSA search engine. This is currently the largest proteome dataset for the OC. A total of 11 proteins, including cochlin, myosin VI, and myosin IX, were identified that when defective are associated with hearing impairment or loss. This study demonstrated the effectiveness of our nanoLC-MS/MS platform for sensitive identification of hearing loss-associated proteins from minute amount of tissue samples.DOI: 10.3390/ijms13078171PMCID: PMC3430228",pubmed,22942697,10.3390/ijms13078171
lowfrequency fluctuation amplitude changes in restingstate brain functional magnetic resonance imaging and its correlation with clinical hearing levels in patients with unilateral hearing impairment,"591. Zhonghua Yi Xue Za Zhi. 2023 Jul 4;103(25):1911-1917. doi: 10.3760/cma.j.cn112137-20221107-02337.[Low-frequency fluctuation amplitude changes in resting-state brain functional magnetic resonance imaging and its correlation with clinical hearing levels in patients with unilateral hearing impairment].[Article in Chinese; Abstract available in Chinese from the publisher]Li HM(1), Han XW(2), Sang CY(1), Sui Y(1), Ma GL(3).Author information:(1)Department of Radiology, Fu Xing Hospital, Capital Medical University, Beijing 100038, China.(2)The Affiliated Drum Tower Hospital of Nanjing University Medical School, Nanjing 210008, China.(3)China-Japan Friendship Clinical Medical College of Peking University, Beijing 100029, China.Objective: To investigate low-frequency fluctuation amplitude changes in resting-state brain fMRI and its correlation with clinical hearing levels in patients with clinical hearing level in patients with unilateral hearing impairment. Methods: Forty-five patients with unilateral hearing impairment[12 males and 33 females, aged 36-67 (46.0±9.7) years], and 31 controls with normal hearing[9 males and 22 females, aged 36-67 (46.0±10.1) years], were retrospectively included. All subjects underwent blood oxygen level-dependent (BOLD) resting-state functional magnetic resonance imaging and high-resolution T1-weighted imaging. The patients were divided into the left-sided hearing impaired group(24 cases), and the right-sided hearing impaired group(21 cases). After data being preprocessed, differences in low frequency amplitude (ALFF) metrics between the evaluated patients and controls were calculated and analyzed, and the statistics were corrected for Gaussian random field (GFR). Results: Overall comparative analysis of patients with hearing impairment showed that one-way ANOVA among the three groups showed abnormal ALFF values only in the right anterior cuneiform lobe (GRF adjusted P=0.002). The ALFF value of the hearing impaired group was higher than that of the control group in one cluster (peak coordinates: X=9, Y=-72, Z=48, T=5.82), involving the left occipital gyrus, right anterior cuneiform lobe, left superior cuneiform lobe, left superior parietal gyrus, and left angular gyrus (GRF adjusted P=0.031). The ALFF value of the hearing impaired group was lower than that of the control group in three clusters (peak coordinates: X=57, Y=-48, Z=-24; T=-4.99; X=45, Y=-66, Z=0, T=-4.06; X=42, Y=-12, Z=36, T=-4.03), involving the right inferior temporal gyrus, the right middle temporal gyrus, and the right precentral gyrus (GRF adjusted P=0.009). Compared with the control group, the ALFF value of the left hearing impairment group was significantly higher than that of the control group in one cluster (peak coordinates: X=-12, Y=-75, Z=45, T=5.78), involving the left anterior cuneiform lobe, right anterior cuneiform lobe, left middle occipital gyrus, left superior parietal gyrus, left superior occipital gyrus, left cuneiform lobe, and right cuneiform lobe (P=0.023 after GRF correction). Compared with the control group, the right hearing impairment group had a significantly higher ALFF value in one cluster (peak coordinates: X=9, Y=-46, Z=22, T=6.06), involving the left middle occipital gyrus, right anterior cuneiform lobe, left cuneiform lobe, right cuneiform lobe, left superior occipital gyrus, and right superior occipital gyrus (GRF adjusted P=0.022); The brain area with reduced ALFF values is located in the right inferior temporal gyrus (GRF adjusted P=0.029). Spearman's two-tailed correlation analysis between ALFF values and pure tone average in the abnormal brain regions showed that ALFF values in the abnormal brain regions correlated to some extent with the pure tone average (PTA) only in the left-sided hearing impaired group(PTA=2 000 Hz, r=0.318,P=0.033;PTA=4 000 Hz,r=0.386,P=0.009). Conclusion: The abnormal neural activity within the brain are different in patients with left-sided and right-sided hearing impairment, and the severity of hearing impairment is related to the difference in functional integration of brain regions.Publisher: 目的： 探讨单侧听力障碍患者静息态脑功能磁共振低频波动振幅（ALFF）差异及其与临床听力的相关性。 方法： 回顾性纳入2020年10月至2022年10月在首都医科大学附属复兴医院就诊的45例单侧听力障碍患者［男12例，女33例，年龄37~67（46.0±9.7）岁］和31名听力正常的对照者［男9名，女22名，年龄36~67（46.0±10.1）岁］。所有受试者均接受血氧水平依赖（BOLD）静息状态功能磁共振成像和高分辨率T1加权成像检查。将患者分为左侧听力障碍组（24例）和右侧听力障碍组（21例）。数据预处理后，计算和分析评估患者和对照者之间ALFF指标的差异，并对统计结果进行高斯随机场（GFR）校正。 结果： 听力障碍患者总体比较分析，三组间单因素方差分析显示仅在右侧楔前叶出现ALFF值异常（GRF校正后P=0.002）；听力障碍组ALFF值高于对照组有1个团簇（峰值坐标为：X=9、Y=-72、Z=48，T=5.82），涉及左侧枕中回、右侧楔前叶、左侧楔叶、左侧枕上回、右侧楔叶、左侧楔前叶、左侧顶上回、左侧角回（GRF校正后P=0.031）；听力障碍组ALFF值低于对照组有3个团簇（峰值坐标分别为：X=57、Y=-48、Z=-24，T=-4.99；X=45、Y=-66、Z=0，T=-4.06；X=42、Y=-12、Z=36，T=-4.03），涉及右侧颞下回、右侧颞中回及右侧中央前回（GRF校正后P=0.009）。左侧听力障碍组与对照组相比，ALFF值高于对照组有1个团簇（峰值坐标为：X=-12、Y=-75、Z=45，T=5.78），涉及左侧楔前叶、右侧楔前叶、左侧枕中回、左侧顶上回、左侧枕上回、左侧楔叶及右侧楔叶（GRF校正后P=0.023）。右侧听力障碍组与对照组相比，ALFF值高于对照组有1个团簇（峰值坐标为：X=9、Y=-46、Z=22，T=6.06），涉及左侧枕中回、右侧楔前叶、左侧楔叶、右侧楔叶、左侧枕上回、右侧枕上回（GRF校正后P<0.022）；ALFF值降低的脑区位于右侧颞下回（GRF校正后P=0.029）。异常脑区ALFF值与纯音平均值（PTA）的Spearman双尾相关分析显示，仅在左侧听力障碍组中异常脑区的ALFF值与PTA具有相关性（PTA=2 000 Hz，r=0.318，P=0.033；PTA=4 000 Hz，r=0.386，P=0.009）。 结论： 左侧与右侧听力障碍患者的大脑内部活动发生异常，且听力受损的严重程度与脑区功能整合的差异有关。.DOI: 10.3760/cma.j.cn112137-20221107-02337",pubmed,37402672,10.3760/cma.j.cn112137-20221107-02337
exposure to acoustic stimuli promotes the development and differentiation of neural stem cells from the cochlear nuclei through the clusterin pathway,"717. Int J Mol Med. 2015 Mar;35(3):637-44. doi: 10.3892/ijmm.2015.2075. Epub 2015 Jan 21.Exposure to acoustic stimuli promotes the development and differentiation of neural stem cells from the cochlear nuclei through the clusterin pathway.Xue T(1), Wei L(2), Zha DJ(1), Qiao L(2), Lu LJ(1), Chen FQ(1), Qiu JH(1).Author information:(1)Department of Otolaryngology, Xijing Hospital, The Fourth Military Medical University, Xi'an, Shaanxi 710032, P.R. China.(2)Department of Obstetrics and Gynecology, Xijing Hospital, The Fourth Military Medical University, Xi'an, Shaanxi 710032, P.R. China.Stem cell therapy has attracted widespread attention for a number of diseases. Recently, neural stem cells (NSCs) from the cochlear nuclei have been identified, indicating a potential direction for the treatment of sensorineural hearing loss. Acoustic stimuli play an important role in the development of the auditory system. In this study, we aimed to determine whether acoustic stimuli induce NSC development and differentiation through the upregulation of clusterin (CLU) in NSCs isolated from the cochlear nuclei. To further clarify the underlying mechanisms involved in the development and differentiation of NSCs exposed to acoustic stimuli, we successfully constructed animal models in which was CLU silenced by an intraperitoneal injection of shRNA targeting CLI. As expected, the NSCs from rats treated with LV-CLU shRNA exhibited a lower proliferation ratio when exposed to an augmented acoustic environment (AAE). Furthermore, the inhibition of cell apoptosis induced by exposure to AAE was abrogated after silencing the expression of the CLU gene. During the differentiation of acoustic stimuli-exposed stem cells into neurons, the number of astrocytes was significantly reduced, as evidenced by the expression of the cell markers, microtubule associated protein‑2 (MAP-2) and glial fibrillary acidic protein (GFAP), which was markedly inhibited when the CLU gene was silenced. Our results indicate that acoustic stimuli may induce the development and differentiation of NSCs from the cochlear nucleus mainly through the CLU pathway. Our study suggests that CLU may be a novel target for the treatment of sensorineural hearing loss.DOI: 10.3892/ijmm.2015.2075PMCID: PMC4314421",pubmed,25605314,10.3892/ijmm.2015.2075
effects of chronic cochlear electrical stimulation after an extended period of profound deafness on primary auditory cortex organization in cats,"528. Eur J Neurosci. 2014 Mar;39(5):811-20. doi: 10.1111/ejn.12445. Epub 2013 Dec 10.Effects of chronic cochlear electrical stimulation after an extended period of profound deafness on primary auditory cortex organization in cats.Fallon JB(1), Shepherd RK, Irvine DR.Author information:(1)Bionics Institute, 384-388 Albert Street, East Melbourne, Vic., 3002, Australia; Department of Otolaryngology, University of Melbourne, Melbourne, Vic., Australia; Medical Bionics Department, University of Melbourne, Melbourne, Vic., Australia.Extended periods of deafness have profound effects on central auditory system function and organization. Neonatal deafening results in loss of the normal cochleotopic organization of the primary auditory cortex (AI), but environmentally-derived intracochlear electrical stimulation, via a cochlear implant, initiated shortly after deafening, can prevent this loss. We investigated whether such stimulation initiated after an extended period of deafness can restore cochleotopy. In two groups of neonatally-deafened cats, a multi-channel intracochlear electrode array was implanted at 8 weeks of age. One group received only minimal stimulation, associated with brief recordings at 4-6-week intervals, over the following 6 months to check the efficacy of the implant. In the other group, this 6-month period was followed by 6 months of near-continuous intracochlear electrical stimulation from a modified clinical cochlear implant system. We recorded multi-unit clusters in the auditory cortex and used two different methods to define the region of interest in the putative AI. There was no evidence of cochleotopy in any of the minimally stimulated animals, confirming our earlier finding. In three of six chronically stimulated cats there was clear evidence of AI cochleotopy, and in a fourth cat in which the majority of penetrations were in the anterior auditory field there was clear evidence of cochleotopy in that field. The finding that chronic intracochlear electrical stimulation after an extended period of deafness is able to restore cochleotopy in some (but not all) cases has implications for the performance of patients implanted after an extended period of deafness.© 2013 Federation of European Neuroscience Societies and John Wiley & Sons Ltd.DOI: 10.1111/ejn.12445",pubmed,24325274,10.1111/ejn.12445
late onset middle ear neuroendocrine tumor presenting with distant metastasis,"Tumors of the ear with neuroendocrine features are a very rare group of neoplasms, with neuroendocrine adenoma representing the more frequent entity found; the prevalence of middle ear neuroendocrine tumors is so low that it has never been determined exactly. Herein we describe the case of a 74 years old woman who presented a pure moderately differentiated middle ear neuroendocrine tumor after a long history of adhesive otitis media. Patient abruptly developed otalgia and a sense of plugged ear. Otomicroscopy revealed a polypoid mass of the external auditory canal. On histopathologic exam a neoplastic proliferation of monomorphous epithelioid cells arranged in small nests and clusters was present, being positive to cytokeratins, chromogranin, synaptophysin and CD56, and negative to S100. Ki-67 proliferation index was 20%. The lesion was diagnosed as a moderately differentiated neuroendocrine tumor (NET grade 2). CT and PET scans highlighted the concurrent presence of locoregional lymph nodes involvement but also at distance liver metastases. The patient underwent chemotherapy and liver repetitions were treated locally. At the time of the writing the patient is in good general condition. We describe an exceeding rare case of a moderately differentiated neuroendocrine tumor with primitivity in the middle ear and concurrent distant metastasis at the time of the diagnosis. This case represents a clinical and histological challenge, for the rarity of the lesion and the aspecific symptoms of the patient. © 2021",scopus,2-s2.0-85103139942,10.1016/j.xocr.2021.100289
speech signal enhancement in cocktail party scenarios by deep learning based virtual sensing of headmounted microphones,"125. Hear Res. 2021 Sep 1;408:108294. doi: 10.1016/j.heares.2021.108294. Epub 2021 Jun 17.Speech signal enhancement in cocktail party scenarios by deep learning based virtual sensing of head-mounted microphones.Fischer T(1), Caversaccio M(1), Wimmer W(2).Author information:(1)Hearing Research Laboratory, ARTORG Center for Biomedical Engineering Research, University of Bern, Bern 3008, Switzerland; Department of ENT, Head and Neck Surgery, Inselspital, Bern University Hospital, University of Bern, Bern 3008, Switzerland.(2)Hearing Research Laboratory, ARTORG Center for Biomedical Engineering Research, University of Bern, Bern 3008, Switzerland; Department of ENT, Head and Neck Surgery, Inselspital, Bern University Hospital, University of Bern, Bern 3008, Switzerland. Electronic address: wilhelm.wimmer@artorg.unibe.ch.The cocktail party effect refers to the human sense of hearing's ability to pay attention to a single conversation while filtering out all other background noise. To mimic this human hearing ability for people with hearing loss, scientists integrate beamforming algorithms into the signal processing path of hearing aids or implants' audio processors. Although these algorithms' performance strongly depends on the number and spatial arrangement of the microphones, most devices are equipped with a small number of microphones mounted close to each other on the audio processor housing. We measured and evaluated the impact of the number and spatial arrangement of hearing aid or head-mounted microphones on the performance of the established Minimum Variance Distortionless Response beamformer in cocktail party scenarios. The measurements revealed that the optimal microphone placement exploits monaural cues (pinna-effect), is close to the target signal, and creates a large distance spread due to its spatial arrangement. However, this microphone placement is impractical for hearing aid or implant users, as it includes microphone positions such as on the forehead. To overcome microphones' placement at impractical positions, we propose a deep virtual sensing estimation of the corresponding audio signals. The results of objective measures and a subjective listening test with 20 participants showed that the virtually sensed microphone signals significantly improved the speech quality, especially in cocktail party scenarios with low signal-to-noise ratios. Subjective speech quality was assessed using a 3-alternative forced choice procedure to determine which of the presented speech mixtures was most pleasant to understand. Hearing aid and cochlear implant (CI) users might benefit from the presented approach using virtually sensed microphone signals, especially in noisy environments.Copyright © 2021 The Author(s). Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2021.108294",pubmed,34182232,10.1016/j.heares.2021.108294
questions and controversies surrounding the perception and neural coding of pitch,"Pitch is a fundamental aspect of auditory perception that plays an important role in our ability to understand speech, appreciate music, and attend to one sound while ignoring others. The questions surrounding how pitch is represented in the auditory system, and how our percept relates to the underlying acoustic waveform, have been a topic of inquiry and debate for well over a century. New findings and technological innovations have led to challenges of some long-standing assumptions and have raised new questions. This article reviews some recent developments in the study of pitch coding and perception and focuses on the topic of how pitch information is extracted from peripheral representations based on frequency-to-place mapping (tonotopy), stimulus-driven auditory-nerve spike timing (phase locking), or a combination of both. Although a definitive resolution has proved elusive, the answers to these questions have potentially important implications for mitigating the effects of hearing loss via devices such as cochlear implants. Copyright © 2023 Oxenham.",scopus,2-s2.0-85147035524,10.3389/fnins.2022.1074752
improving puretone audiometry using probabilistic machine learning classification,,base,eaa067e7ff40e6a8a6debde603644ec387f0cef7fb33032e8602f746028371d5,
improving puretone audiometry using probabilistic machine learning classification ,,base,f67243d7bf355b68663826c6c99284b6a9b5f39402c0e0b138de2a8c97cdda4e,
chinese expert consensus on minimally invasive interventional treatment of trigeminal neuralgia,"Background and purpose: Trigeminal neuralgia is a common condition that is associated with severe pain, which seriously affects the quality of life of patients. When the efficacy of drugs is not satisfactory or adverse drug reactions cannot be tolerated, minimally invasive interventional therapy has become an important treatment because of its simple operation, low risk, high repeatability and low cost. In recent years, minimally invasive interventional treatments, such as radiofrequency thermocoagulation (RF) of the trigeminal nerve and percutaneous microcompression (PMC), have been widely used in the clinic to relieve severe pain in many patients, however, some related problems remain to be addressed. The Pain Association of the Chinese Medical Association organizes and compiles the consensus of Chinese experts to standardize the development of minimally invasive interventional treatment of trigeminal neuralgia to provide a basis for its clinical promotion and application. Materials and methods: The Pain Association of the Chinese Medical Association organizes the Chinese experts to compile a consensus. With reference to the evidence-based medicine (OCEBM) system and the actual situation of the profession, the Consensus Development Committee adopts the nominal group method to adjust the recommended level. Results: Precise imaging positioning and guidance are the keys to ensuring the efficacy and safety of the procedures. RF and PMC are the most widely performed and effective treatments among minimally invasive interventional treatments for trigeminal neuralgia. Conclusions: The pain degree of trigeminal neuralgia is severe, and a variety of minimally invasive intervention methods can effectively improve symptoms. Radiofrequency and percutaneous microcompression may be the first choice for minimally invasive interventional therapy. Copyright © 2022 Fan, Fu, Ma, Tao, Huang, Guo, Huang, Liu, Song, Song, Xiao, Xia and Liu.",scopus,2-s2.0-85137104773,10.3389/fnmol.2022.953765
a reanalysis of mcgurk data suggests that audiovisual fusion in speech perception is subjectdependent,"366. J Acoust Soc Am. 2010 Mar;127(3):1584-94. doi: 10.1121/1.3293001.A reanalysis of McGurk data suggests that audiovisual fusion in speech perception is subject-dependent.Schwartz JL(1).Author information:(1)Department of Speech and Cognition/Institut de la Communication Parlee, GIPSA-Lab, UMR 5216, CNRS, Grenoble University, 38402 Saint Martin d'Heres Cedex, France. jean-luc.schwartz@gipsa-lab.inpg.frAudiovisual perception of conflicting stimuli displays a large level of intersubject variability, generally larger than pure auditory or visual data. However, it is not clear whether this actually reflects differences in integration per se or just the consequence of slight differences in unisensory perception. It is argued that the debate has been blurred by methodological problems in the analysis of experimental data, particularly when using the fuzzy-logical model of perception (FLMP) [Massaro, D. W. (1987). Speech Perception by Ear and Eye: A Paradigm for Psychological Inquiry (Laurence Erlbaum Associates, London)] shown to display overfitting abilities with McGurk stimuli [Schwartz, J. L. (2006). J. Acoust. Soc. Am. 120, 1795-1798]. A large corpus of McGurk data is reanalyzed, using a methodology based on (1) comparison of FLMP and a variant with subject-dependent weights of the auditory and visual inputs in the fusion process, weighted FLMP (WFLMP); (2) use of a Bayesian selection model criterion instead of a root mean square error fit in model assessment; and (3) systematic exploration of the number of useful parameters in the models to compare, attempting to discard poorly explicative parameters. It is shown that WFLMP performs significantly better than FLMP, suggesting that audiovisual fusion is indeed subject-dependent, some subjects being more ""auditory,"" and others more ""visual."" Intersubject variability has important consequences for theoretical understanding of the fusion process, and re-education of hearing impaired people.DOI: 10.1121/1.3293001",pubmed,20329858,10.1121/1.3293001
a mutation in hoxa2 is responsible for autosomalrecessive microtia in an iranian family,"594. Am J Hum Genet. 2008 Apr;82(4):982-91. doi: 10.1016/j.ajhg.2008.02.015.A mutation in HOXA2 is responsible for autosomal-recessive microtia in an Iranian family.Alasti F(1), Sadeghi A, Sanati MH, Farhadi M, Stollar E, Somers T, Van Camp G.Author information:(1)Department of Medical Genetics, University of Antwerp, 2610 Antwerp, Belgium.Erratum in    Am J Hum Genet. 2008 Sep;83(3):424.Microtia, a congenital deformity manifesting as an abnormally shaped or absent external ear, occurs in one out of 8,000-10,000 births. We ascertained a consanguineous Iranian family segregating with autosomal-recessive bilateral microtia, mixed symmetrical severe to profound hearing impairment, and partial cleft palate. Genome-wide linkage analysis localized the responsible gene to chromosome 7p14.3-p15.3 with a maximum multi-point LOD score of 4.17. In this region, homeobox genes from the HOXA cluster were the most interesting candidates. Subsequent DNA sequence analysis of the HOXA1 and HOXA2 homeobox genes from the candidate region identified an interesting HOXA2 homeodomain variant: a change in a highly conserved amino acid (p.Q186K). The variant was not found in 231 Iranian and 109 Belgian control samples. The critical contribution of HoxA2 for auditory-system development has already been shown in mouse models. We built a homology model to predict the effect of this mutation on the structure and DNA-binding activity of the homeodomain by using the program Modeler 8v2. In the model of the mutant homeodomain, the position of the mutant lysine side chain is consistently farther away from a nearby phosphate group; this altered position results in the loss of a hydrogen bond and affects the DNA-binding activity.DOI: 10.1016/j.ajhg.2008.02.015PMCID: PMC2427268",pubmed,18394579,10.1016/j.ajhg.2008.02.015
clinical validation of the russian matrix test  effect of hearing loss age and noise level,"210. Int J Audiol. 2020 Dec;59(12):930-940. doi: 10.1080/14992027.2020.1806368. Epub 2020 Aug 20.Clinical validation of the Russian Matrix test - effect of hearing loss, age, and noise level.Warzybok A(1), Zhilinskaya E(2), Goykhburg M(3), Tavartkiladze G(3), Kollmeier B(1)(4), Boboshko M(2)(5).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4all, Carl-von-Ossietzky Universität Oldenburg, Oldenburg, Germany.(2)Pavlov First St. Petersburg State Medical University, Saint-Petersburg, Russia.(3)National Research Centre for Audiology and Hearing Rehabilitation, Moscow, Russia.(4)HörTech gGmbH, Oldenburg, Germany.(5)Northwest State Medical University named after Mechnikov, Saint-Petersburg, Russia.OBJECTIVE: To validate the Russian matrix sentence test (RUMatrix) for the assessment of speech recognition in quiet and in noise in clinical praxis. The effect of hearing impairment, age, and masking-noise level on speech recognition was examined.DESIGN: All participants underwent pure tone audiometry, a monosyllabic speech test in quiet, and speech recognition measurements with RUMatrix in quiet (SRTQ) and in noise (SRTN).STUDY SAMPLE: One hundred and forty-two listeners divided into four groups: 1. Young normal-hearing listeners, 2. Older normal-hearing listeners, 3. Young hearing-impaired listeners, and 4. Older hearing-impaired listeners.RESULTS: Significant differences between groups of listeners were found in the SRTQ and SRTN. A strong correlation between hearing threshold and SRTQ (R2=0.88, p < 0.001) indicates a strong link between speech recognition in quiet and audibility. The pure-tone average explained less variance in SRTN (R2=0.67, p < 0.001), pointing out an additional influence of suprathreshold distortion. A high test sensitivity of 0.99 was found for SRTN and SRTQ. The monosyllabic test had a low sensitivity (0.21), indicating that the test is not suitable for separating normal-hearing and hearing-impaired listeners.CONCLUSIONS: RuMatrix is a reliable speech recognition assessment tool with a high sensitivity and validity for the main aspects of hearing impairment.DOI: 10.1080/14992027.2020.1806368",pubmed,32815756,10.1080/14992027.2020.1806368
exposure to spoken communication during the covid19 pandemic among children with cochlear implants,"Key Points: Question: Did decreases in exposure to spoken communication, found in the early stages of the COVID-19 pandemic among children using cochlear implants, resolve as lockdowns became more intermittent in later pandemic stages? Findings: In this cohort study, sound environments cataloged using machine learning for cochlear implants were measured by 2746 datalogs for 262 children using cochlear implants before and during 2 years of COVID-19 lockdowns in Ontario, Canada. Due to school closures during lockdowns, school-aged children experienced significantly decreased exposure to spoken language, which has not recovered to the prepandemic baseline. Meaning: This study suggests that school closures due to COVID-19 lockdowns are associated with reduced exposure to spoken communications among children using cochlear implants during sensitive periods of development. Importance: School closures and other COVID-19–related restrictions could decrease children's exposure to speech during important stages of development. Objective: To assess whether significant decreases in exposure to spoken communication found during the initial phase of the COVID-19 pandemic among children using cochlear implants are confirmed for a larger cohort of children and were sustained over the first years of the COVID-19 pandemic. Design, Setting, and Participants: This cohort study used datalogs collected from children with cochlear implants during clinical visits to a tertiary pediatric hospital in Toronto, Ontario, Canada, from January 1, 2018, to November 11, 2021. Children with severe to profound hearing loss using cochlear implants were studied because their devices monitored and cataloged levels and types of sounds during hourly use per day (datalogs) and because their hearing and spoken language development was particularly vulnerable to reduced sound exposure. Statistical analyses were conducted between January 2022 and August 2023. Main Outcomes and Measures: Daily hours of sound were captured by the cochlear implant datalogging system and categorized into 6 auditory scene categories, including speech and speech-in-noise. Time exposed to speech was calculated as the sum of daily hours in speech and daily hours in speech-in-noise. Residual hearing in the ear without an implant of children with unilateral cochlear implants was measured by pure tone audiometry. Mixed-model regression analyses revealed main effects with post hoc adjustment of 95% CIs using the Satterthwaite method. Results: Datalogs (n = 2746) from 262 children (137 with simultaneous bilateral cochlear implants [74 boys (54.0%); mean (SD) age, 5.8 (3.5 years)], 38 with sequential bilateral cochlear implants [24 boys (63.2%); mean (SD) age, 9.1 (4.2) years], and 87 with unilateral cochlear implants [40 boys (46.0%); mean (SD) age, 7.9 (4.6) years]) who were preschool aged (n = 103) and school aged (n = 159) before the COVID-19 pandemic were included in analyses. There was a slight increase in use among preschool-aged bilateral cochlear implant users through the pandemic (early pandemic, 1.4 h/d [95% CI, 0.3-2.5 h/d]; late pandemic, 2.3 h/d [95% CI, 0.6-4.0 h/d]) and little change in use among school-aged bilateral cochlear implant users (early pandemic, −0.6 h/d [95% CI, −1.1 to −0.05 h/d]; late pandemic, −0.3 h/d [95% CI, −0.9 to 0.4 h/d]). However, use decreased during the late pandemic period among school-aged children with unilateral cochlear implants (−1.8 h/d [95% CI,−3.0 to −0.6 h/d]), particularly among children with good residual hearing in the ear without an implant. Prior to the pandemic, children were exposed to speech for approximately 50% of the time they used their cochlear implants (preschool-aged children: bilateral cochlear implants, 46.6% [95% CI, 46.5%-47.2%] and unilateral cochlear implants, 52.1% [95% CI, 50.7%-53.5%]; school-aged children: bilateral cochlear implants, 47.6% [95% CI, 46.8%-48.4%] and unilateral cochlear implants, 51.0% [95% CI, 49.4%-52.6%]). School-aged children in both groups experienced significantly decreased speech exposure in the early pandemic period (bilateral cochlear implants, −12.1% [−14.6% to −9.4%]; unilateral cochlear implants, −15.5% [−20.4% to −10.7%]) and late pandemic periods (bilateral cochlear implants, −5.3% [−8.0% to −2.6%]; unilateral cochlear implants, −11.2% [−15.3% to −7.1%]) compared with the prepandemic baseline. Conclusions and Relevance: This cohort study using datalogs from children using cochlear implants suggests that a sustained reduction in children's access to spoken communication was found during more than 2 years of COVID-19 pandemic-related lockdowns and school closures. This cohort study assesses whether significant decreases in exposure to spoken communication found during the initial phase of the COVID-19 pandemic among children using cochlear implants are confirmed for a larger cohort of children and were sustained over the first years of the COVID-19 pandemic.",cinahl,25743805,10.1001/jamanetworkopen.2023.39042
the application of mir183 family and mesenchymal stem cells a possibility for restoring hearing loss,"Hearing loss as one of the most common disabilities approximately over 5% of the world's population - 360 million people - has disabling hearing loss (328 million adults and 32 million children). Recent developments in stem cell technology provide new opportunities for the treatment of deafness. miRNAs are essential factors of an extensively conserved posttranscriptional process controlling gene expression at mRNA level. Various biological processes such as growth and differentiation are regulated by miRNAs. In this review paper we have discussed about the application of miR-183 family and mesenchymal stem cells as a possibility for restoring hearing loss. In this regards, the web of Science and PubMed databases were searched using the Endnote software for the publications about the application of miR-183 family and mesenchymal stem cells (MSCs) to study hearing loss published from 2000 to 2016. The miR-183 family (miR-183, miR-96, and miR-182) is expressed abundantly in sensory cells in inner ear. miR-183 family is significant for the development and persistence of auditory neurons and hair cell. These four genes, i.e. Sox2, Notch1, Jag1, and Hes1, are potentially the targets of miR-183 family. In studies on animal models such as mouse and zebrafish, the time of Atoh1 expression in the hair cells was found to be the E12/5-E14/5 day, and miR-183 family was reported to begin to express on the E14/5 day. Use of human MSCs in differentiating into hair cells has been investigated, demonstrating that MSCs have neuroregenerative capacity. Cell therapy-targeting regeneration of the auditory neurons and hair cell may therefore be a powerful strategy to cure hearing loss that cannot be reversed by current therapies. A combination of the MSCs, specific growth factors and miR-183 cluster (96-182-183) can increase the potential to differentiate into the auditory neurons and hair cell. © 2017 Medknow Publications. All right reserved.",scopus,2-s2.0-85046703633,10.4103/indianjotol.INDIANJOTOL_105_17
difficulty understanding speech in noise by the hearing impaired underlying causes and technological solutions,"39. Annu Int Conf IEEE Eng Med Biol Soc. 2016 Aug;2016:89-92. doi: 10.1109/EMBC.2016.7590647.Difficulty understanding speech in noise by the hearing impaired: underlying causes and technological solutions.Healy EW, Yoho SE.A primary complaint of hearing-impaired individuals involves poor speech understanding when background noise is present. Hearing aids and cochlear implants often allow good speech understanding in quiet backgrounds. But hearing-impaired individuals are highly noise intolerant, and existing devices are not very effective at combating background noise. As a result, speech understanding in noise is often quite poor. In accord with the significance of the problem, considerable effort has been expended toward understanding and remedying this issue. Fortunately, our understanding of the underlying issues is reasonably good. In sharp contrast, effective solutions have remained elusive. One solution that seems promising involves a single-microphone machine-learning algorithm to extract speech from background noise. Data from our group indicate that the algorithm is capable of producing vast increases in speech understanding by hearing-impaired individuals. This paper will first provide an overview of the speech-in-noise problem and outline why hearing-impaired individuals are so noise intolerant. An overview of our approach to solving this problem will follow.DOI: 10.1109/EMBC.2016.7590647",pubmed,28268288,10.1109/EMBC.2016.7590647
audiome a tiered exome sequencingbased comprehensive gene panel for the diagnosis of heterogeneous nonsyndromic sensorineural hearing loss,"Purpose: Hereditary hearing loss is highly heterogeneous. To keep up with rapidly emerging disease-causing genes, we developed the AUDIOME test for nonsyndromic hearing loss (NSHL) using an exome sequencing (ES) platform and targeted analysis for the curated genes. Methods: A tiered strategy was implemented for this test. Tier 1 includes combined Sanger and targeted deletion analyses of the two most common NSHL genes and two mitochondrial genes. Nondiagnostic tier 1 cases are subjected to ES and array followed by targeted analysis of the remaining AUDIOME genes. Results: ES resulted in good coverage of the selected genes with 98.24% of targeted bases at >15 ×. A fill-in strategy was developed for the poorly covered regions, which generally fell within GC-rich or highly homologous regions. Prospective testing of 33 patients with NSHL revealed a diagnosis in 11 (33%) and a possible diagnosis in 8 cases (24.2%). Among those, 10 individuals had variants in tier 1 genes. The ES data in the remaining nondiagnostic cases are readily available for further analysis. Conclusion: The tiered and ES-based test provides an efficient and cost-effective diagnostic strategy for NSHL, with the potential to reflex to full exome to identify causal changes outside of the AUDIOME test. © 2018, American College of Medical Genetics and Genomics.",scopus,2-s2.0-85052461338,10.1038/gim.2018.48
hearing aid fitting and finetuning based on estimated individual traits,"186. Int J Audiol. 2018 Jun;57(sup3):S139-S145. doi: 10.1080/14992027.2016.1257163. Epub 2016 Nov 22.Hearing aid fitting and fine-tuning based on estimated individual traits.Völker C(1)(2), Ernst SMA(1)(2), Kollmeier B(1)(2).Author information:(1)a Abteilung Medizinische Physik , Carl von Ossietzky Universität Oldenburg , Oldenburg , Germany and.(2)b Cluster of Excellence 'Hearing4all' , Oldenburg , Germany.OBJECTIVE: A generalised concept for hearing aid fitting and fine-tuning based on estimated individual traits is presented along first implementations in this report.DESIGN: To estimate the individual traits, a set of auditory model-based performance measures is used to generate promising candidates within the algorithm's parameter space for a subsequent subjective rating. For the subjective assessment, a fast and intuitive multi-stimulus test denoted as combined discrimination and classification (CoDiCl) is presented to capture user preferences for an optimised setting.STUDY SAMPLE: The estimation of individual traits is shown in an exemplary manner for a multidimensional coherence-based noise reduction algorithm. The dimensionality reduction was performed using differently weighted combinations of speech intelligibility index (SII) and perceived similarity measure (PSM).RESULTS: Nine reasonable alternative algorithm setting candidates were extracted from a model-optimised exploration path (MOEP) for a subsequent subjective rating to potentially differentiate between listeners with different attitudes towards noise suppression and introduced distortions (i.e. ""noise haters"" and ""distortion haters"").CONCLUSIONS: By iteratively improving the agreement between subjective and objective assessment, an objective estimation of subjective traits using appropriate weightings of objective measures may become possible. This will potentially help to efficiently fit modern multidimensional hearing aid algorithms to the individual user.DOI: 10.1080/14992027.2016.1257163",pubmed,27873543,10.1080/14992027.2016.1257163
direct acoustic cochlear implants lead to an improved speech perception gap compared to conventional hearing aid,"265. Otol Neurotol. 2018 Oct;39(9):1147-1152. doi: 10.1097/MAO.0000000000001954.Direct Acoustic Cochlear Implants Lead to an Improved Speech Perception Gap Compared to Conventional Hearing Aid.Maier H(1), Lenarz T(1), Dolležal LV(2), Busch S(1).Author information:(1)Department of Otorhinolaryngology and Cluster of Excellence ""Hearing4all,"" Hannover Medical School.(2)Cochlear Deutschland GmbH & Co. KG, Hannover, Germany.OBJECTIVES: The objectives of this study was to evaluate the aided speech perception in quiet of direct acoustic cochlear implant (DACI) patients and the speech perception gap in comparison with hearing aid users.STUDY DESIGN: Retrospective comparative study.SETTING: Tertiary referral center.PATIENTS: Adults with moderate-to-severe mixed hearing loss who have been implanted with a DACI and fitted with a processor for at least 6 months.INTERVENTION(S): Comparison of aided monosyllabic word scores and speech perception gap of 59 DACI-implanted ears speech perception gap with published data on 208 ears aided with a conventional hearing aid (HA) divided into four different hearing loss groups between 35 and 75 dB HL.MAIN OUTCOME MEASURE(S): Aided monosyllabic word score, predicted maximum monosyllabic word recognition score (PBmax) and speech perception gap.RESULTS: In terms of aided speech perception, DACI patients with cochlear reserves between 45 and 65 dB HL have a significant advantage compared with conventional HA users. A speech perception gap of 11% points for DACI and 21% points for conventional HAs were determined and an approximation of PBmax is achieved by 52% of the DACI patients compared with only 36% of the HA users.CONCLUSIONS: For patients with moderate-to-severe inner ear hearing loss between 45 and 65 dB HL, better speech perception in quiet is obtained with the DACI system. Compared with conventional hearing aids, speech performance with the DACI is closer to the maximally possibly score PBmax.DOI: 10.1097/MAO.0000000000001954",pubmed,30106855,10.1097/MAO.0000000000001954
a prototype for mexican sign language recognition and synthesis in support of a primary care physician,"Few hearing people know and use Mexican Sign Language (MSL). Consequently, this is the main barrier between people who having total or partial hearing loss and hearing people. This study proposes a system that recognizes and animates in real time a set of signs belonging to the semantic field of general medicine consultation services. Therefore, a linkage between a hearing doctor and a deaf patient can be established in a non-intrusive way and with easy dynamic interaction. Our main contribution is a bidirectional translator system for Mexican Sign Language in the context of primary care health services, in addition to basic signs to fingerspell alphabet and numbers as a complement to provide personal information such as name, age, etc. The recognition module uses a Microsoft Kinect sensor to obtain sign trajectories and images to feed hidden Markov Models (HMMs) for processing sign samples in real time. The experiments showed the recognition of 82 different signs by 22 participants. As a result, accuracy and F1 scores average rates of 99% and 88%, respectively, were obtained.",ieee,2169-3536,10.1109/ACCESS.2022.3226696
activitydependent formation and location of voltagegated sodium channel clusters at a cns nerve terminal during postnatal development,"582. J Neurophysiol. 2017 Feb 1;117(2):582-593. doi: 10.1152/jn.00617.2016. Epub 2016 Nov 9.Activity-dependent formation and location of voltage-gated sodium channel clusters at a CNS nerve terminal during postnatal development.Xu J(1), Berret E(1), Kim JH(2)(3).Author information:(1)The Department of Physiology, University of Texas Health Science Center, San Antonio, Texas; and.(2)The Department of Physiology, University of Texas Health Science Center, San Antonio, Texas; and kimjh@uthscsa.edu.(3)Center for Biomedical Neuroscience, University of Texas Health Science Center, San Antonio, Texas.In auditory pathways, the precision of action potential (AP) propagation depends on axon myelination and high densities of voltage-gated Na (Nav) channels clustered at nodes of Ranvier. Changes in Nav channel expression at the heminode, the final node before the nerve terminal, can alter AP invasion into the presynaptic terminal. We studied the activity-dependent formation of Nav channel clusters before and after hearing onset at postnatal day 12 in the rat and mouse auditory brain stem. In rats, the Nav channel cluster at the heminode formed progressively during the second postnatal week, around hearing onset, whereas the Nav channel cluster at the nodes was present before hearing onset. Initiation of heminodal Nav channel clustering was correlated with the expression of scaffolding protein ankyrinG and paranodal protein Caspr. However, in whirler mice with congenital deafness, heminodal Nav channels did not form clusters and maintained broad expression, but Nav channel clustering was normal at the nodes. In addition, a clear difference in the distance from the heminodal Nav channel to the calyx across the mediolateral axis of the medial nucleus of the trapezoid body (MNTB) developed after hearing onset. In the medial MNTB, where neurons respond best to high-frequency sounds, the heminodal Nav channel cluster was located closer to the terminal than in the lateral MNTB, where neurons respond best to low-frequency sounds. Thus sound-mediated neuronal activities are potentially associated with the refinement of the heminode adjacent to the presynaptic terminal in the auditory brain stem.NEW & NOTEWORTHY: Clustering of voltage-gated sodium (Nav) channels and their distribution along the axon, specifically at the unmyelinated axon segment next to the nerve terminal, are essential for tuning propagated action potentials. Nav channel clusters near the nerve terminal and their location as a function of neuronal position along the mediolateral axis are controlled by auditory inputs after hearing onset. Thus sound-mediated neuronal activity influences the tonotopic organization of Nav channels at the nerve terminal in the auditory brain stem.Copyright © 2017 the American Physiological Society.DOI: 10.1152/jn.00617.2016PMCID: PMC5288486",pubmed,27832602,10.1152/jn.00617.2016
noise reduction in cochlear implant signal processing a review and recent developments,"Cochlear implant technology successfully restores hearing function to patients with sensory impairment. Although cochlear implant users generally hear well in quiet, they still find noisy conditions very challenging, hence the need to employ noise reduction algorithms in these systems to enhance the user experience. This paper reviews noise reduction algorithms in cochlear implants. Traditionally, such algorithms have been classified as either single- or multiple-channel, depending on the number of microphones they use. This review retains this general classification in looking at recent papers and extends it to reflect recent interest in machine learning techniques. The review concludes with consideration of promising future areas of research.  © 2008-2011 IEEE.",scopus,2-s2.0-85112612437,10.1109/RBME.2021.3095428
agerelated changes in event related potentials steady state responses and temporal processing in the auditory cortex of mice with severe or mild hearing loss,"254. Hear Res. 2021 Dec;412:108380. doi: 10.1016/j.heares.2021.108380. Epub 2021 Oct 23.Age-related changes in event related potentials, steady state responses and temporal processing in the auditory cortex of mice with severe or mild hearing loss.Rumschlag JA(1), Razak KA(2).Author information:(1)Graduate Neuroscience Program, Riverside, United States.(2)Graduate Neuroscience Program, Riverside, United States; Psychology Department, University of California, Riverside, United States. Electronic address: khaleel@ucr.edu.Age-related changes in auditory processing affect the quality of life of older adults with and without hearing loss. To distinguish between the effects of sensorineural hearing loss and aging on cortical processing, the main goal of the present study was to compare cortical responses using the same stimulus paradigms and recording conditions in two strains of mice (C57BL/6J and FVB) that differ in the degree of age-related hearing loss. Electroencephalogram (EEG) recordings were obtained from freely moving young and old mice using epidural screw electrodes. We measured event related potentials (ERP) and 40 Hz auditory steady-state responses (ASSR). We used a novel stimulus, termed the gap-ASSR stimulus, which elicits an ASSR by rapidly presenting short gaps in continuous noise. By varying the gap widths and modulation depths, we probed the limits of temporal processing in young and old mice. Temporal fidelity of ASSR and gap-ASSR responses were measured as phase consistency across trials (inter-trial phase clustering; ITPC). The old C57 mice, which show severe hearing loss, produced larger ERP amplitudes compared to young mice. Despite robust ERPs, the old C57 mice showed significantly diminished ITPC in the ASSR and gap-ASSR responses, even with 100% modulation depth. The FVB mice, which show mild hearing loss with age, generated similar ERP amplitudes and ASSR ITPC across the age groups tested. However, the old FVB mice showed decreased gap-ASSR responses compared to young mice, particularly for modulation depths <100%. The C57 mice data suggest that severe presbycusis leads to increased gain in the auditory cortex, but with reduced temporal fidelity. The FVB mice data suggest that with mild hearing loss, age-related changes in temporal processing become apparent only when tested with more challenging sounds (shorter gaps and shallower modulation).Copyright © 2021. Published by Elsevier B.V.DOI: 10.1016/j.heares.2021.108380",pubmed,34758398,10.1016/j.heares.2021.108380
cnnlstm hybrid realtime iotbased cognitive approaches for islr with webrtc auditory impaired assistive technology,"795. RETRACTED ARTICLEJ Healthc Eng. 2022 Feb 21;2022:3978627. doi: 10.1155/2022/3978627. eCollection 2022.CNN-LSTM Hybrid Real-Time IoT-Based Cognitive Approaches for ISLR with WebRTC: Auditory Impaired Assistive Technology.Gupta M(1), Thakur N(2), Bansal D(3), Chaudhary G(4), Davaasambuu B(5), Hua Q(6).Author information:(1)Department of Computer Science and Engineering, Chandigarh University, Punjab, India.(2)CSE Department, Bhagwan Parshuram Institute of Technology, New Delhi, India.(3)Department of Electrical and Electronics Engineering Department, Bharati Vidyapeeth's College of Engineering New Delhi, New Delhi, India.(4)Bharati Vidyapeeth's College of Engineering, New Delhi, India.(5)Department of Electronics and Communication Engineering, School of Engineering and Applied Sciences, National University of Mongolia, Ulan Bator, Mongolia.(6)Computer School, Hubei University of Arts and Science, Xiangyang 441000, China.Retraction in    J Healthc Eng. 2023 Dec 13;2023:9894609.In the era of modern technology, people may readily communicate through facial expressions, body language, and other means. As the use of the Internet evolves, it may be a boon to the medical fields. Recently, the Internet of Medical Things (IoMT) has provided a broader platform to handle difficulties linked to healthcare, including people's listening and hearing impairment. Although there are many translators that exist to help people of various linguistic backgrounds communicate more effectively. Using kinesics linguistics, one may assess or comprehend the communications of auditory and hearing-impaired persons who are standing next to each other. When looking at the present COVID-19 scenario, individuals are still linked in some way via online platforms; however, persons with disabilities have communication challenges with online platforms. The work provided in this research serves as a communication bridge inside the challenged community and the rest of the globe. The proposed work for Indian Sign Linguistic Recognition (ISLR) uses three-dimensional convolutional neural networks (3D-CNNs) and long short-term memory (LSTM) technique for analysis. A conventional hand gesture recognition system involves identifying the hand and its location or orientation, extracting certain essential features and applying an appropriate machine learning algorithm to recognise the completed action. In the calling interface of the web application, WebRTC has been implemented. A teleprompting technology is also used in the web app, which transforms sign language into audible sound. The proposed web app's average recognition rate is 97.21%.Copyright © 2022 Meenu Gupta et al.DOI: 10.1155/2022/3978627PMCID: PMC8885272",pubmed,35237390,10.1155/2022/3978627
contribution of acoustic analysis to the detection of vocoid epenthesis in apraxia of speech and other motor speech disorders,"Background: Vocoid epenthesis within consonant clusters has been claimed to contribute to the diagnosis of apraxia of speech. In clinical practice, the clinicians often doubt about the correct production of clusters as the C-C transition may be minimally disrupted. Aims: To demonstrate the value of acoustic analysis in clinical practice as a reliable complement to perceptive judgment. Methods & Procedures: We compared the acoustic signature and the perceptive detection of vocoid epentheses in unvoiced consonant clusters within pseudo-words produced by 40 participants presenting different subtypes of motor speech disorders (including apraxia of speech (AoS) and dysarthria) and matched neurotypical controls. Outcomes & Results: The results indicate that vocoid epenthesis was acoustically visible in 3 out of 10 participants with AoS, and in one out of 30 participants with dysarthria. One-quarter of these vocoid epentheses was not detected via auditory perception by expert listeners (speech and language therapists) who also made false detections. Conclusions: The current results indicate that vocoid epenthesis is not systematic at least in mild AoS. Moreover, an important proportion is misdetected by ear, even by expert clinicians, meaning that visualisation of the acoustic signal can be of precious help. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",scopus,2-s2.0-85106455126,10.1080/02687038.2021.1914815
genetic analysis of potential biomarkers and therapeutic targets in agerelated hearing loss,"6. Hear Res. 2023 Nov;439:108894. doi: 10.1016/j.heares.2023.108894. Epub 2023 Oct 5.Genetic analysis of potential biomarkers and therapeutic targets in age-related hearing loss.Cheng Y(1), Chen W(2), Xu J(1), Liu H(1), Chen T(3), Hu J(4).Author information:(1)Department of Neurology, Peking University Shenzhen Hospital, Shenzhen, China.(2)Department of Neurosurgery, Peking University Shenzhen Hospital, Shenzhen, China.(3)Department of Neurology, Shenzhen Second People's Hospital, Shenzhen, China.(4)Department of Neurology, Peking University Shenzhen Hospital, Shenzhen, China. Electronic address: dochj@163.com.Age-related hearing loss (ARHL) or presbycusis is the phenomenon of hearing loss due to the aging of auditory organs with age. It seriously affects the cognitive function and quality of life of the elderly. This study is based on comprehensive bioinformatic and machine learning methods to identify the critical genes of ARHL and explore its therapy targets and pathological mechanisms. The ARHL and normal samples were from GSE49543 datasets of the Gene Expression Omnibus (GEO) database. Weighted gene co-expression network analysis (WGCNA) was applied to obtain significant modules. The Limma R-package was used to identify differentially expressed genes (DEGs). The 15 common genes of the practical module and DEGs were screened. Functional enrichment analysis suggested that these genes were mainly associated with inflammation, immune response, and infection. Cytoscape software created the protein-protein interaction (PPI) layouts and cytoHubba, support vector machine-recursive feature elimination (SVM-RFE), and random forests (RF) algorithms screened hub genes. After validating the hub gene expressions in GSE6045 and GSE154833 datasets, Clec4n, Mpeg1, and Fcgr3 are highly expressed in ARHL and have higher diagnostic efficacy for ARHL, so they were identified as hub genes. In conclusion, Clec4n, Mpeg1, and Fcgr3 play essential roles in developing ARHL, and they might become vital targets in ARHL diagnosis and anti-inflammatory therapy.Copyright © 2023. Published by Elsevier B.V.DOI: 10.1016/j.heares.2023.108894",pubmed,37844444,10.1016/j.heares.2023.108894
cell typespecific transcriptome analysis reveals a major role for zeb1 and mir200b in mouse inner ear morphogenesis,"655. PLoS Genet. 2011 Sep;7(9):e1002309. doi: 10.1371/journal.pgen.1002309. Epub 2011 Sep 29.Cell type-specific transcriptome analysis reveals a major role for Zeb1 and miR-200b in mouse inner ear morphogenesis.Hertzano R(1), Elkon R, Kurima K, Morrisson A, Chan SL, Sallin M, Biedlingmaier A, Darling DS, Griffith AJ, Eisenman DJ, Strome SE.Author information:(1)Department of Otorhinolaryngology-Head and Neck Surgery, University of Maryland, Baltimore, Maryland, United States of America. rhertzano@smail.umaryland.eduComment in    A Noncoding Point Mutation of Zeb1 Causes Multiple Developmental Malformations and Obesity in Twirler Mice.Cellular heterogeneity hinders the extraction of functionally significant results and inference of regulatory networks from wide-scale expression profiles of complex mammalian organs. The mammalian inner ear consists of the auditory and vestibular systems that are each composed of hair cells, supporting cells, neurons, mesenchymal cells, other epithelial cells, and blood vessels. We developed a novel protocol to sort auditory and vestibular tissues of newborn mouse inner ears into their major cellular components. Transcriptome profiling of the sorted cells identified cell type-specific expression clusters. Computational analysis detected transcription factors and microRNAs that play key roles in determining cell identity in the inner ear. Specifically, our analysis revealed the role of the Zeb1/miR-200b pathway in establishing epithelial and mesenchymal identity in the inner ear. Furthermore, we detected a misregulation of the ZEB1 pathway in the inner ear of Twirler mice, which manifest, among other phenotypes, malformations of the auditory and vestibular labyrinth. The association of misregulation of the ZEB1/miR-200b pathway with auditory and vestibular defects in the Twirler mutant mice uncovers a novel mechanism underlying deafness and balance disorders. Our approach can be employed to decipher additional complex regulatory networks underlying other hearing and balance mouse mutants.DOI: 10.1371/journal.pgen.1002309PMCID: PMC3183091",pubmed,21980309,10.1371/journal.pgen.1002309
functional modeling of the human auditory brainstem response to broadband stimulation,"141. J Acoust Soc Am. 2015 Sep;138(3):1637-59. doi: 10.1121/1.4928305.Functional modeling of the human auditory brainstem response to broadband stimulation.Verhulst S(1), Bharadwaj HM(2), Mehraei G(3), Shera CA(4), Shinn-Cunningham BG(2).Author information:(1)Cluster of Excellence ""Hearing4all"" and Medizinische Physik, Department of Medical Physics and Acoustics, Oldenburg University, Carl-von-Ossietzky Strasse 9-11, 26129 Oldenburg, Germany.(2)Center of Computational Neuroscience and Neural Technology, Boston University, 677 Beacon Street, Boston, Massachusetts 02215, USA.(3)Department of Biomedical Engineering, Boston University, 44 Cummington Street, Boston, Massachusetts 02215, USA.(4)Eaton-Peabody Laboratory, 243 Charles Street, Boston, Massachusetts 02114, USA.Population responses such as the auditory brainstem response (ABR) are commonly used for hearing screening, but the relationship between single-unit physiology and scalp-recorded population responses are not well understood. Computational models that integrate physiologically realistic models of single-unit auditory-nerve (AN), cochlear nucleus (CN) and inferior colliculus (IC) cells with models of broadband peripheral excitation can be used to simulate ABRs and thereby link detailed knowledge of animal physiology to human applications. Existing functional ABR models fail to capture the empirically observed 1.2-2 ms ABR wave-V latency-vs-intensity decrease that is thought to arise from level-dependent changes in cochlear excitation and firing synchrony across different tonotopic sections. This paper proposes an approach where level-dependent cochlear excitation patterns, which reflect human cochlear filter tuning parameters, drive AN fibers to yield realistic level-dependent properties of the ABR wave-V. The number of free model parameters is minimal, producing a model in which various sources of hearing-impairment can easily be simulated on an individualized and frequency-dependent basis. The model fits latency-vs-intensity functions observed in human ABRs and otoacoustic emissions while maintaining rate-level and threshold characteristics of single-unit AN fibers. The simulations help to reveal which tonotopic regions dominate ABR waveform peaks at different stimulus intensities.DOI: 10.1121/1.4928305PMCID: PMC4592442",pubmed,26428802,10.1121/1.4928305
singlecell rnaseq resolves cellular complexity in sensory organs from the neonatal inner ear,"825. Nat Commun. 2015 Oct 15;6:8557. doi: 10.1038/ncomms9557.Single-cell RNA-Seq resolves cellular complexity in sensory organs from the neonatal inner ear.Burns JC(1), Kelly MC(1), Hoa M(1), Morell RJ(2), Kelley MW(1).Author information:(1)Laboratory of Cochlear Development, National Institute on Deafness and Other Communication Disorders, National Institutes of Health, Bethesda, Maryland 20892, USA.(2)Genomics and Computational Biology Core, National Institute on Deafness and Other Communication Disorders, National Institutes of Health, Bethesda, Maryland 20892, USA.In the inner ear, cochlear and vestibular sensory epithelia utilize grossly similar cell types to transduce different stimuli: sound and acceleration. Each individual sensory epithelium is composed of highly heterogeneous populations of cells based on physiological and anatomical criteria. However, limited numbers of each cell type have impeded transcriptional characterization. Here we generated transcriptomes for 301 single cells from the utricular and cochlear sensory epithelia of newborn mice to circumvent this challenge. Cluster analysis indicates distinct profiles for each of the major sensory epithelial cell types, as well as less-distinct sub-populations. Asynchrony within utricles allows reconstruction of the temporal progression of cell-type-specific differentiation and suggests possible plasticity among cells at the sensory-nonsensory boundary. Comparisons of cell types from utricles and cochleae demonstrate divergence between auditory and vestibular cells, despite a common origin. These results provide significant insights into the developmental processes that form unique inner ear cell types.DOI: 10.1038/ncomms9557PMCID: PMC4634134",pubmed,26469390,10.1038/ncomms9557
hearing the impact of mp3 on a survey of middle school students,"491. Lin Chuang Er Bi Yan Hou Tou Jing Wai Ke Za Zhi. 2011 Feb;25(4):151-3.[Hearing the impact of MP3 on a survey of middle school students].[Article in Chinese]Xu Z(1), Li Z, Chen Y, He Y, Chunyu X, Wang F, Zhang P, Gao L, Qiu S, Liu S, Qiao L, Qiu J.Author information:(1)Department of Otolaryngology, the First People's Hospital of Yongkang, Zhejiang Province, Yongkang, 321300, China.OBJECTIVE: To understand the usage of MP3 and effects on hearing of middle school students in Xi'an, and discuss controlling strategies.METHOD: Stratified random cluster sampling method was used in the 1567 middle school students in Xi'an through questionnaire survey, ear examination and hearing examination, data were analysed by the SPSS13.0 statistical software.RESULT: 1) The rate of holding MP3 in the middle school students was 85.2%. Average daily use time was (1.41 +/- 1.11) h. 2) The noise group of pure tone hearing threshold was significantly higher compared with the control group (P<0.01), and increased the detection rate of hearing loss with the increasing use of MP3. 3) The detection rate of symptoms increased with the increasing use of MP3.CONCLUSION: The usage of MP3 can harm hearing in middle school students, which can result in neurasthenic syndrome.",pubmed,21563460,
analysis of hearing aids application in elderly patients,"79. Adv Gerontol. 2023;36(2):265-273.[Analysis of hearing aids application in elderly patients.].[Article in Russian; Abstract available in Russian from the publisher]Boboshko MY(1), Garbaruk ES(1), Golovanova LE(2)(3), Maltseva NV(1), Berdnikova IP(1), Markelov OA(4), Shpakovskaya II(4), Romanov SA(4), Kaplun DI(4).Author information:(1)I.P.Pavlov First Saint-Petersburg State Medical University, 6-8 Lev Tolstoy str., St. Petersburg 197022, Russian Federation, e-mail: boboshkom@gmail.com.(2)I.I.Mechnikov North-Western State Medical University, 41 Kirochnaya str., St. Petersburg 191015, Russian Federation.(3)Saint-Petersburg Geriatric Medico-social Center, Municipal Audiology Center, 21 Rizhskii av., St. Petersburg 190103, Russian Federation.(4)Saint-Petersburg Electrotechnical University, Centre for Digital Communication Technologies, 5 Professora Popova str., St. Petersburg 197022, Russian Federation.The aim of the study is to evaluate the possibility to implement machine learning to create a digital auditory profile for elderly patients and to analyze the hearing aid fitting efficacy depending on involvement of the peripheral and central auditory pathways in a pathological process. Data analysis of 375 people aged 60-93 years is presented. 355 patients with chronic bilateral hearing loss (230 of them used hearing aids) were included in the main group, and 20 normal hearing elderly people were included in the control group. Audiological examination consisted of standard tests (pure tone audiometry, impedancemetry, speech audiometry in quiet) and tests to evaluate the central auditory processing (binaural fusion, dichotic digits, speech audiometry in noise, random gap detection). The Montreal Cognitive Assessment was used to detect cognitive impairment. The hearing aid fitting efficiency was evaluated with COSI questionnaire and speech audiometry in free field. Processing of the results was carried out using Pearson's correlation analysis aimed at creating a polynomial model of a patient's hearing on the basis of the limited test battery. There were close correlations between the state of cognitive functions and age, results of tests to evaluate the central auditory processing, as well as patients' satisfaction of hearing aid. The results of the work indicate the possibility of using computer technologies of data analysis to develop rehabilitation programs for elderly hearing impaired patients.Publisher: Цель исследования — оценка возможности внедрения методов машинного обучения для создания цифрового слухового профиля у пациентов старших возрастных групп и анализа эффективности слухопротезирования в зависимости от вовлеченности в патологический процесс периферических и центральных отделов слуховой системы. Представлены результаты обследования 375 лиц 60–93 лет, из которых в основную группу вошли 355 пациентов с хронической двусторонней тугоухостью (230 из них использовали слуховые аппараты), а в контрольную — 20 человек пожилого возраста с нормальными порогами слуха. Аудиологическое обследование включало базовые методики (тональная пороговая и надпороговая аудиометрия, импедансометрия, речевая аудиометрия в тишине) и методы оценки состояния центральных отделов слуховой системы (тест чередующейся бинаурально речью, дихотический числовой тест, речевая аудиометрия в шуме, тест обнаружения паузы). Диагностику состояния когнитивных функций осуществляли с использованием Монреальской когнитивной шкалы. Эффективность слухопротезирования оценивали посредством анкетирования и речевой аудиометрии в свободном звуковом поле. Обработку результатов проводили с применением корреляционного анализа Пирсона, направленного на создание полиномиальной модели слуха пациента на основе ограниченного набора тестов. Выявлены корреляции состояния когнитивных функций и возраста, выполнения ряда тестов по оценке центральных отделов слуховой системы, а также успешности применения слуховых аппаратов. Результаты работы свидетельствуют о возможности использования компьютерных технологий анализа данных для разработки программ реабилитации пациентов старших возрастных групп с нарушениями слуха.",pubmed,37356105,
ultrathin eardruminspired selfpowered acoustic sensor for vocal synchronization recognition with the assistance of machine learning,"1077/1 “Hearing4all”. MG and HM received travel support to conferences by MED-El and TL declares no competing interests.537. Small. 2022 Apr;18(13):e2106960. doi: 10.1002/smll.202106960. Epub 2022 Feb 5.Ultrathin Eardrum-Inspired Self-Powered Acoustic Sensor for Vocal Synchronization Recognition with the Assistance of Machine Learning.Jiang Y(1)(2), Zhang Y(1)(2), Ning C(1)(2), Ji Q(3), Peng X(1)(2), Dong K(1)(2), Wang ZL(1)(2)(4)(5).Author information:(1)Beijing Institute of Nanoenergy and Nanosystems, Chinese Academy of Sciences, Beijing, 101400, P. R. China.(2)School of Nanoscience and Technology, University of Chinese Academy of Sciences, Beijing, 100049, P. R. China.(3)Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, 100049, P. R. China.(4)CUSTech Institute of Technology, Wenzhou, Zhejiang, 325024, P. R. China.(5)School of Materials Science and Engineering, Georgia Institute of Technology, Atlanta, GA, 30332-0245, USA.With the rapid development of human-machine interfaces, artificial acoustic sensors play an important role in the hearing impaired. Here, an ultrathin eardrum-like triboelectric acoustic sensor (ETAS) is presented consisting of silver-coated nanofibers, whose thickness is only 40 µm. The sensitivity and frequency response range of the ETAS are closely related to the geometric parameters. The ETAS endows a high sensitivity of 228.5 mV Pa-1 at 95 dB, and the ETAS has a broad frequency response ranging from 20 to 5000 Hz, which can be tuned by adjusting the thickness, size, or shape of the sensor. Cooperating with artificial intelligence (AI) algorithms, the ETAS can achieve real-time voice conversion with a high identification accuracy of 92.64%. Under good working property and the AI system, the ETAS simplifies signal processing and reduces the power consumption. This work presents a strategy for self-power auditory systems, which can greatly accelerate the miniaturization of self-powered systems used in wearable electronics, augmented reality, virtual reality, and control hubs for automation.© 2022 Wiley-VCH GmbH.DOI: 10.1002/smll.202106960",pubmed,35122473,10.1002/smll.202106960
speech intelligibility and subjective benefit in singlesided deaf adults after cochlear implantation,"86. Hear Res. 2017 May;348:112-119. doi: 10.1016/j.heares.2017.03.002. Epub 2017 Mar 10.Speech intelligibility and subjective benefit in single-sided deaf adults after cochlear implantation.Finke M(1), Strauß-Schier A(2), Kludt E(3), Büchner A(3), Illg A(2).Author information:(1)Department of Otolaryngology, Hannover Medical School, Germany; Cluster of Excellence ""Hearing4all"", Germany. Electronic address: Finke.Mareike@mh-hannover.de.(2)Department of Otolaryngology, Hannover Medical School, Germany.(3)Department of Otolaryngology, Hannover Medical School, Germany; Cluster of Excellence ""Hearing4all"", Germany.Treatment with cochlear implants (CIs) in single-sided deaf individuals started less than a decade ago. CIs can successfully reduce incapacitating tinnitus on the deaf ear and allow, so some extent, the restoration of binaural hearing. Until now, systematic evaluations of subjective CI benefit in post-lingually single-sided deaf individuals and analyses of speech intelligibility outcome for the CI in isolation have been lacking. For the prospective part of this study, the Bern Benefit in Single-Sided Deafness Questionnaire (BBSS) was administered to 48 single-sided deaf CI users to evaluate the subjectively perceived CI benefit across different listening situations. In the retrospective part, speech intelligibility outcome with the CI up to 12 month post-activation was compared between 100 single-sided deaf CI users and 125 bilaterally implanted CI users (2nd implant). The positive median ratings in the BBSS differed significantly from zero for all items suggesting that most individuals with single-sided deafness rate their CI as beneficial across listening situations. The speech perception scores in quiet and noise improved significantly over time in both groups of CI users. Speech intelligibility with the CI in isolation was significantly better in bilaterally implanted CI users (2nd implant) compared to the scores obtained from single-sided deaf CI users. Our results indicate that CI users with single-sided deafness can reach open set speech understanding with their CI in isolation, encouraging the extension of the CI indication to individuals with normal hearing on the contralateral ear. Compared to the performance reached with bilateral CI users' second implant, speech reception threshold are lower, indicating an aural preference and dominance of the normal hearing ear. The results from the BBSS propose good satisfaction with the CI across several listening situations.Copyright © 2017 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2017.03.002",pubmed,28286233,10.1016/j.heares.2017.03.002
optoacoustic effect is responsible for laserinduced cochlear responses,"703. Sci Rep. 2016 Jun 15;6:28141. doi: 10.1038/srep28141.Optoacoustic effect is responsible for laser-induced cochlear responses.Kallweit N(1)(2), Baumhoff P(3), Krueger A(1)(2), Tinne N(1)(2), Kral A(2)(3), Ripken T(1)(2), Maier H(2)(3).Author information:(1)Laser Zentrum Hannover e.V., Hollerithallee 8, 30419 Hannover, Germany.(2)Cluster of Excellence ""Hearing4all"", Germany.(3)Institute of Audioneurotechnology and Dept. of Experimental Otology, ENT Clinics, Hannover Medical School, Feodor-Lynen-Str. 35, 30625 Hannover, Germany.Optical stimulation of the cochlea with laser light has been suggested as an alternative to conventional treatment of sensorineural hearing loss with cochlear implants. The underlying mechanisms are controversially discussed: The stimulation can either be based on a direct excitation of neurons, or it is a result of an optoacoustic pressure wave acting on the basilar membrane. Animal studies comparing the intra-cochlear optical stimulation of hearing and deafened guinea pigs have indicated that the stimulation requires intact hair cells. Therefore, optoacoustic stimulation seems to be the underlying mechanism. The present study investigates optoacoustic characteristics using pulsed laser stimulation for in vivo experiments on hearing guinea pigs and pressure measurements in water. As a result, in vivo as well as pressure measurements showed corresponding signal shapes. The amplitude of the signal for both measurements depended on the absorption coefficient and on the maximum of the first time-derivative of laser pulse power (velocity of heat deposition). In conclusion, the pressure measurements directly demonstrated that laser light generates acoustic waves, with amplitudes suitable for stimulating the (partially) intact cochlea. These findings corroborate optoacoustic as the basic mechanism of optical intra-cochlear stimulation.DOI: 10.1038/srep28141PMCID: PMC4908384",pubmed,27301846,10.1038/srep28141
a novel audiobased machine learning model for automated detection of collision hazards at construction sites,"Collisions between workers and operating vehicles are the leading source of fatal incidents in the construction industry. One of the most prevalent factors causing contact hazards is the decline in construction workers' auditory situational awareness due to the hearing loss and the complicated nature of construction noises. Thus, a computational technique that can augment the audible sense of a worker can significantly improve safety performance. Since construction machines often generate distinct sound patterns while operating at the construction sites, audio signal processing could be an innovative solution to achieve the goal. Unfortunately, the current body of knowledge regarding automated surveillance in construction still lacks such advanced methods. This paper presents a newly developed auditory surveillance framework using convolutional neural networks (CNNs) that can detect collision hazards by processing acoustic signals in construction sites. The study specifically has two primary contributions: (1) a new labeled dataset of normal and abnormal sound events relating to collision hazards in the construction site, and (2) a novel audio-based machine learning model for automated detection of collision hazards. The model was trained with different network architectures, and its performance was evaluated using various measures, including accuracy, recall, precision, and combined F-measure. The research is expected to help increase the auditory situational awareness of construction workers and consequently enhance construction safety. © 2020 Proceedings of the 37th International Symposium on Automation and Robotics in Construction, ISARC 2020: From Demonstration to Practical Use - To New Stage of Construction Robot. All rights reserved.",scopus,2-s2.0-85109376926,
thinfilm microelectrode stimulation of the cochlea in rats exposed to aminoglycoside induced hearing loss references,"The multi-channel cochlear implant (CI) provides sound and speech perception to thousands of individuals who would otherwise be deaf. Broad activation of auditory nerve fibres when using a CI results in poor frequency discrimination. The CI also provides users with poor amplitude perception due to elicitation of a narrow dynamic range. Provision of more discrete frequency perception and a greater control over amplitude may allow users to better distinguish speech in noise and to segregate sound sources. In this research, thin-film (TF) high density micro-electrode arrays and conventional platinum ring electrode arrays were used to stimulate the cochlea of rats administered sensorineural hearing loss (SNHL) via ototoxic insult, with neural responses taken at 434 multiunit clusters in the central nucleus of the inferior colliculus (CIC). Threshold, dynamic range and broadness of response were used to compare electrode arrays. A stronger current was required to elicit CIC threshold when using the TF array compared to the platinum ring electrode array. TF stimulation also elicited a narrower dynamic range than the PR counterpart. However, monopolar stimulation using the TF array produced more localised CIC responses than other stimulation strategies. These results suggest that individuals with SNHL could benefit from micro stimulation of the cochlea using a monopolar configuration which may provide discrete frequency perception when using TF electrode arrays. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc15&DO=10.1016%2fj.heares.2015.10.003
design of intelligent humancomputer interaction system for hard of hearing and nondisabled people,"Since the hard of hearing cannot communicate effectively with the non-disabled, which may cause various inconveniences. As an essential member of a harmonious society, it is particularly urgent to solve their communication problems with non-disabled people. Effective communication between the hard of hearing and the non-disabled has become possible with the continuous development of artificial intelligence. In this paper, an intelligent human-computer interaction system is designed to solve communication inconvenience between the hard of hearing and the non-disabled. This system combines artificial intelligence with wearable devices and classifies gestures with BP neural network, effectively solving the communication problem between the hard of hearing and the non-disabled.",ieee,1558-1748,10.1109/JSEN.2021.3107949
selecting different amplification for different listening conditions,,cinahl,10500545,NLM8652874
verbal learning and memory in prelingually deaf children with cochlear implants references,"Objective: Deaf children with cochlear implants (CIs) show poorer verbal working memory compared to normal-hearing (NH) peers, but little is known about their verbal learning and memory (VLM) processes involving multi-trial free recall. Design: Children with CIs were compared to NH peers using the California Verbal Learning Test for Children (CVLT-C). Study sample: Participants were 21 deaf (before age 6 months) children (6-16 years old) implanted prior to age 3 years, and 21 age-IQ matched NH peers. Results: Results revealed no differences between groups in number of words recalled. However, CI users showed a pattern of increasing use of serial clustering strategies across learning trials, whereas NH peers decreased their use of serial clustering strategies. In the CI sample (but not in the NH sample), verbal working memory test scores were related to resistance to the build-up of proactive interference, and sentence recognition was associated with performance on the first exposure to the word list and to the use of recency recall strategies. Conclusions: Children with CIs showed robust evidence of VLM comparable to NH peers. However, their VLM processing (especially recency and proactive interference) was related to speech perception outcomes and verbal WM in different ways from NH peers. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc17&DO=10.1080%2f14992027.2018.1481538
proteome analysis of human perilymph using an intraoperative sampling method,"230. J Proteome Res. 2017 May 5;16(5):1911-1923. doi: 10.1021/acs.jproteome.6b00986. Epub 2017 Apr 13.Proteome Analysis of Human Perilymph Using an Intraoperative Sampling Method.Schmitt HA(1)(2), Pich A(3), Schröder A(3), Scheper V(1)(2), Lilli G(1)(2), Reuter G(1)(2), Lenarz T(1)(2).Author information:(1)Department of Otolaryngology, Hannover Medical School , Carl-Neuberg-Str. 1, 30625 Hannover, Germany.(2)Cluster of Excellence of the German Research Foundation (DFG; ""Deutsche Forschungsgemeinschaft"") ""Hearing4all"", Hannover Medical School , Carl-Neuberg-Str. 1, 30625 Hannover, Germany.(3)Core Facility Proteomics, Hannover Medical School , Carl-Neuberg-Str. 1, 30625 Hannover, Germany.The knowledge about the etiology and pathophysiology of sensorineural hearing loss (SNHL) is still very limited. This study aims at the improvement of understanding different types of SNHL by proteome analysis of human perilymph. Sampling of perilymph was established during inner ear surgeries (cochlear implantation, vestibular schwannoma surgeries), and safety of the sampling method was determined by checking hearing threshold with pure-tone audiometry postoperatively. An in-depth shot-gun proteomics approach was performed to identify cochlear proteins and the individual proteome in perilymph of patients. This method enables the identification and quantification of protein composition of perilymph. The proteome of 41 collected perilymph samples with volumes of 1-12 μL was analyzed by data-dependent acquisition, resulting in overall 878 detected protein groups. At least 203 protein groups were solely identified in perilymph, not in reference samples (serum, cerebrospinal fluid), displaying a specific protein pattern for perilymph. Samples were grouped by patient's age and surgery type, leading to the identification of some proteins specific to particular subgroups. Proteins with different abundances between different sample groups were subjected to classification by gene ontology annotations. The identified proteins might serve as biomarkers to develop tools for noninvasive inner ear diagnostics and to elucidate molecular profiles of SNHL.DOI: 10.1021/acs.jproteome.6b00986",pubmed,28282143,10.1021/acs.jproteome.6b00986
acoustic shock controversies,"Background: The diagnosis 'acoustic shock' has been made increasingly in the health care industry in recent years. This paper aims to question the validity of acoustic shock as an organic pathological entity. Methods: The experiences of 16 individuals diagnosed as having acoustic shock, within a medico-legal practice, are reviewed. Results: The commonest symptom was otalgia, followed by noise sensitivity, tinnitus, hearing disturbance and dizziness. Conclusion: The presence of noise-limiting technology in the workplace, the variation in the nature of the acoustic incident involved (ranging from a shriek, through feedback noise, to a male voice), and the marked variation in the time of symptom onset (following the acoustic incident) all suggest that the condition termed acoustic shock is predominantly psychogenic. Cases of pseudohypacusis indicate that malingering is a factor in some cases. Clusters of acoustic shock events occurring in the same call centres suggest that hysteria may play a part. The condition is usually only seen when work-related issues are apparent. Copyright © JLO (1984) Limited 2014.",scopus,2-s2.0-84903556086,10.1017/S0022215114000309
treatment of agerelated hearing loss alters audiovisual integration and restingstate functional connectivity a randomized controlled pilot trial,"114. eNeuro. 2021 Dec 8;8(6):ENEURO.0258-21.2021. doi: 10.1523/ENEURO.0258-21.2021. Print 2021 Nov-Dec.Treatment of Age-Related Hearing Loss Alters Audiovisual Integration and Resting-State Functional Connectivity: A Randomized Controlled Pilot Trial.Rosemann S(1)(2), Gieseler A(2)(3), Tahden M(2)(3), Colonius H(2)(3), Thiel CM(1)(2).Author information:(1)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, Oldenburg 26111, Germany stephanie.rosemann@uni-oldenburg.de christiane.thiel@uni-oldenburg.de.(2)Cluster of Excellence ""Hearing4all,"" Carl von Ossietzky Universität Oldenburg, Oldenburg 26111, Germany.(3)Cognitive Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl von Oldenburg 26111 Universität Oldenburg, Oldenburg 26111, Germany.Untreated age-related hearing loss increases audiovisual integration and impacts resting state functional brain connectivity. Further, there is a relation between crossmodal plasticity and audiovisual integration strength in cochlear implant patients. However, it is currently unclear whether amplification of the auditory input by hearing aids influences audiovisual integration and resting state functional brain connectivity. We conducted a randomized controlled pilot study to investigate how the McGurk illusion, a common measure for audiovisual integration, and resting state functional brain connectivity of the auditory cortex are altered by six-month hearing aid use. Thirty-two older participants with slight-to-moderate, symmetric, age-related hearing loss were allocated to a treatment or waiting control group and measured one week before and six months after hearing aid fitting with functional magnetic resonance imaging. Our results showed a statistical trend for an increased McGurk illusion after six months of hearing aid use. We further demonstrated that an increase in McGurk susceptibility is related to a decreased hearing aid benefit for auditory speech intelligibility in noise. No significant interaction between group and time point was obtained in the whole-brain resting state analysis. However, a region of interest (ROI)-to-ROI analysis indicated that hearing aid use of six months was associated with a decrease in resting state functional connectivity between the auditory cortex and the fusiform gyrus and that this decrease was related to an increase of perceived McGurk illusions. Our study, therefore, suggests that even short-term hearing aid use alters audiovisual integration and functional brain connectivity between auditory and visual cortices.Copyright © 2021 Rosemann et al.DOI: 10.1523/ENEURO.0258-21.2021PMCID: PMC8658542",pubmed,34759049,10.1523/ENEURO.0258-21.2021
resolving the structure of inner ear ribbon synapses with sted microscopy,"752. Synapse. 2015 May;69(5):242-55. doi: 10.1002/syn.21812. Epub 2015 Mar 11.Resolving the structure of inner ear ribbon synapses with STED microscopy.Rutherford MA(1).Author information:(1)Department of Otolaryngology, Central Institute for the Deaf, Washington University School of Medicine, Washington University School of Medicine, St. Louis, Missouri, 63110; Inner Ear Lab, Department of Otolaryngology, University of Göttingen Medical Center, Göttingen, Germany, D-37077.Synapses are diverse in form and function; however, the mechanisms underlying this diversity are poorly understood. To illuminate structure/function relationships, robust analysis of molecular composition and morphology is needed. The molecular-anatomical components of synapses-vesicles, clusters of voltage-gated ion channels in presynaptic densities, arrays of transmitter receptors in postsynaptic densities-are only tens to hundreds of nanometers in size. Measuring the topographies of synaptic proteins requires nanoscale resolution of their molecularly specific labels. Super-resolution light microscopy has emerged to meet this need. Achieving 50 nm resolution in thick tissue, we employed stimulated emission depletion (STED) microscopy to image the functionally and molecularly unique ribbon-type synapses in the inner ear that connect mechano-sensory inner hair cells to cochlear nerve fibers. Synaptic ribbons, bassoon protein, voltage-gated Ca(2+) channels, and glutamate receptors are inhomogeneous in their spatial distributions within synapses; the protein clusters assume variations of shapes typical for each protein specifically at cochlear afferent synapses. Heterogeneity of substructure among these synapses may contribute to functional differences among auditory nerve fibers. The morphology of synaptic voltage-gated Ca(2+) channels matures over development in a way that depends upon bassoon protein, which aggregates in similar form. Functional properties of synaptic transmission appear to depend on voltage-gated Ca(2+) channel cluster morphology and position relative to synaptic vesicles. Super-resolution light microscopy is a group of techniques that complement electron microscopy and conventional light microscopy. Although technical hurdles remain, we are beginning to resolve the details of molecular nanoanatomy that relate mechanistically to synaptic function.© 2015 Wiley Periodicals, Inc.DOI: 10.1002/syn.21812",pubmed,25682928,10.1002/syn.21812
braincontrolled hearing aids for better speech perception in noisy settings,,cinahl,7457472,10.1097/01.hj.0000582432.27323.bd
an automated model for child language impairment prediction using hybrid optimal bilstm,"Children without obvious disabilities (hearing loss/low intellectual capacity) may have language skill development issues due to specific language impairment (SLI), a communication disorder. The SLI has a significant impact on a child's speaking, listening, reading, and writing abilities. SLI is typically known as development language disorder, developmental dysphasia, or language delay. Recently, machine learning as well as deep learning techniques have been quite effective in predicting the early stage of SLI, analyzing the disorder severity, and predicting the treatment efficiency. Existing approaches primarily exploited auditory indicators to diagnose communication disorders, frequently leaving out hidden information acquired in the temporal domain. To overcome this drawback, an optimized Bidirectional Long Short Term Memory (BiLSTM) architecture is presented in this paper to handle the speech dynamics. The Improved Hybrid Aquila Optimizer and Flow Directional algorithm known as IHAOFDA is integrated with the BiLSTM architecture to optimize the hyperparameters of the BiLSTM structure. When assessed using the information from the SLI children in the Laboratory of Artificial Neural Network Applications (LANNA) dataset, the proposed model performs better. The IHAOFDA-optimized BiLSTM architecture improves accuracy in classifying different severity levels such as mild, moderate, and severe. © 2023 IETE.",scopus,2-s2.0-85182810580,10.1080/03772063.2023.2243881
cortical evoked potentials and hearing aids in individuals with auditory dyssynchrony,"106. J Int Adv Otol. 2015 Dec;11(3):236-42. doi: 10.5152/iao.2015.1162.Cortical Evoked Potentials and Hearing Aids in Individuals with Auditory Dys-Synchrony.Yuvaraj P(1), Mannarukrishnaiah J.Author information:(1)Department of Speech Pathology and Audiology, National Insitute of Mental Health and Neurosciences, Bangalore, India. drmjay16@gmail.com.OBJECTIVE: The purpose of the present study was to investigate the relationship between cortical processing of speech and benefit from hearing aids in individuals with auditory dys-synchrony.MATERIALS AND METHODS: Data were collected from 38 individuals with auditory dys-synchrony. Participants were selected based on hearing thresholds, middle ear reflexes, otoacoustic emissions, and auditory brain stem responses. Cortical-evoked potentials were recorded for click and speech. Participants with auditory dys-synchrony were fitted with bilateral multichannel wide dynamic range compression hearing aids. Aided and unaided speech identification scores for 40 words were obtained for each participant.RESULTS: Hierarchical cluster analysis using Ward's method clearly showed four subgroups of participants with auditory dys-synchrony based on the hearing aid benefit score (aided minus unaided speech identification score). The difference in the mean aided and unaided speech identification scores was significantly different in participants with auditory dys-synchrony. However, the mean unaided speech identification scores were not significantly different between the four subgroups. The N2 amplitude and P1 latency of the speech-evoked cortical potentials were significantly different between the four subgroups formed based on hearing aid benefit scores.CONCLUSION: The results indicated that subgroups of individuals with auditory dys-synchrony who benefit from hearing aids exist. Individuals who benefitted from hearing aids showed decreased N2 amplitudes compared with those who did not. N2 amplitude is associated with greater suppression of background noise while processing speech.DOI: 10.5152/iao.2015.1162",pubmed,26915156,10.5152/iao.2015.1162
linguistic profiles of children with ci as compared with children with hearing or specific language impairment references,"Background: The spoken language difficulties of children with moderate or severe to profound hearing loss are mainly related to limited auditory speech perception. However, degraded or filtered auditory input as evidenced in children with cochlear implants (CIs) may result in less efficient or slower language processing as well. To provide insight into the underlying nature of the spoken language difficulties in children with CIs, linguistic profiles of children with CIs are compared with those of hard-of-hearing (HoH) children with conventional hearing aids and children with specific language impairment (SLI). Aims: To examine differences in linguistic abilities and profiles of children with CIs as compared with HoH children and children with SLI, and whether the spoken language difficulties of children with CIs mainly lie in limited auditory perception or in language processing problems. Methods & Procedure: Differences in linguistic abilities and differential linguistic profiles of 47 children with CI, 66 HoH children with moderate to severe hearing loss, and 127 children with SLI are compared, divided into two age cohorts. Standardized Dutch tests were administered. Factor analyses and cluster analyses were conducted to find homogeneous linguistic profiles of the children. Outcomes & Results: The children with CIs were outperformed by their HoH peers and peers with SLI on most linguistic abilities. Concerning the linguistic profiles, the largest group of children with CIs and HoH children shared similar profiles. The profiles observed for most of the children with SLI were different from those of their peers with hearing loss in both age cohorts. Conclusions & Implications: Results suggest that the underlying nature of spoken language problems in most children with CIs manifests in limited auditory perception instead of language processing difficulties. However, there appears to be a subgroup of children with CIs whose linguistic profiles resemble those of children with SLI. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc15&DO=10.1111%2f1460-6984.12228
source localisation of visual evoked potentials in congenitally deaf individuals,"669. Brain Topogr. 2014 May;27(3):412-24. doi: 10.1007/s10548-013-0341-7. Epub 2013 Dec 12.Source localisation of visual evoked potentials in congenitally deaf individuals.Hauthal N(1), Thorne JD, Debener S, Sandmann P.Author information:(1)Neuropsychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all"", European Medical School, University of Oldenburg, 26111, Oldenburg, Germany, nadine.hauthal@uni-oldenburg.de.Previous studies have suggested that individuals deprived of auditory input can compensate with specific superior abilities in the remaining sensory modalities. To better understand the neural basis of deafness-induced changes, the present study used electroencephalography to examine visual functions and cross-modal reorganization of the auditory cortex in deaf individuals. Congenitally deaf participants and hearing controls were presented with reversing chequerboard stimuli that were systematically modulated in luminance ratio. The two groups of participants showed similar modulation of visual evoked potential (VEP) amplitudes (N85, P110) and latencies (P110) as a function of luminance ratio. Analysis of VEPs revealed faster neural processing in deaf participants compared with hearing controls at early stages of cortical visual processing (N85). Deaf participants also showed higher amplitudes (P110) than hearing participants. In contrast to our expectations, the results from VEP source analysis revealed no clear evidence for cross-modal reorganization in the auditory cortex of deaf participants. However, deaf participants tended to show higher activation in posterior parietal cortex (PPC). Moreover, modulation of PPC responses as a function of luminance was also stronger in deaf than in hearing participants. Taken together, these findings are an indication of more efficient neural processing of visual information in the deaf, which may relate to functional changes, in particular in multisensory parietal cortex, as a consequence of early auditory deprivation.DOI: 10.1007/s10548-013-0341-7",pubmed,24337445,10.1007/s10548-013-0341-7
electrical stimulation of the midbrain excites the auditory cortex asymmetrically,"708. Brain Stimul. 2018 Sep-Oct;11(5):1161-1174. doi: 10.1016/j.brs.2018.05.009. Epub 2018 May 17.Electrical stimulation of the midbrain excites the auditory cortex asymmetrically.Quass GL(1), Kurt S(2), Hildebrandt KJ(3), Kral A(2).Author information:(1)Institute of AudioNeuroTechnology (VIANNA), Dept. of Experimental Otology, ENT Clinics, Hannover Medical School, 30625 Hannover, Germany; Cluster of Excellence ""Hearing4all"", Germany. Electronic address: quass.gunnar@mh-hannover.de.(2)Institute of AudioNeuroTechnology (VIANNA), Dept. of Experimental Otology, ENT Clinics, Hannover Medical School, 30625 Hannover, Germany; Cluster of Excellence ""Hearing4all"", Germany.(3)Cluster of Excellence ""Hearing4all"", Germany; Research Center Neurosensory Science, University of Oldenburg, 26111 Oldenburg, Germany.BACKGROUND: Auditory midbrain implant users cannot achieve open speech perception and have limited frequency resolution. It remains unclear whether the spread of excitation contributes to this issue and how much it can be compensated by current-focusing, which is an effective approach in cochlear implants.OBJECTIVE: The present study examined the spread of excitation in the cortex elicited by electric midbrain stimulation. We further tested whether current-focusing via bipolar and tripolar stimulation is effective with electric midbrain stimulation and whether these modes hold any advantage over monopolar stimulation also in conditions when the stimulation electrodes are in direct contact with the target tissue.METHODS: Using penetrating multielectrode arrays, we recorded cortical population responses to single pulse electric midbrain stimulation in 10 ketamine/xylazine anesthetized mice. We compared monopolar, bipolar, and tripolar stimulation configurations with regard to the spread of excitation and the characteristic frequency difference between the stimulation/recording electrodes.RESULTS: The cortical responses were distributed asymmetrically around the characteristic frequency of the stimulated midbrain region with a strong activation in regions tuned up to one octave higher. We found no significant differences between monopolar, bipolar, and tripolar stimulation in threshold, evoked firing rate, or dynamic range.CONCLUSION: The cortical responses to electric midbrain stimulation are biased towards higher tonotopic frequencies. Current-focusing is not effective in direct contact electrical stimulation. Electrode maps should account for the asymmetrical spread of excitation when fitting auditory midbrain implants by shifting the frequency-bands downward and stimulating as dorsally as possible.Copyright © 2018 Elsevier Inc. All rights reserved.DOI: 10.1016/j.brs.2018.05.009",pubmed,29853311,10.1016/j.brs.2018.05.009
randomised controlled trial of treatment of chronic suppurative otitis media in kenyan schoolchildren,"524. Lancet. 1996 Oct 26;348(9035):1128-33. doi: 10.1016/S0140-6736(96)09388-9.Randomised controlled trial of treatment of chronic suppurative otitis media in Kenyan schoolchildren.Smith AW(1), Hatcher J, Mackenzie IJ, Thompson S, Bal I, Macharia I, Mugwe P, Okoth-Olende C, Oburra H, Wanjohi Z.Author information:(1)Hearing Impairment Research Group, Liverpool School of Tropical Medicine, UK.Comment in    Lancet. 1996 Oct 26;348(9035):1113-4.BACKGROUND: The outcomes of treatment of chronic suppurative otitis media (CSOM) are disappointing and uncertain, especially in developing countries. Because CSOM is the commonest cause of hearing impairment in children in these countries, an effective method of management that can be implemented on a wide scale is needed. We report a randomised, controlled trial of treatment of CSOM among children in Kenya; unaffected schoolchildren were taught to administer the interventions.METHODS: We enrolled 524 children with CSOM, aged 5-15 years, from 145 primary schools in Kiambu district of Kenya. The schools were randomly assigned treatments in clusters of five in a ratio of two to dry mopping alone (201 children), two to dry mopping with topical and systemic antibiotics and topical steroids (221 children), and one to no specific treatment (102 children). Schools were matched on factors thought to be related to their socioeconomic status. The primary outcome measures were resolution of otorrhoea and healing of tympanic membranes on otoscopy by 8, 12, and 16 weeks after induction. Absence of perforation was confirmed by tympanometry, and hearing levels were assessed by audiometry. 29 children were withdrawn from the trial because they took non-trial antibiotics. There was no evidence of differences in timing of withdrawals between the groups.FINDINGS: By the 16-week follow-up visit, otorrhoea had resolved in a weighted mean proportion of 51% (95% CI 42-59) of children who received dry mopping with antibiotics, compared with 22% (14-31) of those who received dry mopping alone and 22% (9-35) of controls. Similar differences were recorded by the 8-week and 12-week visits. The weighted mean proportions of children with healing of the tympanic membranes by 16 weeks were 15% (10-21) in the dry-mopping plus antibiotics group, 13% (5-20) in the dry-mopping alone group, and 13% (3-23) in the control group. The proportion with resolution in the dry-mopping alone group did not differ significantly from that in the control group at any time. Hearing thresholds were significantly better for children with no otorrhoea at 16 weeks than for those who had otorrhoea, and were also significantly better for those whose ears had healed than for those with otorrhoea at all times.INTERPRETATION: Our finding that dry mopping plus topical and systemic antibiotics is superior to dry mopping alone contrasts with that of the only previous community-based trial in a developing country, though it accords with findings of most other trials in developed countries. The potential role of antibiotics needs further investigation. Further, similar trials are needed to identify the most cost-effective and appropriate treatment regimen for CSOM in children in developing countries.PIP: 524 children aged 5-15 years with chronic suppurative otitis media (CSOM) were enrolled in a study to determine the effectiveness of different treatment regimens. The subjects were from 145 primary schools in Kenya's Kiambu district. 201 children received dry mopping treatment, 221 received dry mopping with topical and systemic antibiotics and topical steroids, and 102 received no treatment. Participating schools were matched on factors thought to be related to their socioeconomic status. 29 children were withdrawn from the trial for taking non-trial antibiotics, with no evidence observed of differences in the timing of withdrawals between the two groups. At 16 weeks of follow-up, otorrhoea had resolved in a weighted mean proportion of 51% of children who received dry mopping with antibiotics, 22% of children who received dry mopping alone, and 22% of untreated children. Similar differences were observed at 8 and 12 weeks of follow-up. The weighted mean proportions of children with healing of the tympanic membranes by 16 weeks were 15% in the dry-mopping plus antibiotics group, 13% in the dry-mopping alone group, and 13% in the control group. Hearing thresholds were significantly better for children with no otorrhoea at 16 weeks than for those who had otorrhoea, and were also significantly better for those whose ears had healed than for those with otorrhoea at all times.DOI: 10.1016/S0140-6736(96)09388-9",pubmed,8888166,10.1016/S0140-6736(96)09388-9
deep learningbased noise reduction approach to improve speech intelligibility for cochlear implant recipients,"104. Ear Hear. 2018 Jul/Aug;39(4):795-809. doi: 10.1097/AUD.0000000000000537.Deep Learning-Based Noise Reduction Approach to Improve Speech Intelligibility for Cochlear Implant Recipients.Lai YH(1), Tsao Y(2), Lu X(3), Chen F(4), Su YT(5), Chen KC(6)(7), Chen YH(8), Chen LC(7), Po-Hung Li L(7)(9), Lee CH(10).Author information:(1)Department of Biomedical Engineering, National Yang-Ming University, Taipei, Taiwan.(2)Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan.(3)National Institute of Information and Communications Technology, Japan.(4)Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China.(5)Department of Mechatronic Engineering, National Taiwan Normal University, Taipei, Taiwan.(6)Department of Otolaryngology, Far Eastern Memorial Hospital, New Taipei, Taiwan.(7)Department of Otolaryngology, Cheng Hsin General Hospital, Taipei, Taiwan.(8)Department of Internal Medicine, Cheng Hsin General Hospital, Taipei, Taiwan.(9)Faculty of Medicine, School of Medicine, National Yang Ming University, Taipei, Taiwan.(10)School of Electrical and Computer Engineering, Georgia Institute of Technology, Georgia, USA.OBJECTIVE: We investigate the clinical effectiveness of a novel deep learning-based noise reduction (NR) approach under noisy conditions with challenging noise types at low signal to noise ratio (SNR) levels for Mandarin-speaking cochlear implant (CI) recipients.DESIGN: The deep learning-based NR approach used in this study consists of two modules: noise classifier (NC) and deep denoising autoencoder (DDAE), thus termed (NC + DDAE). In a series of comprehensive experiments, we conduct qualitative and quantitative analyses on the NC module and the overall NC + DDAE approach. Moreover, we evaluate the speech recognition performance of the NC + DDAE NR and classical single-microphone NR approaches for Mandarin-speaking CI recipients under different noisy conditions. The testing set contains Mandarin sentences corrupted by two types of maskers, two-talker babble noise, and a construction jackhammer noise, at 0 and 5 dB SNR levels. Two conventional NR techniques and the proposed deep learning-based approach are used to process the noisy utterances. We qualitatively compare the NR approaches by the amplitude envelope and spectrogram plots of the processed utterances. Quantitative objective measures include (1) normalized covariance measure to test the intelligibility of the utterances processed by each of the NR approaches; and (2) speech recognition tests conducted by nine Mandarin-speaking CI recipients. These nine CI recipients use their own clinical speech processors during testing.RESULTS: The experimental results of objective evaluation and listening test indicate that under challenging listening conditions, the proposed NC + DDAE NR approach yields higher intelligibility scores than the two compared classical NR techniques, under both matched and mismatched training-testing conditions.CONCLUSIONS: When compared to the two well-known conventional NR techniques under challenging listening condition, the proposed NC + DDAE NR approach has superior noise suppression capabilities and gives less distortion for the key speech envelope information, thus, improving speech recognition more effectively for Mandarin CI recipients. The results suggest that the proposed deep learning-based NR approach can potentially be integrated into existing CI signal processors to overcome the degradation of speech perception caused by noise.DOI: 10.1097/AUD.0000000000000537",pubmed,29360687,10.1097/AUD.0000000000000537
genomic analysis of the function of the transcription factor gata3 during development of the mammalian inner ear,"773. PLoS One. 2009 Sep 23;4(9):e7144. doi: 10.1371/journal.pone.0007144.Genomic analysis of the function of the transcription factor gata3 during development of the mammalian inner ear.Milo M(1), Cacciabue-Rivolta D, Kneebone A, Van Doorninck H, Johnson C, Lawoko-Kerali G, Niranjan M, Rivolta M, Holley M.Author information:(1)NIHR Cardiovascular Biomedical Research Unit, Sheffield Teaching Hospitals NHS Trust, Sheffield, United Kingdom.We have studied the function of the zinc finger transcription factor gata3 in auditory system development by analysing temporal profiles of gene expression during differentiation of conditionally immortal cell lines derived to model specific auditory cell types and developmental stages. We tested and applied a novel probabilistic method called the gamma Model for Oligonucleotide Signals to analyse hybridization signals from Affymetrix oligonucleotide arrays. Expression levels estimated by this method correlated closely (p<0.0001) across a 10-fold range with those measured by quantitative RT-PCR for a sample of 61 different genes. In an unbiased list of 26 genes whose temporal profiles clustered most closely with that of gata3 in all cell lines, 10 were linked to Insulin-like Growth Factor signalling, including the serine/threonine kinase Akt/PKB. Knock-down of gata3 in vitro was associated with a decrease in expression of genes linked to IGF-signalling, including IGF1, IGF2 and several IGF-binding proteins. It also led to a small decrease in protein levels of the serine-threonine kinase Akt2/PKBbeta, a dramatic increase in Akt1/PKBalpha protein and relocation of Akt1/PKBalpha from the nucleus to the cytoplasm. The cyclin-dependent kinase inhibitor p27(kip1), a known target of PKB/Akt, simultaneously decreased. In heterozygous gata3 null mice the expression of gata3 correlated with high levels of activated Akt/PKB. This functional relationship could explain the diverse function of gata3 during development, the hearing loss associated with gata3 heterozygous null mice and the broader symptoms of human patients with Hearing-Deafness-Renal anomaly syndrome.DOI: 10.1371/journal.pone.0007144PMCID: PMC2742898",pubmed,19774072,10.1371/journal.pone.0007144
largescale training to increase speech intelligibility for hearingimpaired listeners in novel noises,"240. J Acoust Soc Am. 2016 May;139(5):2604. doi: 10.1121/1.4948445.Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises.Chen J(1), Wang Y(1), Yoho SE(2), Wang D(1), Healy EW(2).Author information:(1)Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio 43210, USA.(2)Department of Speech and Hearing Science, The Ohio State University, Columbus, Ohio 43210, USA.Supervised speech segregation has been recently shown to improve human speech intelligibility in noise, when trained and tested on similar noises. However, a major challenge involves the ability to generalize to entirely novel noises. Such generalization would enable hearing aid and cochlear implant users to improve speech intelligibility in unknown noisy environments. This challenge is addressed in the current study through large-scale training. Specifically, a deep neural network (DNN) was trained on 10 000 noises to estimate the ideal ratio mask, and then employed to separate sentences from completely new noises (cafeteria and babble) at several signal-to-noise ratios (SNRs). Although the DNN was trained at the fixed SNR of - 2 dB, testing using hearing-impaired listeners demonstrated that speech intelligibility increased substantially following speech segregation using the novel noises and unmatched SNR conditions of 0 dB and 5 dB. Sentence intelligibility benefit was also observed for normal-hearing listeners in most noisy conditions. The results indicate that DNN-based supervised speech segregation with large-scale training is a very promising approach for generalization to new acoustic environments.DOI: 10.1121/1.4948445PMCID: PMC5392064",pubmed,27250154,10.1121/1.4948445
a cochlear stress hormone axis shapes development and function of the auditory peripherydp   2011,"Corticotropin Releasing Factor (CRF) is the stress hormone that activates the hypothalamic-pituitary-adrenal axis to maintain system-wide homeostasis in response to stress. This thesis describes a CRF signaling system contained in the cochlea and explores its role in auditory processing and protection. To investigate the role of CRF signaling in the cochlea, two transgenic mouse strains were used, each lacking one of the CRF receptors expressed in mammals, CRFR1 or CRFR2. Auditory Brainstem Response (ABR) thresholds and Distortion Product Otoacoustic Emission ii (DPOAE) isoresponse thresholds were measured to assess auditory function. CRFR2 null mice exhibit hypersensitive hearing when raised under quiet conditions, but increased susceptibility to noise-induced hearing loss (NIHL). Immunofluorescent labeling of CRFR2 localizes the receptor to the afferent spiral ganglion neurons and support cells lining the cochlear duct. Genetic ablation of CRFR2 elicits changes in both cell types. Using western blot analysis, alterations in glutamate receptor expression were observed in CRFR2 null mice, suggesting a potential regulatory role for CRFR2 in hair cell to spiral ganglion cell glutamatergic transmission. Also, CRFR2 null mice show deficient expression of connexin proteins important for support cell communication. Connexin deficiency in these mice coincides with altered levels of purinergic receptors and potassium channel proteins, suggesting a potential link between connexin deficiency, altered purinergic signaling, and disrupted potassium homeostasis. Such changes may contribute to the hyperacusis in CRFR2 null mice by increasing the endocochlear drive on mechanotransduction. Auditory physiology reveals a decreased sensitivity in CRFR1 null mice, opposite from the hypersensitivity observed in mice lacking CRFR2. Cochlear expression of CRFR1 was assessed using BAC transgenic mice expressing green fluorescent protein (GFP) under the CRFR1 promoter. CRFR1 expression is largely found in various support cells, with little to no expression in sensory cells. Nonetheless, CRFR1 is expressed in support cells directly juxtaposing the inner and outer hair cells, where it may exert indirect effects on hair cell function. The auditory physiology suggests an afferent basis for the hearing impairment in CRFR1 null mice and therefore, support cell-sensory cell interactions were investigated at the afferent synapse. CRFR1 null mice exhibit a significant reduction in glutamine synthetase (GS) levels suggesting impaired glutamate cycling between the presynaptic inner hair cell and its neighboring CRFR1-positive border cell. GS is a glucocorticoid inducible factor, and to determine if glucocorticoid deficiency underlies the reduced GS expression, mice were chronically treated with corticosterone in their drinking water. Corticosterone treatment restored GS levels in CRFR1 null mice but did not rescue the auditory deficit, indicating that reduced GS levels do not cause this deficit. Assessment of afferent synapse structure revealed normal overlay of presynaptic ribbons and postsynaptic glutamate receptor clusters; however, the distribution of these synapses within the inner hair cell was altered as evidenced by three-dimensional image analysis. Labeling of afferent fibers and terminals using antibodies against the a3 subunit of the sodium-potassium ATPase also revealed targeting defects, and together these results demonstrate an important developmental role for CRFR1 activity in the cochlea. We describe for the first time an HPA equivalent signaling system in the cochlea. Immunolabeling experiments demonstrate expression of pro-opiomelanocortin (POMC), adrenocorticotropic hormone (ACTH), and the ACTH receptor, melanocortin receptor 2 (MC2R) in the support cells and sensory cells of the cochlea. The full role of this HPA axis in auditory processing and protection awaits clarification. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc10&AN=2011-99080-055
genetic mapping of xlinked albinismdeafness syndrome adfn to xq263q27i,"534. Am J Hum Genet. 1990 Jul;47(1):20-7.Genetic mapping of X-linked albinism-deafness syndrome (ADFN) to Xq26.3-q27.I.Shiloh Y(1), Litvak G, Ziv Y, Lehner T, Sandkuyl L, Hildesheimer M, Buchris V, Cremers FP, Szabo P, White BN, et al.Author information:(1)Department of Human Genetics, Sackler School of Medicine, Tel-Aviv University, Israel.X-linked albinism-deafness syndrome (ADFN) was described in one Israeli Jewish family and is characterized by congenital nerve deafness and piebaldness. The ADFN mutation probably affects the migration of neural crest-derived precursors of the melanocytes. As a first step toward identifying the ADFN gene, a linkage study was performed to localize the disease locus on the X chromosome. The family was found to be informative for 11 of 107 RFLPs along the X, and two-point analysis showed four of them--factor 9 (F9), DXS91, DXS37, and DNF1--to have definite or suggestive linkage with ADFN. Multipoint linkage analysis indicated two possible orders within this cluster of loci, neither of which was preferable. In both orders F9 was the most distal, and the best estimate for the location of ADFN was between F9 and the next proximal marker (8.6 cM from F9 [Z = 8.1] or 8.3 cM from F9 [Z = 7.9]). These results suggest that the ADFN is at Xq26.3-q27.1. Disagreement between our data and previous localization of DXS91 at Xq11-q13 was resolved by hybridization of the probe pXG-17, which detects the DXS91 locus, to a panel of somatic cell hybrids containing different portions of the X chromosome. This experiment showed that this locus is definitely at Xq24-q26. Together with the linkage data, our results place DXS91 at Xq26 and underscore the importance of using more than one mapping method for the localization of molecular probes.PMCID: PMC1683749",pubmed,2349949,
reissners membrane in aging menires disease and profound sensorineural deafness,"609. ORL J Otorhinolaryngol Relat Spec. 1990;52(5):273-80. doi: 10.1159/000276150.Reissner's membrane in aging, Menière's disease, and profound sensorineural deafness.Quijano ML(1), Kimura RS, Fowler KB.Author information:(1)Harvard Medical School, Boston, Mass.The characteristics of Reissner's membrane from 47 human cochleae with mild endolymphatic hydrops, profound sensorineural deafness and normal ears were studied by light microscopy. The highest cell densities were observed in the zones adjacent to the limbus spiralis and the stria vascularis. The cell density of Reissner's in normal ears decreased with age concomitant with an increased formation of epithelial cell clusters. In hydrops, the density increased with the exception of the apical turn. However, sporadic loss of the cells in isolated areas of the membrane was observed. The ears with profound deafness showed no significant changes compared with age-related controls. No definite relationship between Reissner's cell density and hair cell loss or strial atrophy was observed.DOI: 10.1159/000276150",pubmed,2234897,10.1159/000276150
nonparametric inference of the area under roc curve under twophase cluster sampling,"Nonparametric inference of the area under ROC curve (AUC) has been well developed either in the presence of verification bias or clustering. However, current nonparametric methods are not able to handle cases where both verification bias and clustering are present. Such a case arises when a two-phase study design is applied to a cohort of subjects (verification bias) where each subject might have multiple test results (clustering). In such cases, the inference of AUC must account for both verification bias and intra-cluster correlation. In the present paper, we propose an IPW AUC estimator that corrects for verification bias and derive a variance formula to account for intra-cluster correlations between disease status and test results. Results of a simulation study indicate that the method that assumes independence underestimates the true variance of the IPW AUC estimator in the presence of intra-cluster correlations. The proposed method, on the other hand, provides a consistent variance estimate for the IPW AUC estimator by appropriately accounting for correlations between true disease statuses and between test results. © 2021 Taylor & Francis Group, LLC.",scopus,2-s2.0-85121697554,10.1080/10543406.2021.2009501
aberrant functional network of smallworld in sudden sensorineural hearing loss with tinnitus,"709. Front Neurosci. 2022 May 19;16:898902. doi: 10.3389/fnins.2022.898902. eCollection 2022.Aberrant Functional Network of Small-World in Sudden Sensorineural Hearing Loss With Tinnitus.Hua JC(1), Xu XM(2), Xu ZG(1), Xu JJ(3), Hu JH(3), Xue Y(1), Wu Y(3).Author information:(1)Department of Otolaryngology, Nanjing Pukou Central Hospital, Pukou Branch Hospital of Jiangsu Province Hospital, Nanjing, China.(2)Department of Radiology, Nanjing First Hospital, Nanjing Medical University, Nanjing, China.(3)Department of Otolaryngology, Nanjing First Hospital, Nanjing Medical University, Nanjing, China.Few researchers investigated the topological properties and relationships with cognitive deficits in sudden sensorineural hearing loss (SNHL) with tinnitus. To explore the topological characteristics of the brain connectome following SNHL from the global level and nodal level, we recruited 36 bilateral SNHL patients with tinnitus and 37 well-matched healthy controls. Every subject underwent pure tone audiometry tests, neuropsychological assessments, and MRI scanning. AAL atlas was employed to divide a brain into 90 cortical and subcortical regions of interest, then investigated the global and nodal properties of ""small world"" network in SNHL and control groups using a graph-theory analysis. The global characteristics include small worldness, cluster coefficient, characteristic path length, local efficiency, and global efficiency. Node properties include degree centrality, betweenness centrality, nodal efficiency, and nodal clustering coefficient. Interregional connectivity analysis was also computed among 90 nodes. We found that the SNHL group had significantly higher hearing thresholds and cognitive impairments, as well as disrupted internal connections among 90 nodes. SNHL group displayed lower AUC of cluster coefficient and path length lambda, but increased global efficiency. The opercular and triangular parts of the inferior frontal gyrus, rectus gyrus, parahippocampal gyrus, precuneus, and amygdala showed abnormal local features. Some of these connectome alterations were correlated with cognitive ability and the duration of SNHL. This study may prove potential imaging biomarkers and treatment targets for future studies.Copyright © 2022 Hua, Xu, Xu, Xu, Hu, Xue and Wu.DOI: 10.3389/fnins.2022.898902PMCID: PMC9160300",pubmed,35663555,10.3389/fnins.2022.898902
brain activation in patients with congenital bilateral hearing impairment,"399. Neuroreport. 2007 Sep 17;18(14):1483-6. doi: 10.1097/WNR.0b013e3282e9a73e.Brain activation in patients with congenital bilateral hearing impairment.Hwang JH(1), Wu CW, Lee CW, Chen JH, Liu TC.Author information:(1)College of Medicine, Graduate Institute of Clinical Medicine, Chiayi, Taiwan.Twelve patients with idiopathic, congenital, symmetric, moderate-to-severe sensorineural hearing loss participated in this study. Functional magnetic resonance imaging was performed while speech sounds were presented to each patient monaurally. Notable blood oxygenation level-dependent responses were clustered mainly in the superior temporal gyrus and transverse temporal gyrus of both hemispheres during right and left ear stimulation. In addition, the middle temporal gyrus of the right hemisphere was activated during right ear stimulation. The activation pattern was very similar to that of participants with normal hearing. Thus, as long as peripheral acoustic stimulation has not been totally absent from childhood, the classical activation pattern can be elicited in patients with congenital bilateral hearing impairment.DOI: 10.1097/WNR.0b013e3282e9a73e",pubmed,17712279,10.1097/WNR.0b013e3282e9a73e
the effect of crossover frequency on binaural hearing performance of adults using electricacoustic stimulation,"414. Cochlear Implants Int. 2019 Jul;20(4):190-206. doi: 10.1080/14670100.2019.1590499. Epub 2019 Mar 18.The effect of cross-over frequency on binaural hearing performance of adults using electric-acoustic stimulation.Incerti PV(1)(2)(3), Ching TY(1)(2), Cowan R(2)(3).Author information:(1)a National Acoustic Laboratories , Australian Hearing , Sydney , NSW 2109 , Australia.(2)b The Hearing CRC , Melbourne , Australia.(3)c Department of Audiology and Speech Pathology , The University of Melbourne , Melbourne , Australia.Objective: To investigate the effect of varying cross-over frequency (CF) settings for electric-acoustic (EA) stimulation in one ear combined with acoustic (A) hearing in the opposite ear on binaural speech perception, localization and functional performance in real life. Methods: Performance with three different CF settings set according to audiometric-based criterion were compared, following a four week familiarisation period with each, in ten adult cochlear implant recipients with residual hearing in both ears. On completion of all trials participants selected their preferred CF setting. Results: On average, CF settings did not have a significant effect on performance scores. However, higher ratings on device usage were associated with the preferred CF settings. Conclusion: Individuals who use EA + A stimulation may benefit from access to different CF settings to achieve maximal device usage.DOI: 10.1080/14670100.2019.1590499",pubmed,30880646,10.1080/14670100.2019.1590499
clustering approach based on psychometrics and auditory eventrelated potentials to evaluate acoustic therapy effects,"Background: Tinnitus is an auditory condition with no effective treatments. Seven of the 25 most widely used treatment are sound based therapies, but no methods to select an appropriate one have been established. Method: Therefore, this investigation aimed to establish a method to select an appropriate acoustic therapy by finding comparable psycho-neurological effects. For that purpose, 71 Mexican volunteers (60 tinnitus sufferers and 11 controls) followed one of five two-month treatments that aimed to (1) diminish the level of attention towards tinnitus via neuro-modulation, (2) habituate to tinnitus by retraining or (3) to relieve distress (binaural beats and music). From the treatment outcomes, six features were defined: (1) hearing loss, (2) anxiety level, (3) stress level, (4) overall amount of neural electrical response to acoustic therapy, (5) EEG channel wherefrom the maximum neural response was obtained, and (6) assigned group. Then, a cluster analysis based on the k-means method was undertaken. Results: As a result, a strong structure (silhouette measure = 0.798) of six clusters showed that tinnitus sufferers who reported diminution of stress and anxiety, and no side effects, mainly proceeded from neuro-modulation treatments. Furthermore, most of tinnitus sufferers who reported increase of anxiety mainly proceeded from tinnitus retraining and binaural beats. Finally, tinnitus sufferers who only reported anxiety diminution mainly proceeded from tinnitus retraining and music therapy. Conclusion: Taken together, these findings are a guideline to select an appropriate therapy according to clinical history and psycho-neurological effects, what has not been proposed until now. © 2022 Elsevier Ltd",scopus,2-s2.0-85128752875,10.1016/j.bspc.2022.103719
spatiotemporal integration of speech reflections in hearingimpaired listeners,"15. Trends Hear. 2022 Jan-Dec;26:23312165221143901. doi: 10.1177/23312165221143901.Spatio-temporal Integration of Speech Reflections in Hearing-Impaired Listeners.Rennies J(1)(2), Warzybok A(3)(2), Kollmeier B(1)(3)(2), Brand T(3)(2).Author information:(1)28439Fraunhofer Institute for Digital Media Technology IDMT, Project Group Hearing, Speech and Audio Technology, Oldenburg, Germany.(2)Cluster of Excellence Hearing4all, Oldenburg, Germany.(3)Medical Physics Group, Department für Medizinische Physik und Akustik, Oldenburg, Germany.Speech recognition in rooms requires the temporal integration of reflections which arrive with a certain delay after the direct sound. It is commonly assumed that there is a certain temporal window of about 50-100 ms, during which reflections can be integrated with the direct sound, while later reflections are detrimental to speech intelligibility. This concept was challenged in a recent study by employing binaural room impulse responses (RIRs) with systematically varied interaural phase differences (IPDs) and amplitude of the direct sound and a variable number of reflections delayed by up to 200 ms. When amplitude or IPD favored late RIR components, normal-hearing (NH) listeners appeared to be capable of focusing on these components rather than on the precedent direct sound, which contrasted with the common concept of considering early RIR components as useful and late components as detrimental. The present study investigated speech intelligibility in the same conditions in hearing-impaired (HI) listeners. The data indicate that HI listeners were generally less able to ""ignore"" the direct sound than NH listeners, when the most useful information was confined to late RIR components. Some HI listeners showed a remarkable inability to integrate across multiple reflections and to optimally ""shift"" their temporal integration window, which was quite dissimilar to NH listeners. This effect was most pronounced in conditions requiring spatial and temporal integration and could provide new challenges for individual prediction models of binaural speech intelligibility.DOI: 10.1177/23312165221143901PMCID: PMC9772954",pubmed,36537084,10.1177/23312165221143901
enhanced audiovisual interactions in the auditory cortex of elderly cochlearimplant users,"560. Hear Res. 2015 Oct;328:133-47. doi: 10.1016/j.heares.2015.08.009. Epub 2015 Aug 21.Enhanced audio-visual interactions in the auditory cortex of elderly cochlear-implant users.Schierholz I(1), Finke M(2), Schulte S(3), Hauthal N(4), Kantzke C(5), Rach S(6), Büchner A(2), Dengler R(5), Sandmann P(5).Author information:(1)Cluster of Excellence ""Hearing4all"", Hannover, Germany; Department of Neurology, Hannover Medical School, Hannover, Germany. Electronic address: Schierholz.Irina@mh-hannover.de.(2)Cluster of Excellence ""Hearing4all"", Hannover, Germany; Department of Otolaryngology, Hannover Medical School, Hannover, Germany.(3)Department of Neurology, Hannover Medical School, Hannover, Germany.(4)Cluster of Excellence ""Hearing4all"", Hannover, Germany; Department of Psychology, European Medical School, University of Oldenburg, Oldenburg, Germany.(5)Cluster of Excellence ""Hearing4all"", Hannover, Germany; Department of Neurology, Hannover Medical School, Hannover, Germany.(6)Department of Epidemiological Methods and Etiological Research, Leibniz Institute of Prevention Research and Epidemiology - BIPS, Bremen, Germany.Auditory deprivation and the restoration of hearing via a cochlear implant (CI) can induce functional plasticity in auditory cortical areas. How these plastic changes affect the ability to integrate combined auditory (A) and visual (V) information is not yet well understood. In the present study, we used electroencephalography (EEG) to examine whether age, temporary deafness and altered sensory experience with a CI can affect audio-visual (AV) interactions in post-lingually deafened CI users. Young and elderly CI users and age-matched NH listeners performed a speeded response task on basic auditory, visual and audio-visual stimuli. Regarding the behavioral results, a redundant signals effect, that is, faster response times to cross-modal (AV) than to both of the two modality-specific stimuli (A, V), was revealed for all groups of participants. Moreover, in all four groups, we found evidence for audio-visual integration. Regarding event-related responses (ERPs), we observed a more pronounced visual modulation of the cortical auditory response at N1 latency (approximately 100 ms after stimulus onset) in the elderly CI users when compared with young CI users and elderly NH listeners. Thus, elderly CI users showed enhanced audio-visual binding which may be a consequence of compensatory strategies developed due to temporary deafness and/or degraded sensory input after implantation. These results indicate that the combination of aging, sensory deprivation and CI facilitates the coupling between the auditory and the visual modality. We suggest that this enhancement in multisensory interactions could be used to optimize auditory rehabilitation, especially in elderly CI users, by the application of strong audio-visually based rehabilitation strategies after implant switch-on.Copyright © 2015 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2015.08.009",pubmed,26302946,10.1016/j.heares.2015.08.009
anophthalmia hearing loss abnormal pituitary development and response to growth hormone therapy in three children with microdeletions of 14q22q23,"Background: Microdeletions of 14q22q23 have been associated with eye abnormalities and pituitary defects. Other phenotypic features in deletion carriers including hearing loss and response to growth hormone therapy are less well recognized. We studied genotype and phenotype of three newly identified children with 14q22q23 deletions, two girls and one boy with bilateral anophthalmia, and compared them with previously published deletion patients and individuals with intragenic defects in genes residing in the region. Results: The three deletions were de novo and ranged in size between 5.8 and 8.9 Mb. All three children lacked one copy of the OTX2 gene and in one of them the deletion involved also the BMP4 gene. All three patients presented partial conductive hearing loss which tended to improve with age. Analysis of endocrine and growth phenotypes showed undetectable anterior pituitary, growth hormone deficiency and progressive growth retardation in all three patients. Growth hormone therapy led to partial catch-up growth in two of the three patients but just prevented further height loss in the third. Conclusions: The pituitary hypoplasia, growth hormone deficiency and growth retardation associated with 14q22q23 microdeletions are very remarkable, and the latter appears to have an atypical response to growth hormone therapy in some of the cases. © 2014 Brisset et al.; licensee BioMed Central Ltd.",scopus,2-s2.0-84898540824,10.1186/1755-8166-7-17
sensitivity to angular and radial source movements as a function of acoustic complexity in normal and impaired hearing,"618. Trends Hear. 2017 Jan-Dec;21:2331216517717152. doi: 10.1177/2331216517717152.Sensitivity to Angular and Radial Source Movements as a Function of Acoustic Complexity in Normal and Impaired Hearing.Lundbeck M(1)(2), Grimm G(1)(2), Hohmann V(1)(2), Laugesen S(3), Neher T(1).Author information:(1)1 Medizinische Physik and Cluster of Excellence 'Hearing4all,' Department of Medical Physics and Acoustics, Oldenburg University, Germany.(2)2 HörTech gGmbH, Oldenburg, Germany.(3)3 Eriksholm Research Centre, Snekkersten, Denmark.In contrast to static sounds, spatially dynamic sounds have received little attention in psychoacoustic research so far. This holds true especially for acoustically complex (reverberant, multisource) conditions and impaired hearing. The current study therefore investigated the influence of reverberation and the number of concurrent sound sources on source movement detection in young normal-hearing (YNH) and elderly hearing-impaired (EHI) listeners. A listening environment based on natural environmental sounds was simulated using virtual acoustics and rendered over headphones. Both near-far ('radial') and left-right ('angular') movements of a frontal target source were considered. The acoustic complexity was varied by adding static lateral distractor sound sources as well as reverberation. Acoustic analyses confirmed the expected changes in stimulus features that are thought to underlie radial and angular source movements under anechoic conditions and suggested a special role of monaural spectral changes under reverberant conditions. Analyses of the detection thresholds showed that, with the exception of the single-source scenarios, the EHI group was less sensitive to source movements than the YNH group, despite adequate stimulus audibility. Adding static sound sources clearly impaired the detectability of angular source movements for the EHI (but not the YNH) group. Reverberation, on the other hand, clearly impaired radial source movement detection for the EHI (but not the YNH) listeners. These results illustrate the feasibility of studying factors related to auditory movement perception with the help of the developed test setup.DOI: 10.1177/2331216517717152PMCID: PMC5548306",pubmed,28675088,10.1177/2331216517717152
development and standardization of auditory lowfrequency word lists in hindi,"Background: Conventionally, it is believed that high-frequency auditory information is important for speech understanding. This is only partly true, as recent studies have demonstrated the importance of low-frequency information. This research was taken up to develop, standardize, and validate auditory low-frequency word lists in Hindi, an Indian language. Material and methods: The first phase of the study involved collection of bisyllabic words followed by verification by a native linguist. Words were then short-listed based on familiarity ratings given by 10 adult native speakers; those words were recorded and the best recorded words selected through subjective and objective analysis. Then, using Fast Fourier Transform and k-means clustering, words with more energy below 1.5 kHz were isolated. Finally, equally difficult 10 word lists were generated by obtaining psychometric function curves. Finally, lists were administered on 40 adult normal hearing participants at 0, 10, 20, and 30 dB sensation level (SL). The word list was also standardized on 100 Hindi speakers at 40 dB SL. Results: Results showed a similar trend of increase in speech identification scores with increase in SL across all lists except list 4. During the final phase, developed lists were validated on 10 simulated low-frequency cochlear hearing loss participants. Hearing loss was simulated using Matlab and National Institute for Occupational Safety and Health (NIOSH) software. Results of validation revealed that auditory low-frequency word lists were sensitive enough to tap the speech understanding difficulty in the simulated condition. Conclusions: The developed word lists can be used clinically to assess communication ability in individuals with rising hearing loss. The word lists also have the potential to assess the performance after amplification provided to individuals with rising hearing loss.",cinahl,2083389X,10.17430/899781
monitoring of the inner ear function during and after cochlear implant insertion using electrocochleography,"236. Trends Hear. 2019 Jan-Dec;23:2331216519833567. doi: 10.1177/2331216519833567.Monitoring of the Inner Ear Function During and After Cochlear Implant Insertion Using Electrocochleography.Haumann S(1)(2), Imsiecke M(1), Bauernfeind G(1)(2), Büchner A(1)(2), Helmstaedter V(1)(2), Lenarz T(1)(2), Salcher RB(1)(2).Author information:(1)1 Department of Otolaryngology, Hannover Medical School, Germany.(2)2 Cluster of Excellence Hearing4All, Hannover, Germany.To preserve residual hearing during cochlear implant (CI) surgery, it is desirable to use intraoperative monitoring of inner ear function (cochlear monitoring), especially during electrode insertion. A promising method is electrocochleography (ECochG). Within this project, the relations between ongoing responses (ORs), recorded extra- and intracochlearly (EC and IC), and preservation of residual hearing were investigated. Before, during, and after insertion of hearing preservation electrodes, intraoperative ECochG recordings were performed EC using a cotton wick electrode and after insertion also IC using the CI electrode (MED-EL) and a research software tool. The stimulation was delivered acoustically using low frequency tone bursts. The recordings were conducted in 10 adult CI recipients. The amplitudes of IC ORs were detected to be larger than EC ORs. Intraoperative EC thresholds correlated highly to preoperative audiometric thresholds at 1000 Hz, IC thresholds highly at 250 Hz and 500 Hz. The correlations of both intraoperative ECochG recordings to postoperative pure tone thresholds were low. When measured postoperatively at the same appointments, IC OR thresholds correlated highly to audiometric pure tone thresholds. For all patients, it was possible to record ORs during or directly after electrode insertion. Consequently, we conclude that we did not observe any cases with severe IC trauma. Delayed hearing loss could not be predicted with our method. Nevertheless, intraoperative ECochG recordings are a promising tool to gain further insight into mechanisms impacting residual hearing. Postoperatively recorded IC OR thresholds seem to be a reliable tool for frequency specific hearing threshold estimation.DOI: 10.1177/2331216519833567PMCID: PMC6435875",pubmed,30909815,10.1177/2331216519833567
typical consonant cluster acquisition in auditoryverbal children with earlyidentified severeprofound hearing loss,"268. Int J Speech Lang Pathol. 2014 Feb;16(1):69-81. doi: 10.3109/17549507.2013.808698. Epub 2013 Sep 3.Typical consonant cluster acquisition in auditory-verbal children with early-identified severe/profound hearing loss.Fulcher A(1), Baker E, Purcell A, Munro N.Author information:(1)The University of Sydney , Sydney , Australia.Early-identified severe/profound hearing loss (HL) following universal newborn hearing screening (UNHS) has been associated with improved speech and language outcomes. However, speech outcome reports have typically been based on broad measures of speech intelligibility and/or singleton consonant accuracy, with little known about production of consonant clusters. Using a prospective design, the range and accuracy of consonant clusters produced by a homogenous cohort of 12 children early-identified with severe/profound HL aged 3- and 4-years were examined. All children demonstrated bilateral aided thresholds within a range of 15-25 dB HL across all frequencies, were optimally amplified with cochlear implants (11/12) or hearing aids (1/12), and attended auditory-verbal (AV) early intervention. Standardized speech and language assessments were administered. Consonant clusters were strategically sampled in single-word and conversational speech contexts. All standard scores for speech, receptive, and expressive language were within normal limits. All children produced consonant clusters commensurate with expectations for typically-developing hearing peers at 3- and 4- years-of-age. Children's production of phonetically complex morphophonemes (final consonant clusters marking grammatical morphemes) was also in keeping with developmental expectations. Factors which contributed to these encouraging outcomes require further investigation.DOI: 10.3109/17549507.2013.808698",pubmed,24001172,10.3109/17549507.2013.808698
kllikers organsupporting cells and cochlear auditory development,"The Kölliker’s organ is a transient cellular cluster structure in the development of the mammalian cochlea. It gradually degenerates from embryonic columnar cells to cuboidal cells in the internal sulcus at postnatal day 12 (P12)–P14, with the cochlea maturing when the degeneration of supporting cells in the Kölliker’s organ is complete, which is distinct from humans because it disappears at birth already. The supporting cells in the Kölliker’s organ play a key role during this critical period of auditory development. Spontaneous release of ATP induces an increase in intracellular Ca2+ levels in inner hair cells in a paracrine form via intercellular gap junction protein hemichannels. The Ca2+ further induces the release of the neurotransmitter glutamate from the synaptic vesicles of the inner hair cells, which subsequently excite afferent nerve fibers. In this way, the supporting cells in the Kölliker’s organ transmit temporal and spatial information relevant to cochlear development to the hair cells, promoting fine-tuned connections at the synapses in the auditory pathway, thus facilitating cochlear maturation and auditory acquisition. The Kölliker’s organ plays a crucial role in such a scenario. In this article, we review the morphological changes, biological functions, degeneration, possible trans-differentiation of cochlear hair cells, and potential molecular mechanisms of supporting cells in the Kölliker’s organ during the auditory development in mammals, as well as future research perspectives. Copyright © 2022 Chen, Gao, Sun and Yang.",scopus,2-s2.0-85140359894,10.3389/fnmol.2022.1031989
altered topological properties of the intrinsic functional brain network in patients with rightsided unilateral hearing loss caused by acoustic neuroma,"Neuroimaging studies have identified alterations in functional connectivity between specific brain regions in patients with unilateral hearing loss (UHL) and different influence of the side of UHL on neural plasticity. However, little is known about changes of whole-brain functional networks in patients with UHL and whether differences exist in topological organization between right-sided UHL (RUHL) and left-sided UHL (LUHL). To address this issue, we employed resting-state fMRI (rs-fMRI) and graph-theoretical approaches to investigate the topological alterations of brain functional connectomes in patients with RUHL and LUHL. Data from 44 patients with UHL (including 22 RUHL patients and 22 LUHL patients) and 37 healthy control subjects (HCs) were collected. Functional brain networks were constructed for each participant, following by graph-theoretical network analyses at connectional and global (e.g., small-worldness) levels. The correlations between brain network topologies and clinical variables were further studied. Using network-based analysis, we found a subnetwork in the visual cortex which had significantly lower connectivity strength in patients with RUHL as compared to HCs. At global level, all participants showed small-world architecture in functional brain networks, however, significantly lower normalized clustering coefficient and small-worldness were observed in patients with RUHL than in HCs. Moreover, these abnormal network metrics were demonstrated to be correlated with the clinical variables and cognitive performance of patients with RUHL. Notably, no significant alterations in the functional brain networks were found in patients with LUHL. Our findings demonstrate that RUHL (rather than LUHL) is accompanied with aberrant topological organization of the functional brain connectome, indicating different pathophysiological mechanisms between RUHL and LUHL from a viewpoint of network topology. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",scopus,2-s2.0-85127681145,10.1007/s11682-022-00658-1
spike generators and cell signaling in the human auditory nerve an ultrastructural superresolution and gene hybridization study,"Background: The human auditory nerve contains 30,000 nerve fibers (NFs) that relay complex speech information to the brain with spectacular acuity. How speech is coded and influenced by various conditions is not known. It is also uncertain whether human nerve signaling involves exclusive proteins and gene manifestations compared with that of other species. Such information is difficult to determine due to the vulnerable, “esoteric,” and encapsulated human ear surrounded by the hardest bone in the body. We collected human inner ear material for nanoscale visualization combining transmission electron microscopy (TEM), super-resolution structured illumination microscopy (SR-SIM), and RNA-scope analysis for the first time. Our aim was to gain information about the molecular instruments in human auditory nerve processing and deviations, and ways to perform electric modeling of prosthetic devices. Material and Methods: Human tissue was collected during trans-cochlear procedures to remove petro-clival meningioma after ethical permission. Cochlear neurons were processed for electron microscopy, confocal microscopy (CM), SR-SIM, and high-sensitive in situ hybridization for labeling single mRNA transcripts to detect ion channel and transporter proteins associated with nerve signal initiation and conductance. Results: Transport proteins and RNA transcripts were localized at the subcellular level. Hemi-nodal proteins were identified beneath the inner hair cells (IHCs). Voltage-gated ion channels (VGICs) were expressed in the spiral ganglion (SG) and axonal initial segments (AISs). Nodes of Ranvier (NR) expressed Nav1.6 proteins, and encoding genes critical for inter-cellular coupling were disclosed. Discussion: Our results suggest that initial spike generators are located beneath the IHCs in humans. The first NRs appear at different places. Additional spike generators and transcellular communication may boost, sharpen, and synchronize afferent signals by cell clusters at different frequency bands. These instruments may be essential for the filtering of complex sounds and may be challenged by various pathological conditions. © Copyright © 2021 Liu, Luque, Li, Schrott-Fischer, Glueckert, Tylstedt, Rajan, Ladak, Agrawal and Rask-Andersen.",scopus,2-s2.0-85103408379,10.3389/fncel.2021.642211
reflections on how tinnitus impacts the lives of children and adolescents,"Objectives: The aim of this study was to generate a conceptual framework describing which aspects of children and adolescents’ lives are affected by chronic tinnitus. Design: Views and experiences of 32 participants from two participant groups informed this study: (a) a tinnitus group, consisting of adults who had experienced tinnitus during childhood and/or adolescence and primary carers of children/ adolescents with tinnitus, and (b) a clinicians’ group, consisting of clinicians who provided care for children/adolescents with tinnitus. Participants produced statements describing aspects of children/adolescents’ lives that may be affected by chronic tinnitus. Key concepts were identified through the processes of sorting the statements and rating them for degree of associated impact. Result: Participants identified 118 unique aspects of the lives of children/adolescents who may be affected by chronic tinnitus. These were clustered into four concepts: (a) emotional well-being, (b) academic performances, (c) social/relations, and (d) auditory/cognitive processing. At a group level, participants rated the impact of tinnitus as above a slight degree but below a moderate degree of impact. However, individual participant’s ratings indicated a range of perceived impact for each statement. Conclusions: The experience of chronic tinnitus during childhood and adolescence extends beyond the mere perception of sound. The perception of tinnitus may impact a child’s emotional well-being, academic performances, social/relational, and auditory/cognitive processing. The impact of tinnitus in one aspect of a child’s life may influence other aspects of their life. While at a group level, participants regarded the impact of tinnitus as “somewhat more than mild” to “less than moderate”; individual participant’s ratings indicate that the impact from chronic tinnitus may be highly individual and highlighted the importance of individual assessment and management. Clinically, tinnitus management during childhood and adolescence may be improved if clinicians consider the impact and manifestation of tinnitus within each child’s daily life and tailor tinnitus education and management strategies accordingly.",cinahl,10590889,10.1044/2021_AJA-20-00178
multiresolution analysis of eventrelated potentials by wavelet decomposition,"156. Brain Cogn. 1995 Apr;27(3):398-438. doi: 10.1006/brcg.1995.1028.Multiresolution analysis of event-related potentials by wavelet decomposition.Samar VJ(1), Swartz KP, Raghuveer MR.Author information:(1)Communication Research Department, National Technical Institute for the Deaf at Rochester Institute of Technology, NY 14623-0887, USA.Wavelet analysis is presented as a new tool for analyzing event-related potentials (ERPs). The wavelet transform expands ERPs into a time-scale representation, which allows the analyst to zoom in on the small scale, fine structure details of an ERP or zoom out to examine the large scale, global waveshape. The time-scale representation is closely related to the more familiar time-frequency representation used in spectrograms of time-varying signals. However, time-scale representations have special properties that make them attractive for many ERP applications. In particular, time-scale representations permit theoretically unlimited time resolution for the detection of short-lived peaks and permit a flexible choice of wavelet basis functions for analyzing different types of ERPs. Generally, time-scale representations offer a formal basis for designing new, specialized filters for various ERP applications. Among recently explored applications of wavelet analysis to ERPs are (a) the precise identification of the time of occurrence of overlapping peaks in the auditory brainstem evoked response; (b) the extraction of single-trial ERPs from background EEG noise; (c) the decomposition of averaged ERP waveforms into orthogonal detail functions that isolate the waveform's experimental behavior in distinct, orthogonal frequency bands; and (d) the use of wavelet transform coefficients to concisely extract important information from ERPs that predicts human signal detection performance. In this tutorial we present an intuitive introduction to wavelets and the wavelet transform, concentrating on the multiresolution approach to wavelet analysis of ERP data. We then illustrate this approach with real data. Finally, we offer some speculations on future applications of wavelet analysis to ERP data.DOI: 10.1006/brcg.1995.1028",pubmed,7626282,10.1006/brcg.1995.1028
a deficit in movementderived sentences in germanspeaking hearingimpaired children,"758. Front Psychol. 2017 Jun 13;8:689. doi: 10.3389/fpsyg.2017.00689. eCollection 2017.A Deficit in Movement-Derived Sentences in German-Speaking Hearing-Impaired Children.Ruigendijk E(1), Friedmann N(2).Author information:(1)Department of Dutch and Cluster of Excellence ""Hearing for All"", University of OldenburgOldenburg, Germany.(2)Language and Brain Lab, Tel Aviv UniversityTel Aviv, Israel.Children with hearing impairment (HI) show disorders in syntax and morphology. The question is whether and how these disorders are connected to problems in the auditory domain. The aim of this paper is to examine whether moderate to severe hearing loss at a young age affects the ability of German-speaking orally trained children to understand and produce sentences. We focused on sentence structures that are derived by syntactic movement, which have been identified as a sensitive marker for syntactic impairment in other languages and in other populations with syntactic impairment. Therefore, our study tested subject and object relatives, subject and object Wh-questions, passive sentences, and topicalized sentences, as well as sentences with verb movement to second sentential position. We tested 19 HI children aged 9;5-13;6 and compared their performance with hearing children using comprehension tasks of sentence-picture matching and sentence repetition tasks. For the comprehension tasks, we included HI children who passed an auditory discrimination task; for the sentence repetition tasks, we selected children who passed a screening task of simple sentence repetition without lip-reading; this made sure that they could perceive the words in the tests, so that we could test their grammatical abilities. The results clearly showed that most of the participants with HI had considerable difficulties in the comprehension and repetition of sentences with syntactic movement: they had significant difficulties understanding object relatives, Wh-questions, and topicalized sentences, and in the repetition of object who and which questions and subject relatives, as well as in sentences with verb movement to second sentential position. Repetition of passives was only problematic for some children. Object relatives were still difficult at this age for both HI and hearing children. An additional important outcome of the study is that not all sentence structures are impaired-passive structures were not problematic for most of the HI children.DOI: 10.3389/fpsyg.2017.00689PMCID: PMC5468451",pubmed,28659836,10.3389/fpsyg.2017.00689
fmri as a preimplant objective tool to predict postimplant oral language outcomes in children with cochlear implants,"470. Ear Hear. 2016 Jul-Aug;37(4):e263-72. doi: 10.1097/AUD.0000000000000259.fMRI as a Preimplant Objective Tool to Predict Postimplant Oral Language Outcomes in Children with Cochlear Implants.Deshpande AK(1), Tan L, Lu LJ, Altaye M, Holland SK.Author information:(1)1Department of Speech-Language-Hearing Sciences, Hofstra University, Hempstead, New York, USA; 2Division of Biomedical Informatics, Cincinnati Children's Hospital Research Foundation, Cincinnati, Ohio, USA; 3School of Computing Sciences and Informatics, 4Department of Environmental Health, College of Medicine, University of Cincinnati, Cincinnati, Ohio, USA; 5Division of Biostatistics and Epidemiology, 6Pediatric Neuroimaging Research Consortium, and 7Department of Pediatric Radiology, Cincinnati Children's Hospital Medical Center, Cincinnati, Ohio, USA.OBJECTIVES: Despite the positive effects of cochlear implantation, postimplant variability in speech perception and oral language outcomes is still difficult to predict. The aim of this study was to identify neuroimaging biomarkers of postimplant speech perception and oral language performance in children with hearing loss who receive a cochlear implant. The authors hypothesized positive correlations between blood oxygen level-dependent functional magnetic resonance imaging (fMRI) activation in brain regions related to auditory language processing and attention and scores on the Clinical Evaluation of Language Fundamentals-Preschool, Second Edition (CELF-P2) and the Early Speech Perception Test for Profoundly Hearing-Impaired Children (ESP), in children with congenital hearing loss.DESIGN: Eleven children with congenital hearing loss were recruited for the present study based on referral for clinical MRI and other inclusion criteria. All participants were <24 months at fMRI scanning and <36 months at first implantation. A silent background fMRI acquisition method was performed to acquire fMRI during auditory stimulation. A voxel-based analysis technique was utilized to generate z maps showing significant contrast in brain activation between auditory stimulation conditions (spoken narratives and narrow band noise). CELF-P2 and ESP were administered 2 years after implantation. Because most participants reached a ceiling on ESP, a voxel-wise regression analysis was performed between preimplant fMRI activation and postimplant CELF-P2 scores alone. Age at implantation and preimplant hearing thresholds were controlled in this regression analysis.RESULTS: Four brain regions were found to be significantly correlated with CELF-P2 scores. These clusters of positive correlation encompassed the temporo-parieto-occipital junction, areas in the prefrontal cortex and the cingulate gyrus. For the story versus silence contrast, CELF-P2 core language score demonstrated significant positive correlation with activation in the right angular gyrus (r = 0.95), left medial frontal gyrus (r = 0.94), and left cingulate gyrus (r = 0.96). For the narrow band noise versus silence contrast, the CELF-P2 core language score exhibited significant positive correlation with activation in the left angular gyrus (r = 0.89; for all clusters, corrected p < 0.05).CONCLUSIONS: Four brain regions related to language function and attention were identified that correlated with CELF-P2. Children with better oral language performance postimplant displayed greater activation in these regions preimplant. The results suggest that despite auditory deprivation, these regions are more receptive to gains in oral language development performance of children with hearing loss who receive early intervention via cochlear implantation. The present study suggests that oral language outcome following cochlear implant may be predicted by preimplant fMRI with auditory stimulation using natural speech.DOI: 10.1097/AUD.0000000000000259",pubmed,26689275,10.1097/AUD.0000000000000259
an individualised acoustically transparent earpiece for hearing devices,"285. Int J Audiol. 2018 Jun;57(sup3):S62-S70. doi: 10.1080/14992027.2017.1294768. Epub 2017 Mar 1.An individualised acoustically transparent earpiece for hearing devices.Denk F(1), Hiipakka M(1), Kollmeier B(1), Ernst SMA(1).Author information:(1)a Medizinische Physik and Cluster of Excellence Hearing4all , University of Oldenburg , Oldenburg , Germany.OBJECTIVE: An important and often still unresolved problem of hearing devices such as assistive listening devices and hearing aids is limited user acceptance - a primary reason is poor conservation quality of the acoustic environment. Approaching a possible solution to this problem, an earpiece prototype is presented and evaluated. The prototype is individually and automatically calibrated in situ to provide acoustical transparency, i.e., achieving an audio perception alike to the open ear.DESIGN: A comprehensive evaluation was performed, comprising technical measurements on an advanced dummy head and listening tests, in which listeners directly compared sound perception through the prototype and a simulated open ear canal reference.STUDY SAMPLE: Ten normal hearing subjects, including five expert listeners, participated in the listening test.RESULTS: The technical evaluation verified good achievement of acoustical transparency. The psychoacoustic results showed that a reliable distinction between the two conditions presented was not possible for relevant communication sounds.CONCLUSION: The prototype can be described as an initial realisation of an acoustically transparent hearing system, i.e. a device that does not disturb the perception of external sounds. In further developments, the device can be considered as the basis for systems integrating high sound quality, hearing support and other desired modifications.DOI: 10.1080/14992027.2017.1294768",pubmed,28635506,10.1080/14992027.2017.1294768
considerations for fitting cochlear implants bimodally and to the singlesided deaf,"137. Trends Hear. 2022 Jan-Dec;26:23312165221108259. doi: 10.1177/23312165221108259.Considerations for Fitting Cochlear Implants Bimodally and to the Single-Sided Deaf.Pieper SH(1)(2), Hamze N(3), Brill S(4), Hochmuth S(5), Exter M(2)(6), Polak M(3), Radeloff A(2)(5)(7), Buschermöhle M(8), Dietz M(1)(2)(7).Author information:(1)Department of Medical Physics and Acoustic, University of Oldenburg, Oldenburg, Germany.(2)Cluster of Excellence Hearing4all, University of Oldenburg, Oldenburg, Germany.(3)MED-EL Medical Electronics GmbH, Innsbruck, Austria.(4)MED-EL Medical Electronics Germany GmbH, Starnberg, Germany.(5)Division of Otorhinolaryngology, University of Oldenburg, Oldenburg, Germany.(6)Hörzentrum Oldenburg gGmbH, Oldenburg, Germany.(7)Research Center Neurosensory Science, University of Oldenburg, Oldenburg, Germany.(8)KIZMO GmbH, Oldenburg, Germany.When listening with a cochlear implant through one ear and acoustically through the other, binaural benefits and spatial hearing abilities are generally poorer than in other bilaterally stimulated configurations. With the working hypothesis that binaural neurons require interaurally matched inputs, we review causes for mismatch, their perceptual consequences, and experimental methods for mismatch measurements. The focus is on the three primary interaural dimensions of latency, frequency, and level. Often, the mismatch is not constant, but rather highly stimulus-dependent. We report on mismatch compensation strategies, taking into consideration the specific needs of the respective patient groups. Practical challenges typically faced by audiologists in the proposed fitting procedure are discussed. While improvement in certain areas (e.g., speaker localization) is definitely achievable, a more comprehensive mismatch compensation is a very ambitious endeavor. Even in the hypothetical ideal fitting case, performance is not expected to exceed that of a good bilateral cochlear implant user.DOI: 10.1177/23312165221108259PMCID: PMC9218456",pubmed,35726211,10.1177/23312165221108259
assessing mindfulnessbased cognitive therapy intervention for tinnitus using behavioural measures and structural mri a pilot study,"Objective: We used a minimally-modified version of Mindfulness-Based Cognitive Therapy (MBCT) to treat symptoms of distress associated with tinnitus. Design: Audiological screening (establishing a baseline) was conducted prior to treatment and at three time-points: pre-intervention, post-intervention and follow-up, 8 weeks after completion of training. MRI tests were also conducted at these three time-points. Study sample: Twenty-one participants were enrolled in the study, of whom 15 completed training and audiological testing and eight completed the MRI portion of the study. Results: Scores on tinnitus-related questionnaires showed a significant decline either from pre- to post-intervention or from pre-intervention to follow-up, despite no significant change during baseline. Voxel-based morphometric analysis of the structural MRI scans revealed clusters in bilateral superior frontal gyrus that exhibited significant increases in grey matter volume over the period of intervention and follow-up. Further, grey matter changes in occipital and cingulate regions correlated with declines in tinnitus handicap. Conclusions: This pilot study supports MBCT as an adequate approach for treating distressing tinnitus and suggests that neuroanatomical changes may reflect reductions in tinnitus-related severity. Although our small sample size precludes drawing strong conclusions, there is potential for assessing neuroanatomical changes due to mindfulness-based interventions in tinnitus.",cinahl,14992027,10.1080/14992027.2019.1629655
mjayaelm a jaya algorithm with mutation and extreme learning machine based approach for sensorineural hearing loss detection,,base,48430a362f84ee43210dd849e7605fcc08c2e4926c5b14526ed972d15c5bf622,
math1 regulates development of the sensory epithelium in the mammalian cochlea,"829. Nat Neurosci. 2004 Dec;7(12):1310-8. doi: 10.1038/nn1349. Epub 2004 Nov 7.Math1 regulates development of the sensory epithelium in the mammalian cochlea.Woods C(1), Montcouquiol M, Kelley MW.Author information:(1)Section on Developmental Neuroscience, National Institute on Deafness and other Communication Disorders, National Institutes of Health, Porter Neuroscience Research Center, Building 35, Bethesda, Maryland 20892, USA.The transcription factor Math1 (encoded by the gene Atoh1, also called Math1) is required for the formation of mechanosensory hair cells in the inner ear; however, its specific molecular role is unknown. Here we show that absence of Math1 in mice results in a complete disruption of formation of the sensory epithelium of the cochlea, including the development of both hair cells and associated supporting cells. In addition, ectopic expression of Math1 in nonsensory regions of the cochlea is sufficient to induce the formation of sensory clusters that contain both hair cells and supporting cells. The formation of these clusters is dependent on inhibitory interactions mediated, most probably, through the Notch pathway, and on inductive interactions that recruit cells to develop as supporting cells through a pathway independent of Math1. These results show that Math1 functions in the developing cochlea to initiate both inductive and inhibitory signals that regulate the overall formation of the sensory epithelia.DOI: 10.1038/nn1349",pubmed,15543141,10.1038/nn1349
enteroviruses and sudden deafness,"354. CMAJ. 2003 May 27;168(11):1421-3.Enteroviruses and sudden deafness.Schattner A(1), Halperin D, Wolf D, Zimhony O.Author information:(1)Hebrew University and Hadassah Medical School, Jerusalem, Israel. amiMD@clalit.org.ilA young, healthy man presented with sudden severe sensorineural hearing loss and tinnitus. The results of the workup and neuroimaging were normal, as were the auditory brain stem responses. Methylprednisolone pulse therapy was associated with significant hearing improvement within 10 days. A history of a short self-limited febrile illness preceding admission (with headache, photophobia, myalgia and fatigue), a raised serum C-reactive protein level and transient leukopenia suggested an infectious cause. Lumbar puncture revealed a mononuclear pleocytosis of the cerebrospinal fluid, with negative cultures but positive polymerase chain reaction test results for enterovirus, which was later cultured from the patient's stool. The patient's wife and baby had had a similar febrile illness without hearing loss 10 days earlier, and an outbreak of enterovirus meningitis was identified in the area, which was associated with familial clustering and echovirus serotype 4 infection. The varied causes of sudden sensorineural hearing loss, which should include enterovirus, are reviewed here.PMCID: PMC155958",pubmed,12771071,
a causal and talkerindependent speaker separationdereverberation deep learning algorithm cost associated with conversion to realtime capable operation,"356. J Acoust Soc Am. 2021 Nov;150(5):3976. doi: 10.1121/10.0007134.A causal and talker-independent speaker separation/dereverberation deep learning algorithm: Cost associated with conversion to real-time capable operation.Healy EW(1), Taherian H(2), Johnson EM(1), Wang D(2).Author information:(1)Department of Speech and Hearing Science, The Ohio State University, Columbus, Ohio 43210, USA.(2)Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio 43210, USA.The fundamental requirement for real-time operation of a speech-processing algorithm is causality-that it operate without utilizing future time frames. In the present study, the performance of a fully causal deep computational auditory scene analysis algorithm was assessed. Target sentences were isolated from complex interference consisting of an interfering talker and concurrent room reverberation. The talker- and corpus/channel-independent model used Dense-UNet and temporal convolutional networks and estimated both magnitude and phase of the target speech. It was found that mean algorithm benefit was significant in every condition. Mean benefit for hearing-impaired (HI) listeners across all conditions was 46.4 percentage points. The cost of converting the algorithm to causal processing was also assessed by comparing to a prior non-causal version. Intelligibility decrements for HI and normal-hearing listeners from non-causal to causal processing were present in most but not all conditions, and these decrements were statistically significant in half of the conditions tested-those representing the greater levels of complex interference. Although a cost associated with causal processing was present in most conditions, it may be considered modest relative to the overall level of benefit.DOI: 10.1121/10.0007134PMCID: PMC8612765",pubmed,34852625,10.1121/10.0007134
local and global spatial organization of interaural level difference and frequency preferences in auditory cortex,"Despite decades of microelectrode recordings, fundamental questions remain about how auditory cortex represents sound-source location. Here, we used in vivo 2-photon calcium imaging to measure the sensitivity of layer II/III neurons in mouse primary auditory cortex (A1) to interaural level differences (ILDs), the principal spatial cue in this species. Although most ILD-sensitive neurons preferred ILDs favoring the contralateral ear, neurons with either midline or ipsilateral preferences were also present. An opponent-channel decoder accurately classified ILDs using the difference in responses between populations of neurons that preferred contralateral-ear-greater and ipsilateral-ear-greater stimuli. We also examined the spatial organization of binaural tuning properties across the imaged neurons with unprecedented resolution. Neurons driven exclusively by contralateral ear stimuli or by binaural stimulation occasionally formed local clusters, but their binaural categories and ILD preferences were not spatially organized on a more global scale. In contrast, the sound frequency preferences of most neurons within local cortical regions fell within a restricted frequency range, and a tonotopic gradient was observed across the cortical surface of individual mice. These results indicate that the representation of ILDs in mouse A1 is comparable to that of most other mammalian species, and appears to lack systematic or consistent spatial order. © The Author 2017.",scopus,2-s2.0-85046086325,10.1093/cercor/bhx295
moroccan sign language recognition based on machine learning,"More than 5% of the world's population (466 million people) suffer from a disabling hearing loss: 4 million are children. People with hearing loss usually communicate through spoken language and can benefit from assistive devices such as cochlear implants. However, deaf people have profound hearing loss and use sign language to communicate with others, which involves little or no hearing. To facilitate communication between deaf people and normal people who do not know sign language, we have proposed in this paper a system that allows textual transcription of sign language. The developed system will be able, in a first step, to recognize the sign language alphabet using machine learning and image processing. Simulation results have shown the efficiency of the developed model.",ieee,2768-0754,10.1109/ISCV54655.2022.9806116
recognition of urdu sign language a systematic review of the machine learning classification,"769. PeerJ Comput Sci. 2022 Feb 18;8:e883. doi: 10.7717/peerj-cs.883. eCollection 2022.Recognition of Urdu sign language: a systematic review of the machine learning classification.Zahid H(1), Rashid M(2), Hussain S(3), Azim F(4), Syed SA(5), Saad A(6).Author information:(1)Faculty of Engineering Science Technology and Management, Department of Biomedical Engineering and Department of Electrical Engineering, Ziauddin University, Karachi, Pakistan.(2)Faculty of Engineering Science Technology and Management, Department of Electrical Engineering and Department of Software Engineering, Ziauddin University, Karachi, Pakistan.(3)HESSA Project, US AID Program, Karachi, Pakistan.(4)Faculty of Engineering Science Technology and Management, Electrical Engineering Department, Ziauddin University, Karachi, Pakistan.(5)Faculty of Engineering Science Technology and Management, Department of Biomedical Engineering, Ziauddin University, Karachi, Pakistan.(6)Computer Science Department, Muhammad Ali Jinnah University, Karachi, Pakistan.BACKGROUND AND OBJECTIVE: Humans communicate with one another using language systems such as written words or body language (movements), hand motions, head gestures, facial expressions, lip motion, and many more. Comprehending sign language is just as crucial as learning a natural language. Sign language is the primary mode of communication for those who have a deaf or mute impairment or are disabled. Without a translator, people with auditory difficulties have difficulty speaking with other individuals. Studies in automatic recognition of sign language identification utilizing machine learning techniques have recently shown exceptional success and made significant progress. The primary objective of this research is to conduct a literature review on all the work completed on the recognition of Urdu Sign Language through machine learning classifiers to date.MATERIALS AND METHODS: All the studies have been extracted from databases, i.e., PubMed, IEEE, Science Direct, and Google Scholar, using a structured set of keywords. Each study has gone through proper screening criteria, i.e., exclusion and inclusion criteria. PRISMA guidelines have been followed and implemented adequately throughout this literature review.RESULTS: This literature review comprised 20 research articles that fulfilled the eligibility requirements. Only those articles were chosen for additional full-text screening that follows eligibility requirements for peer-reviewed and research articles and studies issued in credible journals and conference proceedings until July 2021. After other screenings, only studies based on Urdu Sign language were included. The results of this screening are divided into two parts; (1) a summary of all the datasets available on Urdu Sign Language. (2) a summary of all the machine learning techniques for recognizing Urdu Sign Language.CONCLUSION: Our research found that there is only one publicly-available USL sign-based dataset with pictures versus many character-, number-, or sentence-based publicly available datasets. It was also concluded that besides SVM and Neural Network, no unique classifier is used more than once. Additionally, no researcher opted for an unsupervised machine learning classifier for detection. To the best of our knowledge, this is the first literature review conducted on machine learning approaches applied to Urdu sign language.©2022 Zahid et al.DOI: 10.7717/peerj-cs.883PMCID: PMC9044266",pubmed,35494799,10.7717/peerj-cs.883
hair cell regeneration in the adult budgerigar after kanamycin ototoxicity,"493. Hear Res. 1992 Apr;59(1):46-58. doi: 10.1016/0378-5955(92)90101-r.Hair cell regeneration in the adult budgerigar after kanamycin ototoxicity.Hashino E(1), Tanaka Y, Salvi RJ, Sokabe M.Author information:(1)Hearing Research Laboratories, State University of New York, Buffalo 14214.Adult budgerigars were given kanamycin at a dose of 200 mg/kg/day for 10 successive days. At 1, 7, 14 and 28 days after the drug treatment, the cochleae of the birds were processed for scanning electron microscopy (SEM). Complete degeneration of sensory hair cells was observed in the basal 55-75% of the basilar papilla immediately after the treatment. Regenerating hair cells, characterized by clusters of microvilli and small apical surfaces, were present in the basal end of the papilla as early as one day post-treatment. During the 28 day recovery period, the number of hair cells progressively increased beginning at the base and spreading toward the apex. Although the appearance of the basilar papilla had improved considerably by 28 days post-treatment, the sensory epithelium still contained a number of pathologies, most noticeably, incomplete restoration of hair cell number in the most apical part of the damaged region and the disorganization of hair cell packing. These remaining pathologies may be responsible for the permanent threshold shifts observed in budgerigars exposed to the same dose of kanamycin treatment (Hashino and Sokabe, 1989).DOI: 10.1016/0378-5955(92)90101-r",pubmed,1629046,10.1016/0378-5955(92)90101-r
an exploration of the associations among hearing loss physical health and visual memory in adults from west central alabama references,"Purpose: The purpose of this preliminary study was to explore the associations among hearing loss, physical health, and visual memory in adults living in rural areas, urban clusters, and an urban city in west Central Alabama. Method: Two hundred ninety-seven adults (182 women, 115 men) from rural areas, urban clusters, and an urban city of west Central Alabama completed a hearing assessment, a physical health questionnaire, a hearing handicap measure, and a visual memory test. Results: A greater number of adults with hearing loss lived in rural areas and urban clusters than in an urban area. In addition, poorer physical health was significantly associated with hearing loss. A greater number of individuals with poor physical health who lived in rural towns and urban clusters had hearing loss compared with the adults with other physical health issues who lived in an urban city. Poorer hearing sensitivity resulted in poorer outcomes on the Emotional and Social subscales of the Hearing Handicap Inventory for Adults. And last, visual memory, a working-memory task, was not associated with hearing loss but was associated with educational level. Conclusions: The outcomes suggest that hearing loss is associated with poor physical and emotional health but not with visual-memory skills. A greater number of adults living in rural areas experienced hearing loss compared with adults living in an urban city, and consequently, further research will be necessary to confirm this relationship and to explore the reasons behind it. Also, further exploration of the relationship between cognition and hearing loss in adults living in rural and urban areas will be needed. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc16&DO=10.1044%2f2017_JSLHR-H-16-0369
editorial new discoveries in the benefits and outcomes of cochlear implantation,[No abstract available],scopus,2-s2.0-85143169777,10.3389/fnins.2022.1062582
agerelated decline in cochlear ribbon synapses and its relation to different metrics of auditorynerve activity,"76. Neurobiol Aging. 2021 Dec;108:133-145. doi: 10.1016/j.neurobiolaging.2021.08.019. Epub 2021 Sep 4.Age-related decline in cochlear ribbon synapses and its relation to different metrics of auditory-nerve activity.Steenken F(1), Heeringa AN(1), Beutelmann R(1), Zhang L(1), Bovee S(1), Klump GM(1), Köppl C(2).Author information:(1)Cluster of Excellence ""Hearing4all"" and Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg Oldenburg, Germany.(2)Cluster of Excellence ""Hearing4all"" and Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg Oldenburg, Germany. Electronic address: christine.koeppl@uni-oldenburg.de.Loss of inner hair cell-auditory nerve fiber synapses is considered to be an important early stage of neural presbyacusis. Mass potentials, recorded at the cochlear round window, can be used to derive the neural index (NI), a sensitive measure for pharmacologically-induced synapse loss. Here, we investigate the applicability of the NI for measuring age-related auditory synapse loss in young-adult, middle-aged, and old Mongolian gerbils. Synapse loss, which was progressively evident in the 2 aged groups, correlated weakly with NI when measured at a fixed sound level of 60 dB SPL. However, the NI was confounded by decreases in single-unit firing rates at 60 dB SPL. NI at 30 dB above threshold, when firing rates were similar between age groups, did not correlate with synapse loss. Our results show that synapse loss is poorly reflected in the NI of aged gerbils, particularly if further peripheral pathologies are present. The NI may therefore not be a reliable clinical tool to assess synapse loss in aged humans with peripheral hearing loss.Copyright © 2021 The Authors. Published by Elsevier Inc. All rights reserved.DOI: 10.1016/j.neurobiolaging.2021.08.019",pubmed,34601244,10.1016/j.neurobiolaging.2021.08.019
survey on hand gestures recognition for sign translation using artificial intelligence,"Sign language recognition is the process of converting hand signals used in the sign format into spoken or written language. It involves analysing and deciphering hand and finger gestures using computer vision and machine learning algorithms, then mapping those actions to corresponding words or phrases. There are various uses for sign language recognition, including facilitating communication between hearing and deaf persons, increasing accessibility for those with hearing loss, and improving sign language interpreting education and training programmes. Training data is diverse, covering different signers, lighting conditions, and backgrounds. Collect data with a variety of signing styles and gestures to improve the model's generalization and generate the sign values as vector format. The way we engage with people who primarily communicate through sign language may change significantly if this technology is used. Sign language can be recognized using a variety of techniques, such as sensor-based techniques, computer vision techniques, and hybrid techniques that combine both. Computer vision algorithms use visual data from cameras to identify hand movements and gestures. Sensorbased methods capture motion and record data by attaching sensors to the hands or fingers. Consequently, it may be possible to investigate various approaches to sign language recognition and predict a better method with a greater accuracy rate.",ieee,,10.1109/ICMCSI61536.2024.00047
in silico localization of perilymph proteins enriched in meiere disease using mammalian cochlear singlecell transcriptomics,"780. Otol Neurotol Open. 2023 Mar 9;3(1):e027. doi: 10.1097/ONO.0000000000000027. eCollection 2023 Mar.In Silico Localization of Perilymph Proteins Enriched in Meńier̀e Disease Using Mammalian Cochlear Single-cell Transcriptomics.Arambula AM(1), Gu S(2), Warnecke A(3), Schmitt HA(3), Staecker H(1), Hoa M(2)(4).Author information:(1)Department of Otolaryngology-Head & Neck Surgery, University of Kansas Medical Center, Kansas City, KS.(2)Auditory Development and Restoration Program, National Institute on Deafness and Other Communication Disorders, Bethesda, MD.(3)Department of Otolaryngology and Cluster of Excellence of the German Research Foundation (DFG; ""Deutsche Forschungsgemeinschaft"") ""Hearing4all,"" Hannover Medical School, Hannover, Germany.(4)Department of Otolaryngology-Head and Neck Surgery, Georgetown University Medical Center, Washington, DC.HYPOTHESIS: Proteins enriched in the perilymph proteome of Meńier̀e disease (MD) patients may identify affected cell types. Utilizing single-cell transcriptome datasets from the mammalian cochlea, we hypothesize that these enriched perilymph proteins can be localized to specific cochlear cell types.BACKGROUND: The limited understanding of human inner ear pathologies and their associated biomolecular variations hinder efforts to develop disease-specific diagnostics and therapeutics. Perilymph sampling and analysis is now enabling further characterization of the cochlear microenvironment. Recently, enriched inner ear protein expression has been demonstrated in patients with MD compared to patients with other inner ear diseases. Localizing expression of these proteins to cochlear cell types can further our knowledge of potential disease pathways and subsequent development of targeted therapeutics.METHODS: We compiled previously published data regarding differential perilymph proteome profiles amongst patients with MD, otosclerosis, enlarged vestibular aqueduct, sudden hearing loss, and hearing loss of undefined etiology (controls). Enriched proteins in MD were cross-referenced against published single-cell/single-nucleus RNA-sequencing datasets to localize gene expression to specific cochlear cell types.RESULTS: In silico analysis of single-cell transcriptomic datasets demonstrates enrichment of a unique group of perilymph proteins associated with MD in a variety of intracochlear cells, and some exogeneous hematologic and immune effector cells. This suggests that these cell types may play an important role in the pathology associated with late MD, suggesting potential future areas of investigation for MD pathophysiology and treatment.CONCLUSIONS: Perilymph proteins enriched in MD are expressed by specific cochlear cell types based on in silico localization, potentially facilitating development of disease-specific diagnostic markers and therapeutics.Copyright © 2023 The Authors. Published by Wolters Kluwer Health, Inc. on behalf of Otology & Neurotology, Inc.DOI: 10.1097/ONO.0000000000000027PMCID: PMC10950140",pubmed,38516320,10.1097/ONO.0000000000000027
optimization of sound coding strategies to make singing music more accessible for cochlear implant users,"199. Trends Hear. 2023 Jan-Dec;27:23312165221148022. doi: 10.1177/23312165221148022.Optimization of Sound Coding Strategies to Make Singing Music More Accessible for Cochlear Implant Users.Tahmasebi S(1)(2), Segovia-Martinez M(3), Nogueira W(1)(2).Author information:(1)Department of Otolaryngology, 9177Hannover Medical School, Hannover, Germany.(2)Cluster of Excellence Hearing4all, Hannover, Germany.(3)438962Oticon Medical, Vallauris, France.Cochlear implants (CIs) are implantable medical devices that can partially restore hearing to people suffering from profound sensorineural hearing loss. While these devices provide good speech understanding in quiet, many CI users face difficulties when listening to music. Reasons include poor spatial specificity of electric stimulation, limited transmission of spectral and temporal fine structure of acoustic signals, and restrictions in the dynamic range that can be conveyed via electric stimulation of the auditory nerve. The coding strategies currently used in CIs are typically designed for speech rather than music. This work investigates the optimization of CI coding strategies to make singing music more accessible to CI users. The aim is to reduce the spectral complexity of music by selecting fewer bands for stimulation, attenuating the background instruments by strengthening a noise reduction algorithm, and optimizing the electric dynamic range through a back-end compressor. The optimizations were evaluated through both objective and perceptual measures of speech understanding and melody identification of singing voice with and without background instruments, as well as music appreciation questionnaires. Consistent with the objective measures, results gathered from the perceptual evaluations indicated that reducing the number of selected bands and optimizing the electric dynamic range significantly improved speech understanding in music. Moreover, results obtained from questionnaires show that the new music back-end compressor significantly improved music enjoyment. These results have potential as a new CI program for improved singing music perception.DOI: 10.1177/23312165221148022PMCID: PMC9837293",pubmed,36628453,10.1177/23312165221148022
automatic speech discrimination assessment methods based on eventrelated potentials erp,"26. Sensors (Basel). 2022 Apr 1;22(7):2702. doi: 10.3390/s22072702.Automatic Speech Discrimination Assessment Methods Based on Event-Related Potentials (ERP).Charuthamrong P(1), Israsena P(2), Hemrungrojn S(3), Pan-Ngum S(4).Author information:(1)Interdisciplinary Program of Biomedical Engineering, Faculty of Engineering, Chulalongkorn University, Pathumwan, Bangkok 10330, Thailand.(2)National Electronics and Computer Technology Center, 112 Thailand Science Park, Klong Luang, Pathumthani 12120, Thailand.(3)Department of Psychiatry, Faculty of Medicine, Chulalongkorn University, Pathumwan, Bangkok 10330, Thailand.(4)Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Pathumwan, Bangkok 10330, Thailand.Speech discrimination is used by audiologists in diagnosing and determining treatment for hearing loss patients. Usually, assessing speech discrimination requires subjective responses. Using electroencephalography (EEG), a method that is based on event-related potentials (ERPs), could provide objective speech discrimination. In this work we proposed a visual-ERP-based method to assess speech discrimination using pictures that represent word meaning. The proposed method was implemented with three strategies, each with different number of pictures and test sequences. Machine learning was adopted to classify between the task conditions based on features that were extracted from EEG signals. The results from the proposed method were compared to that of a similar visual-ERP-based method using letters and a method that is based on the auditory mismatch negativity (MMN) component. The P3 component and the late positive potential (LPP) component were observed in the two visual-ERP-based methods while MMN was observed during the MMN-based method. A total of two out of three strategies of the proposed method, along with the MMN-based method, achieved approximately 80% average classification accuracy by a combination of support vector machine (SVM) and common spatial pattern (CSP). Potentially, these methods could serve as a pre-screening tool to make speech discrimination assessment more accessible, particularly in areas with a shortage of audiologists.DOI: 10.3390/s22072702PMCID: PMC9002564",pubmed,35408316,10.3390/s22072702
rapid bilateral improvement in auditory cortex activity in postlingually deafened adults following cochlear implantation,"209. Clin Neurophysiol. 2015 Mar;126(3):594-607. doi: 10.1016/j.clinph.2014.06.029. Epub 2014 Jul 3.Rapid bilateral improvement in auditory cortex activity in postlingually deafened adults following cochlear implantation.Sandmann P(1), Plotz K(2), Hauthal N(3), de Vos M(4), Schönfeld R(2), Debener S(5).Author information:(1)Central Auditory Diagnostics Lab, Department of Neurology, Cluster of Excellence ""Hearing4all"", Hannover Medical School, 30625 Hannover, Germany; Neuropsychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all"", European Medical School, University of Oldenburg, 26111 Oldenburg, Germany. Electronic address: Sandmann.Pascale@mh-hannover.de.(2)ENT Centre, Evangelical Hospital Oldenburg, 26122 Oldenburg, Germany.(3)Neuropsychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all"", European Medical School, University of Oldenburg, 26111 Oldenburg, Germany.(4)Methods in Neurocognitive Psychology, Department of Psychology, Cluster of Excellence ""Hearing4all"", European Medical School, University of Oldenburg, 26111 Oldenburg, Germany.(5)Neuropsychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all"", European Medical School, University of Oldenburg, 26111 Oldenburg, Germany; Research Center Neurosensory Science, University of Oldenburg, 26111 Oldenburg, Germany.OBJECTIVE: Cochlear implants (CIs) can partially restore hearing, but the cortical changes underlying auditory rehabilitation are not well understood.METHODS: This prospective longitudinal study used electroencephalography (EEG) to examine the temporal dynamics of changes in the auditory cortex contralateral and ipsilateral to the CI. Postlingually deafened CI recipients (N=11; mean: 59years) performed an auditory frequency discrimination task after <1week, 8weeks, 15weeks, and 59weeks of CI use.RESULTS: The CI users revealed a remarkable improvement in auditory discrimination ability which was most pronounced over the first eight weeks of CI experience. At the same time, CI users developed N1 auditory event-related potentials (AEP) with significantly enhanced amplitude and decreased latency, both in the auditory cortex contralateral and ipsilateral to the CI. A relationship was found between the duration of deafness and the ipsilateral AEP latency.CONCLUSIONS: Postlingually deafened adult CI users show rapid adaptation of the bilateral auditory cortex. Cortical plasticity is limited after long duration of auditory deprivation.SIGNIFICANCE: The finding of rapid and limited cortical changes in adult CI recipients may be of clinical relevance and can help estimate the role of plasticity for therapeutic gain.Copyright © 2014 International Federation of Clinical Neurophysiology. Published by Elsevier Ireland Ltd. All rights reserved.DOI: 10.1016/j.clinph.2014.06.029",pubmed,25065298,10.1016/j.clinph.2014.06.029
data for machine learningbased genetic diagnosis models for hereditary hearing loss by the gjb2 slc26a4 and mtrnr1 variants ,,base,bbb0899f1648018b0ef8301ff09d980b8303ad8885038340fb1f5b7100d41e53,
data for machine learningbased genetic diagnosis models for hereditary hearing loss by the gjb2 slc26a4 and mtrnr1 variants,,base,69e9300ac278907c26e2e96ca8f63e1b87f054c8b9457e5b9b655456b683982f,
smarca4 mutation causes human otosclerosis and a similar phenotype in mice,"Background Otosclerosis is a common cause of adult-onset progressive hearing loss, affecting 0.3%-0.4% of the population. It results from dysregulation of bone homeostasis in the otic capsule, most commonly leading to fixation of the stapes bone, impairing sound conduction through the middle ear. Otosclerosis has a well-known genetic predisposition including familial cases with apparent autosomal dominant mode of inheritance. While linkage analysis and genome-wide association studies suggested an association with several genomic loci and with genes encoding structural proteins involved in bone formation or metabolism, the molecular genetic pathophysiology of human otosclerosis is yet mostly unknown. Methods Whole-exome sequencing, linkage analysis, generation of CRISPR mutant mice, hearing tests and micro-CT. Results Through genetic studies of kindred with seven individuals affected by apparent autosomal dominant otosclerosis, we identified a disease-causing variant in SMARCA4, encoding a key component of the PBAF chromatin remodelling complex. We generated CRISPR-Cas9 transgenic mice carrying the human mutation in the mouse SMARCA4 orthologue. Mutant Smarca4 +/E1548K mice exhibited marked hearing impairment demonstrated through acoustic startle response and auditory brainstem response tests. Isolated ossicles of the auditory bullae of mutant mice exhibited a highly irregular structure of the incus bone, and their in situ micro-CT studies demonstrated the anomalous structure of the incus bone, causing disruption in the ossicular chain. Conclusion We demonstrate that otosclerosis can be caused by a variant in SMARCA4, with a similar phenotype of hearing impairment and abnormal bone formation in the auditory bullae in transgenic mice carrying the human mutation in the mouse SMARCA4 orthologue.  © 2023 Annals of the Rheumatic Diseases. All rights reserved.",scopus,2-s2.0-85170069610,10.1136/jmg-2023-109264
cochlea ct radiomics predicts chemoradiotherapy induced sensorineural hearing loss in head and neck cancer patients a machine learning and multivariable modelling study,,base,c47240dfa796c93d719ebda1321bb3d819ffb96eb482da42ad1e6e04565cd18d,
middle ear muscle dysfunction as the cause of menieres disease,"The symptoms of Meniere's disease form a distinct cluster: bouts of vertigo, fluctuating hearing loss, low-frequency tinnitus, and a feeling of pressure in the ear. Traditionally, these signature symptoms have pointed to some sort of pathology within the inner ear itself, but here the focus is shifted to the middle ear muscles. These muscles, the tensor tympani and the stapedius, have generally been seen as serving only a secondary protective role in hearing, but in this paper they are identified as vigilant gate-keepers - constantly monitoring acoustic input and dynamically adjusting hearing sensitivity so as to enhance external sounds and suppress internally generated ones. The case is made that this split-second adjustment is accomplished by regulation of inner ear pressure: when the middle ear muscles contract they push the stapes into the oval window and increase the pressure of fluids inside the otic capsule. In turn, hydraulic pressure squeezes hair cells, instantly adjusting their sensitivity. If the middle ear muscles should malfunction - such as from cramp, spasm, or dystonia - the resulting abnormal pressure will disrupt hair cells and produce Meniere's symptoms. A wide-ranging review of Meniere's disease and the middle ear muscles reinforces the link between the two. Since every striated muscle is prone to dystonia - an involuntary contraction involving derangement of its underlying control loop - middle ear muscle dystonia would lead to elevated pressure and abnormal hair cell function. The hypothesis is based on recognizing that the inner ear is a hydrostat - a cavity filled with fluid whose pressure is controlled by the middle ear muscles. Since the fluid is incompressible, even a slight contraction of the muscles can increase the pressure in the labyrinth to 3 kPa. The effect of such a pressure on the sensing cells within is crucial. Outer hair cells carry an internal turgor pressure of about 1 kPa, behaving physically like inflated balloons, and hence contraction of the middle ear muscles can instantly overcome internal cellular pressure, switch off ion channels, and reduce hearing sensitivity. This paper brings together supporting evidence and sets out major implications for Meniere's disease, including possible treatments.",cinahl,2083389X,10.17430/904674
enhancing music recognition using deep learningpowered source separation technology for cochlear implant users,"331. J Acoust Soc Am. 2024 Mar 1;155(3):1694-1703. doi: 10.1121/10.0025057.Enhancing music recognition using deep learning-powered source separation technology for cochlear implant users.Chang YJ(1), Han JY(1), Chu WC(1), Li LP(2)(3)(4)(5), Lai YH(1)(6).Author information:(1)Department of Biomedical Engineering, National Yang Ming Chiao Tung University, Taipei, Taiwan.(2)Faculty of Medicine, School of Medicine, National Yang Ming Chiao Tung University, Taipei, Taiwan.(3)Department of Otolaryngology, Cheng Hsin General Hospital, Taipei, Taiwan.(4)Department of Medical Research, China Medical University Hospital, China Medical University, Taichung, Taiwan.(5)Institute of Brain Science, School of Medicine, National Yang Ming Chiao Tung University, Taipei, Taiwan.(6)Medical Device Innovation Translation Center, National Yang Ming Chiao Tung University, Taipei, Taiwan.Cochlear implant (CI) is currently the vital technological device for assisting deaf patients in hearing sounds and greatly enhances their sound listening appreciation. Unfortunately, it performs poorly for music listening because of the insufficient number of electrodes and inaccurate identification of music features. Therefore, this study applied source separation technology with a self-adjustment function to enhance the music listening benefits for CI users. In the objective analysis method, this study showed that the results of the source-to-distortion, source-to-interference, and source-to-artifact ratios were 4.88, 5.92, and 15.28 dB, respectively, and significantly better than the Demucs baseline model. For the subjective analysis method, it scored higher than the traditional baseline method VIR6 (vocal to instrument ratio, 6 dB) by approximately 28.1 and 26.4 (out of 100) in the multi-stimulus test with hidden reference and anchor test, respectively. The experimental results showed that the proposed method can benefit CI users in identifying music in a live concert, and the personal self-fitting signal separation method had better results than any other default baselines (vocal to instrument ratio of 6 dB or vocal to instrument ratio of 0 dB) did. This finding suggests that the proposed system is a potential method for enhancing the music listening benefits for CI users.© 2024 Acoustical Society of America.DOI: 10.1121/10.0025057",pubmed,38426839,10.1121/10.0025057
characteristics and international comparability of the finnish matrix sentence test in cochlear implant recipients,"223. Int J Audiol. 2015;54 Suppl 2:80-7. doi: 10.3109/14992027.2015.1070309. Epub 2015 Aug 28.Characteristics and international comparability of the Finnish matrix sentence test in cochlear implant recipients.Dietz A(1), Buschermöhle M(2)(3), Sivonen V(4), Willberg T(1), Aarnisalo AA(4), Lenarz T(3)(5), Kollmeier B(2)(3)(6).Author information:(1)a * Department of Otorhinolaryngology , Kuopio University Hospital , Kuopio , Finland.(2)b HörTech gGmbH , Oldenburg , Germany.(3)c Cluster of Excellence, 'Hearing4all' , Oldenburg & Hannover , Germany.(4)d Department of Otorhinolaryngology , Helsinki University Central Hospital , Helsinki , Finland.(5)e Department of Otorhinolaryngology , Head & Neck Surgery , Medizinische Hochschule Hannover , Germany.(6)f Carl von Ossietzky Universität Oldenburg, Medizinische Physik , Oldenburg , Germany.OBJECTIVES: The first Finnish sentence-based speech test in noise--the Finnish matrix sentence test--was recently developed. The aim of this study was to determine the characteristics of the new test with respect to test-retest reliability, speech recognition curve, and international comparability in Finnish cochlear implant (CI) recipients.DESIGN: The speech reception thresholds (SRT) were measured by means of an adaptive test procedure and compared with the results of the traditional Finnish word test. Additional measurements for concurrent slope and SRT estimation were conducted to determine the speech recognition curve and to check the test-retest reliability.STUDY SAMPLE: The measurements were performed on 78 Finnish CI recipients. In a subset of 25 patients, additional measurements for test-retest reliability and slope determination were performed.RESULTS: The mean SRT was -3.5 ± 1.7 dB SNR, with only a weak correlation with the Finnish word test. Test-retest reliability was within ± 1 dB and the mean slope of the speech recognition curve was 14.6 ± 3.6 %/dB. The rehabilitation results were similar to the results published for the German matrix test.CONCLUSIONS: The Finnish matrix test was found to be suitable and efficient in CI recipients with similar characteristics as the German matrix test.DOI: 10.3109/14992027.2015.1070309",pubmed,26364512,10.3109/14992027.2015.1070309
brain metabolic changes in rats following acoustic trauma,"787. Front Neurosci. 2017 Mar 24;11:148. doi: 10.3389/fnins.2017.00148. eCollection 2017.Brain Metabolic Changes in Rats following Acoustic Trauma.He J(1), Zhu Y(1), Aa J(1), Smith PF(2), De Ridder D(3), Wang G(1), Zheng Y(2).Author information:(1)Key Laboratory of Drug Metabolism and Pharmacokinetics, China Pharmaceutical University Nanjing, Jiangsu, China.(2)Department of Pharmacology and Toxicology, School of Biomedical Sciences, University of OtagoDunedin, New Zealand; Brain Health Research Centre, University of OtagoDunedin, New Zealand; Brain Research New ZealandDunedin, New Zealand; Eisdell Moore Centre for Hearing and Balance Research, University of AucklandAuckland, New Zealand.(3)Brain Health Research Centre, University of OtagoDunedin, New Zealand; Brain Research New ZealandDunedin, New Zealand; Eisdell Moore Centre for Hearing and Balance Research, University of AucklandAuckland, New Zealand; Department of Neurosurgery, Dunedin Medical School, University of OtagoOtago, New Zealand.Erratum in    Front Neurosci. 2017 May 08;11:260.Acoustic trauma is the most common cause of hearing loss and tinnitus in humans. However, the impact of acoustic trauma on system biology is not fully understood. It has been increasingly recognized that tinnitus caused by acoustic trauma is unlikely to be generated by a single pathological source, but rather a complex network of changes involving not only the auditory system but also systems related to memory, emotion and stress. One obvious and significant gap in tinnitus research is a lack of biomarkers that reflect the consequences of this interactive ""tinnitus-causing"" network. In this study, we made the first attempt to analyse brain metabolic changes in rats following acoustic trauma using metabolomics, as a pilot study prior to directly linking metabolic changes to tinnitus. Metabolites in 12 different brain regions collected from either sham or acoustic trauma animals were profiled using a gas chromatography mass spectrometry (GC/MS)-based metabolomics platform. After deconvolution of mass spectra and identification of the molecules, the metabolomic data were processed using multivariate statistical analysis. Principal component analysis showed that metabolic patterns varied among different brain regions; however, brain regions with similar functions had a similar metabolite composition. Acoustic trauma did not change the metabolite clusters in these regions. When analyzed within each brain region using the orthogonal projection to latent structures discriminant analysis sub-model, 17 molecules showed distinct separation between control and acoustic trauma groups in the auditory cortex, inferior colliculus, superior colliculus, vestibular nucleus complex (VNC), and cerebellum. Further metabolic pathway impact analysis and the enrichment overview with network analysis suggested the primary involvement of amino acid metabolism, including the alanine, aspartate and glutamate metabolic pathways, the arginine and proline metabolic pathways and the purine metabolic pathway. Our results provide the first metabolomics evidence that acoustic trauma can induce changes in multiple metabolic pathways. This pilot study also suggests that the metabolomic approach has the potential to identify acoustic trauma-specific metabolic shifts in future studies where metabolic changes are correlated with the animal's tinnitus status.DOI: 10.3389/fnins.2017.00148PMCID: PMC5364180",pubmed,28392756,10.3389/fnins.2017.00148
electricacoustic forward masking in cochlear implant users with ipsilateral residual hearing,"282. Hear Res. 2018 Jul;364:25-37. doi: 10.1016/j.heares.2018.04.003. Epub 2018 Apr 9.Electric-acoustic forward masking in cochlear implant users with ipsilateral residual hearing.Imsiecke M(1), Krüger B(2), Büchner A(3), Lenarz T(4), Nogueira W(5).Author information:(1)Department of Otolaryngology, Hannover Medical School, Hanover, Germany. Electronic address: Imsiecke.Marina@mh-hannover.de.(2)Department of Otolaryngology, Hannover Medical School, Hanover, Germany; Cluster of Excellence 'Hearing4All', Hanover, Germany. Electronic address: Krueger.Benjamin@mh-hannover.de.(3)Department of Otolaryngology, Hannover Medical School, Hanover, Germany; Cluster of Excellence 'Hearing4All', Hanover, Germany. Electronic address: Buechner.Andreas@mh-hannover.de.(4)Department of Otolaryngology, Hannover Medical School, Hanover, Germany; Cluster of Excellence 'Hearing4All', Hanover, Germany. Electronic address: Lenarz.Thomas@mh-hannover.de.(5)Department of Otolaryngology, Hannover Medical School, Hanover, Germany; Cluster of Excellence 'Hearing4All', Hanover, Germany. Electronic address: NogueiraVazquez.Waldo@mh-hannover.de.In order to investigate the temporal mechanisms of the auditory system, psychophysical forward masking experiments were conducted in cochlear implant users who had preserved acoustic hearing in the ipsilateral ear. This unique electric-acoustic stimulation (EAS) population allowed the measurement of threshold recovery functions for acoustic or electric probes in the presence of electric or acoustic maskers, respectively. In the electric masking experiment, the forward masked threshold elevation of acoustic probes was measured as a function of the time interval after the offset of the electric masker, i.e. the masker-to-probe interval (MPI). In the acoustic masking experiment, the forward masked threshold elevation of electric probe stimuli was investigated under the influence of a preceding acoustic masker. Since electric pulse trains directly stimulate the auditory nerve, this novel experimental setup allowed the acoustic adaptation properties (attributed to the physiology of the hair cells) to be differentiated from the subsequent processing by more central mechanisms along the auditory pathway. For instance, forward electric masking patterns should result more from the auditory-nerve response to electrical stimulation, while forward acoustic masking patterns should primarily be the result of the recovery from adaptation at the hair-cell neuron interface. Electric masking showed prolonged threshold elevation of acoustic probes, which depended significantly on the masker-to-probe interval. Additionally, threshold elevation was significantly dependent on the similarity between acoustic stimulus frequency and electric place frequency, the electric-acoustic frequency difference (EAFD). Acoustic masking showed a reduced, but statistically significant effect of electric threshold elevation, which did not significantly depend on MPI. Lastly, acoustic masking showed longer decay times than electric masking and a reduced dependency on EAFD. In conclusion, the forward masking patterns observed for combined electric-acoustic stimulation provide further insights into the temporal mechanisms of the auditory system. For instance, the asymmetry in the amount of threshold elevation, the dependency on EAFD and the time constants for the recovery functions of acoustic and electric masking all indicate that there must be several processes with different latencies (e.g. neural adaptation, depression of spontaneous activity, efferent systems) that are involved in forward masking recovery functions.Copyright © 2018 The Authors. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2018.04.003",pubmed,29673567,10.1016/j.heares.2018.04.003
monaural congenital deafness affects aural dominance and degrades binaural processing,"622. Cereb Cortex. 2016 Apr;26(4):1762-77. doi: 10.1093/cercor/bhv351. Epub 2016 Jan 22.Monaural Congenital Deafness Affects Aural Dominance and Degrades Binaural Processing.Tillein J(1), Hubka P(2), Kral A(3).Author information:(1)Cluster of Excellence Hearing4all, Institute of AudioNeuroTechnology and Department of Experimental Otology of the ENT Clinics, Hannover Medical School, Hannover, Germany Department of Otorhinolaryngology, J.W. Goethe University, Frankfurt am Main, Germany MED-EL GmbH, Innsbruck, Austria.(2)Cluster of Excellence Hearing4all, Institute of AudioNeuroTechnology and Department of Experimental Otology of the ENT Clinics, Hannover Medical School, Hannover, Germany.(3)Cluster of Excellence Hearing4all, Institute of AudioNeuroTechnology and Department of Experimental Otology of the ENT Clinics, Hannover Medical School, Hannover, Germany School of Behavioral and Brain Sciences, The University of Texas at Dallas, Richardson, TX, USA.Cortical development extensively depends on sensory experience. Effects of congenital monaural and binaural deafness on cortical aural dominance and representation of binaural cues were investigated in the present study. We used an animal model that precisely mimics the clinical scenario of unilateral cochlear implantation in an individual with single-sided congenital deafness. Multiunit responses in cortical field A1 to cochlear implant stimulation were studied in normal-hearing cats, bilaterally congenitally deaf cats (CDCs), and unilaterally deaf cats (uCDCs). Binaural deafness reduced cortical responsiveness and decreased response thresholds and dynamic range. In contrast to CDCs, in uCDCs, cortical responsiveness was not reduced, but hemispheric-specific reorganization of aural dominance and binaural interactions were observed. Deafness led to a substantial drop in binaural facilitation in CDCs and uCDCs, demonstrating the inevitable role of experience for a binaural benefit. Sensitivity to interaural time differences was more reduced in uCDCs than in CDCs, particularly at the hemisphere ipsilateral to the hearing ear. Compared with binaural deafness, unilateral hearing prevented nonspecific reduction in cortical responsiveness, but extensively reorganized aural dominance and binaural responses. The deaf ear remained coupled with the cortex in uCDCs, demonstrating a significant difference to deprivation amblyopia in the visual system.© The Author 2016. Published by Oxford University Press.DOI: 10.1093/cercor/bhv351PMCID: PMC4785956",pubmed,26803166,10.1093/cercor/bhv351
surgical and audiological outcomes with a new transcutaneous bone conduction device with reduced transducer thickness in children,"319. Eur Arch Otorhinolaryngol. 2023 Oct;280(10):4381-4389. doi: 10.1007/s00405-023-07927-9. Epub 2023 Mar 31.Surgical and audiological outcomes with a new transcutaneous bone conduction device with reduced transducer thickness in children.Willenborg K(1)(2), Lenarz T(3)(4), Busch S(3)(4).Author information:(1)Department of Otolaryngology, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany. willenborg.kerstin@mh-hannover.de.(2)Cluster of Excellence H4A, Hannover, Germany. willenborg.kerstin@mh-hannover.de.(3)Department of Otolaryngology, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany.(4)Cluster of Excellence H4A, Hannover, Germany.PURPOSE: Due to smaller bone thickness, young children with conductive or mixed hearing loss or single-sided deafness were previously most commonly treated with a percutaneous osseointegrated bone-anchored hearing aid (BAHA) or an active middle-ear implant. While the BAHA increases the risk of implant infections, skin infection, overgrowth of the screw or involvement of the implant in head trauma, middle-ear implant surgery involves manipulation of the ossicles with possible risk of surgical trauma. These complications can be omitted with transcutaneous bone conduction implant systems like the MED-EL Bonebridge system. The purpose of this study was to analyze whether the second generation of the Bonebridge (BCI 602) that features a decreased implant thickness with a reduced surgical drilling depth can be implanted safely in young children with good postoperative hearing performance.METHODS: In this study, 14 patients under 12 years were implanted with the second generation of the Bonebridge. Preoperative workup comprised a CT scan, an MRI scan, pure tone audiometry, or alternatively a BERA (bone conduction, air conduction). Since children under 12 years often have a lower bone thickness, the CT was performed to determine the suitability of the temporal bone for optimal implant placement using the Otoplan software.RESULTS: All patients (including three under the age of five) were successfully implanted and showed a good postoperative hearing performance.CONCLUSION: With adequate preoperative workup, this device can be safely implanted in children and even children under 5 years of age and allows for an extension of indication criteria toward younger children.© 2023. The Author(s).DOI: 10.1007/s00405-023-07927-9PMCID: PMC10477095",pubmed,37000276,10.1007/s00405-023-07927-9
risk of hearing loss among multidrugresistant tuberculosis patients according to cumulative aminoglycoside dose,"552. Int J Tuberc Lung Dis. 2020 Jan 1;24(1):65-72. doi: 10.5588/ijtld.19.0062.Risk of hearing loss among multidrug-resistant tuberculosis patients according to cumulative aminoglycoside dose.Hong H(1), Dowdy DW(2), Dooley KE(3), Francis HW(4), Budhathoki C(5), Han HR(6), Farley JE(1).Author information:(1)Johns Hopkins University School of Nursing, Baltimore, MD, The REACH Initiative, Johns Hopkins University School of Nursing, Baltimore, MD.(2)Departments of Epidemiology and International Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, MD.(3)Divisions of Clinical Pharmacology and Infectious Disease, Johns Hopkins University School of Medicine, Baltimore, MD.(4)Division of Head and Neck Surgery and Communication Sciences, Duke University School of Medicine, Durham, NC.(5)Johns Hopkins University School of Nursing, Baltimore, MD.(6)Johns Hopkins University School of Nursing, Baltimore, MD, Center for Cardiovascular and Chronic Care, Johns Hopkins University, Baltimore, MD, USA.SETTING: The ototoxic effects of aminoglycosides (AGs) lead to permanent hearing loss, which is one of the devastating consequences of multidrug-resistant tuberculosis (MDR-TB) treatment. As AG ototoxicity is dose-dependent, the impact of a surrogate measure of AG exposure on AG-induced hearing loss warrants close attention for settings with limited therapeutic drug monitoring.OBJECTIVE: To explore the prognostic impact of cumulative AG dose on AG ototoxicity in patients following initiation of AG-containing treatment for MDR-TB.DESIGN: This prospective cohort study was nested within an ongoing cluster-randomized trial of nurse case management intervention across 10 MDR-TB hospitals in South Africa.RESULTS: The adjusted hazard of AG regimen modification due to ototoxicity in the high-dose group (≥75 mg/kg/week) was 1.33 times higher than in the low-dose group (<75 mg/kg/week, 95%CI 1.09-1.64). The adjusted hazard of developing audiometric hearing loss was 1.34 times higher than in the low-dose group (95%CI 1.01-1.77). Pre-existing hearing loss (adjusted hazard ratio [aHR] 1.71, 95%CI 1.29-2.26) and age (aHR 1.16 per 10 years of age, 95%CI 1.01-1.33) were also associated with an increased risk of hearing loss.CONCLUSION: MDR-TB patients with high AG dose, advanced age and pre-existing hearing loss have a significantly higher risk of AG-induced hearing loss. Those at high risk may be candidates for more frequent monitoring or AG-sparing regimens.DOI: 10.5588/ijtld.19.0062PMCID: PMC7594545",pubmed,32005308,10.5588/ijtld.19.0062
neural circuitwide analysis of changes to gene expression during deafeninginduced birdsong destabilization,"Sensory feedback is required for the stable execution of learned motor skills, and its loss can severely disrupt motor performance. The neural mechanisms that mediate sensorimotor stability have been extensively studied at systems and physiological levels, yet relatively little is known about how disruptions to sensory input alter the molecular properties of associated motor systems. Songbird courtship song, a model for skilled behavior, is a learned and highly structured vocalization that is destabilized following deafening. Here, we sought to determine how the loss of auditory feedback modifies gene expression and its coordination across the birdsong senso-rimotor circuit. To facilitate this system-wide analysis of transcriptional responses, we developed a gene expression profiling approach that enables the construction of hundreds of spatially-defined RNA-sequencing libraries. Using this method, we found that deafening preferentially alters gene expression across birdsong neural circuitry relative to surrounding areas, particularly in premotor and striatal regions. Genes with altered expression are associated with synaptic transmission, neuronal spines, and neuromodulation and show a bias toward expression in glutamatergic neurons and Pvalb/Sst-class GABAergic interneurons. We also found that connected song regions exhibit correla-tions in gene expression that were reduced in deafened birds relative to hearing birds, suggesting that song destabilization alters the inter-region coordination of transcriptional states. Finally, lesioning LMAN, a forebrain afferent of RA required for deafening-induced song plasticity, had the largest effect on groups of genes that were also most affected by deafening. Combined, this integrated transcriptomics analysis demonstrates that the loss of peripheral sensory input drives a distributed gene expression response throughout associated sensorimotor neural circuitry and iden-tifies specific candidate molecular and cellular mechanisms that support the stability and plasticity of learned motor skills. © Colquitt et al.",scopus,2-s2.0-85163236862,10.7554/eLife.85970
defining the biological bases of individual differences in musicality,"836. Philos Trans R Soc Lond B Biol Sci. 2015 Mar 19;370(1664):20140092. doi: 10.1098/rstb.2014.0092.Defining the biological bases of individual differences in musicality.Gingras B(1), Honing H(2), Peretz I(3), Trainor LJ(4), Fisher SE(5).Author information:(1)Department of Cognitive Biology, University of Vienna, Vienna, Austria.(2)Amsterdam Brain and Cognition (ABC), Institute of Logic, Language and Computation (ILLC), University of Amsterdam, Amsterdam, The Netherlands.(3)International Laboratory for Brain, Music and Sound Research, Department of Psychology, University of Montreal, Quebec, Canada.(4)Department of Psychology, Neuroscience and Behaviour, McMaster University, Ontario, Canada.(5)Language and Genetics Department, Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands Donders Institute for Brain, Cognition and Behaviour, Radboud University, Nijmegen, The Netherlands simon.fisher@mpi.nl.Advances in molecular technologies make it possible to pinpoint genomic factors associated with complex human traits. For cognition and behaviour, identification of underlying genes provides new entry points for deciphering the key neurobiological pathways. In the past decade, the search for genetic correlates of musicality has gained traction. Reports have documented familial clustering for different extremes of ability, including amusia and absolute pitch (AP), with twin studies demonstrating high heritability for some music-related skills, such as pitch perception. Certain chromosomal regions have been linked to AP and musical aptitude, while individual candidate genes have been investigated in relation to aptitude and creativity. Most recently, researchers in this field started performing genome-wide association scans. Thus far, studies have been hampered by relatively small sample sizes and limitations in defining components of musicality, including an emphasis on skills that can only be assessed in trained musicians. With opportunities to administer standardized aptitude tests online, systematic large-scale assessment of musical abilities is now feasible, an important step towards high-powered genome-wide screens. Here, we offer a synthesis of existing literatures and outline concrete suggestions for the development of comprehensive operational tools for the analysis of musical phenotypes.© 2015 The Author(s) Published by the Royal Society. All rights reserved.DOI: 10.1098/rstb.2014.0092PMCID: PMC4321133",pubmed,25646515,10.1098/rstb.2014.0092
comparison of interaural electrode pairing methods for bilateral cochlear implants,"384. Trends Hear. 2015 Dec 1;19:2331216515617143. doi: 10.1177/2331216515617143.Comparison of Interaural Electrode Pairing Methods for Bilateral Cochlear Implants.Hu H(1), Dietz M(2).Author information:(1)Medizinische Physik, Universität Oldenburg and Cluster of Excellence ""Hearing4all"", Germany hongmei.hu@uni-oldenburg.de.(2)Medizinische Physik, Universität Oldenburg and Cluster of Excellence ""Hearing4all"", Germany.In patients with bilateral cochlear implants (CIs), pairing matched interaural electrodes and stimulating them with the same frequency band is expected to facilitate binaural functions such as binaural fusion, localization, and spatial release from masking. Because clinical procedures typically do not include patient-specific interaural electrode pairing, it remains the case that each electrode is allocated to a generic frequency range, based simply on the electrode number. Two psychoacoustic techniques for determining interaurally paired electrodes have been demonstrated in several studies: interaural pitch comparison and interaural time difference (ITD) sensitivity. However, these two methods are rarely, if ever, compared directly. A third, more objective method is to assess the amplitude of the binaural interaction component (BIC) derived from electrically evoked auditory brainstem responses for different electrode pairings; a method has been demonstrated to be a potential candidate for bilateral CI users. Here, we tested all three measures in the same eight CI users. We found good correspondence between the electrode pair producing the largest BIC and the electrode pair producing the maximum ITD sensitivity. The correspondence between the pairs producing the largest BIC and the pitch-matched electrode pairs was considerably weaker, supporting the previously proposed hypothesis that whilst place pitch might adapt over time to accommodate mismatched inputs, sensitivity to ITDs does not adapt to the same degree.© The Author(s) 2015.DOI: 10.1177/2331216515617143PMCID: PMC4771032",pubmed,26631108,10.1177/2331216515617143
right ventral stream damage underlies both poststroke aprosodia and amusia,"Background and purpose: This study was undertaken to determine and compare lesion patterns and structural dysconnectivity underlying poststroke aprosodia and amusia, using a data-driven multimodal neuroimaging approach. Methods: Thirty-nine patients with right or left hemisphere stroke were enrolled in a cohort study and tested for linguistic and affective prosody perception and musical pitch and rhythm perception at subacute and 3-month poststroke stages. Participants listened to words spoken with different prosodic stress that changed their meaning, and to words spoken with six different emotions, and chose which meaning or emotion was expressed. In the music tasks, participants judged pairs of short melodies as the same or different in terms of pitch or rhythm. Structural magnetic resonance imaging data were acquired at both stages, and machine learning-based lesion–symptom mapping and deterministic tractography were used to identify lesion patterns and damaged white matter pathways giving rise to aprosodia and amusia. Results: Both aprosodia and amusia were behaviorally strongly correlated and associated with similar lesion patterns in right frontoinsular and striatal areas. In multiple regression models, reduced fractional anisotropy and lower tract volume of the right inferior fronto-occipital fasciculus were the strongest predictors for both disorders, over time. Conclusions: These results highlight a common origin of aprosodia and amusia, both arising from damage and disconnection of the right ventral auditory stream integrating rhythmic–melodic acoustic information in prosody and music. Comorbidity of these disabilities may worsen the prognosis and affect rehabilitation success. © 2021 European Academy of Neurology",scopus,2-s2.0-85117880997,10.1111/ene.15148
auditory crossmodal reorganization in cochlear implant users indicates audiovisual integration,"93. Neuroimage Clin. 2017 Sep 4;16:514-523. doi: 10.1016/j.nicl.2017.09.001. eCollection 2017.Auditory cross-modal reorganization in cochlear implant users indicates audio-visual integration.Stropahl M(1), Debener S(1)(2).Author information:(1)Neuropsychology Lab, Department of Psychology, European Medical School, Carl von Ossietzky University Oldenburg, Germany.(2)Cluster of Excellence Hearing4all Oldenburg, Germany.There is clear evidence for cross-modal cortical reorganization in the auditory system of post-lingually deafened cochlear implant (CI) users. A recent report suggests that moderate sensori-neural hearing loss is already sufficient to initiate corresponding cortical changes. To what extend these changes are deprivation-induced or related to sensory recovery is still debated. Moreover, the influence of cross-modal reorganization on CI benefit is also still unclear. While reorganization during deafness may impede speech recovery, reorganization also has beneficial influences on face recognition and lip-reading. As CI users were observed to show differences in multisensory integration, the question arises if cross-modal reorganization is related to audio-visual integration skills. The current electroencephalography study investigated cortical reorganization in experienced post-lingually deafened CI users (n = 18), untreated mild to moderately hearing impaired individuals (n = 18) and normal hearing controls (n = 17). Cross-modal activation of the auditory cortex by means of EEG source localization in response to human faces and audio-visual integration, quantified with the McGurk illusion, were measured. CI users revealed stronger cross-modal activations compared to age-matched normal hearing individuals. Furthermore, CI users showed a relationship between cross-modal activation and audio-visual integration strength. This may further support a beneficial relationship between cross-modal activation and daily-life communication skills that may not be fully captured by laboratory-based speech perception tests. Interestingly, hearing impaired individuals showed behavioral and neurophysiological results that were numerically between the other two groups, and they showed a moderate relationship between cross-modal activation and the degree of hearing loss. This further supports the notion that auditory deprivation evokes a reorganization of the auditory system even at early stages of hearing loss.DOI: 10.1016/j.nicl.2017.09.001PMCID: PMC5609862",pubmed,28971005,10.1016/j.nicl.2017.09.001
cortical reorganization in postlingually deaf cochlear implant users intramodal and crossmodal considerations,"18. Hear Res. 2017 Jan;343:128-137. doi: 10.1016/j.heares.2016.07.005. Epub 2016 Jul 26.Cortical reorganization in postlingually deaf cochlear implant users: Intra-modal and cross-modal considerations.Stropahl M(1), Chen LC(2), Debener S(2).Author information:(1)Neuropsychology Lab, Department of Psychology, European Medical School, Carl von Ossietzky University Oldenburg, Germany. Electronic address: maren.stropahl@uni-oldenburg.de.(2)Neuropsychology Lab, Department of Psychology, European Medical School, Carl von Ossietzky University Oldenburg, Germany; Cluster of Excellence Hearing4all Oldenburg, Germany.With the advances of cochlear implant (CI) technology, many deaf individuals can partially regain their hearing ability. However, there is a large variation in the level of recovery. Cortical changes induced by hearing deprivation and restoration with CIs have been thought to contribute to this variation. The current review aims to identify these cortical changes in postlingually deaf CI users and discusses their maladaptive or adaptive relationship to the CI outcome. Overall, intra-modal and cross-modal reorganization patterns have been identified in postlingually deaf CI users in visual and in auditory cortex. Even though cross-modal activation in auditory cortex is considered as maladaptive for speech recovery in CI users, a similar activation relates positively to lip reading skills. Furthermore, cross-modal activation of the visual cortex seems to be adaptive for speech recognition. Currently available evidence points to an involvement of further brain areas and suggests that a focus on the reversal of visual take-over of the auditory cortex may be too limited. Future investigations should consider expanded cortical as well as multi-sensory processing and capture different hierarchical processing steps. Furthermore, prospective longitudinal designs are needed to track the dynamics of cortical plasticity that takes place before and after implantation.Copyright © 2016 The Authors. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2016.07.005",pubmed,27473503,10.1016/j.heares.2016.07.005
hearing loss in juvenile rats leads to excessive play fighting and hyperactivity mild cognitive deficits and altered neuronal activity in the prefrontal cortex,"643. Curr Res Neurobiol. 2024 Jan 29;6:100124. doi: 10.1016/j.crneur.2024.100124. eCollection 2024.Hearing loss in juvenile rats leads to excessive play fighting and hyperactivity, mild cognitive deficits and altered neuronal activity in the prefrontal cortex.Jelinek J(1), Johne M(1)(2), Alam M(1), Krauss JK(1), Kral A(2)(3)(4), Schwabe K(1)(2).Author information:(1)Department of Neurosurgery, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany.(2)Cluster of Excellence Hearing4all, German Research Foundation, Hannover, Germany.(3)Institute of AudioNeuroTechnology, Hannover Medical School, Stadtfelddamm 34, 30625, Hanover, Germany.(4)Department of Experimental Otology of the ENT Clinics, Hannover Medical School, Stadtfelddamm 34, 30625, Hannover, Germany.BACKGROUND: In children, hearing loss has been associated with hyperactivity, disturbed social interaction, and risk of cognitive disturbances. Mechanistic explanations of these relations sometimes involve language. To investigate the effect of hearing loss on behavioral deficits in the absence of language, we tested the impact of hearing loss in juvenile rats on motor, social, and cognitive behavior and on physiology of prefrontal cortex.METHODS: Hearing loss was induced in juvenile (postnatal day 14) male Sprague-Dawley rats by intracochlear injection of neomycin under general anesthesia. Sham-operated and non-operated hearing rats served as controls. One week after surgery auditory brainstem response (ABR) measurements verified hearing loss or intact hearing in sham-operated and non-operated controls. All rats were then tested for locomotor activity (open field), coordination (Rotarod), and for social interaction during development in weeks 1, 2, 4, 8, 16, and 24 after surgery. From week 8 on, rats were trained and tested for spatial learning and memory (4-arm baited 8-arm radial maze test). In a final setting, neuronal activity was recorded in the medial prefrontal cortex (mPFC).RESULTS: In the open field deafened rats moved faster and covered more distance than sham-operated and non-operated controls from week 8 on (both p < 0.05). Deafened rats showed significantly more play fighting during development (p < 0.05), whereas other aspects of social interaction, such as following, were not affected. Learning of the radial maze test was not impaired in deafened rats (p > 0.05), but rats used less next-arm entries than other groups indicating impaired concept learning (p < 0.05). In the mPFC neuronal firing rate was reduced and enhanced irregular firing was observed. Moreover, oscillatory activity was altered, both within the mPFC and in coherence of mPFC with the somatosensory cortex (p < 0.05).CONCLUSIONS: Hearing loss in juvenile rats leads to hyperactive behavior and pronounced play-fighting during development, suggesting a causal relationship between hearing loss and cognitive development. Altered neuronal activities in the mPFC after hearing loss support such effects on neuronal networks outside the central auditory system. This animal model provides evidence of developmental consequences of juvenile hearing loss on prefrontal cortex in absence of language as potential confounding factor.© 2024 The Authors.DOI: 10.1016/j.crneur.2024.100124PMCID: PMC11015060",pubmed,38616957,10.1016/j.crneur.2024.100124
realtime recognition of indian sign language,"The real-time sign language recognition system is developed for recognising the gestures of Indian Sign Language (ISL). Generally, sign languages consist of hand gestures and facial expressions. For recognising the signs, the Regions of Interest (ROI) are identified and tracked using the skin segmentation feature of OpenCV. The training and prediction of hand gestures are performed by applying fuzzy c-means clustering machine learning algorithm. The gesture recognition has many applications such as gesture controlled robots and automated homes, game control, Human-Computer Interaction (HCI) and sign language interpretation. The proposed system is used to recognize the real-time signs. Hence it is very much useful for hearing and speech impaired people to communicate with normal people.",ieee,,10.1109/ICCIDS.2019.8862125
mcgurk stimuli for the investigation of multisensory integration in cochlear implant users the oldenburg audio visual speech stimuli olavs,"380. Psychon Bull Rev. 2017 Jun;24(3):863-872. doi: 10.3758/s13423-016-1148-9.McGurk stimuli for the investigation of multisensory integration in cochlear implant users: The Oldenburg Audio Visual Speech Stimuli (OLAVS).Stropahl M(1), Schellhardt S(2), Debener S(3)(4).Author information:(1)Department of Psychology, Neuropsychology Lab, European Medical School, Carl von Ossietzky University of Oldenburg, Ammerländer Herrstraße 114-118, 26129, Oldenburg, Germany. maren.stropahl@uni-oldenburg.de.(2)Institute of Hearing Technology and Audiology, Jade University of Applied Sciences, Oldenburg, Germany.(3)Department of Psychology, Neuropsychology Lab, European Medical School, Carl von Ossietzky University of Oldenburg, Ammerländer Herrstraße 114-118, 26129, Oldenburg, Germany.(4)Cluster of Excellence Hearing4all Oldenburg, Oldenburg, Germany.The concurrent presentation of different auditory and visual syllables may result in the perception of a third syllable, reflecting an illusory fusion of visual and auditory information. This well-known McGurk effect is frequently used for the study of audio-visual integration. Recently, it was shown that the McGurk effect is strongly stimulus-dependent, which complicates comparisons across perceivers and inferences across studies. To overcome this limitation, we developed the freely available Oldenburg audio-visual speech stimuli (OLAVS), consisting of 8 different talkers and 12 different syllable combinations. The quality of the OLAVS set was evaluated with 24 normal-hearing subjects. All 96 stimuli were characterized based on their stimulus disparity, which was obtained from a probabilistic model (cf. Magnotti & Beauchamp, 2015). Moreover, the McGurk effect was studied in eight adult cochlear implant (CI) users. By applying the individual, stimulus-independent parameters of the probabilistic model, the predicted effect of stronger audio-visual integration in CI users could be confirmed, demonstrating the validity of the new stimulus material.DOI: 10.3758/s13423-016-1148-9",pubmed,27562763,10.3758/s13423-016-1148-9
the conspicuous link between ear brain and heartcould neurotrophintreatment of agerelated hearing loss help prevent alzheimers disease and associated amyloid cardiomyopathy,"510. Biomolecules. 2021 Jun 17;11(6):900. doi: 10.3390/biom11060900.The Conspicuous Link between Ear, Brain and Heart-Could Neurotrophin-Treatment of Age-Related Hearing Loss Help Prevent Alzheimer's Disease and Associated Amyloid Cardiomyopathy?Shityakov S(1)(2), Hayashi K(3), Störk S(4), Scheper V(5), Lenarz T(5), Förster CY(1).Author information:(1)Department of Anaesthesiology, Intensive Care, Emergency and Pain Medicine, University Hospital Würzburg, D-97080 Würzburg, Germany.(2)Infochemistry Scientific Center, Laboratory of Chemoinformatics, ITMO University, 191002 Saint-Petersburg, Russia.(3)Advanced Stroke Center, Shimane University Hospital, 89-1 Enya, Shimane, Izumo 693-8501, Japan.(4)Comprehensive Heart Failure Q9 Center, University of Würzburg, D-97080 Würzburg, Germany.(5)Department of Otolaryngology, Hannover Medical School and Cluster of Excellence ""Hearing4All"", 30625 Hannover, Germany.Alzheimer's disease (AD), the most common cause of dementia in the elderly, is a neurodegenerative disorder associated with neurovascular dysfunction and cognitive decline. While the deposition of amyloid β peptide (Aβ) and the formation of neurofibrillary tangles (NFTs) are the pathological hallmarks of AD-affected brains, the majority of cases exhibits a combination of comorbidities that ultimately lead to multi-organ failure. Of particular interest, it can be demonstrated that Aβ pathology is present in the hearts of patients with AD, while the formation of NFT in the auditory system can be detected much earlier than the onset of symptoms. Progressive hearing impairment may beget social isolation and accelerate cognitive decline and increase the risk of developing dementia. The current review discusses the concept of a brain-ear-heart axis by which Aβ and NFT inhibition could be achieved through targeted supplementation of neurotrophic factors to the cochlea and the brain. Such amyloid inhibition might also indirectly affect amyloid accumulation in the heart, thus reducing the risk of developing AD-associated amyloid cardiomyopathy and cardiovascular disease.DOI: 10.3390/biom11060900PMCID: PMC8235707",pubmed,34204299,10.3390/biom11060900
special needs classroom assessment using a sign language communicator casc based on artificial intelligence ai techniques references,"This research focuses on deaf students in the United Arab Emirates. The proposed classroom assessment using sign language communicator (CASC) for special needs students (SN) in the United Arab Emirates is based on artificial intelligence (AI) tools. This research provides essential services for teaching evaluations, learning outcome assessments, and the development of learning environments. CASC model is composed of two models. The first model converts the speech to a sign language, which contains a speech recognizer, sign language recognizer. The second model converts the sign language to written text. This model generates a report for students' understanding and class evaluation in advance before ending the course based on the sign language recognition and image processing tools. This model will have a significantly positive impact on SN students' success and on effective lecturing and optimizing teaching and learning in the classroom. The accuracy of the model is 92%. The analysis of the student's feedback in real-time provides effective instructional strategies. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc22&AN=2023-29001-001
communicaton on mobile phone for the deaf using image recognition,"Sign language is an absolute thing that is mastered by some people, especially people who have hearing or deaf problems. Problems will occur if they want to communicate via mobile devices. They have to use an application that translate sign language to text and vice versa. The research in this field still open because each language will have different sign language. In the other hand the image that was captured by the mobile devices depend on the environment such as brightness and the background. The sign language captured with static background has high accuracy compared with dynamic background that has colorful background. Most of research do not include the dynamic background. Therefore in this study introduced an application used image recognition that can translate sign language with dynamic background into a text so that it can be used to communicate among people who have hearing problems and also between deaf and normal people. The experimental showed that accuracy of our system is 87%.",ieee,,10.1109/ICIMTech50083.2020.9211126
assessing closed captioning quality using a multilayer perceptron,"Closed Captioning (CC) is a telecommunication service to provide Deaf or Hard of Hearing (D/HOH) audiences the text equivalent of what hearing audiences experience in TV. The quality of CC is often interpreted as an accuracy and assessed in the empirical measure of counting number of errors. Although the regulators necessitate certain rules in the factors of CC quality, the D/HOH community members, who are the primary audiences to the CC, are not completely satisfied. One solution to solve this issue can be including the perspective of D/HOH audience in the assessment process. This research made an attempt to design an automatic quality assessment system for CC using artificial neural networks-multilayer perceptron trained from the D/HOH audience's subjective ratings, and the representative values extracted from the caption file, and the transcript file. As an initial stage of research, the trained model was then compared with other statistical regression models.",ieee,,10.1109/AIKE.2018.00010
transplantation of adiposederived stromal cells protects functional and morphological auditory nerve integrity in a model of cochlear implantation,"244. Neuroreport. 2021 Jun 9;32(9):776-782. doi: 10.1097/WNR.0000000000001651.Transplantation of adipose-derived stromal cells protects functional and morphological auditory nerve integrity in a model of cochlear implantation.Radeloff A(1)(2)(3), Nada N(4), El Mahallawi T(4), Kolkaila E(4), Vollmer M(5), Rak K(6), Hagen R(6), Schendzielorz P(6).Author information:(1)Division of Oto-Rhino-Laryngology, Head and Neck Surgery, Carl von Ossietzky-University.(2)Cluster of excellence ""Hearing 4 All"".(3)Research Center Neurosensory Science, Oldenburg, Germany.(4)Department of Oto-Rhino-Laryngology, Head and Neck Surgery, Tanta University Hospitals, Tanta, Egypt.(5)Department of Otol-Rhino-Laryngology, Head and Neck Surgery, University Magdeburg and Leibniz Institute for Neurobiology, Magdeburg.(6)Department of Oto-Rhino-Laryngology, Plastic, Aesthetic and Reconstructive Head and Neck Surgery, University of Würzburg, Germany.Cochlear implants are considered the gold standard therapy for subjects with severe hearing loss and deafness. Cochlear implants bypass the damaged hair cells and directly stimulate spiral ganglion neurons (SGNs) of the auditory nerve. Hence, the presence of functional SGNs is crucial for speech perception in electric hearing with a cochlear implant. In deaf individuals, SGNs progressively degenerate due to the lack of neurotrophic support, normally provided by sensory cells of the inner ear. Adipose-derived stromal cells (ASCs) are known to produce neurotrophic factors. In a guinea pig model of sensory hearing loss and cochlear implantation, ASCs were autologously transplanted into the scala tympani prior to insertion of a cochlear implant on one side. Electrically evoked auditory brain stem responses (eABR) were recorded 8 weeks after cochlear implantation. At conclusion of the experiment, the cochleae were histologically evaluated. Compared to untreated control animals, transplantation of ASCs resulted in an increased number of SGNs and their peripheral neurites. In ASC-transplanted animals, mean eABR thresholds were lower and suprathreshold amplitudes larger, suggesting a larger population of intact auditory nerve fibers. Moreover, when compared to controls, amplitude-level functions of eABRs in ASC transplanted animals demonstrated steeper slopes in response to increasing interphase gaps (IPGs), indicative of better functionality of the auditory nerve. In summary, results suggest that transplantation of autologous ASCs into the deaf inner ear may have protective effects on the survival of SGNs and their peripheral processes and may thus contribute to long-term benefits in speech discrimination performance in cochlear implant subjects.Copyright © 2021 Wolters Kluwer Health, Inc. All rights reserved.DOI: 10.1097/WNR.0000000000001651",pubmed,33994529,10.1097/WNR.0000000000001651
processing of auditory information in forebrain regions after hearing loss in adulthood behavioral and electrophysiological studies in a rat model,"636. Front Neurosci. 2022 Nov 10;16:966568. doi: 10.3389/fnins.2022.966568. eCollection 2022.Processing of auditory information in forebrain regions after hearing loss in adulthood: Behavioral and electrophysiological studies in a rat model.Johne M(1)(2), Helgers SOA(1), Alam M(1), Jelinek J(1), Hubka P(2)(3)(4), Krauss JK(1), Scheper V(2)(5), Kral A(2)(3)(4), Schwabe K(1)(2).Author information:(1)Department of Neurosurgery, Hannover Medical School, Hanover, Germany.(2)Cluster of Excellence Hearing4all, German Research Foundation, Hanover, Germany.(3)Hannover Medical School, Institute of Audioneurotechnology, Hanover, Germany.(4)Department of Experimental Otology of the ENT Clinics, Hannover Medical School, Hanover, Germany.(5)Department of Otolaryngology, Hannover Medical School, Hanover, Germany.BACKGROUND: Hearing loss was proposed as a factor affecting development of cognitive impairment in elderly. Deficits cannot be explained primarily by dysfunctional neuronal networks within the central auditory system. We here tested the impact of hearing loss in adult rats on motor, social, and cognitive function. Furthermore, potential changes in the neuronal activity in the medial prefrontal cortex (mPFC) and the inferior colliculus (IC) were evaluated.MATERIALS AND METHODS: In adult male Sprague Dawley rats hearing loss was induced under general anesthesia with intracochlear injection of neomycin. Sham-operated and naive rats served as controls. Postsurgical acoustically evoked auditory brainstem response (ABR)-measurements verified hearing loss after intracochlear neomycin-injection, respectively, intact hearing in sham-operated and naive controls. In intervals of 8 weeks and up to 12 months after surgery rats were tested for locomotor activity (open field) and coordination (Rotarod), for social interaction and preference, and for learning and memory (4-arms baited 8-arms radial maze test). In a final setting, electrophysiological recordings were performed in the mPFC and the IC.RESULTS: Locomotor activity did not differ between deaf and control rats, whereas motor coordination on the Rotarod was disturbed in deaf rats (P < 0.05). Learning the concept of the radial maze test was initially disturbed in deaf rats (P < 0.05), whereas retesting every 8 weeks did not show long-term memory deficits. Social interaction and preference was also not affected by hearing loss. Final electrophysiological recordings in anesthetized rats revealed reduced firing rates, enhanced irregular firing, and reduced oscillatory theta band activity (4-8 Hz) in the mPFC of deaf rats as compared to controls (P < 0.05). In the IC, reduced oscillatory theta (4-8 Hz) and gamma (30-100 Hz) band activity was found in deaf rats (P < 0.05).CONCLUSION: Minor and transient behavioral deficits do not confirm direct impact of long-term hearing loss on cognitive function in rats. However, the altered neuronal activities in the mPFC and IC after hearing loss indicate effects on neuronal networks in and outside the central auditory system with potential consequences on cognitive function.Copyright © 2022 Johne, Helgers, Alam, Jelinek, Hubka, Krauss, Scheper, Kral and Schwabe.DOI: 10.3389/fnins.2022.966568PMCID: PMC9684731",pubmed,36440269,10.3389/fnins.2022.966568
changes in vowel quality in postlingually deafened cochlear implant users,"489. Audiology. 1997 Sep-Oct;36(5):279-97. doi: 10.3109/00206099709071980.Changes in vowel quality in post-lingually deafened cochlear implant users.Langereis MC(1), Bosman AJ, van Olphen AF, Smoorenburg GF.Author information:(1)Department of Otorhinolaryngology, University Hospital, Utrecht, The Netherlands.The present study addresses the effect of cochlear implantation on vowel production of 20 post-lingually deafened Dutch subjects. All subjects received the Nucleus 22 implant (3 WSP and 17 MSP processors). Speech recordings were made pre-implantation and three and twelve months post-implantation with the implant switched on and off. The first and second formant frequencies were measured for eleven Dutch vowels (monophthongs only) in an h-vowel-t context. Twelve months post-implantation, the results showed an increase in the ranges of the first and second formant frequency covered by the respective vowels when the implant was switched on. The increase in the formant frequency range was most marked for some subjects with a relatively small formant range pre-implantation. Also, at 12 months post-implantation with the implant switched on we found a significant shift of the first and second formant frequency towards the normative values. Moreover, at this time the results showed significantly increased clustering of the respective vowels, suggesting an improvement in the ability to produce phonological contrasts between vowels. Clustering is defined as the ratio of the between-vowel variance of the first and second formant frequency and the within-vowel variance of three tokens of the same vowel.DOI: 10.3109/00206099709071980",pubmed,9305524,10.3109/00206099709071980
panel 1 biotechnology biomedical engineering and new models of otitis media,"Objective: To summarize recently published key articles on the topics of biomedical engineering, biotechnology and new models in relation to otitis media (OM). Data sources: Electronic databases: PubMed, Ovid Medline, Cochrane Library and Clinical Evidence (BMJ Publishing). Review methods: Articles on biomedical engineering, biotechnology, material science, mechanical and animal models in OM published between May 2015 and May 2019 were identified and subjected to review. A total of 132 articles were ultimately included. Results: New imaging technologies for the tympanic membrane (TM) and the middle ear cavity are being developed to assess TM thickness, identify biofilms and differentiate types of middle ear effusions. Artificial intelligence (AI) has been applied to train software programs to diagnose OM with a high degree of certainty. Genetically modified mice models for OM have further investigated what predisposes some individuals to OM and consequent hearing loss. New vaccine candidates protecting against major otopathogens are being explored and developed, especially combined vaccines, targeting more than one pathogen. Transcutaneous vaccination against non-typeable Haemophilus influenzae has been successfully tried in a chinchilla model. In terms of treatment, novel technologies for trans-tympanic drug delivery are entering the clinical domain. Various growth factors and grafting materials aimed at improving healing of TM perforations show promising results in animal models. Conclusion: New technologies and AI applications to improve the diagnosis of OM have shown promise in pre-clinical models and are gradually entering the clinical domain. So are novel vaccines and drug delivery approaches that may allow local treatment of OM. IMPLICATIONS FOR PRACTICE: New diagnostic methods, potential vaccine candidates and the novel trans-tympanic drug delivery show promising results, but are not yet adapted to clinical use. © 2019 Elsevier B.V.",scopus,2-s2.0-85077165747,10.1016/j.ijporl.2019.109833
reading achievement of deaf students challenging the fourth grade ceiling,"588. J Deaf Stud Deaf Educ. 2021 Jun 14;26(3):427-437. doi: 10.1093/deafed/enab013.Reading Achievement of Deaf Students: Challenging the Fourth Grade Ceiling.Mayer C(1), Trezek BJ(2), Hancock GR(3).Author information:(1)Faculty of Education, York University, Toronto, ON, Canada.(2)Department of Rehabilitation Psychology and Special Education, University of Wisconsin-Madison, Madison, WI, USA.(3)Department of Human Development and Quantitative Methodology, University of Maryland, College Park, MD, USA.Historically it has been reported that deaf students do not achieve age-appropriate outcomes in reading, with this performance often being characterized in terms of a fourth grade ceiling. However, given the shifts in the field during the past 20 years (e.g., widespread implementation of newborn hearing screening, advances in hearing technologies), it would be timely to question whether this continues to serve as a meaningful benchmark. To this end, the purpose of this study was to investigate reading outcomes of a Canadian cohort of school-aged deaf learners (N = 70) who all used listening and spoken language as the primary mode of communication. Specifically, the goal was to establish whether their achievement approached that of their hearing age peers and to identify demographic factors influencing performance (i.e., gender, unilateral/bilateral hearing loss, personal amplification, level of auditory functioning, grade placement, additional disabilities, home language). Results indicate that participants obtained standard scores in the average range on both the Basic Reading and Reading Comprehension clusters of the Woodcock Johnson III-Diagnostic Reading Battery (Woodcock et al., 2004), surpassing the fourth grade reading achievement ceiling often reported for this population.© The Author(s) 2021. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.DOI: 10.1093/deafed/enab013",pubmed,34060625,10.1093/deafed/enab013
corticalbrainstem interplay during speech perception in older adults with and without hearing loss,"564. Front Neurosci. 2023 Feb 2;17:1075368. doi: 10.3389/fnins.2023.1075368. eCollection 2023.Cortical-brainstem interplay during speech perception in older adults with and without hearing loss.Lai J(1)(2)(3), Alain C(4)(5), Bidelman GM(1)(2)(6)(7).Author information:(1)Institute for Intelligent Systems, University of Memphis, Memphis, TN, United States.(2)School of Communication Sciences and Disorders, University of Memphis, Memphis, TN, United States.(3)Department of Diagnostic Imaging, St. Jude Children's Research Hospital, Memphis, TN, United States.(4)Rotman Research Institute, Baycrest Centre for Geriatric Care, Toronto, ON, Canada.(5)Department of Psychology, University of Toronto, Toronto, ON, Canada.(6)Department of Speech, Language, and Hearing Sciences, Indiana University, Bloomington, IN, United States.(7)Program in Neuroscience, Indiana University, Bloomington, IN, United States.INTRODUCTION: Real time modulation of brainstem frequency-following responses (FFRs) by online changes in cortical arousal state via the corticofugal (top-down) pathway has been demonstrated previously in young adults and is more prominent in the presence of background noise. FFRs during high cortical arousal states also have a stronger relationship with speech perception. Aging is associated with increased auditory brain responses, which might reflect degraded inhibitory processing within the peripheral and ascending pathways, or changes in attentional control regulation via descending auditory pathways. Here, we tested the hypothesis that online corticofugal interplay is impacted by age-related hearing loss.METHODS: We measured EEG in older adults with normal-hearing (NH) and mild to moderate hearing-loss (HL) while they performed speech identification tasks in different noise backgrounds. We measured α power to index online cortical arousal states during task engagement. Subsequently, we split brainstem speech-FFRs, on a trial-by-trial basis, according to fluctuations in concomitant cortical α power into low or high α FFRs to index cortical-brainstem modulation.RESULTS: We found cortical α power was smaller in the HL than the NH group. In NH listeners, α-FFRs modulation for clear speech (i.e., without noise) also resembled that previously observed in younger adults for speech in noise. Cortical-brainstem modulation was further diminished in HL older adults in the clear condition and by noise in NH older adults. Machine learning classification showed low α FFR frequency spectra yielded higher accuracy for classifying listeners' perceptual performance in both NH and HL participants. Moreover, low α FFRs decreased with increased hearing thresholds at 0.5-2 kHz for clear speech but noise generally reduced low α FFRs in the HL group.DISCUSSION: Collectively, our study reveals cortical arousal state actively shapes brainstem speech representations and provides a potential new mechanism for older listeners' difficulties perceiving speech in cocktail party-like listening situations in the form of a miss-coordination between cortical and subcortical levels of auditory processing.Copyright © 2023 Lai, Alain and Bidelman.DOI: 10.3389/fnins.2023.1075368PMCID: PMC9932544",pubmed,36816123,10.3389/fnins.2023.1075368
amygdala hyperactivity and tonotopic shift after salicylate exposure,"799. Brain Res. 2012 Nov 16;1485:63-76. doi: 10.1016/j.brainres.2012.03.016. Epub 2012 Mar 13.Amygdala hyperactivity and tonotopic shift after salicylate exposure.Chen GD(1), Manohar S, Salvi R.Author information:(1)Center for Hearing and Deafness, State University of New York at Buffalo, 137 Cary Hall, 3435 Main Street, Buffalo, NY 14214, USA. gchen7@buffalo.eduThe amygdala, important in forming and storing memories of aversive events, is believed to play a major role in debilitating tinnitus and hyperacusis. To explore this hypothesis, we recorded from the lateral amygdala (LA) and auditory cortex (AC) before and after treating rats with a dose of salicylate that induces tinnitus and hyperacusis-like behavior. Salicylate unexpectedly increased the amplitude of the local field potential (LFP) in the LA making it hyperactive to sounds≥60 dB SPL. Frequency receptive fields (FRFs) of multiunit (MU) clusters in the LA were also dramatically altered by salicylate. Neuronal activity at frequencies below 10 kHz and above 20 kHz was depressed at low intensities, but was greatly enhanced for stimuli between 10 and 20 kHz (frequencies near the pitch of the salicylate-induced tinnitus in the rat). These frequency-dependent changes caused the FRF of many LA neurons to migrate towards 10-20 kHz thereby amplifying activity from this region. To determine if salicylate-induced changes restricted to the LA would remotely affect neural activity in the AC, we used a micropipette to infuse salicylate (20 μl, 2.8 mM) into the amygdala. Local delivery of salicylate to the amygdala significantly increased the amplitude of the LFP recorded in the AC and selectively enhanced the neuronal activity of AC neurons at the mid-frequencies (10-20 kHz), frequencies associated with the tinnitus pitch. Taken together, these results indicate that systemic salicylate treatment can induce hyperactivity and tonotopic shift in the amygdala and infusion of salicylate into the amygdala can profoundly enhance sound-evoked activity in AC, changes likely to increase the perception and emotional salience of tinnitus and loud sounds. This article is part of a Special Issue entitled: Tinnitus Neuroscience.Copyright © 2012 Elsevier B.V. All rights reserved.DOI: 10.1016/j.brainres.2012.03.016PMCID: PMC5319430",pubmed,22464181,10.1016/j.brainres.2012.03.016
vestibular dysgenesis in mice lacking abr and bcr cdc42racgaps,"The inner ear develops from a simple epithelium (otic placode) into the complex structures specialized for balance (vestibule) and sound (cochlea) detection. Abnormal vestibular and cochlear development is associated with many birth defects. During recent years, considerable progress has been made in understanding the molecular bases of these conditions. To determine the biological function of two closely related GTPase activating proteins for the Cdc42/Rac GTPases, Abr and Bcr, we generated a mouse strain deficient in both of these proteins. Double null mutant mice exhibit hyperactivity, persistent circling, and are unable to swim. These phenotypes are typically found in mice with vestibular defects. Indeed, adult double null mutants display abnormal dysmorphic structures of both the saccule and utricle. Moreover, a total loss of otoconia can be seen in the utricle, whereas in the saccule, otoconia are either missing or their number is drastically decreased and they are abnormally large. Interestingly, both the cochlea and semicircular canals are normal and the double null mutant mice are not deaf. These data demonstrate that Abr and Bcr play important complementary roles during vestibular morphogenesis and that a function of Cdc42/RacGAPs and, therefore, that of the small Rho-related GTPases is critically important for balance and motor coordination. © 2002 Wiley-Liss, Inc.",scopus,2-s2.0-0036203487,10.1002/dvdy.10071
computerassisted ci fitting is the learning capacity of the intelligent agent fox beneficial for speech understanding,"315. Cochlear Implants Int. 2017 Jul;18(4):198-206. doi: 10.1080/14670100.2017.1325093. Epub 2017 May 12.Computer-assisted CI fitting: Is the learning capacity of the intelligent agent FOX beneficial for speech understanding?Meeuws M(1), Pascoal D(1), Bermejo I(1), Artaso M(1), De Ceulaer G(1), Govaerts PJ(1).Author information:(1)a The Eargroup , Herentalsebaan 75, 2100 Antwerp-Deurne, Belgium.OBJECTIVE: The software application FOX ('Fitting to Outcome eXpert') is an intelligent agent to assist in the programing of cochlear implant (CI) processors. The current version utilizes a mixture of deterministic and probabilistic logic which is able to improve over time through a learning effect. This study aimed at assessing whether this learning capacity yields measurable improvements in speech understanding.METHODS: A retrospective study was performed on 25 consecutive CI recipients with a median CI use experience of 10 years who came for their annual CI follow-up fitting session. All subjects were assessed by means of speech audiometry with open set monosyllables at 40, 55, 70, and 85 dB SPL in quiet with their home MAP. Other psychoacoustic tests were executed depending on the audiologist's clinical judgment. The home MAP and the corresponding test results were entered into FOX. If FOX suggested to make MAP changes, they were implemented and another speech audiometry was performed with the new MAP.RESULTS: FOX suggested MAP changes in 21 subjects (84%). The within-subject comparison showed a significant median improvement of 10, 3, 1, and 7% at 40, 55, 70, and 85 dB SPL, respectively. All but two subjects showed an instantaneous improvement in their mean speech audiometric score.DISCUSSION: Persons with long-term CI use, who received a FOX-assisted CI fitting at least 6 months ago, display improved speech understanding after MAP modifications, as recommended by the current version of FOX. This can be explained only by intrinsic improvements in FOX's algorithms, as they have resulted from learning. This learning is an inherent feature of artificial intelligence and it may yield measurable benefit in speech understanding even in long-term CI recipients.DOI: 10.1080/14670100.2017.1325093",pubmed,28498083,10.1080/14670100.2017.1325093
neuroplasticity after cochlear implantation as assessed by the plasma level of mmp9 and its genetic polymorphisms a prospective study of congenitally deaf children,"Background: If it was possible to assay biomarkers of neuroplasticity it might facilitate clinical management of deaf implanted children by identifying those among them who are at risk of speech and language rehabilitation failure. MMP9 is a proteinase involved in neuroplasticity underlying different clinical conditions in human. Material and methods: This was a longitudinal, prospective cohort study of 61 congenitally deaf children who underwent cochlear implantation. We investigated the genetic variants of matrix metalloproteinase 9 (MMP9) and plasma levels of MMP-9 that have been implicated in neuroplasticity after cochlear implantation. Auditory development was assessed by using the LittlEARS Questionnaire (LEAQ) at three follow-up points and the plasma level of MMP-9 was measured at implantation. Results: There was a significant negative correlation between MMP-9 plasma level at implantation and LEAQ score at 18 months follow-up (p < 0.05). Two clusters of good and poor CI performers could be isolated based on this correlation. The prevalence of genetic variants of MMP9 - rs3918242, rs20544, and rs2234681 - in the good performers cluster was different to their prevalence in the poor performers cluster. Conclusions: The study showed that children born deaf who have an MMP-9 plasma level of less than 150 ng/ml at cochlear implantation have a reasonable chance of attaining a high LEAQ score after 18 months of speech and language rehabilitation. It appears that MMP-9 plasma level at cochlear implantation is a promising prognostic marker for CI outcome.",cinahl,2083389X,10.17430/JHS.2022.12.3.5
effect of hearing aid directionality and remote microphone on speech intelligibility in complex listening situations,"135. Trends Hear. 2018 Jan-Dec;22:2331216518804945. doi: 10.1177/2331216518804945.Effect of Hearing Aid Directionality and Remote Microphone on Speech Intelligibility in Complex Listening Situations.Wagener KC(1)(2), Vormann M(1)(2), Latzel M(3), Mülder HE(4).Author information:(1)1 Hörzentrum Oldenburg GmbH, Germany.(2)2 Cluster of Excellence Hearing4all, Oldenburg, Germany.(3)3 Sonova AG, Stäfa, Switzerland.(4)4 Phonak Communications, Murten, Switzerland.Remote microphones (RMs) have been developed to support hearing aid (HA) users in understanding distant talkers. In traditional clinical applications, a drawback of these systems is the deteriorated speech intelligibility in the near field. This study investigates advantages and disadvantages of clinical RM usage and the effects of different directionality settings of the HAs in complex listening situations in the laboratory. Speech intelligibility was investigated in 15 experienced severely hearing impaired participants in a noisy environment using a dual-task test paradigm where the tasks were presented from either a near field or a far field loudspeaker. Primary and secondary tasks were presented simultaneously so attention had to be shared on both tasks. In a second experiment, two speech intelligibility tests were presented from either the near field or the far field loudspeaker. The tests were interleaved to simulate a complex listening situation with shifting attention. Directional HA microphones yielded better performance than omnidirectional microphones (both combined with a RM) in near field when analyzing both tasks of the dual-task experiment separately. Furthermore, the integrated dual-task test results showed better performance with directional HA microphones compared with the omnidirectional setting (both cases in combination with a RM). These findings were confirmed by the results of the interleaved speech intelligibility test.DOI: 10.1177/2331216518804945PMCID: PMC6194921",pubmed,30322342,10.1177/2331216518804945
audiological outcomes of robotassisted cochlear implant surgery,"95. Eur Arch Otorhinolaryngol. 2023 Oct;280(10):4433-4444. doi: 10.1007/s00405-023-07961-7. Epub 2023 Apr 12.Audiological outcomes of robot-assisted cochlear implant surgery.Heuninck E(1), Van de Heyning P(2)(3), Van Rompaey V(2)(3), Mertens G(2)(3), Topsakal V(4).Author information:(1)Department of Otorhinolaryngology Head and Neck Surgery, University Hospital Brussels, Vrije Universiteit Brussel, Brussels Health Campus, Brussels, Belgium. emilie.heuninck@uzbrussel.be.(2)Department of Otorhinolaryngology, Head and Neck Surgery, Antwerp University Hospital, Antwerp, Belgium.(3)Experimental Laboratory of Translational Neurosciences and Dento-Otolaryngology, Faculty of Medicine and Health Sciences, University of Antwerp, Antwerp, Belgium.(4)Department of Otorhinolaryngology Head and Neck Surgery, University Hospital Brussels, Vrije Universiteit Brussel, Brussels Health Campus, Brussels, Belgium.PURPOSE: The main objective of this study is to evaluate the short-term and long-term audiological outcomes in patients who underwent cochlear implantation with a robot-assisted system to enable access to the cochlea, and to compare outcomes with a matched control group of patients who underwent cochlear implantation with conventional access to the cochlea.METHODS: In total, 23 patients were implanted by robot-assisted cochlear implant surgery (RACIS). To evaluate the effectiveness of robotic surgery in terms of audiological outcomes, a statistically balanced control group of conventionally implanted patients was created. Minimal outcome measures (MOM), consisting of pure-tone audiometry, speech understanding in quiet and speech understanding in noise were performed pre-operatively and at 3 months, 6 months, 12 months and 2 years post-activation of the audioprocessor.RESULTS: There was no statistically significant difference in pure-tone audiometry, speech perception in quiet and speech perception in noise between robotically implanted and conventionally implanted patients pre-operatively, 3 months, 6 months, 12 months and 2 years post-activation. A significant improvement in pure-tone hearing thresholds, speech understanding in quiet and speech understanding in noise with the cochlear implant has been quantified as of the first measurements at 3 months and this significant improvement remained stable over a time period of 2 years for HEARO implanted patients.CONCLUSION: Clinical outcomes in robot-assisted cochlear implant surgery are comparable to conventional cochlear implantation. CLINICALTRAILS.GOV TRAIL REGISTRATION NUMBERS: NCT03746613 (date of registration: 19/11/2018), NCT04102215 (date of registration: 25/09/2019).© 2023. The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.DOI: 10.1007/s00405-023-07961-7",pubmed,37043021,10.1007/s00405-023-07961-7
can visualization of internal articulators support speech perception,"This paper describes the contribution to speech perception given by animations of intra-oral articulations. 18 subjects were asked to identify the words in acoustically degraded sentences in three different presentation modes: acoustic signal only, audiovisual with a front view of a synthetic face and an audiovisual with both front face view and a side view, where tongue movements were visible by making parts of the cheek transparent. The augmented reality side-view did not help subjects perform better overall than with the front view only, but it seems to have been beneficial for the perception of palatal plosives, liquids and rhotics, especially in clusters. The results indicate that it cannot be expected that intra-oral animations support speech perception in general, but that information on some articulatory features can be extracted. Animations of tongue movements have hence more potential for use in computer-assisted pronunciation and perception training than as a communication aid for the hearing-impaired. Copyright © 2008 ISCA.",scopus,2-s2.0-84867227459,
ltype calcium channels in the auditory system ltypkalziumkanle im hrsystem,"The voltage-activated L-type calcium channels Cay1.2 and Cay1.3 mediate Ca2+influx into neurons at the soma or at dendrites, whereas they are not observed at the presynapse. Surprisingly, in the inner ear, Cay1.3 is indispensable for signal transmission from the cochlear inner hair cells to the postsynaptic auditory nerve fibers. Due to Cav1.3 channel clustering at ribbons, i.e. specific presynaptic structures of the hair cells, they promote Ca2+influx which triggers calcium-dependent fusion of synaptic vesicles with the plasma membrane. Consequently, mutations in Cacnald, the gene that encodes Cay1.3, cause deafness. Additionally, Cay1.3 plays an important part in the central auditory system. Lack of the channel results in severe changes in auditory pathway cytoarchitecture and in abnormal electrophysiological performance of auditory neurons. Furthermore, developmental refinement of tonotopic inhibitory projections in sound localization circuits is disrupted. These aberrations are associated with abnormal sound processing in the auditory pathway. Cacnald therefore represents a prototypal deafness associated gene, in which mutations result in both peripheral and central auditory deficiencies. This, in turn, has implications for auditory rehabilitation using cochlear implants which address only peripheral dysfunctions. Exploratory research into the closely related Cay1.2 isoform points to an important role of this channel in acoustic trauma. Cay1.2 is mainly expressed in the auditory nerve, but apparently not essential for normal auditory function. Rather, loss of function of the channel does influence the effects of traumatic noise exposure. Loss of this channel induced by noise trauma results in reduced auditory threshold increase. This phenomenon points to the fact that Cay1.2-mediated Ca2+ influx is involved in noise trauma induced damage. Deeper insight into this function might result in new therapeutic approaches. © Springer Verlag 2014.",scopus,2-s2.0-84908086285,
cochlear implantation in an animal model documents cochlear damage at the tip of the implant,"800. Braz J Otorhinolaryngol. 2022 Jul-Aug;88(4):546-555. doi: 10.1016/j.bjorl.2020.07.017. Epub 2020 Sep 20.Cochlear implantation in an animal model documents cochlear damage at the tip of the implant.Andrade JSC(1), Baumhoff P(2), Cruz OLM(3), Lenarz T(2), Kral A(2).Author information:(1)Universidade Federal de São Paulo (UNIFESP), Departamento de Otorrinolaringologia e Cirurgia de Cabeça e Pescoço, São Paulo, SP, Brazil; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (Capes), Brasília, DF, Brazil; Institute of Audioneurotechnology (VIANNA) & Department of Experimental Otology, Department of Otolaryngology, Medical University Hannover, Hannover, Germany. Electronic address: zedmed@gmail.com.(2)Institute of Audioneurotechnology (VIANNA) & Department of Experimental Otology, Department of Otolaryngology, Medical University Hannover, Hannover, Germany; Cluster of Excellence ""Hearing4all"", Hannover, Germany.(3)Universidade Federal de São Paulo (UNIFESP), Departamento de Otorrinolaringologia e Cirurgia de Cabeça e Pescoço, São Paulo, SP, Brazil; Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (Capes), Brasília, DF, Brazil.INTRODUCTION: Electrocochleography has recently emerged as a diagnostic tool in cochlear implant surgery, purposing hearing preservation and optimal electrode positioning.OBJECTIVE: In this experimental study, extra-cochlear potentials were obtained during cochlear implant surgery in guinea pigs. The aim was to determine electrophysiological changes indicating cochlear trauma after cochleostomy and after electrode implantation in different insertion depths.METHODS: Normal-hearing guinea pigs (n = 14) were implanted uni- or bilaterally with a multichannel electrode. The extra-cochlear cochlear nerve action potentials were obtained in response to acoustic stimuli at specific frequencies before and after cochleostomy, and after introduction of the electrode bundle. After the electrophysiological experiments, the guinea pigs were euthanized and microtomography was performed, in order to determine the position of the electrode and to calculate of the depth of insertion. Based on the changes of amplitude and thresholds in relation to the stimulus frequency, the electrophysiological data and the position obtained by the microtomography reconstruction were compared.RESULTS: Cochleostomy promoted a small electrophysiological impact, while electrode insertion caused changes in the amplitude of extra-cochlear electrophysiological potentials over a wide range of frequencies, especially in the deepest insertions. There was, however, preservation of the electrical response to low frequency stimuli in most cases, indicating a limited auditory impact in the intraoperative evaluation. The mean insertion depth of the apical electrodes was 5339.56 μm (±306.45 - 6 inserted contacts) and 4447.75 μm (±290.23 - 5 inserted contacts).CONCLUSIONS: The main electrophysiological changes observed during surgical procedures occurred during implantation of the electrode, especially the deepest insertions, whereas the cochleostomy disturbed the potentials to a lesser extent. While hearing loss was often observed apical to the cochlear implant, it was possible to preserve low frequencies after insertion.Copyright © 2020 Associação Brasileira de Otorrinolaringologia e Cirurgia Cérvico-Facial. Published by Elsevier Editora Ltda. All rights reserved.DOI: 10.1016/j.bjorl.2020.07.017PMCID: PMC9422412",pubmed,33039317,10.1016/j.bjorl.2020.07.017
automated lip reading potential to enhance accessibility in xr,"Existing research indicates that immersive technology will greatly influence the pedagogical practices in the coming years. Due to the visual nature of the XR (eXtended Reality) applications, lack of auditory support systems and high dependence on physical gestures and movements makes immersive tech inaccessible for people with special needs. However, the increasing influence and rise of the Metaverse has instigated an urgency to make XR environments accessible from the vulnerable segments of the society. Deaf and Hard of Hearing (DHH) participants often face social challenges such as illiteracy, unemployment, lack of growth in personal and professional life and social isolation. Recent advancements in Machine Learning have opened opportunities to perform Automated Lip Reading (ALR), which could be used as an assistive tool by DHH people within the Metaverse. This work in progress aspires to contribute towards the implementation of ALR systems in XR educational environments, and provide an ethically aligned design and standardization in its implementation, with ultimate aim to enhance accessibility measures in immersive technologies.  © 2023 Owner/Author.",scopus,2-s2.0-85183330317,10.1145/3633083.3633191
an eventrelated brain potential study of auditory attention in cochlear implant users,"786. Clin Neurophysiol. 2021 Sep;132(9):2290-2305. doi: 10.1016/j.clinph.2021.03.055. Epub 2021 May 20.An event-related brain potential study of auditory attention in cochlear implant users.Schierholz I(1), Schönermark C(2), Ruigendijk E(3), Kral A(4), Kopp B(5), Büchner A(6).Author information:(1)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany; Cluster of Excellence ""Hearing4all"", Germany; Department of Otorhinolaryngology, University of Cologne, Cologne, Germany. Electronic address: irina.schierholz@uk-koeln.de.(2)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany.(3)Cluster of Excellence ""Hearing4all"", Germany; Department for Dutch Studies, Carl von Ossietzky University Oldenburg, Oldenburg, Germany.(4)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany; Cluster of Excellence ""Hearing4all"", Germany; Department of Experimental Otology, Institute for AudioNeuroTechnology (VIANNA), Hannover Medical School, Hannover, Germany.(5)Department of Neurology, Hannover Medical School, Hannover, Germany.(6)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany; Cluster of Excellence ""Hearing4all"", Germany.Comment in    Clin Neurophysiol. 2021 Sep;132(9):2257-2258.OBJECTIVE: Cochlear implants (CIs) provide access to the auditory world for deaf individuals. We investigated whether CIs enforce attentional alterations of auditory cortical processing in post-lingually deafened CI users compared to normal-hearing (NH) controls.METHODS: Event-related potentials (ERPs) were recorded in 40 post-lingually deafened CI users and in a group of 40 NH controls using an auditory three-stimulus oddball task, which included frequent standard tones (Standards) and infrequent deviant tones (Targets), as well as infrequently occurring unique sounds (Novels). Participants were exposed twice to the three-stimulus oddball task, once under the instruction to ignore the stimuli (ignore condition), and once under the instruction to respond to infrequently occurring deviant tones (attend condition).RESULTS: The allocation of attention to auditory oddball stimuli exerted stronger effects on N1 amplitudes at posterior electrodes in response to Standards and to Targets in CI users than in NH controls. Other ERP amplitudes showed similar attentional modulations in both groups (P2 in response to Standards, N2 in response to Targets and Novels, P3 in response to Targets). We also observed a statistical trend for an attenuated attentional modulation of Novelty P3 amplitudes in CI users compared to NH controls.CONCLUSIONS: ERP correlates of enhanced CI-mediated auditory attention are confined to the latency range of the auditory N1, suggesting that enhanced attentional modulation during auditory stimulus discrimination occurs primarily in associative auditory cortices of CI users.SIGNIFICANCE: The present ERP data support the hypothesis of attentional alterations of auditory cortical processing in CI users. These findings may be of clinical relevance for the CI rehabilitation.Copyright © 2021 International Federation of Clinical Neurophysiology. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.clinph.2021.03.055",pubmed,34120838,10.1016/j.clinph.2021.03.055
auditory environment across the life span of cochlear implant users insights from data logging,"320. J Speech Lang Hear Res. 2017 May 24;60(5):1362-1377. doi: 10.1044/2016_JSLHR-H-16-0162.Auditory Environment Across the Life Span of Cochlear Implant Users: Insights From Data Logging.Busch T(1), Vanpoucke F(2), van Wieringen A(3).Author information:(1)KU Leuven, BelgiumCochlear Technology Centre, Mechelen, Belgium.(2)Cochlear Technology Centre, Mechelen, Belgium.(3)KU Leuven, Belgium.PURPOSE: We describe the natural auditory environment of people with cochlear implants (CIs), how it changes across the life span, and how it varies between individuals.METHOD: We performed a retrospective cross-sectional analysis of Cochlear Nucleus 6 CI sound-processor data logs. The logs were obtained from 1,501 people with CIs (ages 0-96 years). They covered over 2.4 million hr of implant use and indicated how much time the CI users had spent in various acoustical environments. We investigated exposure to spoken language, noise, music, and quiet, and analyzed variation between age groups, users, and countries.RESULTS: CI users spent a substantial part of their daily life in noisy environments. As a consequence, most speech was presented in background noise. We found significant differences between age groups for all auditory scenes. Yet even within the same age group and country, variability between individuals was substantial.CONCLUSIONS: Regardless of their age, people with CIs face challenging acoustical environments in their daily life. Our results underline the importance of supporting them with assistive listening technology. Moreover, we found large differences between individuals' auditory diets that might contribute to differences in rehabilitation outcomes. Their causes and effects should be investigated further.DOI: 10.1044/2016_JSLHR-H-16-0162",pubmed,28418532,10.1044/2016_JSLHR-H-16-0162
phenotypic expansion of kmt2drelated disorder beyond kabuki syndrome,"Pathogenic variants in KMT2D, which encodes lysine specific methyltransferase 2D, cause autosomal dominant Kabuki syndrome, associated with distinctive dysmorphic features including arched eyebrows, long palpebral fissures with eversion of the lower lid, large protuberant ears, and fetal finger pads. Most disease-causing variants identified to date are putative loss-of-function alleles, although 15–20% of cases are attributed to missense variants. We describe here four patients (including one previously published patient) with de novo KMT2D missense variants and with shared but unusual clinical findings not typically seen in Kabuki syndrome, including athelia (absent nipples), choanal atresia, hypoparathyroidism, delayed or absent pubertal development, and extreme short stature. These individuals also lack the typical dysmorphic facial features found in Kabuki syndrome. Two of the four patients had severe interstitial lung disease. All of these variants cluster within a 40-amino-acid region of the protein that is located just N-terminal of an annotated coiled coil domain. These findings significantly expand the phenotypic spectrum of features associated with variants in KMT2D beyond those seen in Kabuki syndrome and suggest a possible new underlying disease mechanism for these patients. © 2020 Wiley Periodicals, Inc.",scopus,2-s2.0-85083907753,10.1002/ajmg.a.61518
measuring disability in population based surveys the interrelationship between clinical impairments and reported functional limitations in cameroon and india,"575. PLoS One. 2016 Oct 14;11(10):e0164470. doi: 10.1371/journal.pone.0164470. eCollection 2016.Measuring Disability in Population Based Surveys: The Interrelationship between Clinical Impairments and Reported Functional Limitations in Cameroon and India.Mactaggart I(1), Kuper H(1), Murthy GV(2), Oye J(3), Polack S(1).Author information:(1)International Centre for Evidence in Disability, London School of Hygiene & Tropical Medicine, London, United Kingdom.(2)Indian Institute of Public Health, Hyderabad, India.(3)Sightsavers Cameroon, Yaoundé, Cameroon.PURPOSE: To investigate the relationship between two distinct measures of disability: self-reported functional limitations and objectively-screened clinical impairments.METHODS: We undertook an all age population-based survey of disability in two areas: North-West Cameroon (August/October 2013) and Telangana State, India (Feb/April 2014). Participants were selected for inclusion via two-stage cluster randomised sampling (probability proportionate to size cluster selection and compact segment sampling within clusters). Disability was defined as the presence of self-reported functional limitations across eight domains, or presence of moderate or greater clinical impairments. Clinical impairment screening comprised of visual acuity testing for vision impairment, pure tone audiometry for hearing impairment, musculoskeletal functioning assessment for musculoskeletal impairment, reported seizure history for epilepsy and reported symptoms of clinical depression (depression adults only). Information was collected using structured questionnaires, observations and examinations.RESULTS: Self-reported disability prevalence was 5.9% (95% CI 4.7-7.4) and 7.5% (5.9-9.4) in Cameroon and India respectively. The prevalence of moderate or greater clinical impairments in the same populations were 8.4% (7.5-9.4) in Cameroon and 10.5% (9.4-11.7) in India. Overall disability prevalence (self-report and/or screened positive to a moderate or greater clinical impairment) was 10.5% in Cameroon and 12.2% in India, with limited overlap between the sub-populations identified using the two types of tools. 33% of participants in Cameroon identified to have a disability, and 45% in India, both reported functional limitations and screened positive to objectively-screened impairments, whilst the remainder were identified via one or other tool only. A large proportion of people with moderate or severe clinical impairments did not self-report functional difficulties despite reporting participation restrictions.CONCLUSION: Tools to assess reported functional limitation alone are insufficient to identify all persons with participation restrictions and moderate or severe clinical impairments. A self-reported functional limitation tool followed by clinical screening of all those who report any level of difficulty would identify 94% of people with disabilities in Cameroon and 95% in India, meeting the study criteria.DOI: 10.1371/journal.pone.0164470PMCID: PMC5065175",pubmed,27741320,10.1371/journal.pone.0164470
effectiveness of a webbased support program supr for hearing aid users aged 50 twoarm cluster randomized controlled trial,"749. J Med Internet Res. 2020 Sep 22;22(9):e17927. doi: 10.2196/17927.Effectiveness of a Web-Based SUpport PRogram (SUPR) for Hearing Aid Users Aged 50+: Two-Arm, Cluster Randomized Controlled Trial.Meijerink JF(1), Pronk M(1), Lissenberg-Witte BI(2), Jansen V(3), Kramer SE(1).Author information:(1)Otolaryngology-Head and Neck Surgery, Ear and Hearing, Amsterdam Public Health research institute, Amsterdam UMC, Vrije Universiteit Amsterdam, Amsterdam, Netherlands.(2)Epidemiology and Data Science, Amsterdam UMC, Vrije Universiteit Amsterdam, Amsterdam, Netherlands.(3)Schoonenberg HoorSupport, Dordrecht, Netherlands.BACKGROUND: Hearing aid (HA) use is known to improve health outcomes for people with hearing loss. Despite that, HA use is suboptimal, and communication issues and hearing-related activity limitations and participation restrictions often remain. Web-based self-management communication programs may support people with hearing loss to effectively self-manage the impact of hearing loss in their daily lives.OBJECTIVE: The goal of the research is to examine the short- and long-term effects of a web-based self-management SUpport PRogram (SUPR) on communication strategy use (primary outcome) and a range of secondary outcomes for HA users aged 50 years and older.METHODS: Clients of 36 HA dispensing practices were randomized to SUPR (SUPR recipients; n=180 HA users) and 34 to care as usual (controls; n=163 HA users). SUPR recipients received a practical support booklet and online materials delivered via email over the course of their 6-month HA rehabilitation trajectory. They were encouraged to appoint a communication partner and were offered optional email contact with the HA dispensing practice. The online materials included 3 instruction videos on HA handling, 5 videos on communication strategies, and 3 testimonial videos. Care as usual included a HA fitting rehabilitation trajectory only. Measurements were carried out at baseline, immediately postintervention, 6 months postintervention, and 12 months postintervention. The primary outcome measure was self-reported use of communication strategies (3 subscales of the Communication Profile for the Hearing Impaired [CPHI]). Secondary outcome measures included self-reported personal adjustment to hearing loss (CPHI); use, satisfaction and benefit of HAs and SUPR (use questionnaire; International Outcome Inventory for Hearing Aids [IOI-HA], Alternative Interventions [IOI-AI]); recommendation of HA dispensing services; self-efficacy for HA handling (Measure of Audiologic Rehabilitation Self-Efficacy for Hearing Aids [MARS-HA]); readiness to act on hearing loss (University of Rhode Island Change Assessment adapted for hearing loss [URICA-HL]); and hearing disability (Amsterdam Inventory for Auditory Disability and Handicap [AIADH]).RESULTS: Linear mixed model analyses (intention to treat) showed no significant differences between the SUPR and control group in the course of communication strategy use (CPHI). Immediately postintervention, SUPR recipients showed significantly higher self-efficacy for advanced HA handling than the controls, which was sustained at 12 months (MARS-HA; mean difference immediately postintervention: 5.3, 95% CI 0.3 to 10.4; P=.04). Also, SUPR recipients showed significantly greater HA satisfaction than controls immediately postintervention (IOI-HA; 0.3, 95% CI 0.09 to 0.5; P=.006), which was sustained at 12 months, and significantly greater HA use than the controls immediately postintervention (IOI-HA; 0.3, 95% CI 0.02 to 0.5; P=.03), which was not sustained at 12 months.CONCLUSIONS: This study provides ground to recommend adding SUPR to standard HA dispensing care, as long-term, modest improvements in HA outcomes were observed. Further research is needed to evaluate what adjustments to SUPR are needed to establish long-term effectiveness on outcomes in the psychosocial domain.TRIAL REGISTRATION: ISRCTN77340339; http://www.isrctn.com/ISRCTN77340339.INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): RR2-10.1136/bmjopen-2016-015012.©Janine FJ Meijerink, Marieke Pronk, Birgit I Lissenberg-Witte, Vera Jansen, Sophia E Kramer. Originally published in the Journal of Medical Internet Research (http://www.jmir.org), 22.09.2020.DOI: 10.2196/17927PMCID: PMC7539169",pubmed,32960175,10.2196/17927
altered brain responses to emotional facial expressions in tinnitus patients,"775. Prog Brain Res. 2021;262:189-207. doi: 10.1016/bs.pbr.2021.01.026. Epub 2021 Mar 10.Altered brain responses to emotional facial expressions in tinnitus patients.Rosengarth K(1), Kleinjung T(2), Langguth B(3), Landgrebe M(4), Lohaus F(5), Greenlee MW(6), Hajak G(7), Schmidt NO(8), Schecklmann M(3).Author information:(1)Department of Neurosurgery, University of Regensburg, Regensburg, Germany; Department of Experimental Psychology, University of Regensburg, Regensburg, Germany. Electronic address: katharina.rosengarth@ukr.de.(2)Department of Otorhinolaryngology, University of Zurich, Zürich, Switzerland; Department of Otorhinolaryngology, University of Regensburg, Regensburg, Germany.(3)Department of Psychiatry and Psychotherapy, University of Regensburg, Regensburg, Germany.(4)Department of Psychiatry, Psychosomatics and Psychotherapy, kbo Lech-Mangfall-Hospital Agatharied, Hausham, Germany.(5)Department of Radiotherapy and Radiation Oncology, Faculty of Medicine and University Hospital Carl Gustav Carus, Dresden, Germany.(6)Department of Experimental Psychology, University of Regensburg, Regensburg, Germany.(7)Department of Psychiatry, Psychosomatics and Psychotherapy, Sozialstiftung Bamberg, Bamberg, Germany.(8)Department of Neurosurgery, University of Regensburg, Regensburg, Germany.Tinnitus, the phantom perception of sound, is a frequent disorder that can lead to severe distress and stress-related comorbidity. The pathophysiological mechanisms involved in the etiology of tinnitus are still under exploration. Electrophysiological and functional neuroimaging studies provide increasing evidence for abnormal functioning in auditory but also in non-auditory, e.g., emotional, brain areas. In order to elucidate alterations of affective processing in patients with chronic tinnitus, we used functional magnetic resonance imaging (fMRI) to measure neural responses to emotionally expressive and neutral faces. Twelve patients with chronic tinnitus and a group of 11 healthy controls, matched for age, sex, hearing loss and depressive symptoms were investigated. While viewing emotionally expressive faces compared to neutral faces brain activations in the tinnitus patients differed from those of the controls in a cluster that encompasses the amygdala, the hippocampus and the parahippocampal gyrus bilaterally. Whereas in controls affective faces induced higher brain activation in these regions than neutral faces, these regions in tinnitus patients were deactivated. Our results (1) provide evidence for alterations of affective processing of facial expressions in tinnitus patients indicating general domain-unspecific dysfunctions in emotion processing and (2) indicate the involvement of medial temporal areas in the pathophysiology of tinnitus.Copyright © 2021 Elsevier B.V. All rights reserved.DOI: 10.1016/bs.pbr.2021.01.026",pubmed,33931179,10.1016/bs.pbr.2021.01.026
comparao de sistemas de reconhecimento de voz para incluso de surdos,"For decades, the inclusion of deaf and hard of hearing people in education has been a recurring problem with no effective solution. Based on the growth in the field of artificial intelligence, this article presents a study comparing two voice recognition systems for the development of a class subtitling application for accessibility for deaf students. The tests performed were based on the minimum parameters for the best use of these subtitles in real time.",ieee,2572-1445,10.1109/INDUSCON58041.2023.10375068
outcomes of myringoplasty in australian aboriginal children and factors associated with success a prospective case series,"445. Clin Otolaryngol Allied Sci. 2004 Dec;29(6):606-11. doi: 10.1111/j.1365-2273.2004.00896.x.Outcomes of myringoplasty in Australian Aboriginal children and factors associated with success: a prospective case series.Mak D(1), MacKendrick A, Bulsara M, Coates H, Lannigan F, Lehmann D, Leidwinger L, Weeks S.Author information:(1)Kimberley Public Health Unit, Derby, and School of Population Health, The University of Western Australia, Perth, Australia. makho@bigpond.comThe objective of this study was to assess the outcomes of myringoplasties in Aboriginal children and to identify factors associated with a successful outcome with the use of prospective case series from primary health care clinics and hospitals in four rural and remote regions of Western Australia. All 58 Aboriginal children, aged 5-15 years, who underwent 78 myringoplasties between 1 January 2000 and 30 June 2001 were included in the study. Complete postoperative (post-op) follow-up was achieved following 78% of myringoplasties. The main outcome measures were (a) success, i.e. an intact tympanic membrane and normal hearing six or more months post-op in the operated ear, (b) closure of the perforation, (c) Post-op hearing improvement. Forty-nine per cent of myringoplasties were successful, 72% resulted in closure or reduction in the size of the perforation and 51% resulted in hearing improvement. After controlling for age, sex, clustering and number of previous myringoplasties, no association was observed between success or hearing improvement and perforation size, or the presence of serous aural discharge at the time of surgery. Myringoplasty resulted in hearing improvement and/or perforation closure in a significant proportion of children. Thus, primary school-aged Aboriginal children in whom conservative management of chronic suppurative otitis media has been unsuccessful should have access to myringoplasty because of the positive impact on their socialization, language and learning that results from improved hearing.DOI: 10.1111/j.1365-2273.2004.00896.x",pubmed,15533146,10.1111/j.1365-2273.2004.00896.x
abnormal topological organization of the white matter network in mandarin speakers with congenital amusia,"824. Sci Rep. 2016 May 23;6:26505. doi: 10.1038/srep26505.Abnormal topological organization of the white matter network in Mandarin speakers with congenital amusia.Zhao Y(1), Chen X(1), Zhong S(1), Cui Z(1), Gong G(1), Dong Q(1), Nan Y(1).Author information:(1)State Key Laboratory of Cognitive Neuroscience and Learning &IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, China.Erratum in    Sci Rep. 2016 Jul 22;6:30102.Congenital amusia is a neurogenetic disorder that mainly affects the processing of musical pitch. Brain imaging evidence indicates that it is associated with abnormal structural and functional connections in the fronto-temporal region. However, a holistic understanding of the anatomical topology underlying amusia is still lacking. Here, we used probabilistic diffusion tensor imaging tractography and graph theory to examine whole brain white matter structural connectivity in 31 Mandarin-speaking amusics and 24 age- and IQ-matched controls. Amusics showed significantly reduced global connectivity, as indicated by the abnormally decreased clustering coefficient (Cp) and increased normalized shortest path length (λ) compared to the controls. Moreover, amusics exhibited enhanced nodal strength in the right inferior parietal lobule relative to controls. The co-existence of the lexical tone deficits was associated with even more deteriorated global network efficiency in amusics, as suggested by the significant correlation between the increments in normalized shortest path length (λ) and the insensitivity in lexical tone perception. Our study is the first to reveal reduced global connectivity efficiency in amusics as well as an increase in the global connectivity cost due to the co-existed lexical tone deficits. Taken together these results provide a holistic perspective on the anatomical substrates underlying congenital amusia.DOI: 10.1038/srep26505PMCID: PMC4876438",pubmed,27211239,10.1038/srep26505
can a machine learn to solve our speechinnoise problem,,cinahl,7457472,10.1097/01.hj.0000527210.74559.0b
fungal otitis externa otomycosis associated with aspergillus flavus a case image,"820. Head Neck Pathol. 2024 Feb 9;18(1):5. doi: 10.1007/s12105-023-01606-1.Fungal Otitis Externa (Otomycosis) Associated with Aspergillus Flavus: A Case Image.MacDonald WW(1), Wakely PE Jr(1), Kalmar JR(2), Argyris PP(3).Author information:(1)Department of Pathology, The Ohio State University Wexner Medical Center, James Cancer Hospital and Solove Research Institute, Columbus, OH, USA.(2)Division of Oral and Maxillofacial Pathology, The Ohio State University College of Dentistry, Postle Hall, Room 2191 305 W. 12th Ave, Columbus, OH, USA.(3)Division of Oral and Maxillofacial Pathology, The Ohio State University College of Dentistry, Postle Hall, Room 2191 305 W. 12th Ave, Columbus, OH, USA. argyris.2@osu.edu.A 48-year-old man presented with a chief complaint of intermittent right ear otorrhea of several-month duration, occasional otalgia and progressive unilateral hearing impairment. He also reported frequent episodes of headache and pressure in the sinuses and maxilla. Previous systemic treatment with antibiotics failed to alleviate the symptoms. A head/neck CT showed completely normal mastoid, middle ear and external auditory canal regions without any evidence of opacification or bone erosion. Otoscopic examination of the right ear disclosed aggregates of dried, brown, fibrillar material and debris occluding the external auditory canal and obstructing the otherwise intact tympanic membrane. Dilation of the external auditory canal or thickening of the tympanic membrane were not appreciated. The canal was debrided and the fibrillar material was placed in formalin. Histopathologic examination revealed numerous branching, septated fungal hyphae organized in densely-packed clusters. In other areas, the fungal hyphae abutted or were attached to lamellated collections of orthokeratin. As highlighted by GMS staining, the fungi were morphologically compatible with Aspergillus species. The clinicopathologic findings supported a diagnosis of fungal otitis externa, while the numerous anucleate squamous cells were compatible with colonization of an underlying, probably developing, cholesteatoma. Culture of material isolated from the external auditory canal confirmed the presence of Aspergillus flavus. In this illustrative case, we present the main clinical and microscopic characteristics of Aspergillus-related otomycosis developing in the setting of a tautochronous cholesteatoma.© 2024. The Author(s).DOI: 10.1007/s12105-023-01606-1PMCID: PMC10858010",pubmed,38334859,10.1007/s12105-023-01606-1
combined brainperfusion spect and eeg measurements suggest distinct strategies for speech comprehension in ci users with higher and lower performance,"855. Front Neurosci. 2020 Aug 11;14:787. doi: 10.3389/fnins.2020.00787. eCollection 2020.Combined Brain-Perfusion SPECT and EEG Measurements Suggest Distinct Strategies for Speech Comprehension in CI Users With Higher and Lower Performance.Kessler M(1)(2), Schierholz I(2)(3)(4), Mamach M(2)(5), Wilke F(5), Hahne A(6), Büchner A(2)(3), Geworski L(5), Bengel FM(1), Sandmann P(4), Berding G(1)(2).Author information:(1)Department of Nuclear Medicine, Hannover Medical School, Hanover, Germany.(2)Cluster of Excellence Hearing4all, Hannover Medical School, University of Oldenburg, Oldenburg, Germany.(3)Department of Otorhinolaryngology, Hannover Medical School, Hanover, Germany.(4)Department of Otorhinolaryngology, University of Cologne, Cologne, Germany.(5)Department of Medical Physics and Radiation Protection, Hannover Medical School, Hanover, Germany.(6)Department of Otorhinolaryngology, Faculty of Medicine Carl Gustav Carus, Saxonian Cochlear Implant Center, Technical University Dresden, Dresden, Germany.Cochlear implantation constitutes a successful therapy of inner ear deafness, with the majority of patients showing good outcomes. There is, however, still some unexplained variability in outcomes with a number of cochlear-implant (CI) users, showing major limitations in speech comprehension. The current study used a multimodal diagnostic approach combining single-photon emission computed tomography (SPECT) and electroencephalography (EEG) to examine the mechanisms underlying speech processing in postlingually deafened CI users (N = 21). In one session, the participants performed a speech discrimination task, during which a 96-channel EEG was recorded and the perfusions marker 99mTc-HMPAO was injected intravenously. The SPECT scan was acquired 1.5 h after injection to measure the cortical activity during the speech task. The second session included a SPECT scan after injection without stimulation at rest. Analysis of EEG and SPECT data showed N400 and P600 event-related potentials (ERPs) particularly evoked by semantic violations in the sentences, and enhanced perfusion in a temporo-frontal network during task compared to rest, involving the auditory cortex bilaterally and Broca's area. Moreover, higher performance in testing for word recognition and verbal intelligence strongly correlated to the activation in this network during the speech task. However, comparing CI users with lower and higher speech intelligibility [median split with cutoff + 7.6 dB signal-to-noise ratio (SNR) in the Göttinger sentence test] revealed for CI users with higher performance additional activations of parietal and occipital regions and for those with lower performance stronger activation of superior frontal areas. Furthermore, SPECT activity was tightly coupled with EEG and cognitive abilities, as indicated by correlations between (1) cortical activation and the amplitudes in EEG, N400 (temporal and occipital areas)/P600 (parietal and occipital areas) and (2) between cortical activation in left-sided temporal and bilateral occipital/parietal areas and working memory capacity. These results suggest the recruitment of a temporo-frontal network in CI users during speech processing and a close connection between ERP effects and cortical activation in CI users. The observed differences in speech-evoked cortical activation patterns for CI users with higher and lower speech intelligibility suggest distinct processing strategies during speech rehabilitation with CI.Copyright © 2020 Kessler, Schierholz, Mamach, Wilke, Hahne, Büchner, Geworski, Bengel, Sandmann and Berding.DOI: 10.3389/fnins.2020.00787PMCID: PMC7431776",pubmed,32848560,10.3389/fnins.2020.00787
hearingrelated quality of life in children and adolescents in rural alaska,"803. Laryngoscope Investig Otolaryngol. 2022 Dec 1;8(1):269-278. doi: 10.1002/lio2.973. eCollection 2023 Feb.Hearing-related quality of life in children and adolescents in rural Alaska.Hicks KL(1), Robler SK(2)(3), Simmons RA(4)(5), Ross A(6)(7), Egger JR(5), Emmett SD(3)(5)(6)(7)(8).Author information:(1)Department of Otolaryngology/Head and Neck Surgery University of North Carolina-Chapel Hill Chapel Hill North Carolina USA.(2)Department of Audiology Norton Sound Health Corporation Nome Alaska USA.(3)Department of Otolaryngology-Head and Neck Surgery University of Arkansas for Medical Sciences Little Rock Arkansas USA.(4)Department of Biostatistics & Bioinformatics Duke University Durham North Carolina USA.(5)Duke Global Health Institute Durham North Carolina USA.(6)Department of Head and Neck Surgery and Communication Sciences Duke University School of Medicine Durham North Carolina USA.(7)Center for Health Policy and Inequalities Research, Duke University Durham North Carolina USA.(8)Department of Epidemiology Fay W. Boozman College of Public Health, University of Arkansas for Medical Sciences Little Rock Arkansas USA.OBJECTIVE: This study evaluated the Hearing Environments and Reflection on Quality of Life (HEAR-QL) questionnaire in rural Alaska, including an addendum crafted through community feedback to reflect the local context. The objectives were to assess whether HEAR-QL score was inversely correlated with hearing loss and middle ear disease in an Alaska Native population.METHODS: The HEAR-QL questionnaires for children and adolescents were administered as part of a cluster randomized trial in rural Alaska from 2017 to 2019. Enrolled students completed an audiometric evaluation and HEAR-QL questionnaire on the same day. A cross-sectional evaluation of questionnaire data was utilized.RESULTS: A total of 733 children (ages 7-12 years) and 440 adolescents (ages ≥13 years) completed the questionnaire. Median HEAR-QL scores were similar among children with and without hearing loss (Kruskal-Wallis, p = .39); however, adolescent HEAR-QL scores significantly decreased with increasing hearing loss (p < .001). Median HEAR-QL scores were significantly lower in both children (p = .02) and adolescents (p < .001) with middle ear disease compared with those without. In both children and adolescents, the addendum scores were strongly correlated with total HEAR-QL score (ρSpearman = 0.72 and 0.69, respectively).CONCLUSIONS: The expected negative association between hearing loss and HEAR-QL score was observed in adolescents. However, there was significant variability that could not be explained by hearing loss, and further investigation is warranted. The expected negative association was not observed in children. HEAR-QL scores were associated with middle ear disease in both children and adolescents, making it potentially valuable in populations where the prevalence of ear infections is high.LEVEL OF EVIDENCE: Level 2 Clinicaltrials.gov registration numbers: NCT03309553.© 2022 The Authors. Laryngoscope Investigative Otolaryngology published by Wiley Periodicals LLC on behalf of The Triological Society.DOI: 10.1002/lio2.973PMCID: PMC9948564",pubmed,36846414,10.1002/lio2.973
de wet swanepoel using digital technologies to improve access to hearing health,,cinahl,429686,10.2471/BLT.24.030424
the optimal interimplant interval in pediatric sequential bilateral implantation,"192. Hear Res. 2019 Feb;372:80-87. doi: 10.1016/j.heares.2017.10.010. Epub 2017 Oct 31.The Optimal inter-implant interval in pediatric sequential bilateral implantation.Illg A(1), Sandner C(2), Büchner A(3), Lenarz T(2), Kral A(2), Lesinski-Schiedat A(2).Author information:(1)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany. Electronic address: illg@hoerzentrum-hannover.de.(2)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany.(3)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany; Cluster of Excellence, Hearing4All, Hannover Medical School, Hannover, Germany.An increasing number of children receive bilateral cochlear implants (CIs) sequentially. Outcomes of bilateral implantation show high variability. This retrospective analysis investigates the optimal inter-implant interval. For this purpose, speech comprehension results of 250 children who underwent sequential bilateral cochlear implantation were evaluated. All individuals underwent periodic speech perception testing in quiet and noise. The most recent unilateral data for each side were statistically analyzed. Speech test outcomes were evaluated with reference to age at first implantation and interval between implantations. A statistically significant difference for speech test performance was obtained between the first-implanted ear and the second-implanted ear for all children (expressed as a mean). These outcomes were dependent on the inter-implant interval. There was a significant correlation (r = - 0.497; p = 0.000) between speech test results and the inter-implant interval. Nevertheless, one subgroup of 27 children had the same or better results for the second side as compared with the first. In conclusion, the evaluation of the inter-implant interval and age groups at first implantation showed a preferred interval of up to four years in children under the age of 4 at first implantation. The older the children were at first implantation, the shorter the inter-implant interval had to be. It is as a direct consequence of this interval that children for whom it was longer were also older.Copyright © 2017 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2017.10.010",pubmed,29133013,10.1016/j.heares.2017.10.010
selffocused and somatic attention in patients with tinnitus,,cinahl,10500545,NLM9188071
hcn channels in the mammalian cochlea expression pattern subcellular location and agedependent changes,"91. J Neurosci Res. 2021 Feb;99(2):699-728. doi: 10.1002/jnr.24754. Epub 2020 Nov 12.HCN channels in the mammalian cochlea: Expression pattern, subcellular location, and age-dependent changes.Luque M(1), Schrott-Fischer A(1), Dudas J(1), Pechriggl E(2), Brenner E(2), Rask-Andersen H(3), Liu W(3), Glueckert R(1)(4).Author information:(1)Department of Otorhinolaryngology, Medical University of Innsbruck, Innsbruck, Austria.(2)Department of Anatomy, Histology & Embryology, Division of Clinical & Functional Anatomy, Medical University of Innsbruck, Innsbruck, Austria.(3)Department of Surgical Sciences, Head and Neck Surgery, Section of Otolaryngology, Uppsala University Hospital, Uppsala, Sweden.(4)Tirol Kliniken, University Clinics Innsbruck, Innsbruck, Austria.Neuronal diversity in the cochlea is largely determined by ion channels. Among voltage-gated channels, hyperpolarization-activated cyclic nucleotide-gated (HCN) channels open with hyperpolarization and depolarize the cell until the resting membrane potential. The functions for hearing are not well elucidated and knowledge about localization is controversial. We created a detailed map of subcellular location and co-expression of all four HCN subunits across different mammalian species including CBA/J, C57Bl/6N, Ly5.1 mice, guinea pigs, cats, and human subjects. We correlated age-related hearing deterioration in CBA/J and C57Bl/6N with expression levels of HCN1, -2, and -4 in individual auditory neurons from the same cohort. Spatiotemporal expression during murine postnatal development exposed HCN2 and HCN4 involvement in a critical phase of hair cell innervation. The huge diversity of subunit composition, but lack of relevant heteromeric pairing along the perisomatic membrane and axon initial segments, highlighted an active role for auditory neurons. Neuron clusters were found to be the hot spots of HCN1, -2, and -4 immunostaining. HCN channels were also located in afferent and efferent fibers of the sensory epithelium. Age-related changes on HCN subtype expression were not uniform among mice and could not be directly correlated with audiometric data. The oldest mice groups revealed HCN channel up- or downregulation, depending on the mouse strain. The unexpected involvement of HCN channels in outer hair cell function where HCN3 overlaps prestin location emphasized the importance for auditory function. A better understanding may open up new possibilities to tune neuronal responses evoked through electrical stimulation by cochlear implants.© 2020 The Authors. Journal of Neuroscience Research published by Wiley Periodicals LLC.DOI: 10.1002/jnr.24754PMCID: PMC7839784",pubmed,33181864,10.1002/jnr.24754
influencing factors for the use of earplugs in workers exposed to noise in a city,"363. Zhonghua Lao Dong Wei Sheng Zhi Ye Bing Za Zhi. 2016 Apr 20;34(4):271-4. doi: 10.3760/cma.j.issn.1001-9391.2016.04.008.[Influencing factors for the use of earplugs in workers exposed to noise in a city].[Article in Chinese]Zhu QR(1), Shao YX, Cao CJ, Wu X, Xie WQ, Xu M, Yang L, Xu LW.Author information:(1)Medical College, Hangzhou Normal University, Hangzhou 310000, China.OBJECTIVE: To investigate the current status of hearing loss and the use of earplugs in workers exposed to noise who have been provided earplugs in a city, as well as major influencing factors for the use of earplugs.METHODS: Cluster random sampling was used to conduct a questionnaire survey in workers exposed to noise who had been provided earplugs in 15 enterprises with noise exposure in a city from June to December, 2014.RESULTS: In the workers exposed to noise who had been provided earplugs, the rate of high-frequency anomaly in both ears was 57.8%, and the workers who kept wearing earplugs only accounted for 55.4%. The results of binary logistic regression analysis showed that the protective factors for the use of earplugs included workers' own feeling of hearing condition (OR=1.704), comfort of earplugs (OR= 1.892), enterprise's inspection of the use of earplugs (OR=1.461), workers' knowledge of the function and usage of earplugs (OR=1.581), workers' understanding of the necessity of earplugs (OR=4.482), workers' initiative to search for related data (OR=4.029), the use of earplugs by colleagues (OR=5.071), and reminders from family members or friends (OR=2.678) (all P<0.05).CONCLUSION: The workers exposed to noise in this city have a high rate of abnormal hearing, and only half of the workers keep wearing earplugs during work. The use of earplugs is related to the factors including workers' own feeling of hearing condition, comfort of earplugs, workers' knowledge of protection, the enterprise' s management of hearing protection, and environmental support.DOI: 10.3760/cma.j.issn.1001-9391.2016.04.008",pubmed,27514260,10.3760/cma.j.issn.1001-9391.2016.04.008
use of telemedicine in the remote programming of cochlear implants,"584. Acta Otolaryngol. 2009 May;129(5):533-40. doi: 10.1080/00016480802294369.Use of telemedicine in the remote programming of cochlear implants.Ramos A(1), Rodriguez C, Martinez-Beneyto P, Perez D, Gault A, Falcon JC, Boyle P.Author information:(1)Ear, Neck and Throat Department, Complejo Hospitalario Universitario Insular Materno-Infantil de Gran Canaria, Las Palmas de Gran Canaria, Spain. ramosorl@idecnet.comCONCLUSION: Remote cochlear implant (CI) programming is a viable, safe, user-friendly and cost-effective procedure, equivalent to standard programming in terms of efficacy and user's perception, which can complement the standard procedures. The potential benefits of this technique are outlined.OBJECTIVES: We assessed the technical viability, risks and difficulties of remote CI programming; and evaluated the benefits for the user comparing the standard on-site CI programming versus the remote CI programming.SUBJECTS AND METHODS: The Remote Programming System (RPS) basically consists of completing the habitual programming protocol in a regular CI centre, assisted by local staff, although guided by a remote expert, who programs the CI device using a remote programming station that takes control of the local station through the Internet. A randomized prospective study has been designed with the appropriate controls comparing RPS to the standard on-site CI programming. Study subjects were implanted adults with a HiRes 90K(R) CI with post-lingual onset of profound deafness and 4-12 weeks of device use. Subjects underwent two daily CI programming sessions either remote or standard, on 4 programming days separated by 3 month intervals. A total of 12 remote and 12 standard sessions were completed. To compare both CI programming modes we analysed: program parameters, subjects' auditory progress, subjects' perceptions of the CI programming sessions, and technical aspects, risks and difficulties of remote CI programming.RESULTS: Control of the local station from the remote station was carried out successfully and remote programming sessions were achieved completely and without incidents. Remote and standard program parameters were compared and no significant differences were found between the groups. The performance evaluated in subjects who had been using either standard or remote programs for 3 months showed no significant difference. Subjects were satisfied with both the remote and standard sessions. Safety was proven by checking emergency stops in different conditions. A very small delay was noticed that did not affect the ease of the fitting. The oral and video communication between the local and the remote equipment was established without difficulties and was of high quality.DOI: 10.1080/00016480802294369",pubmed,18649152,10.1080/00016480802294369
fast inhibition slows and desynchronizes mouse auditory efferent neuron activity,"771. bioRxiv [Preprint]. 2024 Jan 23:2023.12.21.572886. doi: 10.1101/2023.12.21.572886.Fast inhibition slows and desynchronizes mouse auditory efferent neuron activity.Fischl M(1)(2), Pederson A(1)(3), Voglewede R(1), Cheng H(4), Drew J(1)(5), Cadenas LT(1), Weisz CJC(1).Author information:(1)Section on Neuronal Circuitry, National Institute on Deafness and Other Communication Disorders, NIH, Bethesda, MD 20892, USA.(2)Current affiliation: Lafayette College, Neuroscience Program, Easton, PA 18042, USA.(3)Current affiliation: The University of Texas at Austin Dell Medical School, Austin, TX 78712, USA.(4)Bioinformatics and Biostatistics Collaboration Core, National Institute on Deafness and Other Communication Disorders, NIH, Bethesda, MD 20892, USA.(5)Current affiliation: Institute for Learning and Brain Sciences, University of Washington, Seattle, WA, 98195, USA.The encoding of acoustic stimuli requires precise neuron timing. Auditory neurons in the cochlear nucleus (CN) and brainstem are well-suited for accurate analysis of fast acoustic signals, given their physiological specializations of fast membrane time constants, fast axonal conduction, and reliable synaptic transmission. The medial olivocochlear (MOC) neurons that provide efferent inhibition of the cochlea reside in the ventral brainstem and participate in these fast neural circuits. However, their modulation of cochlear function occurs over time scales of a slower nature. This suggests the presence of mechanisms that restrict MOC inhibition of cochlear function. To determine how monaural excitatory and inhibitory synaptic inputs integrate to affect the timing of MOC neuron activity, we developed a novel in vitro slice preparation ('wedge-slice'). The wedge-slice maintains the ascending auditory nerve root, the entire CN and projecting axons, while preserving the ability to perform visually guided patch-clamp electrophysiology recordings from genetically identified MOC neurons. The 'in vivo-like' timing of the wedge-slice demonstrates that the inhibitory pathway accelerates relative to the excitatory pathway when the ascending circuit is intact, and the CN portion of the inhibitory circuit is precise enough to compensate for reduced precision in later synapses. When combined with machine learning PSC analysis and computational modeling, we demonstrate a larger suppression of MOC neuron activity when the inhibition occurs with in vivo-like timing. This delay of MOC activity may ensure that the MOC system is only engaged by sustained background sounds, preventing a maladaptive hyper-suppression of cochlear activity.DOI: 10.1101/2023.12.21.572886PMCID: PMC10836066",pubmed,38313270,10.1101/2023.12.21.572886
resveratrol prevents hearing loss and a subregion specific reduction of serotonin reuptake transporter induced by noise exposure in the central auditory system,"726. Front Neurosci. 2023 Mar 24;17:1134153. doi: 10.3389/fnins.2023.1134153. eCollection 2023.Resveratrol prevents hearing loss and a subregion specific- reduction of serotonin reuptake transporter induced by noise exposure in the central auditory system.Cheng LQ(1)(2), Shu FQ(3), Zhang M(1)(2), Kai YZ(1), Tang ZQ(1)(2).Author information:(1)School of Life Sciences, Anhui University, Hefei, China.(2)Key Laboratory of Human Microenvironment and Precision Medicine of Anhui Higher Education Institutes, Anhui University, Hefei, China.(3)Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, China.Prolonged or excessive exposure to noise can lead to hearing loss, tinnitus and hypersensitivity to sound. The effects of noise exposure on main excitatory and inhibitory neurotransmitter systems in auditory pathway have been extensively investigated. However, little is known about aberrant changes in neuromodulator systems caused by noise exposure. In the current study, we exposed 2-month-old mice to a narrow band noise at 116 dB SPL for 6 h or sham exposure, assessed auditory brainstem responses as well as examined the expression of serotonin reuptake transporter (SERT) in the cochlear nucleus (CN), inferior colliculus (IC), and primary auditory cortex (Au1) using immunohistochemistry. We found that noise exposure resulted in a significant increase in hearing thresholds at 4, 8, 16, 24, and 32 kHz, as well as led to a significant reduction of SERT in dorsal cochlear nucleus (DCN), dorsal IC (ICd), external IC (ICe), and Au1 layers I-IV. This reduction of SERT in these subregions of central auditory system was partially recovered 15 or 30 days after noise exposure. Furthermore, we examined efficacy of resveratrol (RSV) on hearing loss and loss of SERT induced by noise exposure. The results demonstrated that RSV treatment significantly attenuated threshold shifts of auditory brainstem responses and loss of SERT in DCN, ICd, ICe, and Au1 layers I-IV. These findings show that noise exposure can cause hearing loss and subregion-specific loss of SERT in the central auditory system, and RSV treatment could attenuate noise exposure-induced hearing loss and loss of SERT in central auditory system.Copyright © 2023 Cheng, Shu, Zhang, Kai and Tang.DOI: 10.3389/fnins.2023.1134153PMCID: PMC10080035",pubmed,37034161,10.3389/fnins.2023.1134153
the use of artificial neural networks to estimate speech intelligibility from acoustic variables a preliminary analysis,"451. J Commun Disord. 1992 Mar;25(1):43-53. doi: 10.1016/0021-9924(92)90013-m.The use of artificial neural networks to estimate speech intelligibility from acoustic variables: a preliminary analysis.Metz DE(1), Schiavetti N, Knight SD.Author information:(1)Department of Communication Research, NTID, Rochester Institute of Technology, NY 14623-0887.Previous research has used regression analysis to attempt to predict the intelligibility of hearing-impaired speakers from acoustic speech parameters. Improvement of prediction may be achieved by the use of computerized artificial neural networks to process mathematically the acoustic input variables as part of the intelligibility process. A preliminary scheme for estimating speech intelligibility from acoustic parameters using a neural network is outlined and preliminary data illustrate its use.DOI: 10.1016/0021-9924(92)90013-m",pubmed,1401230,10.1016/0021-9924(92)90013-m
the application of genome editing in studying hearing loss,"24. Hear Res. 2015 Sep;327:102-8. doi: 10.1016/j.heares.2015.04.016. Epub 2015 May 15.The application of genome editing in studying hearing loss.Zou B(1), Mittal R(1), Grati M(1), Lu Z(2), Shu Y(3), Tao Y(4), Feng Y(5), Xie D(6), Kong W(7), Yang S(8), Chen ZY(9), Liu X(10).Author information:(1)Department of Otolaryngology, University of Miami Miller School of Medicine, Miami, FL 33136, USA.(2)Department of Biology, University of Miami, Miami, FL 33146, USA.(3)Department of Otology and Laryngology, Harvard Medical School and Eaton-Peabody Laboratory, Massachusetts Eye and Ear Infirmary, Boston 02114, USA; Department of Otology and Skull Base Surgery, Eye, Ear, Nose and Throat Hospital, Shanghai Medical College, Fudan University, Shanghai, China.(4)Department of Otology and Laryngology, Harvard Medical School and Eaton-Peabody Laboratory, Massachusetts Eye and Ear Infirmary, Boston 02114, USA; Department of Otolaryngology, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China.(5)Department of Otolaryngology, Xiangya Hospital, Central South University, Changsha, Hunan, China.(6)Department of Otolaryngology-Head and Neck Surgery, The Second Xiangya Hospital Central South University, Changsha, Hunan, China.(7)Department of Otolaryngology, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China.(8)Department of Otolaryngology-Head and Neck Surgery, Chinese PLA General Hospital, Beijing, China.(9)Department of Otology and Laryngology, Harvard Medical School and Eaton-Peabody Laboratory, Massachusetts Eye and Ear Infirmary, Boston 02114, USA. Electronic address: zheng-yi_chen@meei.harvard.edu.(10)Department of Otolaryngology, University of Miami Miller School of Medicine, Miami, FL 33136, USA; Department of Otolaryngology, Xiangya Hospital, Central South University, Changsha, Hunan, China; Department of Otolaryngology-Head and Neck Surgery, The Second Xiangya Hospital Central South University, Changsha, Hunan, China; Department of Otolaryngology-Head and Neck Surgery, Chinese PLA General Hospital, Beijing, China. Electronic address: xliu@med.miami.edu.Targeted genome editing mediated by clustered, regularly interspaced, short palindromic repeat (CRISPR)/CRISPR-associated nuclease 9 (Cas9) technology has emerged as one of the most powerful tools to study gene functions, and with potential to treat genetic disorders. Hearing loss is one of the most common sensory disorders, affecting approximately 1 in 500 newborns with no treatment. Mutations of inner ear genes contribute to the largest portion of genetic deafness. The simplicity and robustness of CRISPR/Cas9-directed genome editing in human cells and model organisms such as zebrafish, mice and primates make it a promising technology in hearing research. With CRISPR/Cas9 technology, functions of inner ear genes can be studied efficiently by the disruption of normal gene alleles through non-homologous-end-joining (NHEJ) mechanism. For genetic hearing loss, CRISPR/Cas9 has potential to repair gene mutations by homology-directed-repair (HDR) or to disrupt dominant mutations by NHEJ, which could restore hearing. Our recent work has shown CRISPR/Cas9-mediated genome editing can be efficiently performed in the mammalian inner ear in vivo. Thus, application of CRISPR/Cas9 in hearing research will open up new avenues for understanding the pathology of genetic hearing loss and provide new routes in the development of treatment to restore hearing. In this review, we describe major methodologies currently used for genome editing. We will highlight applications of these technologies in studies of genetic disorders and discuss issues pertaining to applications of CRISPR/Cas9 in auditory systems implicated in genetic hearing loss.Copyright © 2015 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2015.04.016PMCID: PMC4554948",pubmed,25987504,10.1016/j.heares.2015.04.016
benchmarking ceegrid and solid gelbased electrodes to classify inattentional deafness in a flight simulator,"767. Front Neuroergon. 2022 Jan 6;2:802486. doi: 10.3389/fnrgo.2021.802486. eCollection 2021.Benchmarking cEEGrid and Solid Gel-Based Electrodes to Classify Inattentional Deafness in a Flight Simulator.Somon B(1)(2), Giebeler Y(2)(3), Darmet L(2), Dehais F(1)(2)(4).Author information:(1)Artificial and Natural Intelligence Toulouse Institute, Université de Toulouse, Toulouse, France.(2)Department for Aerospace Vehicles Design and Control, ISAE-SUPAERO, Université de Toulouse, Toulouse, France.(3)Department of Psychology and Ergonomics, Technische Universität Berlin, Berlin, Germany.(4)School of Biomedical Engineering, Science and Health Systems, Drexel University, Philadelphia, PA, United States.Transfer from experiments in the laboratory to real-life tasks is challenging due notably to the inability to reproduce the complexity of multitasking dynamic everyday life situations in a standardized lab condition and to the bulkiness and invasiveness of recording systems preventing participants from moving freely and disturbing the environment. In this study, we used a motion flight simulator to induce inattentional deafness to auditory alarms, a cognitive difficulty arising in complex environments. In addition, we assessed the possibility of two low-density EEG systems a solid gel-based electrode Enobio (Neuroelectrics, Barcelona, Spain) and a gel-based cEEGrid (TMSi, Oldenzaal, Netherlands) to record and classify brain activity associated with inattentional deafness (misses vs. hits to odd sounds) with a small pool of expert participants. In addition to inducing inattentional deafness (missing auditory alarms) at much higher rates than with usual lab tasks (34.7% compared to the usual 5%), we observed typical inattentional deafness-related activity in the time domain but also in the frequency and time-frequency domains with both systems. Finally, a classifier based on Riemannian Geometry principles allowed us to obtain more than 70% of single-trial classification accuracy for both mobile EEG, and up to 71.5% for the cEEGrid (TMSi, Oldenzaal, Netherlands). These results open promising avenues toward detecting cognitive failures in real-life situations, such as real flight.Copyright © 2022 Somon, Giebeler, Darmet and Dehais.DOI: 10.3389/fnrgo.2021.802486PMCID: PMC10790867",pubmed,38235232,10.3389/fnrgo.2021.802486
transient noise reduction using a deep recurrent neural network effects on subjective speech intelligibility and listening comfort,"195. Trends Hear. 2021 Jan-Dec;25:23312165211041475. doi: 10.1177/23312165211041475.Transient Noise Reduction Using a Deep Recurrent Neural Network: Effects on Subjective Speech Intelligibility and Listening Comfort.Keshavarzi M(1)(2)(3), Reichenbach T(1)(4), Moore BCJ(3).Author information:(1)Department of Bioengineering and Centre for Neurotechnology, 4615Imperial College London, London, UK.(2)Centre for Neuroscience in Education, Department of Psychology, 2152University of Cambridge, Cambridge, UK.(3)Cambridge Hearing Group, Department of Psychology, 2152University of Cambridge, Cambridge, UK.(4)Department Artificial Intelligence in Biomedical Engineering, Friedrich-Alexander-University Erlangen- Nuremberg, Erlangen, Germany.A deep recurrent neural network (RNN) for reducing transient sounds was developed and its effects on subjective speech intelligibility and listening comfort were investigated. The RNN was trained using sentences spoken with different accents and corrupted by transient sounds, using the clean speech as the target. It was tested using sentences spoken by unseen talkers and corrupted by unseen transient sounds. A paired-comparison procedure was used to compare all possible combinations of three conditions for subjective speech intelligibility and listening comfort for two relative levels of the transients. The conditions were: no processing (NP); processing using the RNN; and processing using a multi-channel transient reduction method (MCTR). Ten participants with normal hearing and ten with mild-to-moderate hearing loss participated. For the latter, frequency-dependent linear amplification was applied to all stimuli to compensate for individual audibility losses. For the normal-hearing participants, processing using the RNN was significantly preferred over that for NP for subjective intelligibility and comfort, processing using the RNN was significantly preferred over that for MCTR for subjective intelligibility, and processing using the MCTR was significantly preferred over that for NP for comfort for the higher transient level only. For the hearing-impaired participants, processing using the RNN was significantly preferred over that for NP for both subjective intelligibility and comfort, processing using the RNN was significantly preferred over that for MCTR for comfort, and processing using the MCTR was significantly preferred over that for NP for comfort.DOI: 10.1177/23312165211041475PMCID: PMC8642050",pubmed,34606381,10.1177/23312165211041475
prediction of the molecular mechanisms underlying erlong zuoci treatment of agerelated hearing loss via network pharmacologybased analyses combined with experimental validation,"684. Front Pharmacol. 2021 Nov 23;12:719267. doi: 10.3389/fphar.2021.719267. eCollection 2021.Prediction of the Molecular Mechanisms Underlying Erlong Zuoci Treatment of Age-Related Hearing Loss via Network Pharmacology-Based Analyses Combined with Experimental Validation.Liu Q(1), Li N(1), Yang Y(1), Yan X(1), Dong Y(2), Peng Y(2), Shi J(1).Author information:(1)School of Basic Medical Sciences, Shanghai University of Traditional Chinese Medicine, Shanghai, China.(2)Experimental Teaching Center, Shanghai University of Traditional Chinese Medicine, Shanghai, China.Background: The traditional Chinese medicine formula ErLong ZuoCi (ELZC) has been extensively used to treat age-related hearing loss (ARHL) in clinical practice in China for centuries. However, the underlying molecular mechanisms are still poorly understood. Objective: Combine network pharmacology with experimental validation to explore the potential molecular mechanisms underlying ELZC with a systematic viewpoint. Methods: The chemical components of ELZC were collected from the Traditional Chinese Medicine System Pharmacology database, and their possible target proteins were predicted using the SwissTargetPrediction database. The putative ARHL-related target proteins were identified from the database: GeneCards and OMIM. We constructed the drug-target network as well as drug-disease specific protein-protein interaction networks and performed clustering and topological property analyses. Functional annotation and signaling pathways were performed by gene ontology and Kyoto Encyclopedia of Genes and Genomes enrichment analysis. Finally, in vitro experiments were also performed to validate ELZC's key target proteins and treatment effects on ARHL. Results: In total, 63 chemical compounds from ELZC and 365 putative ARHL-related targets were identified, and 1860 ARHL-related targets were collected from the OMIM and GeneCards. A total of 145 shared targets of ELZC and ARHL were acquired by Venn diagram analysis. Functional enrichment analysis suggested that ELZC might exert its pharmacological effects in multiple biological processes, such as cell proliferation, apoptosis, inflammatory response, and synaptic connections, and the potential targets might be associated with AKT, ERK, and STAT3, as well as other proteins. In vitro experiments revealed that ELZC pretreatment could decrease senescence-associated β-galactosidase activity in hydrogen peroxide-induced auditory hair cells, eliminate DNA damage, and reduce cellular senescence protein p21 and p53. Finally, Western blot analysis confirmed that ELZC could upregulate the predicted target ERK phosphorylation. Conclusion: We provide an integrative network pharmacology approach, in combination with in vitro experiments to explore the underlying molecular mechanisms governing ELZC treatment of ARHL. The protective effects of ELZC against ARHL were predicted to be associated with cellular senescence, inflammatory response, and synaptic connections which might be linked to various pathways such as JNK/STAT3 and ERK cascade signaling pathways. As a prosperous possibility, our experimental data suggest phosphorylation ERK is essential for ELZC to prevent degeneration of cochlear.Copyright © 2021 Liu, Li, Yang, Yan, Dong, Peng and Shi.DOI: 10.3389/fphar.2021.719267PMCID: PMC8650627",pubmed,34887749,10.3389/fphar.2021.719267
3d printed cell culture chamber for testing the effect of pumpbased chronic drug delivery on inner ear tissue,"794. Biomolecules. 2022 Apr 17;12(4):589. doi: 10.3390/biom12040589.3D Printed Cell Culture Chamber for Testing the Effect of Pump-Based Chronic Drug Delivery on Inner Ear Tissue.Schwieger J(1)(2)(3), Frisch AS(1)(2), Rau TS(1)(2)(3), Lenarz T(1)(2)(3), Hügl S(1)(2)(3), Scheper V(1)(2)(3).Author information:(1)Department of Otorhinolaryngology, Head and Neck Surgery, Hannover Medical School, 30625 Hannover, Germany.(2)Lower Saxony Center for Biomedical Engineering, Implant Research and Development (NIFE), Stadtfelddamm 34, 30625 Hannover, Germany.(3)Cluster of Excellence ""Hearing4all"" EXC 1077/2, 30625 Hannover, Germany.Cochlear hair cell damage and spiral ganglion neuron (SGN) degeneration are the main causes of sensory neural hearing loss. Cochlear implants (CIs) can replace the function of the hair cells and stimulate the SGNs electrically. The condition of the SGNs and their spatial distance to the CI are key factors for CI-functionality. For a better performance, a high number of neurons and a closer contact to the electrode are intended. Neurotrophic factors are able to enhance SGN survival and neurite outgrowth, and thereby might optimize the electrode-nerve interaction. This would require chronic factor treatment, which is not yet established for the inner ear. Investigations on chronic drug delivery to SGNs could benefit from an appropriate in vitro model. Thus, an inner ear inspired Neurite Outgrowth Chamber (NOC), which allows the incorporation of a mini-osmotic pump for long-term drug delivery, was designed and three-dimensionally printed. The NOC's function was validated using spiral ganglion explants treated with ciliary neurotrophic factor, neurotrophin-3, or control fluid released via pumps over two weeks. The NOC proved to be suitable for explant cultivation and observation of pump-based drug delivery over the examined period, with neurotrophin-3 significantly increasing neurite outgrowth compared to the other groups.DOI: 10.3390/biom12040589PMCID: PMC9032916",pubmed,35454178,10.3390/biom12040589
modelling speech reception thresholds and their improvements due to spatial noise reduction algorithms in bimodal cochlear implant users,"558. Hear Res. 2022 Jul;420:108507. doi: 10.1016/j.heares.2022.108507. Epub 2022 Apr 11.Modelling speech reception thresholds and their improvements due to spatial noise reduction algorithms in bimodal cochlear implant users.Zedan A(1), Jürgens T(2), Williges B(3), Hülsmeier D(4), Kollmeier B(5).Author information:(1)Medizinische Physik and Cluster of Excellence ""Hearing4all,"" Carl-von-Ossietzky Universität Oldenburg, Oldenburg, Germany. Electronic address: ayham.zedan@uni-oldenburg.de.(2)Institut für Akustik, Technische Hochschule Lübeck, Lübeck, Germany. Electronic address: tim.juergens@th-luebeck.de.(3)Medizinische Physik and Cluster of Excellence ""Hearing4all,"" Carl-von-Ossietzky Universität Oldenburg, Oldenburg, Germany; SOUND Lab & Cambridge Hearing Group, Department of Clinical Neurosciences, University of Cambridge, Cambridge, United Kingdom. Electronic address: bw429@medschl.cam.ac.uk.(4)Medizinische Physik and Cluster of Excellence ""Hearing4all,"" Carl-von-Ossietzky Universität Oldenburg, Oldenburg, Germany. Electronic address: david.huelsmeier@uni-oldenburg.de.(5)Medizinische Physik and Cluster of Excellence ""Hearing4all,"" Carl-von-Ossietzky Universität Oldenburg, Oldenburg, Germany. Electronic address: birger.kollmeier@uni-oldenburg.de.Spatial noise reduction algorithms (""beamformers"") can considerably improve speech reception thresholds (SRTs) for bimodal cochlear implant (CI) users. The goal of this study was to model SRTs and SRT-benefit due to beamformers for bimodal CI users. Two existing model approaches varying in computational complexity and binaural processing assumption were compared: (i) the framework of auditory discrimination experiments (FADE) and (ii) the binaural speech intelligibility model (BSIM), both with CI and aided hearing-impaired front-ends. The exact same acoustic scenarios, and open-access beamformers as in the comparison clinical study Zedan et al. (2021) were used to quantify goodness of prediction. FADE was capable of modeling SRTs ab-initio, i.e., no calibration of the model was necessary to achieve high correlations and low root-mean square errors (RMSE) to both, measured SRTs (r = 0.85, RMSE = 2.8 dB) and to measured SRT-benefits (r = 0.96). BSIM achieved somewhat poorer predictions to both, measured SRTs (r = 0.78, RMSE = 6.7 dB) and to measured SRT-benefits (r = 0.91) and needs to be calibrated for matching average SRTs in one condition. Greatest deviations in predictions of BSIM were observed in diffuse multi-talker babble noise, which were not found with FADE. SRT-benefit predictions of both models were similar to instrumental signal-to-noise ratio (iSNR) improvements due to the beamformers. This indicates that FADE is preferrable for modeling absolute SRTs. However, for prediction of SRT-benefit due to spatial noise reduction algorithms in bimodal CI users, the average iSNR is a much simpler approach with similar performance.Copyright © 2022. Published by Elsevier B.V.DOI: 10.1016/j.heares.2022.108507PMCID: PMC9188268",pubmed,35484022,10.1016/j.heares.2022.108507
subgroups of us iraq and afghanistan veterans associations with traumatic brain injury and mental health conditions,"U. S. veterans of Iraq and Afghanistan are known to have a high prevalence of traumatic brain injury (TBI), posttraumatic stress disorder (PTSD), and depression, which are often comorbid and share many symptoms. Attempts to describe this cohort by single diagnoses have limited our understanding of the complex nature of this population. The objective of this study was to identify subgroups of Iraq and Afghanistan veterans (IAVs) with distinct compositions of symptoms associated with TBI, PTSD, and depression. Our cross-sectional, observational study included 303,716 IAVs who received care in the Veterans Health Administration in 2010–2011. Symptoms and conditions were defined using International Classification of Diseases, Ninth Revision codes and symptom-clusters were identified using latent class analysis. We identified seven classes with distinct symptom compositions. One class had low probability of any condition and low health care utilization (HCU) (48 %). Other classes were characterized by high probabilities of mental health comorbidities (14 %); chronic pain and sleep disturbance (20 %); headaches and memory problems (6 %); and auditory problems (2.5 %). Another class had mental health comorbidities and chronic pain (7 %), and the last had high probabilities of most symptoms examined (3 %). These last two classes had the highest likelihood of TBI, PTSD, and depression and were identified as high healthcare utilizers. There are subgroups of IAVs with distinct clusters of symptom that are meaningfully associated with TBI, PTSD, depression, and HCU. Additional studies examining these veteran subgroups could improve our understanding of this complex comorbid patient population. © 2015, Springer Science+Business Media New York.",scopus,2-s2.0-84942368134,10.1007/s11682-015-9402-8
improved speech intelligibility in subjects with stable sensorineural hearing loss following intratympanic dosing of fx322 in a phase 1b study,"206. Otol Neurotol. 2021 Aug 1;42(7):e849-e857. doi: 10.1097/MAO.0000000000003120.Improved Speech Intelligibility in Subjects With Stable Sensorineural Hearing Loss Following Intratympanic Dosing of FX-322 in a Phase 1b Study.McLean WJ(1)(2), Hinton AS(1), Herby JTJ(1), Salt AN(3), Hartsock JJ(3), Wilson S(1), Lucchino DL(1), Lenarz T(4), Warnecke A(4), Prenzler N(4), Schmitt H(4), King S(5), Jackson LE(6), Rosenbloom J(7), Atiee G(8), Bear M(9), Runge CL(10), Gifford RH(11), Rauch SD(12), Lee DJ(12), Langer R(13), Karp JM(14)(15)(16)(17), Loose C(1), LeBel C(1).Author information:(1)Frequency Therapeutics, Woburn, MA & Farmington, CT.(2)Department of Surgery, University of Connecticut School of Medicine, Farmington, CT.(3)Department of Otolaryngology, Central Institute for the Deaf, Fay and Carl Simons Center for Hearing and Deafness, Washington University School of Medicine, Saint Louis, MO.(4)Department of Otolaryngology and Cluster of Excellence of the German Research Foundation ""Hearing4all"", Hannover Medical School, Hannover, Germany.(5)Ear Medical Group.(6)Ear Institute of Texas, San Antonio, TX.(7)Alamo ENT Associates, San Antonio, TX.(8)Worldwide Clinical Trials, San Antonio, TX.(9)Forsythe and Bear LLC, Woodland Hills, CA.(10)Department of Otolaryngology and Communication Sciences, Medical College of Wisconsin, Milwaukee, WI.(11)Department of Hearing and Speech Sciences, Vanderbilt University Medical Center, Nashville, TN.(12)Department of Otolaryngology, Harvard Medical School and Massachusetts Eye and Ear, Boston.(13)Department of Biological Engineering, Massachusetts Institute of Technology, Cambridge, MA.(14)Center for Nanomedicine, Department of Anesthesiology, Perioperative and Pain Medicine, Brigham and Women's Hospital, Harvard Medical School Boston MA.(15)Harvard-MIT Division of Health Science and Technology.(16)Harvard Stem Cell Institute, Harvard University, Cambridge, MA, USA.(17)Broad Institute of MIT and Harvard, Cambridge, MA.OBJECTIVES: There are no approved pharmacologic therapies for chronic sensorineural hearing loss (SNHL). The combination of CHIR99021+valproic acid (CV, FX-322) has been shown to regenerate mammalian cochlear hair cells ex vivo. The objectives were to characterize the cochlear pharmacokinetic profile of CV in guinea pigs, then measure FX-322 in human perilymph samples, and finally assess safety and audiometric effects of FX-322 in humans with chronic SNHL.STUDY DESIGNS: Middle ear residence, cochlear distribution, and elimination profiles of FX-322 were assessed in guinea pigs. Human perilymph sampling following intratympanic FX-322 dosing was performed in an open-label study in cochlear implant subjects. Unilateral intratympanic FX-322 was assessed in a Phase 1b prospective, randomized, double-blinded, placebo-controlled clinical trial.SETTING: Three private otolaryngology practices in the US.PATIENTS: Individuals diagnosed with mild to moderately severe chronic SNHL (≤70 dB standard pure-tone average) in one or both ears that was stable for ≥6 months, medical histories consistent with noise-induced or idiopathic sudden SNHL, and no significant vestibular symptoms.INTERVENTIONS: Intratympanic FX-322.MAIN OUTCOME MEASURES: Pharmacokinetics of FX-322 in perilymph and safety and audiometric effects.RESULTS: After intratympanic delivery in guinea pigs and humans, FX-322 levels in the cochlear extended high-frequency region were observed and projected to be pharmacologically active in humans. A single dose of FX-322 in SNHL subjects was well tolerated with mild, transient treatment-related adverse events (n = 15 FX-322 vs 8 placebo). Of the six patients treated with FX-322 who had baseline word recognition in quiet scores below 90%, four showed clinically meaningful improvements (absolute word recognition improved 18-42%, exceeding the 95% confidence interval determined by previously published criteria). No significant changes in placebo-injected ears were observed. At the group level, FX-322 subjects outperformed placebo group in word recognition in quiet when averaged across all time points, with a mean improvement from baseline of 18.9% (p = 0.029). For words in noise, the treated group showed a mean 1.3 dB signal-to-noise ratio improvement (p = 0.012) relative to their baseline scores while placebo-treated subjects did not (-0.21 dB, p = 0.71).CONCLUSIONS: Delivery of FX-322 to the extended high-frequency region of the cochlea is well tolerated and enhances speech recognition performance in multiple subjects with stable chronic hearing loss.Copyright © 2021 The Author(s). Published by Wolters Kluwer Health, Inc. on behalf of Otology & Neurotology, Inc.DOI: 10.1097/MAO.0000000000003120PMCID: PMC8279894",pubmed,33617194,10.1097/MAO.0000000000003120
predictive coding and stochastic resonance as fundamental principles of auditory phantom perception,"11. Brain. 2023 Dec 1;146(12):4809-4825. doi: 10.1093/brain/awad255.Predictive coding and stochastic resonance as fundamental principles of auditory phantom perception.Schilling A(1)(2), Sedley W(3), Gerum R(2)(4), Metzner C(1), Tziridis K(1), Maier A(5), Schulze H(1), Zeng FG(6), Friston KJ(7), Krauss P(1)(2)(5).Author information:(1)Neuroscience Lab, University Hospital Erlangen, 91054 Erlangen, Germany.(2)Cognitive Computational Neuroscience Group, University Erlangen-Nürnberg, 91058 Erlangen, Germany.(3)Translational and Clinical Research Institute, Newcastle University Medical School, Newcastle upon Tyne NE2 4HH, UK.(4)Department of Physics and Astronomy and Center for Vision Research, York University, Toronto, ON M3J 1P3, Canada.(5)Pattern Recognition Lab, University Erlangen-Nürnberg, 91058 Erlangen, Germany.(6)Center for Hearing Research, Departments of Anatomy and Neurobiology, Biomedical Engineering, Cognitive Sciences, Otolaryngology-Head and Neck Surgery, University of California Irvine, Irvine, CA 92697, USA.(7)Wellcome Centre for Human Neuroimaging, Institute of Neurology, University College London, London WC1N 3AR, UK.Mechanistic insight is achieved only when experiments are employed to test formal or computational models. Furthermore, in analogy to lesion studies, phantom perception may serve as a vehicle to understand the fundamental processing principles underlying healthy auditory perception. With a special focus on tinnitus-as the prime example of auditory phantom perception-we review recent work at the intersection of artificial intelligence, psychology and neuroscience. In particular, we discuss why everyone with tinnitus suffers from (at least hidden) hearing loss, but not everyone with hearing loss suffers from tinnitus. We argue that intrinsic neural noise is generated and amplified along the auditory pathway as a compensatory mechanism to restore normal hearing based on adaptive stochastic resonance. The neural noise increase can then be misinterpreted as auditory input and perceived as tinnitus. This mechanism can be formalized in the Bayesian brain framework, where the percept (posterior) assimilates a prior prediction (brain's expectations) and likelihood (bottom-up neural signal). A higher mean and lower variance (i.e. enhanced precision) of the likelihood shifts the posterior, evincing a misinterpretation of sensory evidence, which may be further confounded by plastic changes in the brain that underwrite prior predictions. Hence, two fundamental processing principles provide the most explanatory power for the emergence of auditory phantom perceptions: predictive coding as a top-down and adaptive stochastic resonance as a complementary bottom-up mechanism. We conclude that both principles also play a crucial role in healthy auditory perception. Finally, in the context of neuroscience-inspired artificial intelligence, both processing principles may serve to improve contemporary machine learning techniques.© The Author(s) 2023. Published by Oxford University Press on behalf of the Guarantors of Brain.DOI: 10.1093/brain/awad255PMCID: PMC10690027",pubmed,37503725,10.1093/brain/awad255
prevalence of hearing impairement in the district of lucknow india,"249. Indian J Public Health. 2011 Apr-Jun;55(2):132-4. doi: 10.4103/0019-557X.85251.Prevalence of hearing impairement in the district of Lucknow, India.Mishra A(1), Verma V, Shukla GK, Mishra SC, Dwivedi R.Author information:(1)Department of Otolaryngology, King George's Medical College, Lucknow, India. anupampenn@yahoo.comA multi-cluster study (survey) was carried out by department of ENT KG Medical University, Lucknow from July 2003 to August 2004 in rural and urban population of Lucknow district to estimate prevalence and causes of hearing impairment in the community. Data included audiological profile and basic ear examination that was analysed through EARFORM software program of WHO. Overall hearing impairment was seen in 15.14% of rural as opposed to 5.9% of urban population. A higher prevalence of disabling hearing impairment (DHI) in elderly and deafness in 0-10 years age group was seen. The prevalence of sensorineural deafness necessitating hearing aids was 20% in rural and 50% in urban areas respectively. The presence of DHI was seen in 1/2 urban subjects and 1/3rd of rural counterparts. The incidence of cerumen / debris was very common in both types of population and the need of surgery was much more amongst rural subjects indicating more advanced / dangerous ear disease.DOI: 10.4103/0019-557X.85251",pubmed,21941050,10.4103/0019-557X.85251
altered neural encoding of vowels in noise does not affect behavioral vowel discrimination in gerbils with agerelated hearing loss,"573. Front Neurosci. 2023 Nov 14;17:1238941. doi: 10.3389/fnins.2023.1238941. eCollection 2023.Altered neural encoding of vowels in noise does not affect behavioral vowel discrimination in gerbils with age-related hearing loss.Heeringa AN(1), Jüchter C(1), Beutelmann R(1), Klump GM(1), Köppl C(1).Author information:(1)Research Centre Neurosensory Science and Cluster of Excellence ""Hearing4all"", Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg, Oldenburg, Germany.INTRODUCTION: Understanding speech in a noisy environment, as opposed to speech in quiet, becomes increasingly more difficult with increasing age. Using the quiet-aged gerbil, we studied the effects of aging on speech-in-noise processing. Specifically, behavioral vowel discrimination and the encoding of these vowels by single auditory-nerve fibers were compared, to elucidate some of the underlying mechanisms of age-related speech-in-noise perception deficits.METHODS: Young-adult and quiet-aged Mongolian gerbils, of either sex, were trained to discriminate a deviant naturally-spoken vowel in a sequence of vowel standards against a speech-like background noise. In addition, we recorded responses from single auditory-nerve fibers of young-adult and quiet-aged gerbils while presenting the same speech stimuli.RESULTS: Behavioral vowel discrimination was not significantly affected by aging. For both young-adult and quiet-aged gerbils, the behavioral discrimination between /eː/ and /iː/ was more difficult to make than /eː/ vs. /aː/ or /iː/ vs. /aː/, as evidenced by longer response times and lower d' values. In young-adults, spike timing-based vowel discrimination agreed with the behavioral vowel discrimination, while in quiet-aged gerbils it did not. Paradoxically, discrimination between vowels based on temporal responses was enhanced in aged gerbils for all vowel comparisons. Representation schemes, based on the spectrum of the inter-spike interval histogram, revealed stronger encoding of both the fundamental and the lower formant frequencies in fibers of quiet-aged gerbils, but no qualitative changes in vowel encoding. Elevated thresholds in combination with a fixed stimulus level, i.e., lower sensation levels of the stimuli for old individuals, can explain the enhanced temporal coding of the vowels in noise.DISCUSSION: These results suggest that the altered auditory-nerve discrimination metrics in old gerbils may mask age-related deterioration in the central (auditory) system to the extent that behavioral vowel discrimination matches that of the young adults.Copyright © 2023 Heeringa, Jüchter, Beutelmann, Klump and Köppl.DOI: 10.3389/fnins.2023.1238941PMCID: PMC10682387",pubmed,38033551,10.3389/fnins.2023.1238941
unilateral deafness in adults effects on communication and social interaction,"Objectives: The aim of this study was to explore the self-reported consequences of profound unilateral deafness regarding communication and social interaction and to compare subjects' speech perception scores to those of normal-hearing individuals who were rendered temporarily unilaterally deaf. Methods: Cross-sectional data from 30 individuals with unilateral deafness and 30 individuals with normal hearing (age, 14 to 75 years) were obtained through structured interviews and tests of audiovisual, auditory-only, and visual-only speech perception. Results: In individuals with permanent unilateral deafness, 93% reported that hearing loss affected communication. Eighty-seven percent reported problems with speech perception in noisy settings. Other consequences were feelings of exclusion, reduced well-being, and extensive use of speech perception strategies. Inducing temporary unilateral deafness (through short-term blocking of one ear) in normal-hearing subjects produced similar effects on speech perception (27% score) as those experienced by unilaterally deaf subjects (25% score). Conclusions: Individuals with unilateral deafness experienced a significant disability in auditory function that affected their communication and social interaction. The major challenges were communicating in situations with background noise, in poor acoustic surroundings, and with limited access to speech-reading or direct listening. Under certain listening conditions, long-standing unilateral deafness seemed to yield no advantage over temporary deafness on one side.",cinahl,34894,
objective tinnitus measurement via fnirs and machine learning,,cinahl,7457472,10.1097/01.hj.0000734228.95140.aa
loss of baiap2l2 destabilizes the transducing stereocilia of cochlear hair cells and leads to deafness,"Key points: Mechanoelectrical transduction at auditory hair cells requires highly specialized stereociliary bundles that project from their apical surface, forming a characteristic graded ‘staircase’ structure. The morphogenesis and maintenance of these stereociliary bundles is a tightly regulated process requiring the involvement of several actin-binding proteins, many of which are still unidentified. We identify a new stereociliary protein, the I-BAR protein BAIAP2L2, which localizes to the tips of the shorter transducing stereocilia in both inner and outer hair cells (IHCs and OHCs). We find that Baiap2l2 deficient mice lose their second and third rows of stereocilia, their mechanoelectrical transducer current, and develop progressive hearing loss, becoming deaf by 8 months of age. We demonstrate that BAIAP2L2 localization to stereocilia tips is dependent on the motor protein MYO15A and its cargo EPS8. We propose that BAIAP2L2 is a new key protein required for the maintenance of the transducing stereocilia in mature cochlear hair cells. Abstract: The transduction of sound waves into electrical signals depends upon mechanosensitive stereociliary bundles that project from the apical surface of hair cells within the cochlea. The height and width of these actin-based stereocilia is tightly regulated throughout life to establish and maintain their characteristic staircase-like structure, which is essential for normal mechanoelectrical transduction. Here, we show that BAIAP2L2, a member of the I-BAR protein family, is a newly identified hair bundle protein that is localized to the tips of the shorter rows of transducing stereocilia in mouse cochlear hair cells. BAIAP2L2 was detected by immunohistochemistry from postnatal day 2.5 (P2.5) throughout adulthood. In Baiap2l2 deficient mice, outer hair cells (OHCs), but not inner hair cells (IHCs), began to lose their third row of stereocilia and showed a reduction in the size of the mechanoelectrical transducer current from just after P9. Over the following post-hearing weeks, the ordered staircase structure of the bundle progressively deteriorates, such that, by 8 months of age, both OHCs and IHCs of Baiap2l2 deficient mice have lost most of the second and third rows of stereocilia and become deaf. We also found that BAIAP2L2 interacts with other key stereociliary proteins involved in normal hair bundle morphogenesis, such as CDC42, RAC1, EPS8 and ESPNL. Furthermore, we show that BAIAP2L2 localization to the stereocilia tips depends on the motor protein MYO15A and its cargo EPS8. We propose that BAIAP2L2 is key to maintenance of the normal actin structure of the transducing stereocilia in mature mouse cochlear hair cells. © 2020 The Authors. The Journal of Physiology published by John Wiley & Sons Ltd on behalf of The Physiological Society.",scopus,2-s2.0-85096764698,10.1113/JP280670
design and evaluation of a cochlear implant strategy based on a phantom channel,"350. PLoS One. 2015 Mar 25;10(3):e0120148. doi: 10.1371/journal.pone.0120148. eCollection 2015.Design and evaluation of a cochlear implant strategy based on a ""Phantom"" channel.Nogueira W(1), Litvak LM(2), Saoji AA(2), Büchner A(1).Author information:(1)Department of Otolaryngology, Medical University Hannover, Cluster of Excellence ""Hearing4all"", Hannover, Germany.(2)Research and Technology Group, Advanced Bionics LLC, Valencia CA, USA.Unbalanced bipolar stimulation, delivered using charge balanced pulses, was used to produce ""Phantom stimulation"", stimulation beyond the most apical contact of a cochlear implant's electrode array. The Phantom channel was allocated audio frequencies below 300 Hz in a speech coding strategy, conveying energy some two octaves lower than the clinical strategy and hence delivering the fundamental frequency of speech and of many musical tones. A group of 12 Advanced Bionics cochlear implant recipients took part in a chronic study investigating the fitting of the Phantom strategy and speech and music perception when using Phantom. The evaluation of speech in noise was performed immediately after fitting Phantom for the first time (Session 1) and after one month of take-home experience (Session 2). A repeated measures of analysis of variance (ANOVA) within factors strategy (Clinical, Phantom) and interaction time (Session 1, Session 2) revealed a significant effect for the interaction time and strategy. Phantom obtained a significant improvement in speech intelligibility after one month of use. Furthermore, a trend towards a better performance with Phantom (48%) with respect to F120 (37%) after 1 month of use failed to reach significance after type 1 error correction. Questionnaire results show a preference for Phantom when listening to music, likely driven by an improved balance between high and low frequencies.DOI: 10.1371/journal.pone.0120148PMCID: PMC4373925",pubmed,25806818,10.1371/journal.pone.0120148
can homeostatic plasticity in deafferented primary auditory cortex lead to travelling waves of excitation,"407. J Comput Neurosci. 2011 Apr;30(2):279-99. doi: 10.1007/s10827-010-0256-1. Epub 2010 Jul 10.Can homeostatic plasticity in deafferented primary auditory cortex lead to travelling waves of excitation?Chrostowski M(1), Yang L, Wilson HR, Bruce IC, Becker S.Author information:(1)McMaster Integrative Neuroscience Discovery & Study, McMaster University, 1280 Main Street West, Hamilton, ON, Canada. chrostm@mcmaster.caTravelling waves of activity in neural circuits have been proposed as a mechanism underlying a variety of neurological disorders, including epileptic seizures, migraine auras and brain injury. The highly influential Wilson-Cowan cortical model describes the dynamics of a network of excitatory and inhibitory neurons. The Wilson-Cowan equations predict travelling waves of activity in rate-based models that have sufficiently reduced levels of lateral inhibition. Travelling waves of excitation may play a role in functional changes in the auditory cortex after hearing loss. We propose that down-regulation of lateral inhibition may be induced in deafferented cortex via homeostatic plasticity mechanisms. We use the Wilson-Cowan equations to construct a spiking model of the primary auditory cortex that includes a novel, mathematically formalized description of homeostatic plasticity. In our model, the homeostatic mechanisms respond to hearing loss by reducing inhibition and increasing excitation, producing conditions under which travelling waves of excitation can emerge. However, our model predicts that the presence of spontaneous activity prevents the development of long-range travelling waves of excitation. Rather, our simulations show short-duration excitatory waves that cancel each other out. We also describe changes in spontaneous firing, synchrony and tuning after simulated hearing loss. With the exception of shifts in characteristic frequency, changes after hearing loss were qualitatively the same as empirical findings. Finally, we discuss possible applications to tinnitus, the perception of sound without an external stimulus.DOI: 10.1007/s10827-010-0256-1",pubmed,20623168,10.1007/s10827-010-0256-1
deafness weakens interareal couplings in the auditory cortex,"817. Front Neurosci. 2021 Jan 21;14:625721. doi: 10.3389/fnins.2020.625721. eCollection 2020.Deafness Weakens Interareal Couplings in the Auditory Cortex.Yusuf PA(1)(2)(3), Hubka P(2)(3), Tillein J(2)(3)(4)(5), Vinck M(6)(7), Kral A(2)(3)(8).Author information:(1)Department of Medical Physics/Medical Technology Core Cluster IMERI, Faculty of Medicine, University of Indonesia, Jakarta, Indonesia.(2)Institute of AudioNeuroTechnology, Hannover Medical School, Hanover, Germany.(3)Department of Experimental Otology of the ENT Clinics, Hannover Medical School, Hanover, Germany.(4)Department of Otorhinolaryngology, Goethe University, Frankfurt am Main, Germany.(5)MedEL Company, Innsbruck, Austria.(6)Ernst Strüngmann Institut for Neuroscience in Cooperation with Max Planck Society, Frankfurt, Germany.(7)Donders Centre for Neuroscience, Radboud University, Department of Neuroinformatics, Nijmegen, Netherlands.(8)Department of Biomedical Sciences, School of Medicine and Health Sciences, Macquarie University, Sydney, NSW, Australia.The function of the cerebral cortex essentially depends on the ability to form functional assemblies across different cortical areas serving different functions. Here we investigated how developmental hearing experience affects functional and effective interareal connectivity in the auditory cortex in an animal model with years-long and complete auditory deprivation (deafness) from birth, the congenitally deaf cat (CDC). Using intracortical multielectrode arrays, neuronal activity of adult hearing controls and CDCs was registered in the primary auditory cortex and the secondary posterior auditory field (PAF). Ongoing activity as well as responses to acoustic stimulation (in adult hearing controls) and electric stimulation applied via cochlear implants (in adult hearing controls and CDCs) were analyzed. As functional connectivity measures pairwise phase consistency and Granger causality were used. While the number of coupled sites was nearly identical between controls and CDCs, a reduced coupling strength between the primary and the higher order field was found in CDCs under auditory stimulation. Such stimulus-related decoupling was particularly pronounced in the alpha band and in top-down direction. Ongoing connectivity did not show such a decoupling. These findings suggest that developmental experience is essential for functional interareal interactions during sensory processing. The outcomes demonstrate that corticocortical couplings, particularly top-down connectivity, are compromised following congenital sensory deprivation.Copyright © 2021 Yusuf, Hubka, Tillein, Vinck and Kral.DOI: 10.3389/fnins.2020.625721PMCID: PMC7858676",pubmed,33551733,10.3389/fnins.2020.625721
convolutional neural network based real time arabic speech recognition to arabic braille for hearing and visually impaired,"172. Front Public Health. 2022 May 27;10:898355. doi: 10.3389/fpubh.2022.898355. eCollection 2022.Convolutional Neural Network Based Real Time Arabic Speech Recognition to Arabic Braille for Hearing and Visually Impaired.Bhatia S(1), Devi A(2), Alsuwailem RI(1), Mashat A(3).Author information:(1)Department of Information Systems, College of Computer Sciences and Information Technology, King Faisal University, Al Hasa, Saudi Arabia.(2)Research Head, AP3 Solutions, Chennai, India.(3)Faculty of Computing and Information Technology, King Abdulaziz University, Rabigh, Saudi Arabia.Natural Language Processing (NLP) is a group of theoretically inspired computer structures for analyzing and modeling clearly going on texts at one or extra degrees of linguistic evaluation to acquire human-like language processing for quite a few activities and applications. Hearing and visually impaired people are unable to see entirely or have very low vision, as well as being unable to hear completely or having a hard time hearing. It is difficult to get information since both hearing and vision, which are crucial organs for receiving information, are harmed. Hearing and visually impaired people are considered to have a substantial information deficit, as opposed to people who just have one handicap, such as blindness or deafness. Visually and hearing-impaired people who are unable to communicate with the outside world may experience emotional loneliness, which can lead to stress and, in extreme cases, serious mental illness. As a result, overcoming information handicap is a critical issue for visually and hearing-impaired people who want to live active, independent lives in society. The major objective of this study is to recognize Arabic speech in real time and convert it to Arabic text using Convolutional Neural Network-based algorithms before saving it to an SD card. The Arabic text is then translated into Arabic Braille characters, which are then used to control the Braille pattern via a Braille display with a solenoid drive. The Braille lettering triggered on the finger was deciphered by visually and hearing challenged participants who were proficient in Braille reading. The CNN, in combination with the ReLU model learning parameters, is fine-tuned for optimization, resulting in a model training accuracy of 90%. The tuned parameters model's testing results show that adding the ReLU activation function to the CNN model improves recognition accuracy by 84 % when speaking Arabic digits.Copyright © 2022 Bhatia, Devi, Alsuwailem and Mashat.DOI: 10.3389/fpubh.2022.898355PMCID: PMC9197259",pubmed,35712297,10.3389/fpubh.2022.898355
a binaural steering beamformer system for enhancing a moving speech source,"444. Trends Hear. 2015 Dec 30;19:2331216515618903. doi: 10.1177/2331216515618903.A Binaural Steering Beamformer System for Enhancing a Moving Speech Source.Adiloğlu K(1), Kayser H(2), Baumgärtel RM(2), Rennebeck S(3), Dietz M(2), Hohmann V(4).Author information:(1)HörTech gGmbH, Oldenburg, Germany Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany k.adiloglu@hoertech.de.(2)Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany Medizinische Physik, Universität Oldenburg, Germany.(3)Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany Medizinische Physik, Universität Oldenburg, Germany Jade Hochschule, Oldenburg, Germany.(4)HörTech gGmbH, Oldenburg, Germany Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany Medizinische Physik, Universität Oldenburg, Germany.In many daily life communication situations, several sound sources are simultaneously active. While normal-hearing listeners can easily distinguish the target sound source from interfering sound sources-as long as target and interferers are spatially or spectrally separated-and concentrate on the target, hearing-impaired listeners and cochlear implant users have difficulties in making such a distinction. In this article, we propose a binaural approach composed of a spatial filter controlled by a direction-of-arrival estimator to track and enhance a moving target sound. This approach was implemented on a real-time signal processing platform enabling experiments with test subjects in situ. To evaluate the proposed method, a data set of sound signals with a single moving sound source in an anechoic diffuse noise environment was generated using virtual acoustics. The proposed steering method was compared with a fixed (nonsteering) method that enhances sound from the frontal direction in an objective evaluation and subjective experiments using this database. In both cases, the obtained results indicated a significant improvement in speech intelligibility and quality compared with the unprocessed signal. Furthermore, the proposed method outperformed the nonsteering method.© The Author(s) 2015.DOI: 10.1177/2331216515618903PMCID: PMC4771043",pubmed,26721924,10.1177/2331216515618903
slope analysis of auditory brainstem responses in children at risk of central auditory processing disorders,"330. Scand Audiol. 1999;28(2):85-90. doi: 10.1080/010503999424806.Slope analysis of Auditory Brainstem Responses in children at risk of central auditory processing disorders.Gopal KV(1), Kowalski J.Author information:(1)Department of Speech and Hearing Sciences, University of North Texas, Denton 76203-5010, USA. gopal@unt.eduThe method of slope vectors was used to quantify Auditory Brainstem Responses (ABR) obtained from nine normal children and nine children at risk for central auditory processing disorders (CAPD) with language impairment, for monaural and binaural stimulation conditions. Slopes thus obtained were subjected to K-Means Cluster Analysis. Distinction between the two groups was obtained only for binaural stimulation conditions, wherein all normal children were grouped under cluster 1 with higher slope values and 6 out of 9 CAPD children were grouped under cluster 2 with lower slopes. The results suggest that there may be several subcategories among children who are found to be at risk for CAPD. One of the subcategories may comprise children who exhibit poor ABR morphology, especially during binaural stimulation conditions, which could be due to binaural interference.DOI: 10.1080/010503999424806",pubmed,10384895,10.1080/010503999424806
speech communication  15th itg conference,The proceedings contain 55 papers. The topics discussed include: ad hoc distributed microphones clustering: a comparative analysis on using coherence and signal-specific features; exploiting an external microphone to improve time-difference-of-arrival estimates for Euclidean distance matrix-based source localization; hearing impairment in crowdsourced speech quality assessments: its effect and screening with digit triplet hearing test; long-term conversation analysis: exploring utility and privacy; towards a natural reproduction of binaural recordings: combining binaural cue adaptation and adaptive crosstalk cancellation; screening of Alzheimer’s dementia up to 12 years ahead from conversational speech of ILSE study; speaker’s articulatory strategy analysis: theoretical framework and preliminary experiment; and speech-based age and gender prediction with transformers.,scopus,2-s2.0-85183638327,
binaural modelbased dynamicrange compression,"22. Int J Audiol. 2018 Jun;57(sup3):S31-S42. doi: 10.1080/14992027.2018.1425554. Epub 2018 Jan 26.Binaural model-based dynamic-range compression.Ernst SMA(1), Kortlang S(1), Grimm G(1)(2), Bisitz T(2), Kollmeier B(1)(2), Ewert SD(1).Author information:(1)a Medizinische Physik and Cluster of Excellence Hearing4all , Carl-von-Ossietzky Universität Oldenburg , Oldenburg , Germany and.(2)b HörTech gGmbH , Oldenburg , Germany.OBJECTIVE: Binaural cues such as interaural level differences (ILDs) are used to organise auditory perception and to segregate sound sources in complex acoustical environments. In bilaterally fitted hearing aids, dynamic-range compression operating independently at each ear potentially alters these ILDs, thus distorting binaural perception and sound source segregation.DESIGN: A binaurally-linked model-based fast-acting dynamic compression algorithm designed to approximate the normal-hearing basilar membrane (BM) input-output function in hearing-impaired listeners is suggested. A multi-center evaluation in comparison with an alternative binaural and two bilateral fittings was performed to assess the effect of binaural synchronisation on (a) speech intelligibility and (b) perceived quality in realistic conditions.STUDY SAMPLE: 30 and 12 hearing impaired (HI) listeners were aided individually with the algorithms for both experimental parts, respectively.RESULTS: A small preference towards the proposed model-based algorithm in the direct quality comparison was found. However, no benefit of binaural-synchronisation regarding speech intelligibility was found, suggesting a dominant role of the better ear in all experimental conditions.CONCLUSION: The suggested binaural synchronisation of compression algorithms showed a limited effect on the tested outcome measures, however, linking could be situationally beneficial to preserve a natural binaural perception of the acoustical environment.DOI: 10.1080/14992027.2018.1425554",pubmed,29373937,10.1080/14992027.2018.1425554
ecap analysis in cochlear implant patients as a function of patients age and electrodedesign,"568. Eur Ann Otorhinolaryngol Head Neck Dis. 2016 Jun;133 Suppl 1:S1-3. doi: 10.1016/j.anorl.2016.04.015. Epub 2016 Jun 1.ECAP analysis in cochlear implant patients as a function of patient's age and electrode-design.Christov F(1), Munder P(2), Berg L(2), Bagus H(2), Lang S(2), Arweiler-Harbeck D(2).Author information:(1)Department for head and neck surgery, Universitätsklinikum, Essen, Germany. Electronic address: florian.christov@uk-essen.de.(2)Department for head and neck surgery, Universitätsklinikum, Essen, Germany.INTRODUCTION: Electric compound action potentials (ECAPs) provide information about the nerve's and device's function in and after cochlear implantation. In general, lower ECAP values are expected to generate better results. Aim was an analysis of ECAPs in the course of time as a function of the patient's age and electrode design.PATIENTS AND METHODS: Between 2008 and 2013, 168 patients of eight defined age groups were included into the investigation. NRTs were measured intraoperatively, after 6 and after 12months.RESULTS: The intraoperative mean value of ECAP was 174.14CL (current level) and decreased after 6months to 156.38CL. Highest ECAPs were achieved intraoperatively in the clusters ""younger than 18months"" (181.04CL) and ""older than 80 years"" (190.45CL). CI 422 showed apparently higher ECAP thresholds (182.69) during surgery than CI 24 RE (171.47) and CI 512 (170.64).CONCLUSION: ECAPs are a well-established method to get information about the CI's and nerve's function during and after surgery. After initial higher values NRTs decrease after 6months and remain stable in the following controls. Very young and older patients tend to have higher thresholds than middle-aged groups. Perimodiolar electrodes are significantly attached to lower values because there is a closer nerve-electrode interaction.Copyright © 2016 Elsevier Masson SAS. All rights reserved.DOI: 10.1016/j.anorl.2016.04.015",pubmed,27262349,10.1016/j.anorl.2016.04.015
prevalence of hearing disorders in china a populationbased survey in four provinces of china,"677. Zhonghua Er Bi Yan Hou Tou Jing Wai Ke Za Zhi. 2016 Nov 7;51(11):819-825. doi: 10.3760/cma.j.issn.1673-0860.2016.11.004.[Prevalence of hearing disorders in China: a population-based survey in four provinces of China].[Article in Chinese]Hu XY(1), Zheng XY(2), Ma FR(3), Long M(1), Han R(1), Zhou LJ(1), Wang F(1), Gong R(2), Pan T(3), Zhang SX(3), Du B(4), Jin P(4), Guo CY(5), Zheng YQ(6), Liu M(7), He LH(8), Qiu JH(9), Xu M(10), Song L(11), Xu XH(12), Liu XW(13), Wang SP(14).Author information:(1)China Rehabilitation Research Center for Deaf Children, Beijing 100029, China.(2)Institute of Population Research, Peking University, Beijing 100871, China.(3)Department of Otorhinolaryngology, Peking University Third Hospital, Beijing 100191, China.(4)Department of Otorhinolaryngology Head and Neck Surgery, the First Hospital of Jilin University, Changchun 130021, China.(5)Language and Hearing Rehabilitation Center of Jilin Province, Changchun 130052, China.(6)Department of Otorhinolaryngology Head and Neck Surgery, Sun Yat-Sen Memorial Hospital, Sun Yat-Sen University, Guangzhou 510120, China.(7)Otorhinolaryngology Hospital of First Affiliated Hospital of Sun Yetsan University, Guangzhou 510080, China.(8)Guangdong Rehabilitation Centre, Guangzhou 510055, China.(9)Department of Otorhinolaryngology Head and Neck Surgery, Xijing Hospital, Xi'an 710032, China.(10)Department of Otorhinolaryngology Head and Neck Surgery, Second Affiliated Hospital, Xi'an Jiaotong University School of Medicine, Xi'an 710004, China.(11)Language and Hearing Rehabilitation Center of Shanxi Province, Xi'an 710016, China.(12)Department of Otorhinolaryngology, Lanzhou General Hospital, Lanzhou Command, Lanzhou 730050, China.(13)Department of Otorhinolaryngology, Lanzhou University Second Hospital, Lanzhou 730030, China.(14)Hearing and Language Rehabilitation Center of Gangsu Province, Lanzhou 730050, China.Objective: To investigate the prevalence, severity of hearing disorders and demographics of people with hearing disorders based on the whole population in Jilin, Guangdong, Shannxi and Gansu provinces in China. Methods: According to "" WHO Ear and Hearing Disorders Survey Protocol"" , 144 clusters were chosen with probability proportional sampling(PPS) method from the four provinces covering 194, 688, 061 residents. Audiological test, otological examination and questionnaire surveying were conducted for all samples from August, 2014 to September, 2015. The hearing disorders were classified according to WHO criteria and classification. Results: Among 47 511 targeted residents, 45, 052 individuals (94.82% response rate) participated in the survey. The standardized prevalence rates of hearing disorders and disabling hearing disorders were 15.84 % and 5.17 % respectively. Almost 50% of people with hearing disorders had no awareness of it or its starting time. There was significant difference in the prevalence among people of different ages, genders, occupations, provinces, marital status and education levels. The prevalence of hearing disorders increased significantly as age grew. People above 60 years old occupied 55.31% of the total hearing disorders. The prevalence of hearing disorders among male, people of low education and those who lost husband or wife, as well as workers and farmers was relatively higher. Conclusions: The prevalence of hearing disorders is high, and hearing disorders are "" invisible"" . Demographics and socioeconomic factors significantly influence the prevalence of hearing disorders.DOI: 10.3760/cma.j.issn.1673-0860.2016.11.004",pubmed,27938607,10.3760/cma.j.issn.1673-0860.2016.11.004
vertebrobasilar and basilar dolichoectasia causing audiovestibular manifestations a case series with a brief literature review,"716. Diagnostics (Basel). 2023 May 16;13(10):1750. doi: 10.3390/diagnostics13101750.Vertebrobasilar and Basilar Dolichoectasia Causing Audio-Vestibular Manifestations: A Case Series with a Brief Literature Review.Frosolini A(1)(2), Fantin F(1), Caragli V(3), Franz L(1)(4), Fermo S(1), Inches I(5), Lovato A(6), Genovese E(3), Marioni G(1), de Filippis C(1).Author information:(1)Phoniatris and Audiology Unit, Department of Neuroscience DNS, University of Padova, 31100 Treviso, Italy.(2)Maxillofacial Surgery Unit, Department of Medical Biotechnologies, University of Siena, 53100 Siena, Italy.(3)Audiology Unit, Department of Diagnostic, Clinical and Public Health Medicine, University of Modena and Reggio Emilia, 41100 Modena, Italy.(4)Artificial Intelligence in Medicine and Innovation in Clinical Research and Methodology (PhD Program), Department of Clinical and Experimental Sciences, University of Brescia, 25100 Brescia, Italy.(5)Neuroradiology Unit, Treviso Hospital, 31100 Treviso, Italy.(6)Otorhinolaryngology Unit, Department of Surgical Specialties, Vicenza Civil Hospital, 36100 Vicenza, Italy.Audio-vestibular symptoms can arise from vertebrobasilar dolichoectasia (VBD) and basilar dolichoectasia (BD). Given the dearth of available information, herein we reported our experience with different audio-vestibular disorders (AVDs) observed in a case series of VBD patients. Furthermore, a literature review analyzed the possible relationships between epidemiological, clinical, and neuroradiological findings and audiological prognosis. The electronic archive of our audiological tertiary referral center was screened. All identified patients had a diagnosis of VBD/BD according to Smoker's criteria and a comprehensive audiological evaluation. PubMed and Scopus databases were searched for inherent papers published from 1 January 2000 to 1 March 2023. Three subjects were found; all of them had high blood pressure, and only the patient with high-grade VBD showed progressive sensorineural hearing loss (SNHL). Seven original studies were retrieved from the literature, overall including 90 cases. AVDs were more common in males and present in late adulthood (mean age 65 years, range 37-71), with symptoms including progressive and sudden SNHL, tinnitus, and vertigo. Diagnosis was made using different audiological and vestibular tests and cerebral MRI. Management was hearing aid fitting and long-term follow-up, with only one case of microvascular decompression surgery. The mechanism by which VBD and BD can cause AVD is debated, with the main hypothesis being VIII cranial nerve compression and vascular impairment. Our reported cases suggested the possibility of central auditory dysfunction of retro-cochlear origin due to VBD, followed by rapidly progressing SNHL and/or unnoticed sudden SNHL. More research is needed to better understand this audiological entity and achieve an evidence-based effective treatment.DOI: 10.3390/diagnostics13101750PMCID: PMC10217288",pubmed,37238234,10.3390/diagnostics13101750
auditory evoked potential in stranded melonheaded whales peponocephala electra with severe hearing loss and possibly caused by anthropogenic noise pollution,"734. Ecotoxicol Environ Saf. 2021 Nov 30;228:113047. doi: 10.1016/j.ecoenv.2021.113047. Online ahead of print.Auditory evoked potential in stranded melon-headed whales (Peponocephala electra): With severe hearing loss and possibly caused by anthropogenic noise pollution.Wang ZT(1), Supin AY(2), Akamatsu T(3), Duan PX(1), Yang YN(1), Wang KX(4), Wang D(5).Author information:(1)Key Laboratory of Aquatic Biodiversity and Conservation of the Chinese Academy of Sciences, Institute of Hydrobiology, Chinese Academy of Sciences, 7 South Donghu Road, Wuhan 430072, China.(2)Institute of Ecology and Evolution of the Russian Academy of Sciences, Moscow 119071, Russia.(3)Ocean Policy Research Institute, the Sasakawa Peace Foundation, Tokyo, Japan.(4)Key Laboratory of Aquatic Biodiversity and Conservation of the Chinese Academy of Sciences, Institute of Hydrobiology, Chinese Academy of Sciences, 7 South Donghu Road, Wuhan 430072, China. Electronic address: wangk@ihb.ac.cn.(5)Key Laboratory of Aquatic Biodiversity and Conservation of the Chinese Academy of Sciences, Institute of Hydrobiology, Chinese Academy of Sciences, 7 South Donghu Road, Wuhan 430072, China. Electronic address: wangd@ihb.ac.cn.Highly concentrated live mass stranding events of dolphins and whales happened in the eastern coast of China between June and October 2021. The current study adopted the non-invasive auditory evoked-potential technique to investigate the hearing threshold of a stranded melon headed whale (Peponocephala electra) at a frequency range of between 9.5 and 181 kHz. It was found that, at the frequency range of from 10 to 100 kHz, hearing thresholds for the animal were between 20 and 65 dB higher than those of its phylogenetically closest species (Pygmy killer whale). The severe hearing loss in the melon headed whale was probably caused by transient intense anthropogenic sonar or chronic shipping noise exposures. The hearing loss could have been the cause for the observed temporal and spatial clustered stranding events. Therefore, there is need for noise mitigation strategies to reduce noise exposure levels for marine mammals in the coastal areas of China.Copyright © 2021 The Authors. Published by Elsevier Inc. All rights reserved.DOI: 10.1016/j.ecoenv.2021.113047",pubmed,34861441,10.1016/j.ecoenv.2021.113047
phosphatidylinositol 4kinase  is required for the ciliogenesis of zebrafish otic vesicle,"The primary cilium, an important microtubule-based organelle, protrudes from nearly all the vertebrate cells. The motility of cilia is necessary for various developmental and physiological processes. Phosphoinositides (PIs) and its metabolite, PtdIns(4,5)P2, have been revealed to contribute to cilia assembly and disassembly. As an important kinase of the PI pathway and signaling, phosphatidylinositol 4-kinase β (PI4KB) is the one of the most extensively studied phosphatidylinositol 4-kinase isoform. However, its potential roles in organ development remain to be characterized. To investigate the developmental role of Pi4kb, especially its function on zebrafish ciliogenesis, we generated pi4kb deletion mutants using clustered regularly interspaced short palindromic repeats (CRISPR)/CRISPR-associated protein 9 technique. The homozygous pi4kb mutants exhibit an absence of primary cilia in the inner ear, neuromasts, and pronephric ducts accompanied by severe edema in the eyes and other organs. Moreover, smaller otic vesicle, malformed semicircular canals, and the insensitivity on sound stimulation were characteristics of pi4kb mutants. At the protein level, both in vivo and in vitro analyses revealed that synthesis of Pi4p was greatly reduced owing to the loss of Pi4kb. In addition, the expression of the Pi4kb-binding partner of neuronal calcium sensor-1, as well as the phosphorylation of phosphatidylinositol-4-phosphate downstream effecter of Akt, was significantly inhibited in pi4kb mutants. Taken together, our work uncovers a novel role of Pi4kb in zebrafish inner ear development and the functional formation of hearing ability by determining hair cell ciliogenesis. © 2020 Institute of Genetics and Developmental Biology, Chinese Academy of Sciences, and Genetics Society of China",scopus,2-s2.0-85098501647,10.1016/j.jgg.2020.07.007
perceived listening effort and speech intelligibility in reverberation and noise for hearingimpaired listeners,"208. Int J Audiol. 2016 Dec;55(12):738-747. doi: 10.1080/14992027.2016.1219774. Epub 2016 Sep 14.Perceived listening effort and speech intelligibility in reverberation and noise for hearing-impaired listeners.Schepker H(1)(2), Haeder K(2)(3), Rennies J(2)(4), Holube I(2)(3).Author information:(1)a Signal Processing Group, Department of Medical Physics and Acoustics , University of Oldenburg , Oldenburg , Germany.(2)b Cluster of Excellence ""Hearing4All"" , Oldenburg , Germany.(3)c Institute of Hearing Technology and Audiology , Jade University of Applied Sciences , Oldenburg , Germany , and.(4)d Project Group Hearing, Speech and Audio Technology , Fraunhofer Institute for Digital Media Technology IDMT , Oldenburg , Germany.OBJECTIVE: The purpose of this study was to assess perceived listening effort and speech intelligibility in reverberant and noisy conditions for hearing-impaired listeners for conditions that are similar according to the speech transmission index (STI).DESIGN: Scaled listening effort was measured in four different conditions at five different STI generated using various relative contributions of noise and reverberant interferences. Intelligibility was measured for a subset of conditions.STUDY SAMPLE: Twenty mildly to moderately hearing-impaired listeners.RESULTS: In general, listening effort decreased and speech intelligibility increased with increasing STI. For simulated impulse responses consisting of white Gaussian noise exponentially decaying in time, a good agreement between conditions of different relative contributions of noise and reverberation was found. For real impulse responses, the STI slightly overestimated the effect of reverberation on the perceived listening effort and underestimated its effect on speech intelligibility. Including the average hearing loss in the calculation of the STI led to a better agreement between STI predictions and subjective data.CONCLUSION: Speech intelligibility and listening effort provide complementary tools to evaluate speech perception over a broad range of acoustic scenarios. In addition, when incorporating hearing loss information the STI provides a rough prediction of listening effort in these acoustic scenarios.DOI: 10.1080/14992027.2016.1219774",pubmed,27627181,10.1080/14992027.2016.1219774
antiinflammatory and otoprotective effect of the small heat shock protein alpha bcrystallin hspb5 in experimental pneumococcal meningitis,"706. Front Neurol. 2019 Jun 10;10:570. doi: 10.3389/fneur.2019.00570. eCollection 2019.Anti-inflammatory and Oto-Protective Effect of the Small Heat Shock Protein Alpha B-Crystallin (HspB5) in Experimental Pneumococcal Meningitis.Erni ST(1)(2)(3)(4), Fernandes G(1)(2)(3), Buri M(1)(2), Perny M(1)(2)(3), Rutten RJ(5), van Noort JM(6), Senn P(7), Grandgirard D(1)(2), Roccio M(2)(3)(8), Leib SL(1)(2).Author information:(1)Neuroinfection Laboratory, Institute for Infectious Diseases, University of Bern, Bern, Switzerland.(2)Cluster for Regenerative Neuroscience, DBMR, University of Bern, Bern, Switzerland.(3)Laboratory of Inner Ear Research, DBMR, University of Bern, Bern, Switzerland.(4)Graduate School for Cellular and Biomedical Sciences, University of Bern, Bern, Switzerland.(5)Audion Therapeutics, Amsterdam, Netherlands.(6)Delta Crystallon BV, Leiden, Netherlands.(7)Service d'oto-rhino-laryngologie (ORL) et de chirurgie cervico-faciale, Département des Neurosciences Cliniques, Hôpitaux Universitaires de Genève, Geneva, Switzerland.(8)Department of Otorhinolaryngology, Head & Neck Surgery, Inselspital, Bern, Switzerland.Sensorineural hearing loss is the most common long-term deficit after pneumococcal meningitis (PM), occurring in up to 30% of surviving patients. The infection and the following overshooting inflammatory host response damage the vulnerable sensory cells of the inner ear, resulting in loss of hair cells and spiral ganglion neurons, ultimately leading to elevated hearing thresholds. Here, we tested the oto-protective properties of the small heat shock protein alpha B-crystallin (HspB5) with previously reported anti-inflammatory, anti-apoptotic and neuroprotective functions, in an experimental model of PM-induced hearing loss. We analyzed the effect of local and systemic delivery of HspB5 in an infant rat model of PM, as well as ex vivo, using whole mount cultures. Cytokine secretion profile, hearing thresholds and inner ear damage were assessed at predefined stages of the disease up to 1 month after infection. PM was accompanied by elevated pro-inflammatory cytokine concentrations in the cerebrospinal fluid (CSF), leukocyte and neutrophil infiltration in the perilymphatic spaces of the cochlea with neutrophils extracellular trap formation during the acute phase of the disease. Elevated hearing thresholds were measured after recovery from meningitis. Intracisternal but not intraperitoneal administration of HspB5 significantly reduced the levels of TNF-α, IL-6 IFN-γ and IL-10 in the acute phase of the disease. This resulted in a greater outer hair cell survival, as well as improved hearing thresholds at later stages. These results suggest that high local concentrations of HspB5 are needed to prevent inner ear damage in acute PM. HspB5 represents a promising therapeutic option to improve the auditory outcome and counteract hearing loss after PM.DOI: 10.3389/fneur.2019.00570PMCID: PMC6573805",pubmed,31244750,10.3389/fneur.2019.00570
assessment of hearing screening programmes across 47 countries or regions i provision of newborn hearing screening references,"Objectives: Newborn hearing screening (NHS) varies regarding number and type of tests, location, age, professionals and funding. We compared the provision of existing screening programmes. Design: A questionnaire containing nine domains: demography, administration, existing screening, coverage, tests, diagnosis, treatment, cost and adverse effects, was presented to hearing screening experts. Responses were verified. Clusters were identified based on number of screening steps and use of OAE or aABR, either for all infants or for well and high-risk infants (dual-protocol). Study sample: Fifty-two experts completed the questionnaire sufficiently: 40 European countries, Russia, Malawi, Rwanda, India and China. Results: It took considerable effort to find experts for all countries with sufficient time and knowledge. Data essential for evaluation are often not collected. Infants are first screened in maternity wards in most countries. Human development index and health expenditure were high among countries with dual protocols, three screening steps, including aABR, and low among countries without NHS and countries using OAE for all infants. Nationwide implementation of NHS took 6 years, on average. Conclusion: The extent and complexity of NHS programmes are primarily related to health expenditure and HDI. Data collection should be improved to facilitate comparison of NHS programmes across borders. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc20&DO=10.1080%2f14992027.2021.1886350
subtle alterations of vestibulomotor functioning in conductive hearing loss,"642. Front Neurosci. 2023 Aug 29;17:1057551. doi: 10.3389/fnins.2023.1057551. eCollection 2023.Subtle alterations of vestibulomotor functioning in conductive hearing loss.Manno FAM(1)(2)(3)(4), Cheung P(4), Basnet V(4), Khan MS(4), Mao Y(5), Pan L(5), Ma V(6), Cho WC(6), Tian S(7), An Z(7), Feng Y(7)(8)(9), Cai YL(4), Pienkowski M(10), Lau C(3)(4).Author information:(1)Department of Physics, East Carolina University, Greenville, NC, United States.(2)Department of Biomedical Engineering, Center for Imaging Science, Whiting School of Engineering, Johns Hopkins University, Baltimore, MD, United States.(3)Center for Advanced Nuclear Safety and Sustainable Development, City University of Hong Kong, Kowloon, Hong Kong SAR, China.(4)Department of Physics, City University of Hong Kong, Kowloon, Hong Kong SAR, China.(5)Department of Nautical Injury Prevention, Faculty of Navy Medicine, Second Military Medical University, Shanghai, China.(6)Department of Clinical Oncology, Queen Elizabeth Hospital, Kowloon, Hong Kong SAR, China.(7)School of Biomedical Engineering, Southern Medical University, Guangzhou, China.(8)Guangdong Provincial Key Laboratory of Medical Image Processing and Guangdong Province Engineering Laboratory for Medical Imaging and Diagnostic Technology, Southern Medical University, Guangzhou, China.(9)Key Laboratory of Mental Health of the Ministry of Education, Guangdong-Hong Kong-Macao Greater Bay Area Center for Brain Science and Brain-Inspired Intelligence, Guangdong Province Key Laboratory of Psychiatric Disorders, Department of Neurobiology, School of Basic Medical Sciences, Southern Medical University, Guangzhou, China.(10)Osborne College of Audiology, Salus University, Elkins Park, PA, United States.INTRODUCTION: Conductive hearing loss (CHL) attenuates the ability to transmit air conducted sounds to the ear. In humans, severe hearing loss is often accompanied by alterations to other neural systems, such as the vestibular system; however, the inter-relations are not well understood. The overall goal of this study was to assess vestibular-related functioning proxies in a rat CHL model.METHODS: Male Sprague-Dawley rats (N=134, 250g, 2months old) were used in a CHL model which produced a >20dB threshold shift induced by tympanic membrane puncture. Auditory brainstem response (ABRs) recordings were used to determine threshold depth at different times before and after CHL. ABR threshold depths were assessed both manually and by an automated ABR machine learning algorithm. Vestibular-related functioning proxy assessment was performed using the rotarod, balance beam, elevator vertical motion (EVM) and Ferris-wheel rotation (FWR) assays.RESULTS: The Pre-CHL (control) threshold depth was 27.92dB±11.58dB compared to the Post-CHL threshold depth of 50.69dB±13.98dB (mean±SD) across the frequencies tested. The automated ABR machine learning algorithm determined the following threshold depths: Pre-CHL=24.3dB, Post-CHL same day=56dB, Post-CHL 7 days=41.16dB, and Post-CHL 1 month=32.5dB across the frequencies assessed (1, 2, 4, 8, 16, and 32kHz). Rotarod assessment of motor function was not significantly different between pre and post-CHL (~1week) rats for time duration (sec) or speed (RPM), albeit the former had a small effect size difference. Balance beam time to transverse was significantly longer for post-CHL rats, likely indicating a change in motor coordination. Further, failure to cross was only noted for CHL rats. The defection count was significantly reduced for CHL rats compared to control rats following FWR, but not EVM. The total distance traveled during open-field examination after EVM was significantly different between control and CHL rats, but not for FWR. The EVM is associated with linear acceleration (acting in the vertical plane: up-down) stimulating the saccule, while the FWR is associated with angular acceleration (centrifugal rotation about a circular axis) stimulating both otolith organs and semicircular canals; therefore, the difference in results could reflect the specific vestibular-organ functional role.DISCUSSION: Less movement (EVM) and increase time to transverse (balance beam) may be associated with anxiety and alterations to defecation patterns (FWR) may result from autonomic disturbances due to the impact of hearing loss. In this regard, vestibulomotor deficits resulting in changes in balance and motion could be attributed to comodulation of auditory and vestibular functioning. Future studies should manipulate vestibular functioning directly in rats with CHL.Copyright © 2023 Manno, Cheung, Basnet, Khan, Mao, Pan, Ma, Cho, Tian, An, Feng, Cai, Pienkowski and Lau.DOI: 10.3389/fnins.2023.1057551PMCID: PMC10495589",pubmed,37706156,10.3389/fnins.2023.1057551
detection of a novel setbp1 variant in a chinese neonate with schinzelgiedion syndrome,"Schinzel–Giedion syndrome (SGS) is a multiple malformation syndrome characterized by typical facial features, severe neurodevelopmental delay, and multiple congenital abnormalities. SGS is associated with de novo pathogenic variants in the SETBP1 gene. In specific, SETBP1 variants in over 50 patients with classical or non-classical SGS were clustered within exon 4. A male Chinese neonate with dysmorphic facial features, nervous system disorders, and organ malformations at birth was examined in this study and long-term followed-up. Whole-exome sequencing was performed to identify any underlying pathogenic variants in the proband. Additionally, we reviewed the literature that documents the main clinical features and underlying variants of all patients genetically diagnosed with SGS. The neonate had a characteristic midface retraction, abnormal electroencephalogram waveforms, and genital abnormalities. The patient did not initially develop hydronephrosis or undergo a comprehensive skeletal assessment. Six months after birth, the patient had an epileptic seizure and experienced persistent neurodevelopmental delay with auditory and visual abnormalities. Color Doppler ultrasonography at 18 months revealed hydronephrosis and bilateral widening of the lateral ventricles. The patient died suddenly 20.5 months after birth. Whole-exome sequencing revealed a heterozygous de novo variant (c.2605A > G:p.S869G) in exon 4 degradation sequence in SETBP1. The reported de novo heterozygous variant in SETBP1 (c.2605A > G:p.S869G) broadens the knowledge of the scientific community's on the possible SGS genetic alterations. To the best of our knowledge, this is the first report of SETBP1 variant (c.2605A > G:p.S869G) in SGS. The clinical manifestations of neonatal SGS are atypical, and genetic testing is crucial for diagnosis. Long-term follow-up should be conducted after diagnosis to optimize the therapeutic interventions. Copyright © 2022 Yang, Liu, Chen, Lin, Wang, Chen, Wang and Yan.",scopus,2-s2.0-85138250452,10.3389/fped.2022.920741
string data mining of gwas data in canine hereditary pigmentassociated deafness,"843. Vet Anim Sci. 2020 May 12;9:100118. doi: 10.1016/j.vas.2020.100118. eCollection 2020 Jun.STRING data mining of GWAS data in canine hereditary pigment-associated deafness.Kelly-Smith M(1), Strain GM(1).Author information:(1)Comparative Biomedical Sciences, School of Veterinary Medicine, Louisiana State University, Baton Rouge, LA 70803 USA.Most canine deafness is linked to white pigmentation caused by the piebald locus, shown to be the gene MITF (melanocyte inducing transcription factor), but studies have failed to identify a deafness cause. The coding regions of MITF have not been shown to be mutated in deaf dogs, leading us to pursue genes acting on or controlled by MITF. We have genotyped DNA from 502 deaf and hearing Australian cattle dogs, Dalmatians, and English setters, breeds with a high deafness prevalence. Genome-wide significance was not attained in any of our analyses, but we did identify several suggestive associations. Genome-wide association studies (GWAS) in complex hereditary disorders frequently fail to identify causative gene variants, so advanced bioinformatics data mining techniques are needed to extract information to guide future studies. STRING diagrams are graphical representations of known and predicted networks of protein-protein interactions, identifying documented relationships between gene proteins based on the scientific literature, to identify functional gene groupings to pursue for further scrutiny. The STRING program predicts associations at a preset confidence level and suggests biological functions based on the identified genes. Starting with (1) genes within 500 kb of GWAS-suggested SNPs, (2) known pigmentation genes, (3) known human deafness genes, and (4) genes identified from proteomic analysis of the cochlea, we generated STRING diagrams that included these genes. We then reduced the number of genes by excluding genes with no relationship to auditory function, pigmentation, or relevant structures, and identified clusters of genes that warrant further investigation.© 2020 The Authors.DOI: 10.1016/j.vas.2020.100118PMCID: PMC7386748",pubmed,32734119,10.1016/j.vas.2020.100118
a novel modelbased hearing compensation design using a gradientfree optimization method,"481. Neural Comput. 2005 Dec;17(12):2648-71. doi: 10.1162/089976605774320575.A novel model-based hearing compensation design using a gradient-free optimization method.Chen Z(1), Becker S, Bondy J, Bruce IC, Haykin S.Author information:(1)Department of Electrical and Computer Engineering, McMaster University, Hamilton, Ontario L85 4k1, Canada. zhechen@soma.ece.mcmaster.caWe propose a novel model-based hearing compensation strategy and gradient-free optimization procedure for a learning-based hearing aid design. Motivated by physiological data and normal and impaired auditory nerve models, a hearing compensation strategy is cast as a neural coding problem, and a Neurocompensator is designed to compensate for the hearing loss and enhance the speech. With the goal of learning the Neurocompensator parameters, we use a gradient-free optimization procedure, an improved version of the ALOPEX that we have developed, to learn the unknown parameters of the Neurocompensator. We present our methodology, learning procedure, and experimental results in detail; discussion is also given regarding the unsupervised learning and optimization methods.DOI: 10.1162/089976605774320575",pubmed,16212766,10.1162/089976605774320575
relation between hearing abilities and preferred playback settings for speech perception in complex listening conditions,"73. Int J Audiol. 2022 Nov;61(11):965-974. doi: 10.1080/14992027.2021.1980233. Epub 2021 Oct 6.Relation between hearing abilities and preferred playback settings for speech perception in complex listening conditions.Kubiak AM(1), Rennies J(1), Ewert SD(2), Kollmeier B(2).Author information:(1)Fraunhofer IDMT, Project Group Hearing, Speech and Audio Technology, Cluster of Excellence ""Hearing4all"", Oldenburg, Germany.(2)Medizinische Physik, Cluster of Excellence Hearing4all, Carl von Ossietzky Universität, Oldenburg, Germany.OBJECTIVE: This study investigated if individual preferences with respect to the trade-off between a good signal-to-noise ratio and a distortion-free speech target were stable across different masking conditions and if simple adjustment methods could be used to identify subjects as either ""noise haters"" or ""distortions haters"".DESIGN: In each masking condition, subjects could adjust the target speech level according to their preferences by employing (i) linear gain or gain at the cost of (ii) clipping distortions or (iii) compression distortions. The comparison of these processing conditions allowed investigating the preferred trade-off between distortions and noise disturbance.STUDY SAMPLE: Thirty subjects differing widely in hearing status (normal-hearing to moderately impaired) and age (23-85 years).RESULTS: High test-retest stability of individual preferences was found for all modification schemes. The preference adjustments suggested that subjects could be consistently categorised along a scale from ""noise haters"" to ""distortion haters"", and this preference trait remained stable through all maskers, spatial conditions, and types of distortions.CONCLUSIONS: Employing quick self-adjustment to collect listening preferences in complex listening conditions revealed a stable preference trait along the ""noise vs. distortions"" tolerance dimension. This could potentially help in fitting modern hearing aid algorithms to the individual user.DOI: 10.1080/14992027.2021.1980233",pubmed,34612124,10.1080/14992027.2021.1980233
preoperative visual measures of verbal learning and memory and their relations to speech recognition after cochlear implantation,"152. Ear Hear. 2022 May/Jun;43(3):993-1002. doi: 10.1097/AUD.0000000000001155.Preoperative Visual Measures of Verbal Learning and Memory and their Relations to Speech Recognition After Cochlear Implantation.Ray C(1), Pisoni DB(2), Lu E(1), Kronenberger WG(2)(3), Moberly AC(1).Author information:(1)Department of Otolaryngology, The Ohio State University, Columbus, OH.(2)Department of Psychological and Brain Sciences, Indiana University, Bloomington, IN.(3)Department of Psychiatry, Indiana University School of Medicine, Indianapolis, IN.OBJECTIVES: This study examined the performance of a group of adult cochlear implant (CI) candidates (CIC) on visual tasks of verbal learning and memory. Preoperative verbal learning and memory abilities of the CIC group were compared with a group of older normal-hearing (ONH) control participants. Relations between preoperative verbal learning and memory measures and speech recognition outcomes after 6 mo of CI use were also investigated for a subgroup of the CICs.DESIGN: A group of 80 older adult participants completed a visually presented multitrial free recall task. Measures of word recall, repetition learning, and the use of self-generated organizational strategies were collected from a group of 49 CICs, before cochlear implantation, and a group of 31 ONH controls. Speech recognition outcomes were also collected from a subgroup of 32 of the CIC participants who returned for testing 6 mo after CI activation.RESULTS: CICs demonstrated poorer verbal learning performance compared with the group of ONH control participants. Among the preoperative verbal learning and memory measures, repetition learning slope and measures of self-generated organizational clustering strategies were the strongest predictors of post-CI speech recognition outcomes.CONCLUSIONS: Older adult CI candidates present with verbal learning and memory deficits compared with older adults without hearing loss, even on visual tasks that are independent from the direct effects of audibility. Preoperative verbal learning and memory processes reflecting repetition learning and self-generated organizational strategies in free recall were associated with speech recognition outcomes 6 months after implantation. The pattern of results suggests that visual measures of verbal learning may be a useful predictor of outcomes in postlingual adult CICs.Copyright © 2022 Wolters Kluwer Health, Inc. All rights reserved.DOI: 10.1097/AUD.0000000000001155PMCID: PMC9010345",pubmed,35319518,10.1097/AUD.0000000000001155
concurrent gradients of ribbon volume and ampareceptor patch volume in cochlear afferent synapses on gerbil inner hair cells,"705. Hear Res. 2018 Jul;364:81-89. doi: 10.1016/j.heares.2018.03.028. Epub 2018 Apr 1.Concurrent gradients of ribbon volume and AMPA-receptor patch volume in cochlear afferent synapses on gerbil inner hair cells.Zhang L(1), Engler S(1), Koepcke L(2), Steenken F(1), Köppl C(3).Author information:(1)Cluster of Excellence ""Hearing4all"" and Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg, 26129 Oldenburg, Germany.(2)Department of Neuroscience, Computational Neuroscience, Carl von Ossietzky University Oldenburg, 26129 Oldenburg, Germany.(3)Cluster of Excellence ""Hearing4all"" and Research Centre Neurosensory Science, Department of Neuroscience, School of Medicine and Health Science, Carl von Ossietzky University Oldenburg, 26129 Oldenburg, Germany. Electronic address: christine.koeppl@uol.de.The Mongolian gerbil is a classic animal model for age-related hearing loss. As a prerequisite for studying age-related changes, we characterized cochlear afferent synaptic morphology in young adult gerbils, using immunolabeling and quantitative analysis of confocal microscopic images. Cochlear wholemounts were triple-labeled with a hair-cell marker, a marker of presynaptic ribbons, and a marker of postsynaptic AMPA-type glutamate receptors. Seven cochlear positions covering an equivalent frequency range from 0.5 - 32 kHz were evaluated. The spatial positions of synapses were determined in a coordinate system with reference to their individual inner hair cell. Synapse numbers confirmed previous reports for gerbils (on average, 20-22 afferents per inner hair cell). The volumes of presynaptic ribbons and postsynaptic glutamate receptor patches were positively correlated: larger ribbons associated with larger receptor patches and smaller ribbons with smaller patches. Furthermore, the volumes of both presynaptic ribbons and postsynaptic receptor patches co-varied along the modiolar-pillar and the longitudinal axes of their hair cell. The gradients in ribbon volume are consistent with previous findings in cat, guinea pig, mouse and rat and further support a role in differentiating the physiological properties of type I afferents. However, the positive correlation between the volumes of pre- and postsynaptic elements in the gerbil is different to the opposing gradients found in the mouse, suggesting species-specific differences in the postsynaptic AMPA receptors that are unrelated to the fundamental classes of type I afferents.Copyright © 2018 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2018.03.028",pubmed,29631778,10.1016/j.heares.2018.03.028
braininformed speech separation biss for enhancement of target speaker in multitalker speech perception,"92. Neuroimage. 2020 Dec;223:117282. doi: 10.1016/j.neuroimage.2020.117282. Epub 2020 Aug 20.Brain-informed speech separation (BISS) for enhancement of target speaker in multitalker speech perception.Ceolini E(1), Hjortkjær J(2), Wong DDE(3), O'Sullivan J(4), Raghavan VS(4), Herrero J(5), Mehta AD(5), Liu SC(6), Mesgarani N(7).Author information:(1)University of Zürich and ETH Zürich, Institute of Neuroinformatics, Switzerland. Electronic address: enea.ceolini@ini.uzh.ch.(2)Department of Health Technology, Danmarks Tekniske Universitet DTU, Kongens Lyngby, Denmark; Danish Research Centre for Magnetic Resonance, Copenhagen University Hospital Hvidovre, Hvidovre, Denmark.(3)Laboratoire des Systèmes Perceptifs, CNRS, UMR 8248, Paris, France; Département d'Études Cognitives, École Normale Supérieure, PSL Research University, Paris, France.(4)Department of Electrical Engineering, Columbia University, New York, NY, USA; Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA.(5)Department of Neurosurgery, Hofstra-Northwell School of Medicine and Feinstein Institute for Medical Research, Manhasset, New York, NY, USA.(6)University of Zürich and ETH Zürich, Institute of Neuroinformatics, Switzerland.(7)Department of Electrical Engineering, Columbia University, New York, NY, USA; Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA. Electronic address: nima@ee.columbia.edu.Hearing-impaired people often struggle to follow the speech stream of an individual talker in noisy environments. Recent studies show that the brain tracks attended speech and that the attended talker can be decoded from neural data on a single-trial level. This raises the possibility of ""neuro-steered"" hearing devices in which the brain-decoded intention of a hearing-impaired listener is used to enhance the voice of the attended speaker from a speech separation front-end. So far, methods that use this paradigm have focused on optimizing the brain decoding and the acoustic speech separation independently. In this work, we propose a novel framework called brain-informed speech separation (BISS)1 in which the information about the attended speech, as decoded from the subject's brain, is directly used to perform speech separation in the front-end. We present a deep learning model that uses neural data to extract the clean audio signal that a listener is attending to from a multi-talker speech mixture. We show that the framework can be applied successfully to the decoded output from either invasive intracranial electroencephalography (iEEG) or non-invasive electroencephalography (EEG) recordings from hearing-impaired subjects. It also results in improved speech separation, even in scenes with background noise. The generalization capability of the system renders it a perfect candidate for neuro-steered hearing-assistive devices.Copyright © 2020. Published by Elsevier Inc.DOI: 10.1016/j.neuroimage.2020.117282PMCID: PMC8056438",pubmed,32828921,10.1016/j.neuroimage.2020.117282
emerging approaches for restoration of hearing and vision,"16. Physiol Rev. 2020 Oct 1;100(4):1467-1525. doi: 10.1152/physrev.00035.2019. Epub 2020 Mar 19.Emerging Approaches for Restoration of Hearing and Vision.Kleinlogel S(1), Vogl C(1), Jeschke M(1), Neef J(1), Moser T(1).Author information:(1)Institute for Physiology, University of Bern, Bern, Switzerland; Pre-Synaptogenesis and Intracellular Transport in Hair Cells Group, Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany; Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany; Collaborative Research Center 889, University of Göttingen, Göttingen, Germany; Cognitive Hearing in Primates Group, Auditory Neuroscience and Optogenetics Laboratory, Deutsches Primatenzentrum GmbH, Göttingen, Germany; Auditory Neuroscience and Optogenetics Laboratory, Deutsches Primatenzentrum GmbH, Göttingen, Germany; and Multiscale Bioimaging Cluster of Excellence (MBExC), University of Göttingen, Göttingen, Germany.Impairments of vision and hearing are highly prevalent conditions limiting the quality of life and presenting a major socioeconomic burden. For a long time, retinal and cochlear disorders have remained intractable for causal therapies, with sensory rehabilitation limited to glasses, hearing aids, and electrical cochlear or retinal implants. Recently, the application of gene therapy and optogenetics to eye and ear has generated hope for a fundamental improvement of vision and hearing restoration. To date, one gene therapy for the restoration of vision has been approved, and ongoing clinical trials will broaden its application including gene replacement, genome editing, and regenerative approaches. Moreover, optogenetics, i.e., controlling the activity of cells by light, offers a more general alternative strategy. Over little more than a decade, optogenetic approaches have been developed and applied to better understand the function of biological systems, while protein engineers have identified and designed new opsin variants with desired physiological features. Considering potential clinical applications of optogenetics, the spotlight is on the sensory systems, particularly the eye and ear. Multiple efforts have been undertaken to restore lost or hampered function in the eye and ear. Optogenetic stimulation promises to overcome fundamental shortcomings of electrical stimulation, namely, poor spatial resolution and cellular specificity, and accordingly to deliver more detailed sensory information. This review aims to provide a comprehensive reference on current gene therapeutic and optogenetic research relevant to the restoration of hearing and vision. We will introduce gene-therapeutic approaches and discuss the biotechnological and optoelectronic aspects of optogenetic hearing and vision restoration.DOI: 10.1152/physrev.00035.2019",pubmed,32191560,10.1152/physrev.00035.2019
association of speech processor technology and speech recognition outcomes in adult cochlear implant users,"Objective: Determine association of advancements in speech processor technology with improvements in speech recognition outcomes. Study Design: Retrospective cohort. Setting: Tertiary referral center. Patients: Adult unilateral cochlear implant (CI) recipients. Intervention: Increasing novelty of speech processor defined by year of market availability. Main Outcome Measures: Consonant-Nucleus-Consonant (CNC) and Hearing in Noise Test (HINT) in quiet. Results: From 1991 to 2016, 1,111 CNC scores and 1,121 HINT scores were collected from 351 patients who had complete data. Mean post-implantation CNC score was 53.8% and increased with more recent era of implantation ( p<0.001, analysis of variance [ANOVA]). Median HINT score was 87.0% and did not significantly vary with implantation era ( p=0.06, ANOVA). Multivariable generalized linear models were fitted to estimate the effect of speech processor novelty on CNC and HINT scores, each accounting for clustering of scores within patients and characteristics known to influence speech recognition outcomes. Each 5-year increment in speech processor novelty was independently associated with an increase in CNC score by 2.85% (95% confidence limits [CL] 0.26, 5.44%) and was not associated with change in HINT scores ( p=0.30). Conclusion: Newer speech processors are associated with improved CNC scores independent of the year of device implantation and expanding candidacy criteria. The lack of association with HINT scores can be attributed to a ceiling effect, suggesting that HINT in quiet may not be an informative test of speech recognition in the modern CI recipient. The implications of these findings with respect to appropriate interval of speech processor upgrades are discussed.  © 2019, Otology & Neurotology, Inc.",scopus,2-s2.0-85066061553,10.1097/MAO.0000000000002172
the finnish simplified matrix sentence test for the assessment of speech intelligibility in the elderly,"153. Int J Audiol. 2020 Oct;59(10):763-771. doi: 10.1080/14992027.2020.1741704. Epub 2020 Mar 18.The Finnish simplified matrix sentence test for the assessment of speech intelligibility in the elderly.Willberg T(1)(2), Kärtevä K(1), Zokoll M(3)(4), Buschermöhle M(5), Sivonen V(6), Aarnisalo A(6), Löppönen H(1)(7), Kollmeier B(3)(8), Dietz A(7).Author information:(1)Institute of Clinical Medicine, University of Eastern Finland, Kuopio, Finland.(2)Department of Otorhinolaryngology, Turku University Hospital, Turku, Finland.(3)Medizinische Physik and Cluster of Excellence Hearing4all, Carl von Ossietzky University, Oldenburg, Germany.(4)Hörzentrum Oldenburg GmbH, Oldenburg, Germany.(5)Klinisches Innovationszentrum für Medizintechnik Oldenburg, Oldenburg, Germany.(6)Department of Otorhinolaryngology, Helsinki University Hospital, Finland.(7)Department of Otorhinolaryngology, Kuopio University Hospital, Finland.(8)HörTech gGmbH, Oldenburg, Germany.Objective: A simplified version of the Finnish matrix sentence test (FMST) was developed to improve the reliability of hearing diagnostic for children and for patients with limited working memory capacity and/or vocabulary.Design: Study 1 evaluated the word matrix of the Finnish simplified matrix sentence test (FINSIMAT) to rule out systematic differences between the new FINSIMAT test lists, and to provide reference values for normal-hearing (NH) young adults (YA). In Study 2, the FINSIMAT and the FMST were evaluated in elderly listeners with mild-to-moderate hearing impairment (HI).Study sample: Twenty NH YAs participated in Study 1, and 16 elderly HI adults participated in Study 2.Results: For NH YAs, the reference speech reception threshold (SRT50) estimate and the slope for the FINSIMAT were -11.2 ± 1.0 dB signal-to-noise ratio (SNR) and 19.4 ± 1.9%/dB SNR. For the elderly HI listeners, the mean SRT50 estimates for the FINSIMAT and FMST were -4.1 and -3.6 dB SNR, respectively. The correlation between the FMST and FINSIMAT results was strong (r2 = 0.78, p < 0.001).Conclusion: The FINSIMAT showed comparable characteristics to the FMST and proved feasible for measurements in elderly HI listeners.DOI: 10.1080/14992027.2020.1741704",pubmed,32186403,10.1080/14992027.2020.1741704
neural degeneration in normalaging human cochleas machinelearning counts and 3d mapping in archival sections,"56. J Assoc Res Otolaryngol. 2023 Oct;24(5):499-511. doi: 10.1007/s10162-023-00909-y. Epub 2023 Nov 13.Neural Degeneration in Normal-Aging Human Cochleas: Machine-Learning Counts and 3D Mapping in Archival Sections.Wu PZ(1)(2), O'Malley JT(3)(4), Liberman MC(3)(4).Author information:(1)Eaton-Peabody Laboratories, Massachusetts Eye and Ear Infirmary, 243 Charles St., Boston, MA, 02114-3096, USA. Peizhe-Wu@uiowa.edu.(2)Department of Otolaryngology, Harvard Medical School, Boston, MA, 02115, USA. Peizhe-Wu@uiowa.edu.(3)Eaton-Peabody Laboratories, Massachusetts Eye and Ear Infirmary, 243 Charles St., Boston, MA, 02114-3096, USA.(4)Department of Otolaryngology, Harvard Medical School, Boston, MA, 02115, USA.Quantifying the survival patterns of spiral ganglion cells (SGCs), the cell bodies of auditory-nerve fibers, is critical to studies of sensorineural hearing loss, especially in human temporal bones. The classic method of manual counting is tedious, and, although stereology approaches can be faster, they can only be used to estimate total cell numbers per cochlea. Here, a machine-learning algorithm that automatically identifies, counts, and maps the SGCs in digitized images of semi-serial human temporal-bone sections not only speeds the analysis, with no loss of accuracy, but also allows 3D visualization of the SGCs and fine-grained mapping to cochlear frequency. Applying the algorithm to 62 normal-aging human ears shows significantly faster degeneration of SGCs in the basal than the apical half of the cochlea. Comparison to fiber counts in the same ears shows that the fraction of surviving SGCs lacking a peripheral axon steadily increases with age, reaching more than 50% in the apical cochlea and almost 66% in basal regions.© 2023. The Author(s) under exclusive licence to Association for Research in Otolaryngology.DOI: 10.1007/s10162-023-00909-yPMCID: PMC10695900",pubmed,37957485,10.1007/s10162-023-00909-y
darf a datareduced fade version for simulations of speech recognition thresholds with real hearing aids,"200. Hear Res. 2021 May;404:108217. doi: 10.1016/j.heares.2021.108217. Epub 2021 Feb 22.DARF: A data-reduced FADE version for simulations of speech recognition thresholds with real hearing aids.Hülsmeier D(1), Schädler MR(2), Kollmeier B(2).Author information:(1)Medizinische Physik and Cluster of Excellence Hearing4all, CvO Universität Oldenburg, Oldenburg 26129, Germany. Electronic address: david.huelsmeier@uni-oldenburg.de.(2)Medizinische Physik and Cluster of Excellence Hearing4all, CvO Universität Oldenburg, Oldenburg 26129, Germany.Developing and selecting hearing aids is a time consuming process which is simplified by using objective models. Previously, the framework for auditory discrimination experiments (FADE) accurately simulated benefits of hearing aid algorithms with root mean squared prediction errors below 3 dB. One FADE simulation requires several hours of (un)processed signals, which is obstructive when the signals have to be recorded. We propose and evaluate a data-reduced FADE version (DARF) which facilitates simulations with signals that cannot be processed digitally, but that can only be recorded in real-time. DARF simulates one speech recognition threshold (SRT) with about 30 min of recorded and processed signals of the (German) matrix sentence test. Benchmark experiments were carried out to compare DARF and standard FADE exhibiting small differences for stationary maskers (1 dB), but larger differences with strongly fluctuating maskers (5 dB). Hearing impairment and hearing aid algorithms seemed to reduce the differences. Hearing aid benefits were simulated in terms of speech recognition with three pairs of real hearing aids in silence (≥8 dB), in stationary and fluctuating maskers in co-located (stat. 2 dB; fluct. 6 dB), and spatially separated speech and noise signals (stat. ≥8 dB; fluct. 8 dB). The simulations were plausible in comparison to data from literature, but a comparison with empirical data is still open. DARF facilitates objective SRT simulations with real devices with unknown signal processing in real environments. Yet, a validation of DARF for devices with unknown signal processing is still pending since it was only tested with three similar devices. Nonetheless, DARF could be used for improving as well as for developing or model-based fitting of hearing aids.Copyright © 2021 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2021.108217",pubmed,33706223,10.1016/j.heares.2021.108217
longterm communication outcomes for children receiving cochlear implants younger than 12 months a multicenter study,"416. Otol Neurotol. 2016 Feb;37(2):e82-95. doi: 10.1097/MAO.0000000000000915.Long-term Communication Outcomes for Children Receiving Cochlear Implants Younger Than 12 Months: A Multicenter Study.Dettman SJ(1), Dowell RC, Choo D, Arnott W, Abrahams Y, Davis A, Dornan D, Leigh J, Constantinescu G, Cowan R, Briggs RJ.Author information:(1)*University of Melbourne, HEARing CRC, Cochlear Implant Clinic, Royal Victorian Eye and Ear Hospital †University of Melbourne, HEARing CRC ‡Hear and Say Centre §The Shepherd Centre ||Cochlear Implant Clinic, Royal Victorian Eye and Ear Hospital ¶University of Melbourne, HEARing CRC #University of Melbourne, HEARing CRC, Royal Victorian Eye and Ear Hospital, East Melbourne, Victoria, Australia.OBJECTIVE: Examine the influence of age at implant on speech perception, language, and speech production outcomes in a large unselected paediatric cohort.STUDY DESIGN: This study pools available assessment data (collected prospectively and entered into respective databases from 1990 to 2014) from three Australian centers.PATIENTS: Children (n = 403) with congenital bilateral severe to profound hearing loss who received cochlear implants under 6 years of age (excluding those with acquired onset of profound hearing loss after 12 mo, those with progressive hearing loss and those with mild/moderate/severe additional cognitive delay/disability).MAIN OUTCOME MEASURE(S): Speech perception; open-set words (scored for words and phonemes correct) and sentence understanding at school entry and late primary school time points. Language; PLS and PPVT standard score equivalents at school entry, CELF standard scores. Speech Production; DEAP percentage accuracy of vowels, consonants, phonemes-total and clusters, and percentage word-intelligibility at school entry.RESULTS: Regression analysis indicated a significant effect for age-at-implant for all outcome measures. Cognitive skills also accounted for significant variance in all outcome measures except open-set phoneme scores. ANOVA with Tukey pairwise comparisons examined group differences for children implanted younger than 12 months (Group 1), between 13 and 18 months (Group 2), between 19 and 24 months (Group 3), between 25 and 42 months (Group 4), and between 43 and 72 months (Group 5). Open-set speech perception scores for Groups 1, 2, and 3 were significantly higher than Groups 4 and 5. Language standard scores for Group 1 were significantly higher than Groups 2, 3, 4, and 5. Speech production outcomes for Group 1 were significantly higher than scores obtained for Groups 2, 3, and 4 combined. Cross tabulation and χ2 tests supported the hypothesis that a greater percentage of Group 1 children (than Groups 2, 3, 4, or 5) demonstrated language performance within the normative range by school entry.CONCLUSIONS: Results support provision of cochlear implants younger than 12 months of age for children with severe to profound hearing loss to optimize speech perception and subsequent language acquisition and speech production accuracy.DOI: 10.1097/MAO.0000000000000915",pubmed,26756160,10.1097/MAO.0000000000000915
spatiotemporal convolutions and video vision transformers for signerindependent sign language recognition,"Sign language is a vital tool of communication for individuals who are deaf or hard of hearing. Sign language recognition (SLR) technology can assist in bridging the communication gap between deaf and hearing individuals. However, existing SLR systems are typically signer-dependent, requiring training data from the specific signer for accurate recognition. This presents a significant challenge for practical use, as collecting data from every possible signer is not feasible. This research focuses on developing a signer-independent isolated SLR system to address this challenge. The system implements two model variants on the signer-independent datasets: an R(2+ I)D spatiotemporal convolutional block and a Video Vision transformer. These models learn to extract features from raw sign language videos from the LSA64 dataset and classify signs without needing handcrafted features, explicit segmentation or pose estimation. Overall, the R(2+1)D model architecture significantly outperformed the ViViT architecture for signer-independent SLR on the LSA64 dataset. The R(2+1)D model achieved a near-perfect accuracy of 99.53% on the unseen test set, with the ViViT model yielding an accuracy of 72.19 %. Proving that spatiotemporal convolutions are effective at signer-independent SLR.",ieee,,10.1109/icABCD59051.2023.10220534
novel crisprcas12abased genetic diagnostic approach for slc26a4 mutationrelated hereditary hearing loss,"69. Eur J Med Genet. 2022 Feb;65(2):104406. doi: 10.1016/j.ejmg.2021.104406. Epub 2021 Dec 27.Novel CRISPR/Cas12a-based genetic diagnostic approach for SLC26A4 mutation-related hereditary hearing loss.Jin X(1), Zhang L(1), Wang X(2), An L(1), Huang S(3), Dai P(3), Gao H(4), Ma X(5).Author information:(1)National Research Institute for Family Planning, Beijing, China; National Human Genetic Resources Center, Beijing, China.(2)Shanghai Institute for Advanced Immunochemical Studies, ShanghaiTech University, Shanghai, China; School of Life Science and Technology, ShanghaiTech University, Shanghai, China.(3)Department of Otolaryngology, PLA General Hospital, Beijing, China.(4)National Research Institute for Family Planning, Beijing, China; National Human Genetic Resources Center, Beijing, China. Electronic address: gaohuafang@nrifp.org.cn.(5)National Research Institute for Family Planning, Beijing, China; National Human Genetic Resources Center, Beijing, China. Electronic address: genetic@263.net.cn.Hereditary hearing loss is a common defect of the auditory nervous system with high-incidence, seriously affecting the quality of life of the patients. The clinical manifestations of SLC26A4 mutation-related hearing loss are congenital sensorineural or mixed deafness. Sensitive and specific SLC26A4 mutation detection in the early clinical stage is key for the early indication of potential hearing loss in the lack of effective treatment. Using clustered regularly interspaced short palindromic repeats (CRISPR)-based nucleic acid detection technology, we designed a fast and sensitive detection system for SLC26A4 pathogenic mutations (c.919-2A > G, c.2168A > G and c.1229C > T). This recombinase-aided amplification-based detection system allows rapid target gene amplification and, in combination with the CRISPR-based nucleic acid testing (NAT) system, mutation site detection. Moreover, mismatches were introduced in CRISPR-derived RNA (crRNA) to increase signal differences between the wild-type genes and mutant genes. A total of 64 samples were examined using this approach and all results were verified using Sanger sequencing. The detection results were consistent with the polymerase chain reaction-Sanger sequencing results. Overall, this CRISPR-based NAT technology provides a sensitive and fast new approach for the detection of hereditary deafness and provides a crRNA optimization strategy for single-nucleotide polymorphism detection, which could be helpful for the clinical diagnosis of SLC26A4 mutation-related hereditary hearing loss.Copyright © 2021. Published by Elsevier Masson SAS.DOI: 10.1016/j.ejmg.2021.104406",pubmed,34968750,10.1016/j.ejmg.2021.104406
visuotactile interactions in the congenitally deaf a behavioral and eventrelated potential study,"844. Front Integr Neurosci. 2015 Jan 21;8:98. doi: 10.3389/fnint.2014.00098. eCollection 2014.Visuo-tactile interactions in the congenitally deaf: a behavioral and event-related potential study.Hauthal N(1), Debener S(2), Rach S(3), Sandmann P(4), Thorne JD(1).Author information:(1)Neuropsychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all,"" European Medical School, University of Oldenburg Oldenburg, Germany.(2)Neuropsychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all,"" European Medical School, University of Oldenburg Oldenburg, Germany ; Research Center Neurosensory Science, University of Oldenburg Oldenburg, Germany.(3)Research Center Neurosensory Science, University of Oldenburg Oldenburg, Germany ; Experimental Psychology Lab, Department of Psychology, European Medical School, University of Oldenburg Oldenburg, Germany ; Department of Epidemiological Methods and Etiologic Research, Leibniz Institute for Prevention Research and Epidemiology - BIPS Bremen, Germany.(4)Neuropsychology Lab, Department of Psychology, Cluster of Excellence ""Hearing4all,"" European Medical School, University of Oldenburg Oldenburg, Germany ; Department of Neurology, Cluster of Excellence ""Hearing4all,"" Hannover Medical School Hannover, Germany.Auditory deprivation is known to be accompanied by alterations in visual processing. Yet not much is known about tactile processing and the interplay of the intact sensory modalities in the deaf. We presented visual, tactile, and visuo-tactile stimuli to congenitally deaf and hearing individuals in a speeded detection task. Analyses of multisensory responses showed a redundant signals effect that was attributable to a coactivation mechanism in both groups, although the redundancy gain was less in the deaf. In line with these behavioral results, on a neural level, there were multisensory interactions in both groups that were again weaker in the deaf. In hearing but not deaf participants, somatosensory event-related potential N200 latencies were modulated by simultaneous visual stimulation. A comparison of unisensory responses between groups revealed larger N200 amplitudes for visual and shorter N200 latencies for tactile stimuli in the deaf. Furthermore, P300 amplitudes were also larger in the deaf. This group difference was significant for tactile and approached significance for visual targets. The differences in visual and tactile processing between deaf and hearing participants, however, were not reflected in behavior. Both the behavioral and electroencephalography (EEG) results suggest more pronounced multisensory interaction in hearing than in deaf individuals. Visuo-tactile enhancements could not be explained by perceptual deficiency, but could be partly attributable to inverse effectiveness.DOI: 10.3389/fnint.2014.00098PMCID: PMC4300915",pubmed,25653602,10.3389/fnint.2014.00098
vesicular glutamatergic transmission in noiseinduced loss and repair of cochlear ribbon synapses,"529. J Neurosci. 2019 Jun 5;39(23):4434-4447. doi: 10.1523/JNEUROSCI.2228-18.2019. Epub 2019 Mar 29.Vesicular Glutamatergic Transmission in Noise-Induced Loss and Repair of Cochlear Ribbon Synapses.Kim KX(1), Payne S(1), Yang-Hood A(1), Li SZ(1), Davis B(1)(2), Carlquist J(1), V-Ghaffari B(1), Gantz JA(1), Kallogjeri D(1), Fitzpatrick JAJ(3), Ohlemiller KK(1), Hirose K(1), Rutherford MA(4).Author information:(1)Department of Otolaryngology, Washington University School of Medicine.(2)Program in Audiology and Communication Sciences, Washington University School of Medicine, St. Louis, Missouri 63110, and.(3)Washington University Center for Cellular Imaging, Department of Neuroscience, Department of Cell Biology and Physiology, Department of Biomedical Engineering, Washington University School of Medicine, St. Louis, Missouri 63110.(4)Department of Otolaryngology, Washington University School of Medicine, RutherfordM@ent.wustl.edu.Noise-induced excitotoxicity is thought to depend on glutamate. However, the excitotoxic mechanisms are unknown, and the necessity of glutamate for synapse loss or regeneration is unclear. Despite absence of glutamatergic transmission from cochlear inner hair cells in mice lacking the vesicular glutamate transporter-3 (Vglut3KO ), at 9-11 weeks, approximately half the number of synapses found in Vglut3WT were maintained as postsynaptic AMPA receptors juxtaposed with presynaptic ribbons and voltage-gated calcium channels (CaV1.3). Synapses were larger in Vglut3KO than Vglut3WT In Vglut3WT and Vglut3+/- mice, 8-16 kHz octave-band noise exposure at 100 dB sound pressure level caused a threshold shift (∼40 dB) and a loss of synapses (>50%) at 24 h after exposure. Hearing threshold and synapse number partially recovered by 2 weeks after exposure as ribbons became larger, whereas recovery was significantly better in Vglut3WT Noise exposure at 94 dB sound pressure level caused auditory threshold shifts that fully recovered in 2 weeks, whereas suprathreshold hearing recovered faster in Vglut3WT than Vglut3+/- These results, from mice of both sexes, suggest that spontaneous repair of synapses after noise depends on the level of Vglut3 protein or the level of glutamate release during the recovery period. Noise-induced loss of presynaptic ribbons or postsynaptic AMPA receptors was not observed in Vglut3KO , demonstrating its dependence on vesicular glutamate release. In Vglut3WT and Vglut3+/-, noise exposure caused unpairing of presynaptic ribbons and presynaptic CaV1.3, but not in Vglut3KO where CaV1.3 remained clustered with ribbons at presynaptic active zones. These results suggest that, without glutamate release, noise-induced presynaptic Ca2+ influx was insufficient to disassemble the active zone. However, synapse volume increased by 2 weeks after exposure in Vglut3KO , suggesting glutamate-independent mechanisms.SIGNIFICANCE STATEMENT Hearing depends on glutamatergic transmission mediated by Vglut3, but the role of glutamate in synapse loss and repair is unclear. Here, using mice of both sexes, we show that one copy of the Vglut3 gene is sufficient for noise-induced threshold shift and loss of ribbon synapses, but both copies are required for normal recovery of hearing function and ribbon synapse number. Impairment of the recovery process in mice having only one functional copy suggests that glutamate release may promote synapse regeneration. At least one copy of the Vglut3 gene is necessary for noise-induced synapse loss. Although the excitotoxic mechanism remains unknown, these findings are consistent with the presumption that glutamate is the key mediator of noise-induced synaptopathy.Copyright © 2019 the authors.DOI: 10.1523/JNEUROSCI.2228-18.2019PMCID: PMC6554621",pubmed,30926748,10.1523/JNEUROSCI.2228-18.2019
can haptic stimulation enhance music perception in hearingimpaired listeners,"712. Front Neurosci. 2021 Aug 31;15:723877. doi: 10.3389/fnins.2021.723877. eCollection 2021.Can Haptic Stimulation Enhance Music Perception in Hearing-Impaired Listeners?Fletcher MD(1)(2).Author information:(1)University of Southampton Auditory Implant Service, Faculty of Engineering and Physical Sciences, University of Southampton, Southampton, United Kingdom.(2)Institute of Sound and Vibration Research, Faculty of Engineering and Physical Sciences, University of Southampton, Southampton, United Kingdom.Cochlear implants (CIs) have been remarkably successful at restoring hearing in severely-to-profoundly hearing-impaired individuals. However, users often struggle to deconstruct complex auditory scenes with multiple simultaneous sounds, which can result in reduced music enjoyment and impaired speech understanding in background noise. Hearing aid users often have similar issues, though these are typically less acute. Several recent studies have shown that haptic stimulation can enhance CI listening by giving access to sound features that are poorly transmitted through the electrical CI signal. This ""electro-haptic stimulation"" improves melody recognition and pitch discrimination, as well as speech-in-noise performance and sound localization. The success of this approach suggests it could also enhance auditory perception in hearing-aid users and other hearing-impaired listeners. This review focuses on the use of haptic stimulation to enhance music perception in hearing-impaired listeners. Music is prevalent throughout everyday life, being critical to media such as film and video games, and often being central to events such as weddings and funerals. It represents the biggest challenge for signal processing, as it is typically an extremely complex acoustic signal, containing multiple simultaneous harmonic and inharmonic sounds. Signal-processing approaches developed for enhancing music perception could therefore have significant utility for other key issues faced by hearing-impaired listeners, such as understanding speech in noisy environments. This review first discusses the limits of music perception in hearing-impaired listeners and the limits of the tactile system. It then discusses the evidence around integration of audio and haptic stimulation in the brain. Next, the features, suitability, and success of current haptic devices for enhancing music perception are reviewed, as well as the signal-processing approaches that could be deployed in future haptic devices. Finally, the cutting-edge technologies that could be exploited for enhancing music perception with haptics are discussed. These include the latest micro motor and driver technology, low-power wireless technology, machine learning, big data, and cloud computing. New approaches for enhancing music perception in hearing-impaired listeners could substantially improve quality of life. Furthermore, effective haptic techniques for providing complex sound information could offer a non-invasive, affordable means for enhancing listening more broadly in hearing-impaired individuals.Copyright © 2021 Fletcher.DOI: 10.3389/fnins.2021.723877PMCID: PMC8439542",pubmed,34531717,10.3389/fnins.2021.723877
endtoend chinese lipreading recognition based on multimodal fusion,"With around 1.5 billion people worldwide suffering from hearing impairment, it is particularly important to communicate between non-disabled people and people with hearing or speech impairment and to build a barrier-free society. Multi-modal learning provides an excellent artificial intelligence channel for this purpose. In this article, we create an End-to-end Chinese Lip-Reading Recognition System based on multi-modal fusion to implement Chinese lip translation in order to facilitate communication between individuals with hearing impairment. Our system adopts the End-to-end Audio-visual feature fusion Lip-reading Recognition Architecture (EALRA), with feature extraction based on a MobileNet0.25 tuned CNN skeleton and the encoder back-end using the Conformer self-attentive convolution encoder for modelling. The largest Chinese Mandarin Lip-Reading (CMLR) was selected as the dataset for the empirical study, and the performance metric for Chinese lip recognition was the character error rate (CER). The results of our experiments show that the CER metric of EALRA in the lip-recognition model is 8.0, which is on average 23.74% lower than the CER metrics of other lip-recognition models, indicating that EALRA performs better in fusing image features and audio features.",ieee,,10.1109/ICFTIC57696.2022.10075247
tone discrimination in cantonesespeaking children using a cochlear implant,"484. Clin Linguist Phon. 2002 Mar;16(2):79-99. doi: 10.1080/02699200110109802.Tone discrimination in Cantonese-speaking children using a cochlear implant.Barry JG(1), Blamey PJ, Martin LF, Lee KY, Tang T, Ming YY, Van Hasselt CA.Author information:(1)University of Melbourne, Melbourne, Victoria, Australia. j.barry@medoto.unimelb.edu.auMost tone perception tests for Cantonese-speaking cochlear implant users have been based on tone identification tasks which require significant cognitive development to be successfully completed. Results from such tests suggest that cochlear implant child users are performing at about chance level and may not be receiving much information about pitch using the implant. This paper reports on the ability of cochlear implant child users to discriminate pitch variations in Cantonese by using an experimental procedure based on play audiometry. As part of the study, the usefulness of higher rates of electrode stimulation for aiding tone discrimination is also examined. Cochlear implant users are shown to derive sufficient information about pitch to discriminate most tone contrasts relatively successfully, with performance being most variable for contrasts involving tones clustered in the lower register of the speaker's fundamental frequency range. Contrary to hypothesis, higher electrode stimulation rates are not found to offer significant benefits for aiding pitch discrimination.DOI: 10.1080/02699200110109802",pubmed,11987495,10.1080/02699200110109802
level coding by phase duration and asymmetric pulse shape reduce channel interactions in cochlear implants,"850. Hear Res. 2020 Oct;396:108070. doi: 10.1016/j.heares.2020.108070. Epub 2020 Sep 4.Level coding by phase duration and asymmetric pulse shape reduce channel interactions in cochlear implants.Quass GL(1), Baumhoff P(2), Gnansia D(3), Stahl P(3), Kral A(4).Author information:(1)Institute for AudioNeuroTechnology (VIANNA), ENT Clinics, Hannover Medical School, 30625 Hannover, Germany; Cluster of Excellence ""Hearing4All"" (EXC 2177). Electronic address: quass.gunnar@mh-hannover.de.(2)Institute for AudioNeuroTechnology (VIANNA), ENT Clinics, Hannover Medical School, 30625 Hannover, Germany.(3)Oticon Medical, 06220 Vallauris, France.(4)Institute for AudioNeuroTechnology (VIANNA), ENT Clinics, Hannover Medical School, 30625 Hannover, Germany; Cluster of Excellence ""Hearing4All"" (EXC 2177).Conventional loudness coding with CIs by pulse current amplitude has a disadvantage: Increasing the stimulation current increases the spread of excitation in the auditory nerve, resulting in stronger channel interactions at high stimulation levels. These limit the number of effective information channels that a CI user can perceive. Stimulus intensity information (loudness) can alternatively be transmitted via pulse phase duration. We hypothesized that loudness coding by phase duration avoids the increase in the spread of the electric field and thus leads to less channel interactions at high stimulation levels. To avoid polarity effects, we combined this coding with pseudomonophasic stimuli. To test whether this affects the spread of excitation, 16 acutely deafened guinea pigs were implanted with CIs and neural activity from the inferior colliculus was recorded while stimulating with either biphasic, amplitude-coded pulses, or pseudomonophasic, duration- or amplitude-coded pulses. Pseudomonophasic stimuli combined with phase duration loudness coding reduced the lowest response thresholds and the spread of excitation. We investigated the channel interactions at suprathreshold levels by computing the phase-locking to a pulse train in the presence of an interacting pulse train on a different electrode on the CI. Pseudomonophasic pulses coupled with phase duration loudness coding reduced the interference by 4-5% compared to biphasic pulses, depending on the place of stimulation. This effect of pseudomonophasic stimuli was achieved with amplitude coding only in the basal cochlea, indicating a distance- or volume dependent effect. Our results show that pseudomonophasic, phase-duration-coded stimuli slightly reduce channel interactions, suggesting a potential benefit for speech understanding in humans.Copyright © 2020. Published by Elsevier B.V.DOI: 10.1016/j.heares.2020.108070",pubmed,32950954,10.1016/j.heares.2020.108070
relating fmri and pet signals to neural activity by means of largescale neural models,"514. Neuroinformatics. 2004;2(2):251-66. doi: 10.1385/NI:2:2:251.Relating fMRI and PET signals to neural activity by means of large-scale neural models.Horwitz B(1).Author information:(1)Section on Brain Imaging and Modeling, National Institute on Deafness and Other Communication Disorders, National Institutes of Health, Bethesda, MD, USA. horwitz@helix.nih.govThis article reviews the four ways by which large-scale, neurobiologically realistic modeling can be used in conjunction with functional neuroimaging data, especially that obtained by functional magnetic resonance imaging (fMRI) and positron emission tomography (PET), to help investigators understand the neural bases for sensorimotor and cognitive functions. The conceptually distinct purposes served are:(1) formulating and implementing specific hypotheses about how neuronal populations mediate a task, which will be illustrated using models of visual and auditory object processing; (2) determining how well an experimental design paradigm or analysis method works, which will be illustrated by examining event-related fMRI; (3) investigating the meaning in neural terms of macro-level concepts, which will be illustrated using functional connectivity; and (4) combining different types of macroscopic data with one another, which will be illustrated using transcranial magnetic stimulation (TMS) and PET.DOI: 10.1385/NI:2:2:251",pubmed,15319520,10.1385/NI:2:2:251
pax1 is essential for development and function of the human thymus,"We investigated the molecular and cellular basis of severe combined immunodeficiency (SCID) in six patients with otofaciocervical syndrome type 2 who failed to attain T cell reconstitution after allogeneic hematopoietic stem cell transplantation, despite successful engraftment in three of them. We identified rare biallelic PAX1 rare variants in all patients. We demonstrated that these mutant PAX1 proteins have an altered conformation and flexibility of the paired box domain and reduced transcriptional activity. We generated patient-derived induced pluripotent stem cells and differentiated them into thymic epithelial progenitor cells and found that they have an altered transcriptional profile, including for genes involved in the development of the thymus and other tissues derived from pharyngeal pouches. These results identify biallelic, loss-of-function PAX1 mutations as the cause of a syndromic form of SCID due to altered thymus development.  © 2020 The Authors.",scopus,2-s2.0-85081071846,10.1126/sciimmunol.aax1036
prevalence of paediatric chronic suppurative otitis media and hearing impairment in rural malawi a crosssectional survey,"292. PLoS One. 2017 Dec 21;12(12):e0188950. doi: 10.1371/journal.pone.0188950. eCollection 2017.Prevalence of paediatric chronic suppurative otitis media and hearing impairment in rural Malawi: A cross-sectional survey.Hunt L(1), Mulwafu W(2), Knott V(3), Ndamala CB(4), Naunje AW(4), Dewhurst S(5), Hall A(6), Mortimer K(1).Author information:(1)Liverpool School of Tropical Medicine, Liverpool, United Kingdom.(2)College of Medicine, Blantyre, Malawi.(3)Sheffield Teaching Hospitals NHS Foundation Trust, Sheffield, United Kingdom.(4)Malawi-Liverpool-Wellcome Trust Clinical Research Program, Blantyre, Malawi.(5)University Hospitals Leicester NHS Foundation Trust, Leicester, United Kingdom.(6)Independent Scholar, Sheffield, United Kingdom.OBJECTIVE: To estimate the prevalence of World Health Organization-defined chronic suppurative otitis media (CSOM) and mild hearing impairment in a population representative sample of school-entry age children in rural Malawi. A secondary objective was to explore factors associated with CSOM in this population.METHODS: We performed a community-based cross-sectional study of children aged 4-6 years in Chikhwawa District, Southern Malawi, utilising a village-level cluster design. Participants underwent a structured clinical assessment, including video-otoscopy and screening audiometry. Diagnoses were made remotely by two otolaryngologists who independently reviewed clinical data and images collected in the field. Hearing impairment was classified as failure to hear a pure tone of 25dB or greater at 1, 2 or 4kHz.RESULTS: We recruited 281 children across 10 clusters. The prevalence estimates of CSOM, unilateral hearing impairment and bilateral hearing impairment were 5.4% (95%CI 2.2-8.6), 24.5% (95%CI 16.3-30.0), and 12.5% (95%CI 6.2-16.9) respectively. Middle ear disease was seen in 46.9% of children with hearing impairment. A trend towards increased risk of CSOM was observed with sleeping in a house with >2 other children.INTERPRETATION: We found a high burden of middle ear disease and preventable hearing impairment in our sample of school-entry age children in rural Malawi. There are important public health implications of these findings as CSOM and hearing impairment can affect educational outcomes, and may impact subsequent development. The identification and management of middle ear disease and hearing impairment represent major unmet needs in this population.DOI: 10.1371/journal.pone.0188950PMCID: PMC5739401",pubmed,29267304,10.1371/journal.pone.0188950
a rare case of angiolymphoid hyperplasia with eosinophilia with a new effective treatment,"754. Cureus. 2023 Jun 5;15(6):e39966. doi: 10.7759/cureus.39966. eCollection 2023 Jun.A Rare Case of Angiolymphoid Hyperplasia With Eosinophilia With a New Effective Treatment.Hasan U(1), Ahmed N(2), Malik T(1), Shah SA(1), Subhan U(1).Author information:(1)Dermatology, Pakistan Navy Station (PNS) Shifa Hospital, Karachi, PAK.(2)Dermatology, Bahria University of Health Sciences, Pakistan Navy Station (PNS) Shifa Hospital, Karachi, PAK.Angiolymphoid hyperplasia with eosinophilia (ALHE) is a benign locally proliferating lesion of unknown etiology, composed of vascular channels lined by endothelial cells, surrounded by lymphocytes and eosinophils. It presents clinically as a cluster of skin to violaceous-colored nodules on the head and neck, particularly in and around the ear. We present the case of a 50-year-old, Pakistani woman with unilateral multiple nodular lesions for eight years in the left ear concha and postauricular area causing complete obliteration of the external auditory meatus with conductive hearing loss of the left ear for seven years. Biopsy showed lymphoid follicles and dilated blood vessels with mixed infiltrate predominantly eosinophils corresponding to the diagnosis of angiolymphoid hyperplasia with eosinophilia. Surgical excision was not feasible, and there was no response to topical steroids. The patient was started on beta blockers. After three months, postauricular lesions completely resolved, and the size of the rest of the nodules decreased markedly; then hearing loss also recovered. Our objective in this study is to emphasize the importance of considering beta blockers for the treatment of ALHE.Copyright © 2023, Hasan et al.DOI: 10.7759/cureus.39966PMCID: PMC10320733",pubmed,37415992,10.7759/cureus.39966
coherent coding of enhanced interaural cues improves sound localization in noise with bilateral cochlear implants,"406. Trends Hear. 2018 Jan-Dec;22:2331216518781746. doi: 10.1177/2331216518781746.Coherent Coding of Enhanced Interaural Cues Improves Sound Localization in Noise With Bilateral Cochlear Implants.Williges B(1), Jürgens T(1)(2), Hu H(1), Dietz M(1)(3).Author information:(1)1 Medizinische Physik and Cluster of Excellence ""Hearing4all,"" Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany.(2)2 Institute of Acoustics, University of Applied Sciences Lübeck, Lübeck, Germany.(3)3 National Centre for Audiology, School of Communication Sciences and Disorders, Western University, London, Ontario, Canada.Bilateral cochlear implant (BCI) users only have very limited spatial hearing abilities. Speech coding strategies transmit interaural level differences (ILDs) but in a distorted manner. Interaural time difference (ITD) information transmission is even more limited. With these cues, most BCI users can coarsely localize a single source in quiet, but performance quickly declines in the presence of other sound. This proof-of-concept study presents a novel signal processing algorithm specific for BCIs, with the aim to improve sound localization in noise. The core part of the BCI algorithm duplicates a monophonic electrode pulse pattern and applies quasistationary natural or artificial ITDs or ILDs based on the estimated direction of the dominant source. Three experiments were conducted to evaluate different algorithm variants: Experiment 1 tested if ITD transmission alone enables BCI subjects to lateralize speech. Results showed that six out of nine BCI subjects were able to lateralize intelligible speech in quiet solely based on ITDs. Experiments 2 and 3 assessed azimuthal angle discrimination in noise with natural or modified ILDs and ITDs. Angle discrimination for frontal locations was possible with all variants, including the pure ITD case, but for lateral reference angles, it was only possible with a linearized ILD mapping. Speech intelligibility in noise, limitations, and challenges of this interaural cue transmission approach are discussed alongside suggestions for modifying and further improving the BCI algorithm.DOI: 10.1177/2331216518781746PMCID: PMC6048749",pubmed,29956589,10.1177/2331216518781746
crossmodal reorganization in cochlear implant users auditory cortex contributes to visual face processing,"650. Neuroimage. 2015 Nov 1;121:159-70. doi: 10.1016/j.neuroimage.2015.07.062. Epub 2015 Jul 26.Cross-modal reorganization in cochlear implant users: Auditory cortex contributes to visual face processing.Stropahl M(1), Plotz K(2), Schönfeld R(2), Lenarz T(3), Sandmann P(4), Yovel G(5), De Vos M(6), Debener S(7).Author information:(1)Neuropsychology Lab, Department of Psychology, Carl von Ossietzky University Oldenburg, Germany. Electronic address: maren.stropahl@uni-oldenburg.de.(2)Department of Phoniatrics, Pediatric Audiology and Neurootology, Evangelisches Krankenhaus Oldenburg, Germany.(3)Department of Otolaryngology, Hannover Medical School, Germany; Cluster of Excellence Hearing4all Oldenburg, Germany.(4)Cluster of Excellence Hearing4all Oldenburg, Germany; Department of Neurology, Hannover Medical School, Germany.(5)Department of Psychology, Tel Aviv University, Tel Aviv, Israel.(6)Cluster of Excellence Hearing4all Oldenburg, Germany; Department of Engineering Science, University of Oxford, UK; Methods in Cognitive Psychology, Department of Psychology, Carl von Ossietzky University Oldenburg, Germany.(7)Neuropsychology Lab, Department of Psychology, Carl von Ossietzky University Oldenburg, Germany; Cluster of Excellence Hearing4all Oldenburg, Germany.There is converging evidence that the auditory cortex takes over visual functions during a period of auditory deprivation. A residual pattern of cross-modal take-over may prevent the auditory cortex to adapt to restored sensory input as delivered by a cochlear implant (CI) and limit speech intelligibility with a CI. The aim of the present study was to investigate whether visual face processing in CI users activates auditory cortex and whether this has adaptive or maladaptive consequences. High-density electroencephalogram data were recorded from CI users (n=21) and age-matched normal hearing controls (n=21) performing a face versus house discrimination task. Lip reading and face recognition abilities were measured as well as speech intelligibility. Evaluation of event-related potential (ERP) topographies revealed significant group differences over occipito-temporal scalp regions. Distributed source analysis identified significantly higher activation in the right auditory cortex for CI users compared to NH controls, confirming visual take-over. Lip reading skills were significantly enhanced in the CI group and appeared to be particularly better after a longer duration of deafness, while face recognition was not significantly different between groups. However, auditory cortex activation in CI users was positively related to face recognition abilities. Our results confirm a cross-modal reorganization for ecologically valid visual stimuli in CI users. Furthermore, they suggest that residual takeover, which can persist even after adaptation to a CI is not necessarily maladaptive.Copyright © 2015 Elsevier Inc. All rights reserved.DOI: 10.1016/j.neuroimage.2015.07.062",pubmed,26220741,10.1016/j.neuroimage.2015.07.062
cd44 is a marker for the outer pillar cells in the early postnatal mouse inner ear,"696. J Assoc Res Otolaryngol. 2010 Sep;11(3):407-18. doi: 10.1007/s10162-010-0211-x. Epub 2010 Apr 13.CD44 is a marker for the outer pillar cells in the early postnatal mouse inner ear.Hertzano R(1), Puligilla C, Chan SL, Timothy C, Depireux DA, Ahmed Z, Wolf J, Eisenman DJ, Friedman TB, Riazuddin S, Kelley MW, Strome SE.Author information:(1)Department of Otorhinolaryngology-Head and Neck Surgery, University of Maryland, 16 South Eutaw Street, Baltimore, MD 21201, USA. rhertzano@smail.umaryland.eduCluster of differentiation antigens (CD proteins) are classically used as immune cell markers. However, their expression within the inner ear is still largely undefined. In this study, we explored the possibility that specific CD proteins might be useful for defining inner ear cell populations. mRNA expression profiling of microdissected auditory and vestibular sensory epithelia revealed 107 CD genes as expressed in the early postnatal mouse inner ear. The expression of 68 CD genes was validated with real-time RT-PCR using RNA extracted from microdissected sensory epithelia of cochleae, utricles, saccules, and cristae of newborn mice. Specifically, CD44 was identified as preferentially expressed in the auditory sensory epithelium. Immunohistochemistry revealed that within the early postnatal organ of Corti, the expression of CD44 is restricted to outer pillar cells. In order to confirm and expand this finding, we characterized the expression of CD44 in two different strains of mice with loss- and gain-of-function mutations in Fgfr3 which encodes a receptor for FGF8 that is essential for pillar cell development. We found that the expression of CD44 is abolished from the immature pillar cells in homozygous Fgfr3 knockout mice. In contrast, both the outer pillar cells and the aberrant Deiters' cells in the Fgfr3 ( P244R/ ) (+) mice express CD44. The deafness phenotype segregating in DFNB51 families maps to a linkage interval that includes CD44. To study the potential role of CD44 in hearing, we characterized the auditory system of CD44 knockout mice and sequenced the entire open reading frame of CD44 of affected members of DFNB51 families. Our results suggest that CD44 does not underlie the deafness phenotype of the DFNB51 families. Finally, our study reveals multiple potential new cell type-specific markers in the mouse inner ear and identifies a new marker for outer pillar cells.DOI: 10.1007/s10162-010-0211-xPMCID: PMC2914240",pubmed,20386946,10.1007/s10162-010-0211-x
hair cell mechanotransduction regulates spontaneous activity and spiral ganglion subtype specification in the auditory system,"637. Cell. 2018 Aug 23;174(5):1247-1263.e15. doi: 10.1016/j.cell.2018.07.008. Epub 2018 Aug 2.Hair Cell Mechanotransduction Regulates Spontaneous Activity and Spiral Ganglion Subtype Specification in the Auditory System.Sun S(1), Babola T(1), Pregernig G(2), So KS(2), Nguyen M(2), Su SM(2), Palermo AT(2), Bergles DE(1), Burns JC(3), Müller U(4).Author information:(1)The Solomon Snyder Department of Neuroscience and Department of Otolaryngology, Head and Neck Surgery, Johns Hopkins University School of Medicine, 725 N. Wolfe Street, Baltimore, MD 21205, USA.(2)Decibel Therapeutics, 1325 Boylston Street, Suite 500, Boston, MA 02215, USA.(3)Decibel Therapeutics, 1325 Boylston Street, Suite 500, Boston, MA 02215, USA. Electronic address: jburns@decibeltx.com.(4)The Solomon Snyder Department of Neuroscience and Department of Otolaryngology, Head and Neck Surgery, Johns Hopkins University School of Medicine, 725 N. Wolfe Street, Baltimore, MD 21205, USA. Electronic address: umuelle3@jhmi.edu.Comment in    Nat Rev Neurosci. 2018 Oct;19(10):579.Type I spiral ganglion neurons (SGNs) transmit sound information from cochlear hair cells to the CNS. Using transcriptome analysis of thousands of single neurons, we demonstrate that murine type I SGNs consist of subclasses that are defined by the expression of subsets of transcription factors, cell adhesion molecules, ion channels, and neurotransmitter receptors. Subtype specification is initiated prior to the onset of hearing during the time period when auditory circuits mature. Gene mutations linked to deafness that disrupt hair cell mechanotransduction or glutamatergic signaling perturb the firing behavior of SGNs prior to hearing onset and disrupt SGN subtype specification. We thus conclude that an intact hair cell mechanotransduction machinery is critical during the pre-hearing period to regulate the firing behavior of SGNs and their segregation into subtypes. Because deafness is frequently caused by defects in hair cells, our findings have significant ramifications for the etiology of hearing loss and its treatment.Copyright © 2018 Elsevier Inc. All rights reserved.DOI: 10.1016/j.cell.2018.07.008PMCID: PMC6429032",pubmed,30078710,10.1016/j.cell.2018.07.008
a suboscine bird eastern phoebe sayornis phoebe develops normal song without auditory feedback,"Imitative song development, its requisite auditory feedback, and the underlying neural control of learned song are becoming increasingly well known in songbirds, but the evolution of these characteristics from songbird ancestors is poorly understood. Suboscine flycatchers, which belong to the evolutionary sister group of the oscine songbirds (in the same order, Passeriformes), are thought not to imitate songs from other individuals. This study therefore examines the role of auditory feedback in song development and provides preliminary comments on neural control. Four eastern phoebes, Sayornis phoebe, were collected at 10-12 days of age and hand-reared in the laboratory; at approximately 35 days of age, before they began to sing, the birds were bilaterally deafened by removal of the cochlea. Songs of these phoebes, two males and two females, were judged to be normal when compared with songs of males recorded in nature and to songs of laboratory-reared, intact males and females. Like several non-passerines (representatives of Galliformes and Columbiformes), the eastern phoebe requires no auditory feedback for normal vocal development. Brain sections of phoebes contain no obvious cell clusters like the forebrain song nuclei of songbirds. If some of these nuclei mediate auditory feedback control of song development, the apparent absence of these nuclei in the phoebe is consistent with its ability to develop normal song without auditory feedback. © 1991 The Association for the Study of Animal Behaviour.",scopus,2-s2.0-0026293290,10.1016/S0003-3472(05)80047-8
a mutation in the srrm4 gene causes alternative splicing defects and deafness in the bronx waltzer mouse,"590. PLoS Genet. 2012;8(10):e1002966. doi: 10.1371/journal.pgen.1002966. Epub 2012 Oct 4.A mutation in the Srrm4 gene causes alternative splicing defects and deafness in the Bronx waltzer mouse.Nakano Y(1), Jahan I, Bonde G, Sun X, Hildebrand MS, Engelhardt JF, Smith RJ, Cornell RA, Fritzsch B, Bánfi B.Author information:(1)Department of Anatomy and Cell Biology, Carver College of Medicine, University of Iowa, Iowa City, IA, USA.Sensory hair cells are essential for hearing and balance. Their development from epithelial precursors has been extensively characterized with respect to transcriptional regulation, but not in terms of posttranscriptional influences. Here we report on the identification and functional characterization of an alternative-splicing regulator whose inactivation is responsible for defective hair-cell development, deafness, and impaired balance in the spontaneous mutant Bronx waltzer (bv) mouse. We used positional cloning and transgenic rescue to locate the bv mutation to the splicing factor-encoding gene Ser/Arg repetitive matrix 4 (Srrm4). Transcriptome-wide analysis of pre-mRNA splicing in the sensory patches of embryonic inner ears revealed that specific alternative exons were skipped at abnormally high rates in the bv mice. Minigene experiments in a heterologous expression system confirmed that these skipped exons require Srrm4 for inclusion into the mature mRNA. Sequence analysis and mutagenesis experiments showed that the affected transcripts share a novel motif that is necessary for the Srrm4-dependent alternative splicing. Functional annotations and protein-protein interaction data indicated that the encoded proteins cluster in the secretion and neurotransmission pathways. In addition, the splicing of a few transcriptional regulators was found to be Srrm4 dependent, and several of the genes known to be targeted by these regulators were expressed at reduced levels in the bv mice. Although Srrm4 expression was detected in neural tissues as well as hair cells, analyses of the bv mouse cerebellum and neocortex failed to detect splicing defects. Our data suggest that Srrm4 function is critical in the hearing and balance organs, but not in all neural tissues. Srrm4 is the first alternative-splicing regulator to be associated with hearing, and the analysis of bv mice provides exon-level insights into hair-cell development.DOI: 10.1371/journal.pgen.1002966PMCID: PMC3464207",pubmed,23055939,10.1371/journal.pgen.1002966
a reafferent and feedforward model of song syntax generation in the bengalese finch,"522. J Comput Neurosci. 2011 Nov;31(3):509-32. doi: 10.1007/s10827-011-0318-z. Epub 2011 Mar 15.A reafferent and feed-forward model of song syntax generation in the Bengalese finch.Hanuschkin A(1), Diesmann M, Morrison A.Author information:(1)Functional Neural Circuits Group, Faculty of Biology, Albert-Ludwig University of Freiburg, Schänzlestrasse 1, 79104 Freiburg, Germany. hanuschkin@bccn.uni-freiburg.deAdult Bengalese finches generate a variable song that obeys a distinct and individual syntax. The syntax is gradually lost over a period of days after deafening and is recovered when hearing is restored. We present a spiking neuronal network model of the song syntax generation and its loss, based on the assumption that the syntax is stored in reafferent connections from the auditory to the motor control area. Propagating synfire activity in the HVC codes for individual syllables of the song and priming signals from the auditory network reduce the competition between syllables to allow only those transitions that are permitted by the syntax. Both imprinting of song syntax within HVC and the interaction of the reafferent signal with an efference copy of the motor command are sufficient to explain the gradual loss of syntax in the absence of auditory feedback. The model also reproduces for the first time experimental findings on the influence of altered auditory feedback on the song syntax generation, and predicts song- and species-specific low frequency components in the LFP. This study illustrates how sequential compositionality following a defined syntax can be realized in networks of spiking neurons.DOI: 10.1007/s10827-011-0318-zPMCID: PMC3232349",pubmed,21404048,10.1007/s10827-011-0318-z
visual influences on auditory behavioral neural and perceptual processes a review,"704. J Assoc Res Otolaryngol. 2021 Jul;22(4):365-386. doi: 10.1007/s10162-021-00789-0. Epub 2021 May 20.Visual Influences on Auditory Behavioral, Neural, and Perceptual Processes: A Review.Opoku-Baah C(1)(2), Schoenhaut AM(1)(2), Vassall SG(1)(2), Tovar DA(1)(2), Ramachandran R(2)(3)(4)(5), Wallace MT(6)(7)(8)(9)(10)(11).Author information:(1)Neuroscience Graduate Program, Vanderbilt University, Nashville, TN, USA.(2)Vanderbilt Brain Institute, Vanderbilt University, Nashville, TN, USA.(3)Department of Psychology, Vanderbilt University, Nashville, TN, USA.(4)Department of Hearing and Speech, Vanderbilt University Medical Center, Nashville, TN, USA.(5)Vanderbilt Vision Research Center, Nashville, TN, USA.(6)Vanderbilt Brain Institute, Vanderbilt University, Nashville, TN, USA. mark.wallace@vanderbilt.edu.(7)Department of Psychology, Vanderbilt University, Nashville, TN, USA. mark.wallace@vanderbilt.edu.(8)Department of Hearing and Speech, Vanderbilt University Medical Center, Nashville, TN, USA. mark.wallace@vanderbilt.edu.(9)Vanderbilt Vision Research Center, Nashville, TN, USA. mark.wallace@vanderbilt.edu.(10)Department of Psychiatry and Behavioral Sciences, Vanderbilt University Medical Center, Nashville, TN, USA. mark.wallace@vanderbilt.edu.(11)Department of Pharmacology, Vanderbilt University, Nashville, TN, USA. mark.wallace@vanderbilt.edu.In a naturalistic environment, auditory cues are often accompanied by information from other senses, which can be redundant with or complementary to the auditory information. Although the multisensory interactions derived from this combination of information and that shape auditory function are seen across all sensory modalities, our greatest body of knowledge to date centers on how vision influences audition. In this review, we attempt to capture the state of our understanding at this point in time regarding this topic. Following a general introduction, the review is divided into 5 sections. In the first section, we review the psychophysical evidence in humans regarding vision's influence in audition, making the distinction between vision's ability to enhance versus alter auditory performance and perception. Three examples are then described that serve to highlight vision's ability to modulate auditory processes: spatial ventriloquism, cross-modal dynamic capture, and the McGurk effect. The final part of this section discusses models that have been built based on available psychophysical data and that seek to provide greater mechanistic insights into how vision can impact audition. The second section reviews the extant neuroimaging and far-field imaging work on this topic, with a strong emphasis on the roles of feedforward and feedback processes, on imaging insights into the causal nature of audiovisual interactions, and on the limitations of current imaging-based approaches. These limitations point to a greater need for machine-learning-based decoding approaches toward understanding how auditory representations are shaped by vision. The third section reviews the wealth of neuroanatomical and neurophysiological data from animal models that highlights audiovisual interactions at the neuronal and circuit level in both subcortical and cortical structures. It also speaks to the functional significance of audiovisual interactions for two critically important facets of auditory perception-scene analysis and communication. The fourth section presents current evidence for alterations in audiovisual processes in three clinical conditions: autism, schizophrenia, and sensorineural hearing loss. These changes in audiovisual interactions are postulated to have cascading effects on higher-order domains of dysfunction in these conditions. The final section highlights ongoing work seeking to leverage our knowledge of audiovisual interactions to develop better remediation approaches to these sensory-based disorders, founded in concepts of perceptual plasticity in which vision has been shown to have the capacity to facilitate auditory learning.© 2021. The Author(s).DOI: 10.1007/s10162-021-00789-0PMCID: PMC8329114",pubmed,34014416,10.1007/s10162-021-00789-0
universal automated classification of the acoustic startle reflex using machine learning,"735. Hear Res. 2023 Feb;428:108667. doi: 10.1016/j.heares.2022.108667. Epub 2022 Dec 15.Universal automated classification of the acoustic startle reflex using machine learning.Fawcett TJ(1), Longenecker RJ(2), Brunelle DL(3), Berger JI(4), Wallace MN(5), Galazyuk AV(6), Rosen MJ(6), Salvi RJ(7), Walton JP(8).Author information:(1)Global Center for Hearing and Speech Research, University of South Florida, Tampa, FL, USA; Research Computing, University of South Florida, Tampa, FL, USA. Electronic address: tfawcett@usf.edu.(2)Sound Pharmaceuticals Inc, 4010 Stone Way N., Suite 120, Seattle, WA 98103, USA.(3)Global Center for Hearing and Speech Research, University of South Florida, Tampa, FL, USA.(4)Department of Neurosurgery, University of Iowa Hospitals and Clinics, Iowa City, IA, USA.(5)Hearing Sciences, School of Medicine, University of Nottingham, Nottingham, UK.(6)Hearing Research Group, Department of Anatomy and Neurobiology, Northeast Ohio Medical University, Rootstown, OH, USA.(7)Center for Hearing and Deafness, University at Buffalo, University of Buffalo, USA.(8)Global Center for Hearing and Speech Research, University of South Florida, Tampa, FL, USA; Department of Medical Engineering, University of South Florida, Tampa, FL, USA; Department of Communication Sciences and Disorders, University of South Florida, Tampa, FL, USA. Electronic address: jwalton1@usf.edu.The startle reflex (SR), a robust, motor response elicited by an intense auditory, visual, or somatosensory stimulus has been widely used as a tool to assess psychophysiology in humans and animals for almost a century in diverse fields such as schizophrenia, bipolar disorder, hearing loss, and tinnitus. Previously, SR waveforms have been ignored, or assessed with basic statistical techniques and/or simple template matching paradigms. This has led to considerable variability in SR studies from different laboratories, and species. In an effort to standardize SR assessment methods, we developed a machine learning algorithm and workflow to automatically classify SR waveforms in virtually any animal model including mice, rats, guinea pigs, and gerbils obtained with various paradigms and modalities from several laboratories. The universal features common to SR waveforms of various species and paradigms are examined and discussed in the context of each animal model. The procedure describes common results using the SR across species and how to fully implement the open-source R implementation. Since SR is widely used to investigate toxicological or pharmaceutical efficacy, a detailed and universal SR waveform classification protocol should be developed to aid in standardizing SR assessment procedures across different laboratories and species. This machine learning-based method will improve data reliability and translatability between labs that use the startle reflex paradigm.Copyright © 2022. Published by Elsevier B.V.DOI: 10.1016/j.heares.2022.108667PMCID: PMC10734095",pubmed,36566642,10.1016/j.heares.2022.108667
evoked otoacoustic emissions and middle ear function,"434. Laryngorhinootologie. 1994 Mar;73(3):118-22. doi: 10.1055/s-2007-997092.[Evoked otoacoustic emissions and middle ear function].[Article in German]Rödel R(1), Breuer T.Author information:(1)Universitäts-Hals-Nasen-Ohren-Klinik Bonn.Transiently evoked otoacustic emissions (TEOAE) recorded in 98 ears were compared depending on the results of tympanometry. The values of middle ear pressure in patients without detectable TEOAE are significantly smaller compared to those of patients with detectable TEOAE. In cases of detectable TEOAE, the amplitudes and frequency components were compared to the results of tympanometry. When maximal compliances are shifted towards negative values of middle ear pressure, the amplitudes of TEOAE are reduced with a loss of low frequency components. Small compliance values result in reduced amplitudes of TEOAE with the loss of low frequency components. By means of cluster analysis in the scatter plot of middle ear compliance and pressure, a classification is obtained separating two groups representing significantly groups of patients with and without detectable TEOAE. Summing up, it must be stated that there is an effect of middle ear function on TEOAE which must be taken into consideration when TEOAE are used in clinical application.DOI: 10.1055/s-2007-997092",pubmed,8172629,10.1055/s-2007-997092
unlocking the basis of tinnitus sound therapy informational masking and tinnitus adaptation  year 2,,cinahl,8976368,
prenatal exposure to tobacco and alcohol alters development of the neonatal auditory system,"251. Dev Neurosci. 2021;43(6):358-375. doi: 10.1159/000518130. Epub 2021 Jul 19.Prenatal Exposure to Tobacco and Alcohol Alters Development of the Neonatal Auditory System.Sininger YS(1)(2), Condon CG(3), Gimenez LA(3), Shuffrey LC(3)(4), Myers MM(3)(4)(5), Elliott AJ(6)(7), Thai T(3), Nugent JD(3)(4), Pini N(3)(4), Sania A(4), Odendaal HJ(8), Angal J(6)(7), Tobacco D(6)(7), Hoffman HJ(9), Simmons DD(10), Fifer WP(3)(4)(5).Author information:(1)Department of Head & Neck Surgery, University of California, Los Angeles, California, USA.(2)C&Y Consultants, Santa Fe, New Mexico, USA.(3)Division of Developmental Neuroscience, New York State Psychiatric Institute, New York, New York, USA.(4)Department of Psychiatry, Columbia University Irving Medical Center, New York, New York, USA.(5)Department of Pediatrics, Columbia University Irving Medical Center, New York, New York, USA.(6)Center for Pediatric & Community Research, Avera Research Institute, Sioux Falls, South Dakota, USA.(7)Department of Pediatrics, University of South Dakota School of Medicine, Sioux Falls, South Dakota, USA.(8)Department of Obstetrics and Gynaecology, Faculty of Medicine and Health Science, Stellenbosch University, Cape Town, South Africa.(9)Epidemiology and Statistics Program, National Institute on Deafness and Other Communication Disorders (NIDCD), National Institutes of Health (NIH), Bethesda, Maryland, USA.(10)Department of Biology, Baylor University, Waco, Texas, USA.Prenatal exposures to alcohol (PAE) and tobacco (PTE) are known to produce adverse neonatal and childhood outcomes including damage to the developing auditory system. Knowledge of the timing, extent, and combinations of these exposures on effects on the developing system is limited. As part of the physiological measurements from the Safe Passage Study, Auditory Brainstem Responses (ABRs) and Transient Otoacoustic Emissions (TEOAEs) were acquired on infants at birth and one-month of age. Research sites were in South Africa and the Northern Plains of the U.S. Prenatal information on alcohol and tobacco exposure was gathered prospectively on mother/infant dyads. Cluster analysis was used to characterize three levels of PAE and three levels of PTE. Repeated-measures ANOVAs were conducted for newborn and one-month-old infants for ABR peak latencies and amplitudes and TEOAE levels and signal-to-noise ratios. Analyses controlled for hours of life at test, gestational age at birth, sex, site, and other exposure. Significant main effects of PTE included reduced newborn ABR latencies from both ears. PTE also resulted in a significant reduction of ABR peak amplitudes elicited in infants at 1-month of age. PAE led to a reduction of TEOAE amplitude for 1-month-old infants but only in the left ear. Results indicate that PAE and PTE lead to early disruption of peripheral, brainstem, and cortical development and neuronal pathways of the auditory system, including the olivocochlear pathway.© 2021 S. Karger AG, Basel.DOI: 10.1159/000518130",pubmed,34348289,10.1159/000518130
predicting hearing loss from otoacoustic emissions using an artificial neural network,"338. S Afr J Commun Disord. 2002;49:28-39.Predicting hearing loss from otoacoustic emissions using an artificial neural network.de Waal R(1), Hugo R, Soer M, Krüger JJ.Author information:(1)Department of Communication Pathology & Electronic Engineering, University of Pretoria.Normal and impaired pure tone thresholds (PTTs) were predicted from distortion product otoacoustic emissions (DPOAEs) using a feed-forward artificial neural network (ANN) with a back-propagation training algorithm. The ANN used a map of present and absent DPOAEs from eight DPgrams, (2f1-f2 = 406-4031 Hz) to predict PTTs at 0.5, 1, 2 and 4 kHz. With normal hearing as < 25 dB HL, prediction accuracy of normal hearing was 94% at 500, 88% at 1000, 88% at 2000 and 93% at 4000 Hz. Prediction of hearing-impaired categories was less accurate, due to insufficient data for the ANN to train on. This research indicates the possibility of accurately predicting hearing ability within 10 dB in normal hearing individuals and in hearing-impaired listeners with DPOAEs and ANNs from 500-4000 Hz.",pubmed,14968700,
the role of sperm associated antigen 6 gene in morphological changes of inner ear development and signal regulation of auditory organs in mice,"Aim: In order to analyze the effects of the deletion of the Sperm Associated Antigen 6 (Spag6) gene on the inner ear and the auditory system of mice. Method: Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)/Cas9 technology was utilized, and the Spag6 gene knockout mouse model was constructed. Self-breeding was carried out to obtain the F1 generation Spag6 homozygous mouse (Spag−/−), the Spag6 heterozygous mouse (Spag+/−) and the Spag6 wild mouse (Spag+/+) were obtained. Polymerase Chain Reaction (PCR) technology was used to verify the three genotypes of mice. The mean hearing threshold of Spag−/− mice, Spag+/− mice and Spag+/+ mice after different intensity of sound stimulation was detected. The inner ear cochlea tissues of Spag−/− mice and Spag+/+ mice were collected and paraffin sections were made. Morphological differences of inner ear tissues in mice were analyzed by hematoxylin – eosin (HE) staining. Immunofluorescence staining was used to analyze the number of hair cells in the inner ear of mice. Apoptosis of mouse inner ear hair cells was analyzed by TdT-mediated dUTP Nick-End Labeling (TUNEL) staining Result: It was found that the hearing of Spag−/− mice decreased significantly compared with that of Spag+/+ mice (P < 0.01), and the hearing of Spag+/− mice decreased significantly compared with that of Spag+/− mice (P < 0.05). After HE staining and immunofluorescence staining, hair cells in the inner ear cochlea of Spag−/− mice were found to be defective. The apoptosis detection results of TUNEL staining indicated that the number of apoptosis of hair cells in inner ear cochlea of Spag−/− mice was significantly higher than that of Spag+/+ mice, and the average optical density of Corti apparatus of Spag−/− mice was significantly higher than that of Spag+/+ mice (P < 0.01). Conclusion: The results showed that the loss of the Spag6 gene accelerated the apoptosis of hair cells in the cochlear tissue of the inner ear of mice, thereby affecting the auditory system of mice. The significance of this study is to lay the foundation for future studies on the effects of Spag6 on deafness. © 2019 The Authors",scopus,2-s2.0-85077337605,10.1016/j.jksus.2019.12.015
agerelated inflammation and oxidative stress in the cochlea are exacerbated by longterm shortduration noise stimulation,"617. Front Aging Neurosci. 2022 Apr 5;14:853320. doi: 10.3389/fnagi.2022.853320. eCollection 2022.Age-Related Inflammation and Oxidative Stress in the Cochlea Are Exacerbated by Long-Term, Short-Duration Noise Stimulation.Fuentes-Santamaría V(1)(2), Alvarado JC(1)(2), Mellado S(1)(2), Melgar-Rojas P(1)(2), Gabaldón-Ull MC(1)(2), Cabanes-Sanchis JJ(1)(2), Juiz JM(1)(2)(3).Author information:(1)Instituto de Investigación en Discapacidades Neurológicas (IDINE), Albacete, Spain.(2)Facultad de Medicina, Universidad de Castilla-La Mancha, Albacete, Spain.(3)Department of Otolaryngology, Hannover Medical School, NIFE-VIANNA, Cluster of Excellence Hearing4all-German Research Foundation, Hanover, Germany.We have previously reported that young adult rats exposed to daily, short-duration noise for extended time periods, develop accelerated presbycusis starting at 6 months of age. Auditory aging is associated with progressive hearing loss, cell deterioration, dysregulation of the antioxidant defense system, and chronic inflammation, among others. To further characterize cellular and molecular mechanisms at the crossroads between noise and age-related hearing loss (ARHL), 3-month-old rats were exposed to a noise-accelerated presbycusis (NAP) protocol and tested at 6 and 16 months of age, using auditory brainstem responses, Real-Time Reverse Transcription-Quantitative PCR (RT-qPCR) and immunocytochemistry. Chronic noise-exposure leading to permanent auditory threshold shifts in 6-month-old rats, resulted in impaired sodium/potassium activity, degenerative changes in the lateral wall and spiral ganglion, increased lipid peroxidation, and sustained cochlear inflammation with advancing age. Additionally, at 6 months, noise-exposed rats showed significant increases in the gene expression of antioxidant enzymes (superoxide dismutase 1/2, glutathione peroxidase 1, and catalase) and inflammation-associated molecules [ionized calcium binding adaptor molecule 1, interleukin-1 beta (IL-1β), and tumor necrosis factor-alpha]. The levels of IL-1β were upregulated in the spiral ganglion and spiral ligament, particularly in type IV fibrocytes; these cells showed decreased levels of connective tissue growth factor and increased levels of 4-hydroxynonenal. These data provide functional, structural and molecular evidence that age-noise interaction contributes to exacerbating presbycusis in young rats by leading to progressive dysfunction and early degeneration of cochlear cells and structures. These findings contribute to a better understanding of NAP etiopathogenesis, which is essential as it affects the life quality of young adults worldwide.Copyright © 2022 Fuentes-Santamaría, Alvarado, Mellado, Melgar-Rojas, Gabaldón-Ull, Cabanes-Sanchis and Juiz.DOI: 10.3389/fnagi.2022.853320PMCID: PMC9016828",pubmed,35450058,10.3389/fnagi.2022.853320
loudness summation of equal loud narrowband signals in normalhearing and hearingimpaired listeners,"136. Int J Audiol. 2018 Jun;57(sup3):S71-S80. doi: 10.1080/14992027.2017.1380848. Epub 2017 Oct 3.Loudness summation of equal loud narrowband signals in normal-hearing and hearing-impaired listeners.Ewert SD(1), Oetting D(1)(2).Author information:(1)a Medizinische Physik and Cluster of Excellence Hearing4all , Universität Oldenburg , Oldenburg , Germany and.(2)b Project Group Hearing, Speech and Audio Technology of the Fraunhofer IDMT and Cluster of Excellence Hearing4all , Oldenburg , Germany.OBJECTIVE: Loudness perception of binaural broadband signals, e.g. speech shaped noise, shows large individual differences using frequency-dependent amplification which was adjusted to restore the loudness perception of monaural narrowband signals in hearing-impaired (HI) listeners. To better understand and quantify this highly individual effect, loudness perception of broadband stimuli consisting of a number of spectrally separated narrowband components which where individually adjusted to equal loudness is of interest.DESIGN: Based on categorical loudness scaling, the loudness of an equal categorical loudness noise (ECLN) consisting of six third-octave noises was assessed. For loudness categories ""medium"" und ""very loud"" the required narrowband loudness was analysed.STUDY SAMPLE: Nine normal-hearing (NH) and ten HI listeners.RESULTS: HI listeners showed lower narrowband loudness values compared to NH listeners, indicating an increased spectral loudness summation. More than 50% of the HI listeners showed higher binaural spectral loudness summation compared to NH listeners. The amount of binaural spectral loudness summation was highly correlated (r2 = 0.92) with the loudness level at ""very loud"" of aided speech shaped noise.CONCLUSIONS: The suggested ECLN measurement is suited to assess individual (binaural) broadband loudness in aided conditions, providing valuable information for hearing-aid fitting.DOI: 10.1080/14992027.2017.1380848",pubmed,28971746,10.1080/14992027.2017.1380848
association between speech recognition in noise and risk factors of cardiovascular disease,"34. Audiol Neurootol. 2021;26(5):368-377. doi: 10.1159/000513551. Epub 2021 Mar 2.Association between Speech Recognition in Noise and Risk Factors of Cardiovascular Disease.Goderie T(1), van Wier MF(2), Stam M(2), Lissenberg-Witte BI(3), Merkus P(2), Smits C(2), Kramer SE(2).Author information:(1)Department of Otolaryngology-Head and Neck surgery, Section Ear and Hearing, Amsterdam Public Health Research Institute, Amsterdam UMC, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands, t.goderie@amsterdamumc.nl.(2)Department of Otolaryngology-Head and Neck surgery, Section Ear and Hearing, Amsterdam Public Health Research Institute, Amsterdam UMC, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands.(3)Department of Epidemiology and Data Science, Amsterdam UMC, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands.INTRODUCTION: Risk factors for cardiovascular disease (CVD) are associated with sensorineural hearing loss. CVD risk factors are known to cluster and interact, thereby increasing the cumulative risk for CVD. Previously, using the database of the Netherlands Longitudinal Study on Hearing (NL-SH), an association was found between a history of smoking and an increased decline in speech recognition in noise over 10 years of follow-up. Prospectively limited data are available on the association between CVD risk factors, interactions of these risk factors, and hearing loss. In this study, data from the NL-SH were used to study the association between CVD risk factors and speech recognition in noise longitudinally.METHODS: Baseline, 5-year, and 10-year follow-up data of the NL-SH were included. The NL-SH is a web-based prospective cohort study which started in 2006. Participants were aged 18-70 years at baseline. Speech recognition in noise was determined with an online digit-triplet speech-in-noise test. In addition, participants completed online questionnaires on demographic, lifestyle, and health-related characteristics. The association of the ability to recognize speech in noise with CVD risk factors (i.e., obesity, rheumatoid arthritis [RA], hypertension, diabetes mellitus, and dyslipidemia) was analyzed longitudinally. We also analyzed the interaction between these risk factors (including age, sex, and history of smoking) and speech recognition in noise.RESULTS: None of the CVD risk factors or interactions of 2 CVD risk factors was significantly associated with a decline in SRT over time. Obesity (p = 0.016), RA (p = 0.027), and hypertension (p = 0.044) were associated with overall higher (more unfavorable) SRTs. No overall interactions between CVD risk factors were found.CONCLUSION: Obesity, RA, and hypertension were overall associated with a higher SRT, but no longitudinal associations between these or other CVD factors with SRTs were found. Also, no interactions between 2 CVD risk factors and SRTs were found. Although no longitudinal associations between CVD risk factors and decline in SRTs were found, clinicians should be alert about the concurrent association between CVD risk factors and hearing loss.© 2021 The Author(s) Published by S. Karger AG, Basel.DOI: 10.1159/000513551",pubmed,33652431,10.1159/000513551
family cluster of cholesteatoma,"Objective: We report an extremely rare case of family clustering of cholesteatoma. Method: Case reports and a review of the world literature concerning cholesteatoma and heredity are presented. Results: The family consists of parents and seven siblings of whom the mother and three sons have been surgically treated for cholesteatoma. All cholesteatomas in the family are acquired and all have a history of otitis media. Cholesteatomas occur with an incidence of 5/100 000 in Greenland, corresponding to two to three new cholesteatoma patients per year among the 57 000 inhabitants of Greenland. The family is very exceptional and interesting for further research concerning heredity in the pathogenesis of acquired cholesteatoma. Conclusion: To our knowledge this is the first report in the world literature of family clustering of acquired cholesteatoma. This case indicates that hereditary factors interplay with other factors in the pathogenesis of cholesteatoma. © 2007 JLO (1984) Limited.",scopus,2-s2.0-33846846521,10.1017/S0022215106004117
low frequency bisyllabic wordlists in a southindian language kannada development standardization and validation,"Objective: The present study aimed to develop, standardize and validate low frequency bi-syllabic wordlists in Kannada, a South-Indian language. Study design: The study was conducted in three different phases. The Kannada low frequency wordlists were developed in Phase I. The procedure involved collecting bi-syllabic familiar words, recording them, selecting the words dominant in low frequency energy by acoustical (Fast Fourier) transform and statistical means (k-means clustering) then generating equivalent wordlists using psychometric function. In Phase II, all the wordlists developed were standardized through estimation of speech identification scores in 100 individuals with normal hearing and through re-verification of equivalence of wordlists’ difficulty level by obtaining psychometric function. Finally, during Phase III, lists developed were evaluated for usefulness by administering them along with conventional phonemically-balanced Kannada wordlist on 10 individuals with cochlear hearing loss having rising audiometric configurations (i.e. more loss at lower frequencies). Results: Phase I resulted in development of seven psychometrically equivalent wordlists. Speech identification scores on 100 individuals with normal hearing showed mean scores greater than 95% for all the lists at 40 dB SL. No statistical difference was noted across wordlists. Further, individuals with rising cochlear hearing loss (RCHL) performed significantly poorer when compared to normal hearing counterparts across wordlists except low frequency wordlist 4 and phonemically balanced wordlist. Conclusions: The study utilized a unique procedure for the development of wordlists which can serve as guidelines for further research. The study has resulted in standardization (along with generation of normative data) and successful validation of the lists (except list 4) developed on a clinical population, i.e. individuals with RCHL. Given the lack of availability and the current clinical/research need of such test materials, the wordlists generated from this study can be useful. © 2017 International Association of Physicians in Audiology.",scopus,2-s2.0-85012040245,10.1080/21695717.2017.1283909
fetal alcohol syndrome,"Maternal alcohol consumption during pregnancy is known to produce a spectrum of morphological and neurocognitive outcomes in the offspring. The most severely affected on the spectrum exhibit a cluster of birth defects called fetal alcohol syndrome, which is characterized by a unique pattern of anomalies on the face, prenatal and/or postnatal growth deficiency, and evidence of central nervous system (CNS) dysfunction (Jones et al, Lancet 1:1267–1271, 1973). The characteristic pattern of malformations on the face includes a smooth philtrum, thin upper lip, and short palpebral fissures (see Fig. 1). Children with FASD are usually small in stature, with their height and weight falling below the 10th percentile. The deleterious effects of alcohol on the central nervous system are evidenced by microcephaly and cognitive and behavioral deficits. Children with prenatal alcohol exposure have also been observed to exhibit birth defects involving other systems such as cardiac (e.g., atrial and ventricular septal defects), skeletal (e.g., clinodactyly and camptodactyly), ocular (e.g., strabismus), and auditory (e.g., conductive hearing loss). However, the majority of children on the spectrum display only some or none of the above physical features but exhibit evidence of CNS dysfunction. The term, “alcohol-related neurodevelopmental disorder” (ARND), is used to label neurodevelopmental difficulties in those alcohol-exposed children without clinically discernable physical anomalies (Stratton et al (eds) Fetal alcohol syndrome: diagnosis, epidemiology, prevention, and treatment. National Academy Press, Washington, DC, 1996). Although not a diagnostic label, the term “fetal alcohol spectrum disorders” (FASDs) has been introduced to denote the full spectrum of morphological and neurocognitive outcomes resulting from prenatal alcohol exposure. While estimated prevalence rates of FAS range from.5 to 2 cases per 1,000 live births, the rate of FASD is estimated at 1 per 100 (Sampson et al, Teratology 56:317–326, 1997). © Springer Science+Business Media New York 2013, 2016 and Springer Nature Switzerland AG 2022.",scopus,2-s2.0-85160128660,10.1007/978-3-030-88832-9_90
a realtime interactive nonverbal communication system through semantic feature extraction as an interlingua,"There has been a growing interest in the use of networked virtual environment (NVE) technology to implement telepresence that allows participants to interact with each other in shared cyberspace. In addition, nonverbal language has attracted increased attention because of its association with more natural human communication, and especially sign languages play an important role for the hearing impaired. This paper proposes a novel real-time nonverbal communication system by introducing an artificial intelligence method into the NVE. We extract semantic information as an interlingua from the input text through natural language processing, and then transmit this semantic feature extraction (SFE) to the three-dimensional (3-D) articulated humanoid models prepared for each client in remote locations. Once the SFE is received, the virtual human is animated by the synthesized SFE. Experiments with Japanese and Chinese sign languages show this system makes the real-time animation of avatars available for the participants when chatting with each other. The communication is more natural since it is not just based on text or predefined gesture icons. This proposed system is suitable for sign language distance training as well.",ieee,1558-2426,10.1109/TSMCA.2003.818461
functional consequences of poor binaural hearing in development evidence from children with unilateral hearing loss and children receiving bilateral cochlear implants,"531. Trends Hear. 2021 Jan-Dec;25:23312165211051215. doi: 10.1177/23312165211051215.Functional Consequences of Poor Binaural Hearing in Development: Evidence From Children With Unilateral Hearing Loss and Children Receiving Bilateral Cochlear Implants.McSweeny C(1), Cushing SL(1)(2)(3), Campos JL(4)(5), Papsin BC(1)(2)(3), Gordon KA(1)(2).Author information:(1)Archie's Cochlear Implant Lab, 7979Hospital for Sick Children, Toronto, Ontario, Canada.(2)Department of Otolaryngology, Head & Neck Surgery, Faculty of Medicine, University of Toronto, Ontario, Canada.(3)Department of Otolaryngology, Head & Neck Surgery, 7979Hospital for Sick Children, Toronto, Ontario, Canada.(4)KITE-Toronto Rehabilitation Institute, Toronto, Ontario, Canada.(5)Department of Psychology, University of Toronto, Toronto, Ontario, Canada.Poor binaural hearing in children was hypothesized to contribute to related cognitive and academic deficits. Children with unilateral hearing have normal hearing in one ear but no access to binaural cues. Their cognitive and academic deficits could be unique from children receiving bilateral cochlear implants (CIs) at young ages who have poor access to spectral cues and impaired binaural sensitivity. Both groups are at risk for vestibular/balance deficits which could further contribute to memory and learning challenges. Eighty-eight children (43 male:45 female, aged 9.89  ±  3.40 years), grouped by unilateral hearing loss (n = 20), bilateral CI (n = 32), and typically developing (n = 36), completed a battery of sensory, cognitive, and academic tests. Analyses revealed that children in both hearing loss groups had significantly poorer skills (accounting for age) on most tests than their normal hearing peers. Children with unilateral hearing loss had more asymmetric speech perception than children with bilateral CIs (p < .0001) but balance and language deficits (p = .0004, p < .0001, respectively) were similar in the two hearing loss groups (p > .05). Visuospatial memory deficits occurred in both hearing loss groups (p = .02) but more consistently across tests in children with unilateral hearing loss. Verbal memory was not significantly different than normal (p > .05). Principal component analyses revealed deficits in a main cluster of visuospatial memory, oral language, mathematics, and reading measures (explaining 46.8% data variability). The remaining components revealed clusters of self-reported hearing, balance and vestibular function, and speech perception deficits. The findings indicate significant developmental impacts of poor binaural hearing in children.DOI: 10.1177/23312165211051215PMCID: PMC8527588",pubmed,34661482,10.1177/23312165211051215
premium versus entrylevel hearing aids using group concept mapping to investigate the drivers of preference,"221. Int J Audiol. 2022 Dec;61(12):1003-1017. doi: 10.1080/14992027.2021.2009923. Epub 2021 Dec 9.Premium versus entry-level hearing aids: using group concept mapping to investigate the drivers of preference.Saleh HK(1)(2), Folkeard P(2), Van Eeckhoutte M(2)(3)(4), Scollie S(2)(5).Author information:(1)Health & Rehabilitation Sciences, Western University, London, Ontario, Canada.(2)National Centre for Audiology, Western University, London, Ontario, Canada.(3)Hearing Systems, Department of Health Technology, Technical University of Denmark, Kongens, Lyngby.(4)Ear, Nose, Throat (ENT) & Audiology Clinic, Rigshospitalet, Copenhagen University Hospital, Denmark.(5)Communication Sciences and Disorders, Faculty of Health Sciences, Western University, London, Ontario, Canada.OBJECTIVES: To investigate the difference in outcome measures and drivers of user preference between premium and entry-level hearing aids using group concept mapping.DESIGN: A single-blind crossover trial was conducted. Aided behavioural outcomes measured were loudness rating, speech/consonant recognition, and speech quality. Preference between hearing aids was measured with a 7-point Likert scale. Group concept mapping was utilised to investigate preference results. Participants generated statements based on what influenced their preferences. These were sorted into categories with underlying themes. Participants rated each statement on a 5-point Likert scale of importance.STUDY SAMPLE: Twenty-three adult participants (mean: 62.4 years; range: 24-78) with mild to moderately severe bilateral SNHL (PTA500-4000 Hz > 20 dB HL).RESULTS: A total of 83 unique statements and nine distinct clusters, with underlying themes driving preference, were generated. Clusters that differed significantly in importance between entry-level and premium hearing aid choosers were: Having access to smartphone application-based user-controlled settings, the ability to stream calls and music, and convenience features such as accessory compatibility.CONCLUSION: This study has identified non-signal-processing factors which significantly influenced preference for a premium hearing aid over an entry-level hearing aid, indicating the importance of these features as drivers of user preference.DOI: 10.1080/14992027.2021.2009923",pubmed,34883040,10.1080/14992027.2021.2009923
a convolutional neural networkbased framework for analysis and assessment of nonlinguistic sound classification and enhancement for normal hearing and cochlear implant listeners,"217. J Acoust Soc Am. 2022 Nov;152(5):2720. doi: 10.1121/10.0014955.A convolutional neural network-based framework for analysis and assessment of non-linguistic sound classification and enhancement for normal hearing and cochlear implant listeners.Shekar RCMC(1), Hansen JHL(1).Author information:(1)Cochlear Implant Processing Laboratory-Center for Robust Speech Systems (CRSS-CILab), University of Texas at Dallas, Richardson, Texas 75080, USA.Naturalistic sounds encode salient acoustic content that provides situational context or subject/system properties essential for acoustic awareness, autonomy, safety, and improved quality of life for individuals with sensorineural hearing loss. Cochlear implants (CIs) are an assistive hearing device that restores auditory function in hearing impaired individuals. Most CI research advancements have focused on improving speech recognition in noisy, reverberant, or time-varying diverse environments. Relatively few studies have explored non-linguistic sound (NLS) perception among CIs, and those that have carried out such studies generally reported poor perception, suggesting a clear deficit in current CI sound processing systems. In this study, a convolutional neural network (CNN)-based NLS classification model is used as a framework to compare unprocessed and CI-simulated NLS classification and evaluate NLS perception targeted algorithms among CI listeners. Additionally, a NLS enhancement algorithm that focuses on improving identifiability and perception among CI listeners is proposed. The proposed NLS enhancement algorithm is evaluated based on identifiability performance using the CI-simulated NLS classification model. The proposed NLS classification framework was able to achieve near human-level performance with no significant effect of classification modality (model vs human subject) and achieved mean classification scores of 85.86% for NH (p = 0.3758) and 65.25% for CI (p = 0.1725). Among the four different feature-based methods of the proposed NLS enhancement algorithm, the ""harmonicity""-based one achieved highest mean classification accuracy of 63.75%, when compared to baseline, and demonstrated significant improvement in performance (p = 0.0403). The resulting proposed comparative NLS classification framework contributes toward (i) advancement of NLS recognition studies, (ii) mitigation of CI user recruitment constraints and listener evaluation with NH listeners, (iii) development of a community shared testbed for comparative NLS studies, and (iv) advancement of NLS enhancement studies (identifiability and perceptual factors) among CI listeners.DOI: 10.1121/10.0014955PMCID: PMC9637023",pubmed,36456299,10.1121/10.0014955
disrupted local neural activity and functional connectivity in subjective tinnitus patients evidence from restingstate fmri study,"Purpose: The study aimed to investigate the abnormal alterations of both the intra-regional brain activity and inter-regional functional connectivity (FC) in patients with subjective tinnitus (ST) using resting-state functional MRI (rs-fMRI) methods.Methods: Twenty-five ST patients and 25 normal controls (NCs) were included and underwent resting-state functional magnetic resonance imaging scans. ReHo, fALFF, and seed-based FC were calculated and compared between ST patients and NCs. Meanwhile, correlation analyses were calculated between altered connectivity and clinical data in ST patients.Results: Compared with NCs, ST patients exhibited increased ReHo and fALFF values in the right middle temporal gyrus (MTG), and the ReHo values were also increased in the right cuneus. In contrast, decreased ReHo values in ST patients were observed in the right middle frontal gyrus (MFG) and left cerebellar anterior lobe. Considering these brain areas with altered ReHo and fALFF clusters as seeds, the right MTG (ReHo) exhibited decreased connectivity with the right MFG, lingual gyrus, and left cerebellar posterior lobe, besides, the right cuneus showed decreased connectivity with the right MTG. In ST patients, the decreased FC between the right MTG (ReHo) and the right MFG was also positively correlated with the Tinnitus handicap inventory score (r = 0.675, P = 0.001).Conclusion: The present study revealed that ST patients had altered regional neural activity and inter-regional connectivity in partial auditory and non-auditory brain regions, mainly involving the default mode network and audio-visual network, which could further improve our understanding of the neuroimaging mechanism in ST.",cinahl,283940,10.1007/s00234-018-2087-0
impedance changes and fibrous tissue growth after cochlear implantation are correlated and can be reduced using a dexamethasone eluting electrode,"475. PLoS One. 2016 Feb 3;11(2):e0147552. doi: 10.1371/journal.pone.0147552. eCollection 2016.Impedance Changes and Fibrous Tissue Growth after Cochlear Implantation Are Correlated and Can Be Reduced Using a Dexamethasone Eluting Electrode.Wilk M(1)(2), Hessler R(3), Mugridge K(3), Jolly C(3), Fehr M(2), Lenarz T(1)(4), Scheper V(1)(4).Author information:(1)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.(2)Clinic for Exotic Pets, Reptiles, Pet and Feral Birds, University of Veterinary Medicine, Foundation, Hannover, Germany.(3)MED-EL GmbH, Innsbruck, Austria.(4)Cluster of Excellence ""Hearing4all"", Hannover Medical School, Hannover, Germany.BACKGROUND: The efficiency of cochlear implants (CIs) is affected by postoperative connective tissue growth around the electrode array. This tissue formation is thought to be the cause behind post-operative increases in impedance. Dexamethasone (DEX) eluting CIs may reduce fibrous tissue growth around the electrode array subsequently moderating elevations in impedance of the electrode contacts.METHODS: For this study, DEX was incorporated into the silicone of the CI electrode arrays at 1% and 10% (w/w) concentration. Electrodes prepared by the same process but without dexamethasone served as controls. All electrodes were implanted into guinea pig cochleae though the round window membrane approach. Potential additive or synergistic effects of electrical stimulation (60 minutes) were investigated by measuring impedances before and after stimulation (days 0, 7, 28, 56 and 91). Acoustically evoked auditory brainstem responses were recorded before and after CI insertion as well as on experimental days 7, 28, 56, and 91. Additionally, histology performed on epoxy embedded samples enabled measurement of the area of scala tympani occupied with fibrous tissue.RESULTS: In all experimental groups, the highest levels of fibrous tissue were detected in the basal region of the cochlea in vicinity to the round window niche. Both DEX concentrations, 10% and 1% (w/w), significantly reduced fibrosis around the electrode array of the CI. Following 3 months of implantation impedance levels in both DEX-eluting groups were significantly lower compared to the control group, the 10% group producing a greater effect. The same effects were observed before and after electrical stimulation.CONCLUSION: To our knowledge, this is the first study to demonstrate a correlation between the extent of new tissue growth around the electrode and impedance changes after cochlear implantation. We conclude that DEX-eluting CIs are a means to reduce this tissue reaction and improve the functional benefits of the implant by attenuating electrode impedance.DOI: 10.1371/journal.pone.0147552PMCID: PMC4739581",pubmed,26840740,10.1371/journal.pone.0147552
a pathogenic deletion in forkhead box l1 foxl1 identifies the first otosclerosis otsc gene,"Otosclerosis is a bone disorder of the otic capsule and common form of late-onset hearing impairment. Considered a complex disease, little is known about its pathogenesis. Over the past 20 years, ten autosomal dominant loci (OTSC1-10) have been mapped but no genes identified. Herein, we map a new OTSC locus to a 9.96 Mb region within the FOX gene cluster on 16q24.1 and identify a 15 bp coding deletion in Forkhead Box L1 co-segregating with otosclerosis in a Caucasian family. Pre-operative phenotype ranges from moderate to severe hearing loss to profound sensorineural loss requiring a cochlear implant. Mutant FOXL1 is both transcribed and translated and correctly locates to the cell nucleus. However, the deletion of 5 residues in the C-terminus of mutant FOXL1 causes a complete loss of transcriptional activity due to loss of secondary (alpha helix) structure. FOXL1 (rs764026385) was identified in a second unrelated case on a shared background. We conclude that FOXL1 (rs764026385) is pathogenic and causes autosomal dominant otosclerosis and propose a key inhibitory role for wildtype Foxl1 in bone remodelling in the otic capsule. New insights into the molecular pathology of otosclerosis from this study provide molecular targets for non-invasive therapeutic interventions. © 2021, The Author(s).",scopus,2-s2.0-85116915029,10.1007/s00439-021-02381-1
reorganized brain functional network topology in presbycusis,"Purpose: Presbycusis is characterized by bilateral sensorineural hearing loss at high frequencies and is often accompanied by cognitive decline. This study aimed to identify the topological reorganization of brain functional network in presbycusis with/without cognitive decline by using graph theory analysis approaches based on resting-state functional magnetic resonance imaging (rs-fMRI). Methods: Resting-state fMRI scans were obtained from 30 presbycusis patients with cognitive decline, 30 presbycusis patients without cognitive decline, and 50 age-, sex-, and education-matched healthy controls. Graph theory was applied to analyze the topological properties of brain functional networks including global and nodal metrics, modularity, and rich-club organization. Results: At the global level, the brain functional networks of all participants were found to possess small-world properties. Also, significant group differences in global network metrics were observed among the three groups such as clustering coefficient, characteristic path length, normalized characteristic path length, and small-worldness. At the nodal level, several nodes with abnormal betweenness centrality, degree centrality, nodal efficiency, and nodal local efficiency were detected in presbycusis patients with/without cognitive decline. Changes in intra-modular connections in frontal lobe module and inter-modular connections in prefrontal subcortical lobe module were found in presbycusis patients exposed to modularity analysis. Rich-club nodes were reorganized in presbycusis patients, while the connections among them had no significant group differences. Conclusion: Presbycusis patients exhibited topological reorganization of the whole-brain functional network, and presbycusis patients with cognitive decline showed more obvious changes in these topological properties than those without cognitive decline. Abnormal changes of these properties in presbycusis patients may compensate for cognitive impairment by mobilizing additional neural resources. Copyright © 2022 Guan, Xu, Chen, Xing, Xu, Shang, Xu, Wu and Yan.",scopus,2-s2.0-85132200152,10.3389/fnagi.2022.905487
the effect of shortterm ventilation tubes versus watchful waiting on hearing in young children with persistent otitis media with effusion a randomized trial,"Objective: To study the effect of short-term ventilation tubes in children aged 1 to 2 yr with screening-detected, bilateral otitis media with effusion (OME) persisting for 4 to 6 mo, as compared with watchful waiting.Design: Multi-center randomized controlled trial (N = 187) with two treatment arms: short-term ventilation tubes versus watchful waiting. Young children underwent auditory screening; those with persistent (4 to 6 mo) bilateral OME were recruited.Results: The mean duration of effusion over 1-yr follow-up was 142 days (36%) in the ventilation tube (VT) group versus 277 days (70%) in the watchful waiting (WW) group. After 6 mo of follow-up, the pure-tone average in the VT group was 5.6 dB A better than that in the WW group. After 12 mo, most of the advantage in the VT group had disappeared. After the insertion of ventilation tubes, the children with poorer hearing levels at randomization improved more than the children with better hearing levels. The largest difference in hearing levels was found between the children in the VT group whose ventilation tubes remained in situ and the children in the WW group. In the VT children with recurrence of OME, the hearing levels again increased, but remained slightly lower than those in the infants with persistent OME in the WW group.Conclusions: Ventilation tubes have a beneficial effect on hearing in the short run (6 mo); this effect, however, largely disappears in the long run (12 mo). This is probably due to partial recurrent OME in the VT group and to partial spontaneous recovery in the WW group.",cinahl,1960202,10.1097/00003446-200106000-00003
a robotic voice simulator and the interactive training for hearingimpaired people,"651. J Biomed Biotechnol. 2008;2008:768232. doi: 10.1155/2008/768232.A robotic voice simulator and the interactive training for hearing-impaired people.Sawada H(1), Kitani M, Hayashi Y.Author information:(1)Department of Intelligent Mechanical Systems Engineering, Faculty of Engineering, Kagawa University, Japan. sawada@eng.kagawa-u.ac.jpA talking and singing robot which adaptively learns the vocalization skill by means of an auditory feedback learning algorithm is being developed. The robot consists of motor-controlled vocal organs such as vocal cords, a vocal tract and a nasal cavity to generate a natural voice imitating a human vocalization. In this study, the robot is applied to the training system of speech articulation for the hearing-impaired, because the robot is able to reproduce their vocalization and to teach them how it is to be improved to generate clear speech. The paper briefly introduces the mechanical construction of the robot and how it autonomously acquires the vocalization skill in the auditory feedback learning by listening to human speech. Then the training system is described, together with the evaluation of the speech training by auditory impaired people.DOI: 10.1155/2008/768232PMCID: PMC2279150",pubmed,18389073,10.1155/2008/768232
kilquist syndrome a novel syndromic hearing loss disorder caused by homozygous deletion of slc12a2,"Syndromic sensorineural hearing loss is multigenic and associated with malformations of the ear and other organ systems. Herein we describe a child admitted to the NIH Undiagnosed Diseases Program with global developmental delay, sensorineural hearing loss, gastrointestinal abnormalities, and absent salivation. Next-generation sequencing revealed a uniparental isodisomy in chromosome 5, and a 22 kb homozygous deletion in SLC12A2, which encodes for sodium, potassium, and chloride transporter in the basolateral membrane of secretory epithelia. Functional studies using patient-derived fibroblasts showed truncated SLC12A2 transcripts and markedly reduced protein abundance when compared with control. Loss of Slc12a2 in mice has been shown to lead to deafness, abnormal neuronal growth and migration, severe gastrointestinal abnormalities, and absent salivation. Together with the described phenotype of the Slc12a2-knockout mouse model, our results suggest that the absence of functional SLC12A2 causes a new genetic syndrome and is crucial for the development of auditory, neurologic, and gastrointestinal tissues. © 2019 Wiley Periodicals, Inc.",scopus,2-s2.0-85062797843,10.1002/humu.23722
speech understanding and listening effort in cochlear implant users  microphone beamformers lead to significant improvements in noisy environments,"169. Cochlear Implants Int. 2020 Jan;21(1):1-8. doi: 10.1080/14670100.2019.1661567. Epub 2019 Oct 7.Speech understanding and listening effort in cochlear implant users - microphone beamformers lead to significant improvements in noisy environments.Büchner A(1), Schwebs M(1), Lenarz T(1).Author information:(1)Department of Otolaryngology and Cluster of Excellence 'Hearing4all', Medical University of Hannover, Hannover, Germany.Objectives: To evaluate the effect of microphone directionality, i.e. beamforming, on speech understanding in noise with the SONNET audio processor.Methods: Speech reception thresholds (SRTs) were tested in three different microphone settings (omnidirectional, adaptive, and fixed beamformer (natural)) and assessed via the Oldenburg Sentence Test and the Just Understanding Speech Test. Subjects rated the listening effort needed to understand speech in different signal-to-noise ratios (-10, -5, 0, 5, 10, 15 dB SNR) via a Visual Analogue Scale. For all test methods, speech was presented at 0° azimuth while fixed and uncorrelated masking noise was presented simultaneously from five loudspeakers positioned at ±70°, ±135°, and 180° azimuth.Results: Compared to the omnidirectional mode, significant improvements (p<0.001) were shown in mean SRTs for both the natural (3.3 dB SNR) and adaptive (5.2 dB SNR) settings. Using the natural or the adaptive setting required significantly less listening effort than using the omnidirectional setting for the SNR conditions -5 dB SNR (p=0.002) and 0 dB SNR (p<0.001).Discussion: The beamformer settings significantly improved speech understanding in noise over the omnidirectional setting. Due our multi-speaker test setup, we conclude that beamforming should yield significantly better and less stressful speech understanding in demanding real-life listening situations.DOI: 10.1080/14670100.2019.1661567",pubmed,31590612,10.1080/14670100.2019.1661567
a brief survey on dataset and method used for sign language detection,"Although speech is still a most common form of communication some people have difficulty in listening and speaking. For people with such disabilities, communication poses a big barrier. To solve this issue researcher have been trying to build state of art machine learning and deep learning model so that those people with disabilities can communicate easily, in such a context most of the researchers worked on Indian Sign Language which is one of the most difficult tasks to study because, in contrast to American Sign Language, it is still in its fancy. To perform Indian Sign Language recognition many researchers have built various Artificial Intelligence based models. This paper explains some of the Artificial Intelligence based techniques to solve the problem of communication for those people who have difficulty in listening or speaking.",ieee,2687-7767,10.1109/UPCON59197.2023.10434904
enext an iot and ai driven solution to the pluggedear pandemic,"According to World Health Organization (WHO), more than a billion people are at risk due to the unsafe use of earphones. This article proposes the concept of a next-generation earphone called “eNext.” Unlike existing traditional earphones eNext comprises artificial intelligence (AI) and IoT-driven technologies in order to overcome the hearing health hazards, ensure pedestrian safety, and provide health monitoring facilities. Additionally, this article formulates an IoT-based technical layout of the proposed system model and provides possible AI-driven solutions to implement the proposed services.",ieee,2327-4662,10.1109/JIOT.2023.3244800
interactions between amplitude modulation and frequency modulation processing effects of age and hearing loss,"257. J Acoust Soc Am. 2016 Jul;140(1):121. doi: 10.1121/1.4955078.Interactions between amplitude modulation and frequency modulation processing: Effects of age and hearing loss.Paraouty N(1), Ewert SD(2), Wallaert N(1), Lorenzi C(1).Author information:(1)Laboratoire des Systèmes Perceptifs (CNRS UMR 8248), Institut d'Etude de la Cognition, Ecole normale supérieure, Paris Sciences et Lettres Research University, 29 rue d'Ulm, 75005 Paris, France.(2)Medizinische Physik and Cluster of Excellence Hearing4All, Universität Oldenburg, 26111 Oldenburg, Germany.Frequency modulation (FM) and amplitude modulation (AM) detection thresholds were measured for a 500-Hz carrier frequency and a 5-Hz modulation rate. For AM detection, FM at the same rate as the AM was superimposed with varying FM depth. For FM detection, AM at the same rate was superimposed with varying AM depth. The target stimuli always contained both amplitude and frequency modulations, while the standard stimuli only contained the interfering modulation. Young and older normal-hearing listeners, as well as older listeners with mild-to-moderate sensorineural hearing loss were tested. For all groups, AM and FM detection thresholds were degraded in the presence of the interfering modulation. AM detection with and without interfering FM was hardly affected by either age or hearing loss. While aging had an overall detrimental effect on FM detection with and without interfering AM, there was a trend that hearing loss further impaired FM detection in the presence of AM. Several models using optimal combination of temporal-envelope cues at the outputs of off-frequency filters were tested. The interfering effects could only be predicted for hearing-impaired listeners. This indirectly supports the idea that, in addition to envelope cues resulting from FM-to-AM conversion, normal-hearing listeners use temporal fine-structure cues for FM detection.DOI: 10.1121/1.4955078",pubmed,27475138,10.1121/1.4955078
on the cognitive neurodynamics of listening effort a phase clustering analysis of largescale neural correlates,"523. Annu Int Conf IEEE Eng Med Biol Soc. 2009;2009:2078-81. doi: 10.1109/IEMBS.2009.5333956.On the cognitive neurodynamics of listening effort: a phase clustering analysis of large-scale neural correlates.Strauss DJ(1), Corona-Strauss FI, Bernarding C, Reith W, Latzel M, Froehlich M.Author information:(1)Computational Diagnostics & Biocybernetics Unit, Saarland University Hospital and Saarland University of Applied Sciences, Homburg/Saar, Germany. strauss@cdb-unit.deAn increased listening effort represents a major problem in humans with hearing impairment. Neurodiagnostic methods for an objective listening effort estimation could revolutionize auditory rehabilitation. However the cognitive neurodynamics of listening effort is not understood and research related its neural correlates is still in its infancy. In this paper we present a phase clustering analysis of large-scale listening effort correlates in auditory late responses (ALRs). For this we apply the complex wavelet transform as well as tight Gabor Frame (TGF) operators. We show (a) that phase clustering on the unit circle can separate ALR data from auditory paradigms which require a graduated effort for their solution; (b) the application of TGFs for an inverse artificial phase stabilization at the alpha/theta-border enlarges the endogenously driven listening effort correlates in the reconstructed time- domain waveforms. It is concluded that listening effort correlates can be extracted from ALR sequences using an instantaneous phase clustering analysis, at least by means of the applied experimental pure tone paradigm.DOI: 10.1109/IEMBS.2009.5333956",pubmed,19964575,10.1109/IEMBS.2009.5333956
development of an expert system for pediatric auditory brainstem response interpretation,"467. J Am Acad Audiol. 1993 May;4(3):163-71.Development of an expert system for pediatric auditory brainstem response interpretation.Tharpe AM(1), Biswas G, Hall JW 3rd.Author information:(1)Division of Hearing and Speech Sciences, Vanderbilt University School of Medicine, Nashville, Tennessee.Expert systems are computer programs which incorporate artificial intelligence technology and are created to emulate the decision-making abilities of human experts. The advantage of such systems lies in their ability to capture and model expert problem solving knowledge in a domain and make it available to an unlimited number of consumers in an economic and efficient way. The purpose of this project was to develop an expert system to interpret infant auditory brainstem response data as entered by the user. The resulting system provides diagnostic conclusions regarding hearing status, type of hearing loss, and brainstem function at an accuracy level equal to that of a human expert.",pubmed,8318707,
spectral and binaural loudness summation for hearingimpaired listeners,"197. Hear Res. 2016 May;335:179-192. doi: 10.1016/j.heares.2016.03.010. Epub 2016 Mar 19.Spectral and binaural loudness summation for hearing-impaired listeners.Oetting D(1), Hohmann V(2), Appell JE(3), Kollmeier B(4), Ewert SD(2).Author information:(1)Project Group Hearing, Speech and Audio Technology of the Fraunhofer IDMT and Cluster of Excellence Hearing4all, Oldenburg, Germany; Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, 26111 Oldenburg, Germany. Electronic address: dirk.oetting@idmt.fraunhofer.de.(2)Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, 26111 Oldenburg, Germany.(3)Project Group Hearing, Speech and Audio Technology of the Fraunhofer IDMT and Cluster of Excellence Hearing4all, Oldenburg, Germany.(4)Project Group Hearing, Speech and Audio Technology of the Fraunhofer IDMT and Cluster of Excellence Hearing4all, Oldenburg, Germany; Medizinische Physik and Cluster of Excellence Hearing4all, Universität Oldenburg, 26111 Oldenburg, Germany.Sensorineural hearing loss typically results in a steepened loudness function and a reduced dynamic range from elevated thresholds to uncomfortably loud levels for narrowband and broadband signals. Restoring narrowband loudness perception for hearing-impaired (HI) listeners can lead to overly loud perception of broadband signals and it is unclear how binaural presentation affects loudness perception in this case. Here, loudness perception quantified by categorical loudness scaling for nine normal-hearing (NH) and ten HI listeners was compared for signals with different bandwidth and different spectral shape in monaural and in binaural conditions. For the HI listeners, frequency- and level-dependent amplification was used to match the narrowband monaural loudness functions of the NH listeners. The average loudness functions for NH and HI listeners showed good agreement for monaural broadband signals. However, HI listeners showed substantially greater loudness for binaural broadband signals than NH listeners: on average a 14.1 dB lower level was required to reach ""very loud"" (range 30.8 to -3.7 dB). Overall, with narrowband loudness compensation, a given binaural loudness for broadband signals above ""medium loud"" was reached at systematically lower levels for HI than for NH listeners. Such increased binaural loudness summation was not found for loudness categories below ""medium loud"" or for narrowband signals. Large individual variations in the increased loudness summation were observed and could not be explained by the audiogram or the narrowband loudness functions.Copyright © 2016 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2016.03.010",pubmed,27006003,10.1016/j.heares.2016.03.010
generation of a human deafness sheep model using the crisprcas system,"CRISPR/Cas9 system is a promising method for the generation of human disease models by genome editing in non-conventional experimental animals. Medium/large-sized animals like sheep have several advantages to study human diseases and medicine. Here, we present a protocol that describes the generation of an otoferlin edited sheep model via CRISPR-assisted single-stranded oligodinucleotide-mediated Homology-Directed Repair (HDR), through direct cytoplasmic microinjection in in vitro produced zygotes. Otoferlin is a protein expressed in the cochlear inner hair cells, with different mutations at the OTOF gene being the major cause of nonsyndromic recessive auditory neuropathy spectrum disorder in humans. By using this protocol, we reported for the first time an OTOF KI model in sheep with 17.8% edited lambs showing indel mutations, and 61.5% of them bearing knock-in mutations by HDR. The reported method establishes the bases to produce a deafness model to test novel therapies in human disorders related to OTOF mutations. © 2022, The Author(s), under exclusive license to Springer Science+Business Media, LLC, part of Springer Nature.",scopus,2-s2.0-85132827224,10.1007/978-1-0716-2301-5_12
heterozygous mutation of ush1gsans in mice causes earlyonset progressive hearing loss which is recovered by reconstituting the strainspecific mutation in cdh23,"Most clinical reports have suggested that patients with congenital profound hearing loss have recessive mutations in deafness genes, whereas dominant alleles are associated with progressive hearing loss (PHL). Jackson shaker (Ush1gjs) is a mouse model of recessive deafness that exhibits congenital profound deafness caused by the homozygous mutation of Ush1g/Sans on chromosome 11. We found that C57BL/6J-Ush1gjs/+ heterozygous mice exhibited early-onset PHL (ePHL) accompanied by progressive degeneration of stereocilia in the cochlear outer hair cells. Interestingly, ePHL did not develop in mutant mice with the C3H/HeN background, thus suggesting that other genetic factors are required for ePHL development. Therefore, we performed classical genetic analyses and found that the occurrence of ePHL in Ush1gjs/+ mice was associated with an interval in chromosome 10 that contains the cadherin 23 gene (Cdh23), which is also responsible for human deafness. To confirm this mutation effect, we generated C57BL/6J-Ush1gjs/+, Cdh23c.753A/G double-heterozygous mice by using the CRISPR/Cas9-mediated Cdh23c.753A>G knock-in method. The Cdh23c.753A/G mice harbored a one-base substitution (A for G), and the homozygous A allele caused moderate hearing loss with aging. Analyses revealed the complete recovery of ePHL and stereocilia degeneration in C57BL/6J-Ush1gjs/+ mice. These results clearly show that the development of ePHL requires at least two mutant alleles of the Ush1g and Cdh23 genes. Our results also suggest that because the SANS and CDH23 proteins form a complex in the stereocilia, the interaction between these proteins may play key roles in the maintenance of stereocilia and the prevention of ePHL. © The Author 2016. Published by Oxford University Press. All rights reserved.",scopus,2-s2.0-84992208934,10.1093/hmg/ddw078
the impact of tinnitus distress on cognition,"533. Sci Rep. 2021 Jan 26;11(1):2243. doi: 10.1038/s41598-021-81728-0.The impact of tinnitus distress on cognition.Neff P(1)(2), Simões J(1), Psatha S(3), Nyamaa A(3), Boecking B(3), Rausch L(3), Dettling-Papargyris J(4), Funk C(4), Brueggemann P(3), Mazurek B(5).Author information:(1)Department of Psychiatry and Psychotherapy, University of Regensburg, Regensburg, Germany.(2)University Research Priority Program 'Dynamics of Healthy Aging', University of Zürich, Zürich, Switzerland.(3)Tinnitus-Zentrum, Charité - Universitätsmedizin, Berlin, Germany.(4)Terzo Institute, ISMA AG, Sonneberg, Germany.(5)Tinnitus-Zentrum, Charité - Universitätsmedizin, Berlin, Germany. birgit.mazurek@charite.de.Tinnitus is the chronic perception of a phantom sound with different levels of related distress. Past research has elucidated interactions of tinnitus distress with audiological, affective and further clinical variables. The influence of tinnitus distress on cognition is underinvestigated. Our study aims at investigating specific influences of tinnitus distress and further associated predictors on cognition in a cohort of n = 146 out-ward clinical tinnitus patients. Age, educational level, hearing loss, Tinnitus Questionnaire (TQ) score, tinnitus duration, speech in noise (SIN), stress, anxiety and depression, and psychological well-being were included as predictors of a machine learning regression approach (elastic net) in three models with scores of a multiple choice vocabulary test (MWT-B), or two trail-making tests (TMT-A and TMT-B), as dependent variables. TQ scores predicted lower MWT-B scores and higher TMT-B test completion time. Stress, emotional, and psychological variables were not found to be relevant predictors in all models with the exception of small positive influences of SIN and depression on TMT-B. Effect sizes were small to medium for all models and predictors. Results are indicative of specific influence of tinnitus distress on cognitive performance, especially on general or crystallized intelligence and executive functions. More research is needed at the delicate intersection of tinnitus distress and cognitive skills needed in daily functioning.DOI: 10.1038/s41598-021-81728-0PMCID: PMC7838303",pubmed,33500489,10.1038/s41598-021-81728-0
genetic basis for susceptibility to noiseinduced hearing loss in mice,"273. Hear Res. 2001 May;155(1-2):82-90. doi: 10.1016/s0378-5955(01)00250-7.Genetic basis for susceptibility to noise-induced hearing loss in mice.Davis RR(1), Newlander JK, Ling X, Cortopassi GA, Krieg EF, Erway LC.Author information:(1)Hearing Loss Prevention Section, Division of Applied Research and Technology, National Institute for Occupational Safety and Health Centers for Disease Control and Prevention, Mailstop C-27, 4676 Columbia Parkway, Cincinnati OH 45226, USA. rrdl@cdc.govThe C57BL/6J (B6) and DBA/2J (D2) inbred strains of mice exhibit an age-related hearing loss (AHL) due to a recessive gene (Ahl) that maps to Chromosome 10. The Ahl gene is also implicated in the susceptibility to noise-induced hearing loss (NIHL). The B6 mice (Ahl/Ahl) are more susceptible to NIHL than the CBA/CaJ (CB) mice (+(Ahl)). The B6xD2.F(1) hybrid mice (Ahl/Ahl) are more susceptible to NIHL than the CBxB6.F(1) mice (+/Ahl) [Erway et al., 1996. Hear. Res. 93, 181-187]. These genetic effects implicate the Ahl gene as contributing to NIHL susceptibility. The present study demonstrates segregation for the putative Ahl gene and mapping of such a gene to Chromosome 10, consistent with other independent mapping of Ahl for AHL in 10 strains of mice [Johnson et al., 2000. Genomics 70, 171-180]. The present study was based on a conventional cross between two inbred strains, CBxB6.F(1) backcrossed to B6 with segregation for the putative +/Ahl:Ahl/Ahl. These backcross progeny were exposed to 110 dB SPL noise for 8 h. All of the progeny were tested for auditory evoked brainstem responses and analyzed for any significant permanent threshold shift of NIHL. Cluster analyses were used to distinguish the two putative genotypes, the least affected with NIHL (+/Ahl) and most affected with PTS (Ahl/Ahl). Approximately 1/2 of the backcross progeny exhibited PTS, particularly at 16 kHz. These mice were genotyped for two D10Mit markers. Quantitative trait loci analyses (log of the odds=15) indicated association of the genetic factor within a few centiMorgan of the best evidence for Ahl [Johnson et al., 2000. Genomics 70, 171-180]. All of the available evidence supports a role for the Ahl gene in both AHL and NIHL among these strains of mice.DOI: 10.1016/s0378-5955(01)00250-7",pubmed,11335078,10.1016/s0378-5955(01)00250-7
amikacin concentrations predictive of ototoxicity in multidrugresistant tuberculosis patients,"393. Antimicrob Agents Chemother. 2015 Oct;59(10):6337-43. doi: 10.1128/AAC.01050-15. Epub 2015 Jul 27.Amikacin Concentrations Predictive of Ototoxicity in Multidrug-Resistant Tuberculosis Patients.Modongo C(1), Pasipanodya JG(2), Zetola NM(3), Williams SM(4), Sirugo G(5), Gumbo T(6).Author information:(1)Division of Infectious Diseases, University of Pennsylvania, Philadelphia, Pennsylvania, USA Botswana-University of Pennsylvania Partnership, Gaborone, Botswana.(2)Office of Global Health, University of Texas Southwestern Medical Center, Dallas, Texas, USA.(3)Division of Infectious Diseases, University of Pennsylvania, Philadelphia, Pennsylvania, USA Botswana-University of Pennsylvania Partnership, Gaborone, Botswana Department of Medicine, University of Botswana, Gaborone, Botswana.(4)Department of Genetics, Geisel School of Medicine, Dartmouth College, Hanover, New Hampshire, USA.(5)Centro di Ricerca, Ospedale San Pietro Fatebenefratelli, Rome, Italy.(6)Office of Global Health, University of Texas Southwestern Medical Center, Dallas, Texas, USA Department of Medicine, University of Cape Town, Observatory, South Africa Tawanda.Gumbo@BaylorHealth.edu.Aminoglycosides, such as amikacin, are used to treat multidrug-resistant tuberculosis. However, ototoxicity is a common problem and is monitored using peak and trough amikacin concentrations based on World Health Organization recommendations. Our objective was to identify clinical factors predictive of ototoxicity using an agnostic machine learning method. We used classification and regression tree (CART) analyses to identify clinical factors, including amikacin concentration thresholds that predicted audiometry-confirmed ototoxicity among 28 multidrug-resistant pulmonary tuberculosis patients in Botswana. Amikacin concentrations were measured for all patients. The quantitative relationship between predictive factors and the probability of ototoxicity were then identified using probit analyses. The primary predictors of ototoxicity on CART analyses were cumulative days of therapy, followed by cumulative area under the concentration-time curve (AUC), which improved on the primary predictor by 87%. The area under the receiver operating curve was 0.97 on the test set. Peak and trough were not predictors in any tree. When algorithms were forced to pick peak and trough as primary predictors, the area under the receiver operating curve fell to 0.46. Probit analysis revealed that the probability of ototoxicity increased sharply starting after 6 months of therapy to near maximum at 9 months. A 10% probability of ototoxicity occurred with a threshold cumulative AUC of 87,232 days · mg · h/liter, while that of 20% occurred at 120,000 days · mg · h/liter. Thus, cumulative amikacin AUC and duration of therapy, and not peak and trough concentrations, should be used as the primary decision-making parameters to minimize the likelihood of ototoxicity in multidrug-resistant tuberculosis.Copyright © 2015, Modongo et al.DOI: 10.1128/AAC.01050-15PMCID: PMC4576092",pubmed,26248372,10.1128/AAC.01050-15
aging but not agerelated hearing loss dominates the decrease of parvalbumin immunoreactivity in the primary auditory cortex of mice,"202. eNeuro. 2020 May 8;7(3):ENEURO.0511-19.2020. doi: 10.1523/ENEURO.0511-19.2020. Print 2020 May/Jun.Aging But Not Age-Related Hearing Loss Dominates the Decrease of Parvalbumin Immunoreactivity in the Primary Auditory Cortex of Mice.Rogalla MM(1), Hildebrandt KJ(2).Author information:(1)Department of Neuroscience, Division of Auditory Neuroscience, and Cluster of Excellence, Hearing4all, Carl von Ossietzky University, Oldenburg 26129, Germany meike.rogalla@uni-oldenburg.de.(2)Department of Neuroscience, Division of Auditory Neuroscience, and Cluster of Excellence, Hearing4all, Carl von Ossietzky University, Oldenburg 26129, Germany.Alterations in inhibitory circuits of the primary auditory cortex (pAC) have been shown to be an aspect of aging and age-related hearing loss (AHL). Several studies reported a decline in parvalbumin (PV) immunoreactivity in aged rodent pAC of animals displaying AHL and conclude a relationship between reduced sensitivity and declined PV immunoreactivity. However, it remains elusive whether AHL or a general molecular aging is causative for decreased PV immunoreactivity. In this study, we aimed to disentangle the effects of AHL and general aging on PV immunoreactivity patterns in inhibitory interneurons of mouse pAC. We compared young and old animals of a mouse line with AHL (C57BL/6) and a mutant (C57B6.CAST-Cdh23Ahl+ ) that is not vulnerable to AHL according to their hearing status by measuring auditory brainstem responses (ABRs) and by an immunohistochemical evaluation of the PV immunoreactivity patterns in two dimensions (rostro-caudal and layer) in the pAC. Although AHL could be confirmed by ABR measurements for the C57BL/6 mice, both aged strains showed a similar reduction of PV+ positive interneurons in both, number and density. The pattern of reduction across the rostro-caudal axis and across cortical layers was similar for both aged lines. Our results demonstrate that a reduced PV immunoreactivity is a sign of general, molecular aging and not related to AHL.Copyright © 2020 Rogalla and Hildebrandt.DOI: 10.1523/ENEURO.0511-19.2020PMCID: PMC7210488",pubmed,32327469,10.1523/ENEURO.0511-19.2020
factors in clientclinician interaction that influence hearing aid adoption,"The influence of client–clinician interactions has not been emphasized in hearing health care, despite the extensive evidence of the impact of the provider–patient interaction on health outcomes. The purpose of this study was to identify factors in the client–clinician interaction that may influence hearing aid adoption. Thirteen adults who had received a hearing aid recommendation within the previous 3 months and 10 audiologists participated in a study to generate, sort, and rate the importance of factors in client–clinician interaction that may influence the hearing aid purchase decision. A concept mapping approach was used to define meaningful clusters of factors. Quantitative analysis and qualitative interpretation of the statements resulted in eight concepts. The concepts in order of their importance are (a) Ensuring client comfort, (b) Understanding and meeting client needs, (c) Client-centered traits and actions, (d) Acknowledging client as an individual, (e) Imposing undue pressure and discomfort, (f) Conveying device information by clinician, (g) Supporting choices and shared decision making, and (h) Factors in client readiness. Two overarching themes of client-centered interaction and client empowerment were identified. Results highlight the influence of the client–clinician interaction in hearing aid adoption and suggest the possibility of improving hearing aid adoption by empowering clients through a client-centered interaction. © 2011, SAGE Publications. All rights reserved.",scopus,2-s2.0-84857727049,10.1177/1084713811430217
the neural bases of tinnitus lessons from deafness and cochlear implants,"604. J Neurosci. 2020 Sep 16;40(38):7190-7202. doi: 10.1523/JNEUROSCI.1314-19.2020.The Neural Bases of Tinnitus: Lessons from Deafness and Cochlear Implants.Knipper M(1), van Dijk P(2)(3), Schulze H(4), Mazurek B(5), Krauss P(4), Scheper V(6)(7), Warnecke A(6)(7), Schlee W(8), Schwabe K(6)(7), Singer W(9), Braun C(10), Delano PH(11), Fallgatter AJ(12), Ehlis AC(12), Searchfield GD(13)(14), Munk MHJ(12)(15), Baguley DM(16)(17), Rüttiger L(9).Author information:(1)University of Tübingen, Department of Otolaryngology, Head and Neck Surgery, Tübingen Hearing Research Center, Molecular Physiology of Hearing, 72076 Tübingen, Germany marlies.knipper@uni-tuebingen.de p.van.dijk@umcg.nl.(2)Department of Otorhinolaryngology/Head and Neck Surgery, University of Groningen, University Medical Center Groningen, 9700 AB Groningen, The Netherlands marlies.knipper@uni-tuebingen.de p.van.dijk@umcg.nl.(3)Graduate School of Medical Sciences (Research School of Behavioural and Cognitive Neurosciences), University of Groningen, 9700 AB Groningen, The Netherlands.(4)Experimental Otolaryngology, Neuroscience Laboratory, University Hospital Erlangen, Friedrich-Alexander University Erlangen-Nürnberg, 91054 Erlangen, Germany.(5)Charité-Universitätsmedizin Berlin, Tinnituszentrum, 10117 Berlin, Germany.(6)Department of Otorhinolaryngology, Head and Neck Surgery, Hannover Medical School, 30625 Hannover, Germany.(7)Cluster of Excellence ""Hearing4all"" of the German Research Foundation, 30625 Hannover, Germany.(8)Department of Psychiatry and Psychotherapy, University of Regensburg, 93053 Regensburg, Germany.(9)University of Tübingen, Department of Otolaryngology, Head and Neck Surgery, Tübingen Hearing Research Center, Molecular Physiology of Hearing, 72076 Tübingen, Germany.(10)MEG Center, University Hospital Tübingen, 72076 Tübingen, Germany.(11)Departments of Otolaryngology and Neuroscience, Faculty of Medicine, University of Chile, 15782 Santiago, Chile.(12)Department of Psychiatry, University of Tübingen, 72076 Tübingen, Germany.(13)Eisdell Moore Centre, Audiology Section, University of Auckland, 1546 Auckland, New Zealand.(14)Brain Research New Zealand, Centre for Brain Research, University of Auckland, 1142 Auckland, New Zealand.(15)Department of Biology, Technical University Darmstadt, 64287 Darmstadt, Germany.(16)Hearing Sciences, Division of Clinical Neuroscience, School of Medicine, University of Nottingham, NG15DU Nottingham, United Kingdom.(17)NIHR Nottingham Biomedical Research Centre, University of Nottingham, NG72UH Nottingham, United Kingdom.Subjective tinnitus is the conscious perception of sound in the absence of any acoustic source. The literature suggests various tinnitus mechanisms, most of which invoke changes in spontaneous firing rates of central auditory neurons resulting from modification of neural gain. Here, we present an alternative model based on evidence that tinnitus is: (1) rare in people who are congenitally deaf, (2) common in people with acquired deafness, and (3) potentially suppressed by active cochlear implants used for hearing restoration. We propose that tinnitus can only develop after fast auditory fiber activity has stimulated the synapse formation between fast-spiking parvalbumin positive (PV+) interneurons and projecting neurons in the ascending auditory path and coactivated frontostriatal networks after hearing onset. Thereafter, fast auditory fiber activity promotes feedforward and feedback inhibition mediated by PV+ interneuron activity in auditory-specific circuits. This inhibitory network enables enhanced stimulus resolution, attention-driven contrast improvement, and augmentation of auditory responses in central auditory pathways (neural gain) after damage of slow auditory fibers. When fast auditory fiber activity is lost, tonic PV+ interneuron activity is diminished, resulting in the prolonged response latencies, sudden hyperexcitability, enhanced cortical synchrony, elevated spontaneous γ oscillations, and impaired attention/stress-control that have been described in previous tinnitus models. Moreover, because fast processing is gained through sensory experience, tinnitus would not exist in congenital deafness. Electrical cochlear stimulation may have the potential to reestablish tonic inhibitory networks and thus suppress tinnitus. The proposed framework unites many ideas of tinnitus pathophysiology and may catalyze cooperative efforts to develop tinnitus therapies.Copyright © 2020 the authors.DOI: 10.1523/JNEUROSCI.1314-19.2020PMCID: PMC7534911",pubmed,32938634,10.1523/JNEUROSCI.1314-19.2020
hearing loss alters the subcellular distribution of presynaptic gad and postsynaptic gabaa receptors in the auditory cortex,"546. Cereb Cortex. 2008 Dec;18(12):2855-67. doi: 10.1093/cercor/bhn044. Epub 2008 Apr 9.Hearing loss alters the subcellular distribution of presynaptic GAD and postsynaptic GABAA receptors in the auditory cortex.Sarro EC(1), Kotak VC, Sanes DH, Aoki C.Author information:(1)Center for Neural Science, New York University, New York, NY 10003, USA.We have shown previously that auditory experience regulates the maturation of excitatory synapses in the auditory cortex (ACx). In this study, we used electron microscopic immunocytochemistry to determine whether the heightened excitability of the ACx following neonatal sensorineural hearing loss (SNHL) also involves pre- or postsynaptic alterations of GABAergic synapses. SNHL was induced in gerbils just prior to the onset of hearing (postnatal day 10). At P17, the gamma-aminobutyri acid type A (GABA(A)) receptor's beta2/3-subunit (GABA(A)beta2/3) clusters residing at plasma membranes in layers 2/3 of ACx was reduced significantly in size (P < 0.05) and number (P < 0.005), whereas the overall number of immunoreactive puncta (intracellular + plasmalemmal) remained unchanged. The reduction of GABA(A)beta2/3 was observed along perikaryal plasma membranes of excitatory neurons but not of GABAergic interneurons. This cell-specific change can contribute to the enhanced excitability of SNHL ACx. Presynaptically, GABAergic axon terminals were significantly larger but less numerous and contained 47% greater density of glutamic acid decarboxylase immunoreactivity (P < 0.05). This suggests that GABA synthesis may be upregulated by a retrograde signal arising from lowered levels of postsynaptic GABA(A)R. Thus, both, the pre- and postsynaptic sides of inhibitory synapses that form upon pyramidal neurons of the ACx are regulated by neonatal auditory experience.DOI: 10.1093/cercor/bhn044PMCID: PMC2583158",pubmed,18403398,10.1093/cercor/bhn044
hand gesture recognition with augmented reality and leap motion controller,"The use of hand gestures is one of the commonly used communication approaches in human daily life, especially for the deaf and dumb. Hand gesture recognition can be adopted in human-computer interaction for converting hand gestures into words or sentences. Unfortunately, the same gesture may have diverse meanings in different countries. With the aim of eliminating the communication barriers between hearing-impaired communities and the general people, an efficient interaction user interface created with the augmented reality technique and leap motion controller for hand gesture recognition and translation is proposed in this paper. Five hand gestures captured by a leap motion controller were used for learning and recognizing through machine learning methodologies, including Support Vector Machine, K-Nearest Neighbor, Convolutional Neural Network, Deep Neural Network and Decision Tree. The experimental results from different classifiers reveal the practicability of employing hand gesture recognition in text translation. The hand gesture recognition system should be capable of reducing the communication gap between hearing disabilities and the public so as to avoid deaf and mute people being isolated from society.",ieee,,10.1109/IEEM50564.2021.9672611
the text analysis software for hearingimpaired persons,"Using information technologies for hearing-impaired persons allows society to include citizens that were to some extent isolated because of communication difficulties. Although different applications exist to help such persons, it is always actual to have a nationally-oriented product. Being properly modeled with consideration of needs a person with hearing impairments, such application with allow improving living standards not only for such persons, but also their families and close ones.",ieee,2766-3639,10.1109/CSIT52700.2021.9648605
investigation on noise exposure level and health status of workers in transportation equipment manufacturing industry,"631. Zhonghua Lao Dong Wei Sheng Zhi Ye Bing Za Zhi. 2021 Jul 20;39(7):498-502. doi: 10.3760/cma.j.cn121094-20200513-00258.[Investigation on noise exposure level and health status of workers in transportation equipment manufacturing industry].[Article in Chinese; Abstract available in Chinese from the publisher]Hu SQ(1), Hu WJ(2), Yang S(1), Zhu XH(1), Sun K(1), Jiang SS(1), Qiu YX(1), Li XD(1).Author information:(1)Labor Health Occupational Disease Prevention Center of Zhuzhou, Zhuzhou 412011, China.(2)National Insitute of Occupational Health and Poison Control, Chinese Center for Disease Control and Prevention, Beijing 100050, China.Objective: To explore the noise exposure level and the health status of workers in transportation equipment manufacturing industry, and provide a scientific basis for guidance and implementation of intervention measures. Methods: From January to December in 2019, a total of 2088 noise workers from a large enterprise were selected by cluster sampling method in railway transportation equipment manufacturing, automobile manufacturing and aerospace aircraft manufacturing enterprises. The worker's noise exposure level was detected. Occupational health checkups were performed on the noise workers including electrical audiometry, blood pressure and electrocardiogram. χ(2) test and trend χ(2) test were used to analyze the data. Results: The noise exposure level of 66.9% (1396/2088) workers exceeded 85 dB (A) , and the median noise level was 87.9 (84.3-90.3) dB (A) . Among them, workers of railway transportation equipment manufacturing enterprises had the highest noise exposure level[89.9 (87.8-91.6) dB (A) ]. The detection rate of high-frequency hearing loss, abnormal blood pressure and abnormal electrocardiogram of noise workers were 15.7% (327/2088) , 18.1% (378/2088) and 6.1% (128/2088) , respectively. The differences in the detection rates of high-frequency hearing loss, abnormal blood pressure, and abnormal electrocardiogram in workers of railway transportation equipment manufacturing enterprises, automobile manufacturing enterprises, and aerospace manufacturing enterprises were statistically significant (P<0.05) . Workers of railway transportation equipment manufacturing enterprises had higher detection rates of high-frequency hearing loss (17.6%, 186/1056) . Workers of aerospace manufacturing enterprises had higher detection rates of abnormal blood pressure and abnormal electrocardiogram (26.3%, 169/642; 10.0%, 64/642) . The differences in the detection rates of high-frequency hearing loss, abnormal blood pressure and abnormal electrocardiogram of noise workers were statistically significant in different age and working age groups, and gradually increased with age and working age (P<0.05) . The difference in the detection rate of high-frequency hearing loss of noise workers was statistically significant in different noise intensity groups, and the overall trend was increasing (P<0.05) . Conclusion: The transportation equipment manufacturing industry has serious noise hazards, especially the railway transportation equipment manufacturing industry. Long-term occupational noise exposure can adversely affect workers' hearing and cardiovascular system. Enterprises should strengthen occupational health inspections, and at the same time, take personal protective measures to protect the health of workers.Publisher: 目的： 探讨交通运输设备制造行业工人噪声接触水平及其健康状况，为指导和实施干预措施提供科学依据。 方法： 于2019年1至12月，采用整群抽样方法，在铁路运输设备制造、汽车制造、航天航空器制造企业各选取1家大型企业的噪声作业工人作为调查对象，共2 088人。测定噪声作业岗位的噪声接触水平，并对噪声作业工人进行纯音测听、血压和心电图测量，采用χ(2)检验和趋势χ(2)检验等统计方法对所得数据进行分析。 结果： 工人噪声接触水平超过85 dB（A）占66.9%（1 396/2 088），噪声声级为87.9（84.3~90.3）dB（A），其中铁路运输设备制造企业工人噪声声级最大[89.9（87.8~91.6）dB（A）]。噪声作业工人高频听力损失、血压异常、心电图异常检出率分别为15.7%（327/2 088）、18.1%（378/2 088）、6.1%（128/2 088）。铁路运输设备制造、汽车制造、航天航空器制造企业工人的高频听力损失、血压异常和心电图异常检出率差异均有统计学意义（P<0.05）。铁路运输设备制造企业工人高频听力损失检出率较高（17.6%，186/1 056），航天航空器制造企业工人血压异常和心电图异常检出率较高（26.3%，169/642；10.0%，64/642）。噪声作业工人高频听力损失、血压异常和心电图异常检出率在不同年龄和工龄组的差异均有统计学意义，且随着年龄、工龄增加而逐渐升高（P<0.05），噪声作业工人高频听力损失检出率在不同噪声接触水平组的差异有统计学意义，且总体呈上升趋势（P<0.05）。 结论： 交通运输设备制造行业噪声危害严重，尤其是铁路运输设备制造行业。长期职业接触噪声会对工人听力及心血管产生不良影响，企业应加强职业健康检查，同时做好个体防护措施，保护劳动者健康。.DOI: 10.3760/cma.j.cn121094-20200513-00258",pubmed,34365758,10.3760/cma.j.cn121094-20200513-00258
stem cell based drug delivery for protection of auditory neurons in a guinea pig model of cochlear implantation,"670. Front Cell Neurosci. 2019 May 14;13:177. doi: 10.3389/fncel.2019.00177. eCollection 2019.Stem Cell Based Drug Delivery for Protection of Auditory Neurons in a Guinea Pig Model of Cochlear Implantation.Scheper V(1)(2)(3), Hoffmann A(3)(4), Gepp MM(5)(6), Schulz A(5), Hamm A(3)(4), Pannier C(1), Hubka P(3)(7), Lenarz T(1)(2)(3), Schwieger J(1)(3).Author information:(1)Department of Otolaryngology, Hannover Medical School, Hanover, Germany.(2)Cluster of Excellence 'Hearing4all', German Research Foundation, Bonn, Germany.(3)Lower Saxony Centre for Biomedical Engineering, Implant Research and Development (NIFE), Hanover, Germany.(4)Department of Orthopaedic Surgery, Hannover Medical School, Hanover, Germany.(5)Fraunhofer Institute for Biomedical Engineering IBMT, Sulzbach, Germany.(6)Fraunhofer Project Center for Stem Cell Process Engineering, Würzburg, Germany.(7)Department of Experimental Otology, Hannover Medical School, Hanover, Germany.Background: The success of a cochlear implant (CI), which is the standard therapy for patients suffering from severe to profound sensorineural hearing loss, depends on the number and excitability of spiral ganglion neurons (SGNs). Brain-derived neurotrophic factor (BDNF) has a protective effect on SGNs but should be applied chronically to guarantee their lifelong survival. Long-term administration of BDNF could be achieved using genetically modified mesenchymal stem cells (MSCs), but these cells should be protected - by ultra-high viscous (UHV-) alginate ('alginate-MSCs') - from the recipient immune system and from uncontrolled migration. Methods: Brain-derived neurotrophic factor-producing MSCs were encapsulated in UHV-alginate. Four experimental groups were investigated using guinea pigs as an animal model. Three of them were systemically deafened and (unilaterally) received one of the following: (I) a CI; (II) an alginate-MSC-coated CI; (III) an injection of alginate-embedded MSCs into the scala tympani followed by CI insertion and alginate polymerization. Group IV was normal hearing, with CI insertion in both ears and a unilateral injection of alginate-MSCs. Using acoustically evoked auditory brainstem response measurements, hearing thresholds were determined before implantation and before sacrificing the animals. Electrode impedance was measured weekly. Four weeks after implantation, the animals were sacrificed and the SGN density and degree of fibrosis were evaluated. Results: The MSCs survived being implanted for 4 weeks in vivo. Neither the alginate-MSC injection nor the coating affected electrode impedance or fibrosis. CI insertion with and without previous alginate injection in normal-hearing animals resulted in increased hearing thresholds within the high-frequency range. Low-frequency hearing loss was additionally observed in the alginate-injected and implanted cochleae, but not in those treated only with a CI. In deafened animals, the alginate-MSC coating of the CI significantly prevented SGN from degeneration, but the injection of alginate-MSCs did not. Conclusion: Brain-derived neurotrophic factor-producing MSCs encapsulated in UHV-alginate prevent SGNs from degeneration in the form of coating on the CI surface, but not in the form of an injection. No increase in fibrosis or impedance was detected. Further research and development aimed at verifying long-term mechanical and biological properties of coated electrodes in vitro and in vivo, in combination with chronic electrical stimulation, is needed before the current concept can be tested in clinical trials.DOI: 10.3389/fncel.2019.00177PMCID: PMC6527816",pubmed,31139049,10.3389/fncel.2019.00177
amplitude growth functions of auditory nerve responses to electric pulse stimulation with varied interphase gaps in cochlear implant users with ipsilateral residual hearing,"744. Trends Hear. 2021 Jan-Dec;25:23312165211014137. doi: 10.1177/23312165211014137.Amplitude Growth Functions of Auditory Nerve Responses to Electric Pulse Stimulation With Varied Interphase Gaps in Cochlear Implant Users With Ipsilateral Residual Hearing.Imsiecke M(1), Büchner A(1)(2), Lenarz T(1)(2), Nogueira W(1)(2).Author information:(1)Clinic for Otorhinolaryngology, Hannover Medical School, Hannover, Germany.(2)Cluster of Excellence ""Hearing4All,"" Hannover, Germany.Amplitude growth functions (AGFs) of electrically evoked compound action potentials (eCAPs) with varying interphase gaps (IPGs) were measured in cochlear implant users with ipsilateral residual hearing (electric-acoustic stimulation [EAS]). It was hypothesized that IPG effects on AGFs provide an objective measure to estimate neural health. This hypothesis was tested in EAS users, as residual low-frequency hearing might imply survival of hair cells and hence better neural health in apical compared to basal cochlear regions. A total of 16 MED-EL EAS subjects participated, as well as a control group of 16 deaf cochlear implant users. The IPG effect on the AGF characteristics of slope, threshold, dynamic range, and stimulus level at 50% maximum eCAP amplitude (level50%) was investigated. AGF threshold and level50% were significantly affected by the IPG in both EAS and control group. The magnitude of AGF characteristics correlated with electrode impedance and electrode-modiolus distance (EMD) in both groups. In contrast, the change of the AGF characteristics with increasing IPG was independent of these electrode-specific measures. The IPG effect on the AGF level50% in both groups, as well as on the threshold in EAS users, correlated with the duration of hearing loss, which is a predictor of neural health. In EAS users, a significantly different IPG effect on level50% was found between apical and medial electrodes. This outcome is consistent with our hypothesis that the influence of IPG effects on AGF characteristics provides a sensitive measurement and may indicate better neural health in the apex compared to the medial cochlear region in EAS users.DOI: 10.1177/23312165211014137PMCID: PMC8243142",pubmed,34181493,10.1177/23312165211014137
relationship between memory load and listening demands in agerelated hearing impairment,"60. Neural Plast. 2021 Jun 4;2021:8840452. doi: 10.1155/2021/8840452. eCollection 2021.Relationship between Memory Load and Listening Demands in Age-Related Hearing Impairment.Pauquet J(1), Thiel CM(1)(2), Mathys C(3)(4), Rosemann S(1)(2).Author information:(1)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität, 26111 Oldenburg, Germany.(2)Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, 26111 Oldenburg, Germany.(3)Institute of Radiology and Neuroradiology, Evangelisches Krankenhaus, Carl von Ossietzky Universität Oldenburg, 26122 Oldenburg, Germany.(4)Research Center Neurosensory Science, Carl von Ossietzky Universität Oldenburg, 26111 Oldenburg, Germany.Age-related hearing loss has been associated with increased recruitment of frontal brain areas during speech perception to compensate for the decline in auditory input. This additional recruitment may bind resources otherwise needed for understanding speech. However, it is unknown how increased demands on listening interact with increasing cognitive demands when processing speech in age-related hearing loss. The current study used a full-sentence working memory task manipulating demands on working memory and listening and studied untreated mild to moderate hard of hearing (n = 20) and normal-hearing age-matched participants (n = 19) with functional MRI. On the behavioral level, we found a significant interaction of memory load and listening condition; this was, however, similar for both groups. Under low, but not high memory load, listening condition significantly influenced task performance. Similarly, under easy but not difficult listening conditions, memory load had a significant effect on task performance. On the neural level, as measured by the BOLD response, we found increased responses under high compared to low memory load conditions in the left supramarginal gyrus, left middle frontal gyrus, and left supplementary motor cortex regardless of hearing ability. Furthermore, we found increased responses in the bilateral superior temporal gyri under easy compared to difficult listening conditions. We found no group differences nor interactions of group with memory load or listening condition. This suggests that memory load and listening condition interacted on a behavioral level, however, only the increased memory load was reflected in increased BOLD responses in frontal and parietal brain regions. Hence, when evaluating listening abilities in elderly participants, memory load should be considered as it might interfere with the assessed performance. We could not find any further evidence that BOLD responses for the different memory and listening conditions are affected by mild to moderate age-related hearing loss.Copyright © 2021 Julia Pauquet et al.DOI: 10.1155/2021/8840452PMCID: PMC8195652",pubmed,34188676,10.1155/2021/8840452
sjdocx1aor10117700034894231206902 textendash supplemental material for machine learning models for predicting sudden sensorineural hearing loss outcome a systematic review ,,base,339d1c86ba9b03c04ea9601039955558f463946d6b770ff8bd40e1ab92f3d595,
are there good days and bad days for hearing quantifying daytoday intraindividual speech perception variability in older and younger adults,"555. J Exp Psychol Hum Percept Perform. 2023 Nov;49(11):1377-1394. doi: 10.1037/xhp0001159.Are there good days and bad days for hearing? Quantifying day-to-day intraindividual speech perception variability in older and younger adults.Kuhlmann I(1), Angonese G(1), Thiel C(1), Kollmeier B(1), Hildebrandt A(1).Author information:(1)Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität.Moment-to-moment variations in hearing and speech perception have long been observed. Depending on the researcher's theoretical position, the observed fluctuations have been attributed to measurement error or to internal, nonsensory factors such as fluctuations in attention. While cognitive performance has been shown to fluctuate from day to day over longer time, such fluctuations have not been quantified for speech perception, despite being well-recognized by clinical audiologists and hearing-impaired patients. In three studies, we aimed to explore and quantify the magnitude of daily variability in speech perception and to investigate whether such variability goes beyond test unreliability. We also asked whether intraindividual variability depends on overall speech perception performance as observed in different groups of individuals. Older adults with objective hearing impairment and mostly hearing aids (N₁ = 45), with subjective hearing problems but no hearing aids (N₂ = 113), and younger adults without hearing problems (N₃ = 20) participated in three ecological momentary assessment studies. They performed a digit-in-noise test two to three times a day for several weeks. Variance heterogeneous linear mixed-effects models indicated reliable intraindividual variability in speech perception and substantial individual differences in daily variability. A protective factor against daily fluctuations is a higher average speech perception. These studies show that day-to-day variations in speech perception cannot simply be attributed to test unreliability and pave the way for investigating how psychological states that do not vary from moment-to-moment, but rather from day to day, predict variations in speech perception. (PsycInfo Database Record (c) 2023 APA, all rights reserved).DOI: 10.1037/xhp0001159",pubmed,37870818,10.1037/xhp0001159
a talkerindependent deep learning algorithm to increase intelligibility for hearingimpaired listeners in reverberant competing talker conditions,"290. J Acoust Soc Am. 2020 Jun;147(6):4106. doi: 10.1121/10.0001441.A talker-independent deep learning algorithm to increase intelligibility for hearing-impaired listeners in reverberant competing talker conditions.Healy EW(1), Johnson EM(1), Delfarah M(2), Wang D(2).Author information:(1)Department of Speech and Hearing Science, and Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, Ohio 43210, USA.(2)Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio 43210, USA.Deep learning based speech separation or noise reduction needs to generalize to voices not encountered during training and to operate under multiple corruptions. The current study provides such a demonstration for hearing-impaired (HI) listeners. Sentence intelligibility was assessed under conditions of a single interfering talker and substantial amounts of room reverberation. A talker-independent deep computational auditory scene analysis (CASA) algorithm was employed, in which talkers were separated and dereverberated in each time frame (simultaneous grouping stage), then the separated frames were organized to form two streams (sequential grouping stage). The deep neural networks consisted of specialized convolutional neural networks, one based on U-Net and the other a temporal convolutional network. It was found that every HI (and normal-hearing, NH) listener received algorithm benefit in every condition. Benefit averaged across all conditions ranged from 52 to 76 percentage points for individual HI listeners and averaged 65 points. Further, processed HI intelligibility significantly exceeded unprocessed NH intelligibility. Although the current utterance-based model was not implemented as a real-time system, a perspective on this important issue is provided. It is concluded that deep CASA represents a powerful framework capable of producing large increases in HI intelligibility for potentially any two voices.DOI: 10.1121/10.0001441PMCID: PMC7314568",pubmed,32611178,10.1121/10.0001441
the microrna18396182 cluster is essential for stereociliary bundle formation and function of cochlear sensory hair cells,"756. Sci Rep. 2018 Dec 21;8(1):18022. doi: 10.1038/s41598-018-36894-z.The microRNA-183/96/182 Cluster is Essential for Stereociliary Bundle Formation and Function of Cochlear Sensory Hair Cells.Geng R(1)(2), Furness DN(3), Muraleedharan CK(4), Zhang J(5)(6), Dabdoub A(7), Lin V(7), Xu S(8).Author information:(1)Department of Ophthalmology, Visual and Anatomical Sciences, School of Medicine, Wayne State University, Detroit, Michigan, USA. ruishuang@gmail.com.(2)Department of Otolaryngology, School of Medicine, Wayne State University, Detroit, Michigan, USA. ruishuang@gmail.com.(3)School of Life Sciences, Keele University, Keele, Staffs, ST5 5BG, United Kingdom.(4)Department of Ophthalmology, Visual and Anatomical Sciences, School of Medicine, Wayne State University, Detroit, Michigan, USA.(5)Department of Otolaryngology, School of Medicine, Wayne State University, Detroit, Michigan, USA.(6)Department of Communication Sciences and Disorders, College of Liberal Arts and Sciences, Wayne State University, Detroit, Michigan, USA.(7)Biological Science, Sunnybrook Research Institute, Toronto, Ontario, Canada.(8)Department of Ophthalmology, Visual and Anatomical Sciences, School of Medicine, Wayne State University, Detroit, Michigan, USA. sxu@med.wayne.edu.The microRNA (miR)-183/96/182 cluster plays important roles in the development and functions of sensory organs, including the inner ear. Point-mutations in the seed sequence of miR-96 result in non-syndromic hearing loss in both mice and humans. However, the lack of a functionally null mutant has hampered the evaluation of the cluster's physiological functions. Here we have characterized a loss-of-function mutant mouse model (miR-183CGT/GT), in which the miR-183/96/182 cluster gene is inactivated by a gene-trap (GT) construct. The homozygous mutant mice show profound congenital hearing loss with severe defects in cochlear hair cell (HC) maturation, alignment, hair bundle formation and the checkboard-like pattern of the cochlear sensory epithelia. The stereociliary bundles retain an immature appearance throughout the cochlea at postnatal day (P) 3 and degenerate soon after. The organ of Corti of mutant newborn mice has no functional mechanoelectrical transduction. Several predicted target genes of the miR-183/96/182 cluster that are known to play important roles in HC development and function, including Clic5, Rdx, Ezr, Rac1, Myo1c, Pvrl3 and Sox2, are upregulated in the cochlea. These results suggest that the miR-183/96/182 cluster is essential for stereociliary bundle formation, morphogenesis and function of the cochlear HCs.DOI: 10.1038/s41598-018-36894-zPMCID: PMC6303392",pubmed,30575790,10.1038/s41598-018-36894-z
recent advances in hearing conservation programmes a systematic review,"157. S Afr J Commun Disord. 2020 Mar 3;67(2):e1-e11. doi: 10.4102/sajcd.v67i2.675.Recent advances in hearing conservation programmes: A systematic review.Moroe NF(1), Khoza-Shangase K.Author information:(1)Department of Speech Pathology and Audiology, Faculty of Humanities, University of the Witwatersrand, Johannesburg. nomfundo.moroe@wits.ac.za.BACKGROUND: Current evidence from low- and middle-income (LAMI) countries, such as South Africa, indicates that occupational noise-induced hearing loss (ONIHL) continues to be a health and safety challenge for the mining industry. There is also evidence of hearing conservation programmes (HCPs) being implemented with limited success.OBJECTIVES: The aim of this study was to explore and document current evidence reflecting recent advances in HCPs in order to identify gaps within the South African HCPs.METHOD: A systematic literature review was conducted in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analysis. Electronic databases including Sage, Science Direct, PubMed, Scopus MEDLINE, ProQuest and Google Scholar were searched for potential studies published in English between 2010 and 2019 reporting on recent advances in HCPs within the mining industry.RESULTS: The study findings revealed a number of important recent advances internationally, which require deliberation for possible implementation within the South African HCPs context. These advances have been presented under seven themes: (1) the use of metrics, (2) pharmacological interventions and hair cell regeneration, (3) artificial neural network, (4) audiology assessment measures, (5) noise monitoring advances, (6) conceptual approaches to HCPs and (7) buying quiet.CONCLUSION: The study findings raise important advances that may have significant implications for HCPs in LAMI countries where ONIHL remains a highly prevalent occupational health challenge. Establishing feasibility and efficacy of these advances in these contexts to ensure contextual relevance and responsiveness is one of the recommendations to facilitate the success of HCPs targets.DOI: 10.4102/sajcd.v67i2.675PMCID: PMC7136823",pubmed,32129659,10.4102/sajcd.v67i2.675
neural correlates of semantic and syntactic processing in german sign language,"654. Neuroimage. 2019 Oct 15;200:231-241. doi: 10.1016/j.neuroimage.2019.06.025. Epub 2019 Jun 17.Neural correlates of semantic and syntactic processing in German Sign Language.Stroh AL(1), Rösler F(2), Dormal G(2), Salden U(2), Skotara N(2), Hänel-Faulhaber B(3), Röder B(2).Author information:(1)Biological Psychology and Neuropsychology, University of Hamburg, Germany. Electronic address: anna-lena.stroh@uni-hamburg.de.(2)Biological Psychology and Neuropsychology, University of Hamburg, Germany.(3)Biological Psychology and Neuropsychology, University of Hamburg, Germany; Special Education, University of Hamburg, Germany.The study of deaf and hearing native users of signed languages can offer unique insights into how biological constraints and environmental input interact to shape the neural bases of language processing. Here, we use functional magnetic resonance imaging (fMRI) to address two questions: (1) Do semantic and syntactic processing in a signed language rely on anatomically and functionally distinct neural substrates as it has been shown for spoken languages? and (2) Does hearing status affect the neural correlates of these two types of linguistic processing? Deaf and hearing native signers performed a sentence judgement task on German Sign Language (Deutsche Gebärdensprache: DGS) sentences which were correct or contained either syntactic or semantic violations. We hypothesized that processing of semantic and syntactic violations in DGS relies on distinct neural substrates as it has been shown for spoken languages. Moreover, we hypothesized that effects of hearing status are observed within auditory regions, as deaf native signers have been shown to activate auditory areas to a greater extent than hearing native signers when processing a signed language. Semantic processing activated low-level visual areas and the left inferior frontal gyrus (IFG), suggesting both modality-dependent and independent processing mechanisms. Syntactic processing elicited increased activation in the right supramarginal gyrus (SMG). Moreover, psychophysiological interaction (PPI) analyses revealed a cluster in left middle occipital regions showing increased functional coupling with the right SMG during syntactic relative to semantic processing, possibly indicating spatial processing mechanisms that are specific to signed syntax. Effects of hearing status were observed in the right superior temporal cortex (STC): deaf but not hearing native signers showed greater activation for semantic violations than for syntactic violations in this region. Taken together, the present findings suggest that the neural correlates of language processing are partly determined by biological constraints, but that they may additionally be influenced by the unique processing demands of the language modality and different sensory experiences.Copyright © 2019 Elsevier Inc. All rights reserved.DOI: 10.1016/j.neuroimage.2019.06.025",pubmed,31220577,10.1016/j.neuroimage.2019.06.025
betterear glimpsing with symmetricallyplaced interferers in bilateral cochlear implant users,"228. J Acoust Soc Am. 2018 Apr;143(4):2128. doi: 10.1121/1.5030918.Better-ear glimpsing with symmetrically-placed interferers in bilateral cochlear implant users.Hu H(1), Dietz M(1), Williges B(1), Ewert SD(1).Author information:(1)Medizinische Physik, Carl von Ossietzky Universität Oldenburg and Cluster of Excellence ""Hearing4all,"" Küpkersweg 74, 26129, Oldenburg, Germany.For a frontal target in spatially symmetrically placed interferers, normal hearing (NH) listeners can use ""better-ear glimpsing"" to select time-frequency segments with favorable signal-to-noise ratio in either ear. With an ideal monaural better-ear mask (IMBM) processing, some studies showed that NH listeners can reach similar performance as in the natural binaural listening condition, although interaural phase differences at low frequencies can further improve performance. In principle, bilateral cochlear implant (BiCI) listeners could use the same better-ear glimpsing, albeit without exploiting interaural phase differences. Speech reception thresholds of NH and BiCI listeners were measured in three interferers (speech-shaped stationary noise, nonsense speech, or single talker) either co-located with the target, symmetrically placed at ±60°, or independently presented to each ear, with and without IMBM processing. Furthermore, a bilateral noise vocoder based on the BiCI electrodogram was used in the same NH listeners. Headphone presentation and direct stimulation with head-related transfer functions for spatialization were used in NH and BiCI listeners, respectively. Compared to NH listeners, both NH listeners with vocoder and BiCI listeners showed strongly reduced binaural benefit from spatial separation. However, both groups greatly benefited from IMBM processing as part of the stimulation strategy.DOI: 10.1121/1.5030918",pubmed,29716260,10.1121/1.5030918
loss of kv31 tonotopicity and alterations in camp response elementbinding protein signaling in central auditory neurons of hearing impaired mice,"429. J Neurosci. 2004 Feb 25;24(8):1936-40. doi: 10.1523/JNEUROSCI.4554-03.2004.Loss of Kv3.1 tonotopicity and alterations in cAMP response element-binding protein signaling in central auditory neurons of hearing impaired mice.von Hehn CA(1), Bhattacharjee A, Kaczmarek LK.Author information:(1)Department of Pharmacology, Yale University School of Medicine, New Haven, Connecticut 06520, USA.The promoter for the kv3.1 potassium channel gene is regulated by a Ca2+-cAMP responsive element, which binds the transcription factor cAMP response element-binding protein (CREB). Kv3.1 is expressed in a tonotopic gradient within the medial nucleus of the trapezoid body (MNTB) of the auditory brainstem, where Kv3.1 levels are highest at the medial end, which corresponds to high auditory frequencies. We have compared the levels of Kv3.1, CREB, and the phosphorylated form of CREB (pCREB) in a mouse strain that maintains good hearing throughout life, CBA/J (CBA), with one that suffers early cochlear hair cell loss, C57BL/6 (BL/6). A gradient of Kv3.1 immunoreactivity in the MNTB was detected in both young (6 week) and older (8 month) CBA mice. Although no gradient of CREB was detected, pCREB-immunopositive cells were grouped together in distinct clusters along the tonotopic axis. The same pattern of Kv3.1, CREB, and pCREB localization was also found in young BL/6 mice at a time (6 weeks) when hearing is normal. In contrast, at 8 months, when hearing is impaired, the gradient of Kv3.1 was abolished. Moreover, in the older BL/6 mice there was a decrease in CREB expression along the tonotopic axis, and the pattern of pCREB labeling appeared random, with no discrete clusters of pCREB-positive cells along the tonotopic axis. Our findings are consistent with the hypothesis that ongoing activity in auditory brainstem neurons is necessary for the maintenance of Kv3.1 tonotopicity through the CREB pathway.DOI: 10.1523/JNEUROSCI.4554-03.2004PMCID: PMC6730406",pubmed,14985434,10.1523/JNEUROSCI.4554-03.2004
atlasbased segmentation of temporal bone surface structures,"635. Int J Comput Assist Radiol Surg. 2019 Aug;14(8):1267-1273. doi: 10.1007/s11548-019-01978-2. Epub 2019 Apr 25.Atlas-based segmentation of temporal bone surface structures.Powell KA(1), Kashikar T(2), Hittle B(3), Stredney D(3), Kerwin T(3), Wiet GJ(4).Author information:(1)Department of Biomedical Informatics, The Ohio State University, Columbus, OH, 43210, USA. kimerly.powell@osumc.edu.(2)Ohio University Heritage College of Osteopathic, Ohio University, Athens, OH, USA.(3)Interface Laboratory, The Ohio State University, Columbus, OH, USA.(4)Department of Otolaryngology, Nationwide Children's Hospital and The Ohio State University, Columbus, OH, USA.PURPOSE: To develop a time-efficient automated segmentation approach that could identify surface structures on the temporal bone for use in surgical simulation software and preoperative surgical training.METHODS: An atlas-based segmentation approach was developed to segment the tegmen, sigmoid sulcus, exterior auditory canal, interior auditory canal, and posterior canal wall in normal temporal bone CT images. This approach was tested in images of 20 cadaver bones (10 left, 10 right). The results of the automated segmentation were compared to manual segmentation using quantitative metrics of similarity, Mahalanobis distance, average Hausdorff distance, and volume similarity.RESULTS: The Mahalanobis distance was less than 0.232 mm for all structures. The average Hausdorff distance was less than 0.464 mm for all structures except the posterior canal wall and external auditory canal for the right bones. Volume similarity was 0.80 or greater for all structures except the sigmoid sulcus that was 0.75 for both left and right bones. Visually, the segmented structures were accurate and similar to that manually traced by an expert observer.CONCLUSIONS: An atlas-based approach using a deformable registration of a Gaussian-smoothed temporal bone image and refinements using surface landmarks was successful in segmenting surface structures of temporal bone anatomy for use in pre-surgical planning and training.DOI: 10.1007/s11548-019-01978-2",pubmed,31025245,10.1007/s11548-019-01978-2
severity of hyperacusis predicts individual differences in speech perception in williams syndrome,"Williams Syndrome (WS) is a neurodevelopmental disorder of genetic origin, characterised by relative proficiency in language in the face of serious impairment in several other domains. Individuals with WS display an unusual sensitivity to noise, known as hyperacusis. In this study, we examined the extent to which hyperacusis interferes with the perception of speech in children and adults with WS. Participants were required to discriminate words which differed in one consonant of a cluster when these contrasts were embedded in a background of noise. Although the introduction of noise interfered with performance on a consonant cluster discrimination task equally in the WS and control groups, the severity of hyperacusis significantly predicted individual variability in speech perception within the WS group. These results suggest that alterations in sensitivity to input mediate atypical pathways for language development in WS, where hyperacusis exerts an important influence together with other non-auditory factors.",cinahl,9642633,10.1111/j.1365-2788.2011.01411.x
plasticity of synaptic endings in the cochlear nucleus following noiseinduced hearing loss is facilitated in the adult fgf2 overexpressor mouse,"649. Eur J Neurosci. 2007 Aug;26(3):666-80. doi: 10.1111/j.1460-9568.2007.05695.x. Epub 2007 Jul 25.Plasticity of synaptic endings in the cochlear nucleus following noise-induced hearing loss is facilitated in the adult FGF2 overexpressor mouse.D'Sa C(1), Gross J, Francone VP, Morest DK.Author information:(1)Department of Neuroscience, University of Connecticut Health Center, Farmington, CT 06030, USA.In adult mammals a single exposure to loud noise can damage cochlear hair cells and initiate subsequent episodes of degeneration of axonal endings in the cochlear nucleus (CN). Possible mechanisms are loss of trophic support and/or excitotoxicity. Fibroblast growth factor 2 (FGF2), important for development, might be involved in either mechanism. To test this hypothesis, we noise-exposed FGF2 overexpressor mice and observed the effects on synaptic endings by immunolabelling for SV2, a synaptic vesicle protein, at 1, 2, 4, and 8 weeks after noise exposure. SV2 staining was observed in two major locations; perisomatic, representing axo-somatic terminals, and neuropil, representing axo-dendritic terminals. The wildtype (WT) lost both perisomatic and neuropil clusters with an intervening period of modest recovery for the perisomatic. In contrast, in the overexpressor, the perisomatic clusters remained unchanged after intervening periods of increase. The neuropil clusters underwent a period of initial decline, followed by a transient recovery and ultimate decline. Changes in SV2 immunostaining correlated with changes in vesicular glutamate and GABA transporters at synapses and, in the overexpressor, with staining changes for FGF2 and FGF receptor 1. These molecules may contribute to the synaptic reorganization after noise damage; they may protect and/or aid recovery of synapses after overstimulation.DOI: 10.1111/j.1460-9568.2007.05695.x",pubmed,17651425,10.1111/j.1460-9568.2007.05695.x
comparison of cnnbased speech dereverberation using neural vocoder,"Reverberation degrades the speech quality and intelligibility, particularly for hearing impaired people. In an automatic speech recognition (ASR) system, a dereverberation technique, which removes reverberation, is widely employed as a pre-processing to increase the performance of the ASR system. In this paper, we compare the performance of the CNN-based dereverberation method by applying various vocoders. The U-Net architecture is employed as the dereverberation technique. WaveGlow, MelGAN, and Griffin Lim are used as vocoders. Such vocoders play a role in converting speech features into speech samples in time domain, and are capable of generating high-quality speech from mel-spectrograms. In order to compare the results, PESQ was measured. As a result, it was confirmed that PESQ was higher than that of the reverberant speech when speech was synthesized with the reverberation removal and vocoder.",ieee,,10.1109/ICAIIC51459.2021.9415259
an automatic method to develop music with music segment and long short term memory for tinnitus music therapy,"Tinnitus is a perception of sound when no external sound is present. It has seriously affected patients' life. Music is an option to relieve tinnitus in clinic, as it can bring enjoyment to listeners. However, existing music used in tinnitus therapies has limited duration, it is usually repetitively played during the long-term treatment and may not be helpful for relaxation. Moreover, individualized preferences of patients are ignored in most cases. Both of them may hinder tinnitus relief. Although existing methods can synthesize specific music that has unlimited duration and is not repetitively played, the synthesized music has defects with pitch mutations and long pitch durations. Moreover, characteristics of these synthesized music have not been confirmed by tinnitus patients. Therefore, this study presents an automatic method to develop the specific music based on music segments from existing music and long short term memory (LSTM). Numerical results indicate that specific music developed in this study not only retains characteristics of original music, but also overcome the defects above. Besides, a total of 30 tinnitus patients and 10 tinnitus-free volunteers participated the auditory experiment. Auditory results are consistent with numerical results and also suggest that tinnitus patients can perceive feelings that are conducive to tinnitus relief after listening preferred music. Therefore, the developed music presents a possible complement to tinnitus treatment in clinic.",ieee,2169-3536,10.1109/ACCESS.2020.3013339
audio to indian and american sign language converter using machine translation and nlp technique,"People with hearing loss use Sign Language as their mother tongue communication. Unlike acoustic hearing, sign language is a visual language that uses body language and physical communication to communicate effectively thoughts. It usually consists of hand gestures and facial expressions. Generally, communicating with different disabled people seems very difficult. This is because it takes a long time to learn the language, not just the language, people who are not disabled but also people who are. Establishment to communicate in such cases, both parties need to know sign language, or they use a human translator to make communication possible. Information Technologies with their own modern methods such as artificial intelligence, cloud computing has an amazing role to play in improving communication people with speech impairments and ordinary people. Sign language recognition can be done in two ways, glove-based or vision-based recognition. The solution proposed in this paper will produce software that takes over input in the form of speech and indicates appropriate Sign Language. The software is developed in Python platform to convert speech to Indian and American sign languages (ISL and ASL) which provides hearing impairment assistant. This software can be useful in many areas, such as in educational institutes, hospitals, police stations, and for general everyday life conversation.",ieee,,10.1109/ICICICT54557.2022.9917614
frameshift mutation of timm8a1 gene in mouse leads to an abnormal mitochondrial structure in the brain correlating with hearing and memory impairment,"Background Deafness-dystonia-optic neuronopathy (DDON) syndrome is a progressive X-linked recessive disorder characterised by deafness, dystonia, ataxia and reduced visual acuity. The causative gene deafness/dystonia protein 1 (DDP1)/translocase of the inner membrane 8A (TIMM8A) encodes a mitochondrial intermembrane space chaperon. The molecular mechanism of DDON remains unclear, and detailed information on animal models has not been reported yet. Methods and results We characterized a family with DDON syndrome, in which the affected members carried a novel hemizygous variation in the DDP1 gene (NM_004085.3, c.82C>T, p.Q28X). We then generated a mouse line with the hemizygous mutation (p.I23fs49X) in the Timm8a1 gene using the clustered regularly interspaced short palindromic repeats /Cas9 technology. The deficient DDP1 protein was confirmed by western blot assay. Electron microscopic analysis of brain samples from the mutant mice indicated abnormal mitochondrial structure in several brain areas. However, Timm8a1 I23fs49X/y mutation did not affect the import of mitochondria inner member protein Tim23 and outer member protein Tom40 as well as the biogenesis of the proteins in the mitochondrial oxidative phosphorylation system and the manganese superoxide dismutase (MnSOD / SOD-2). The male mice with Timm8a1 I23fs49X/y mutant exhibited less weight gain, hearing impairment and cognitive deficit. Conclusion Our study suggests that frameshift mutation of the Timm8a1 gene in mice leads to an abnormal mitochondrial structure in the brain, correlating with hearing and memory impairment. Taken together, we have successfully generated a mouse model bearing loss-of-function mutation in Timm8a1. © 2021 BMJ Publishing Group. All rights reserved.",scopus,2-s2.0-85094946649,10.1136/jmedgenet-2020-106925
dual drug delivery in cochlear implants in vivo study of dexamethasone combined with diclofenac or immunophilin inhibitor mm284 in guinea pigs,"857. Pharmaceutics. 2023 Feb 22;15(3):726. doi: 10.3390/pharmaceutics15030726.Dual Drug Delivery in Cochlear Implants: In Vivo Study of Dexamethasone Combined with Diclofenac or Immunophilin Inhibitor MM284 in Guinea Pigs.Behrends W(1)(2), Wulf K(3), Raggl S(4), Fröhlich M(1)(5), Eickner T(3), Dohr D(6), Esser KH(2), Lenarz T(1)(7), Scheper V(1)(7), Paasche G(1)(7).Author information:(1)Department of Otolaryngology, Hannover Medical School, 30625 Hannover, Germany.(2)Auditory Neuroethology and Neurobiology, Institute of Zoology, University of Veterinary Medicine Hannover Foundation, 30559 Hannover, Germany.(3)Institute for Biomedical Engineering, Rostock University Medical Center, 18119 Rostock, Germany.(4)MED-EL Medical Electronics, 6020 Innsbruck, Austria.(5)MED-EL Research Center, 30625 Hannover, Germany.(6)Department of Otorhinolaryngology, Head and Neck Surgery ""Otto Körner"", Rostock University Medical Center, 18057 Rostock, Germany.(7)Hearing4all Cluster of Excellence, Hannover Medical School, 30625 Hannover, Germany.Cochlear implants are well established to treat severe hearing impairments. Despite many different approaches to reduce the formation of connective tissue after electrode insertion and to keep electrical impedances low, results are not yet satisfying. Therefore, the aim of the current study was to combine the incorporation of 5% dexamethasone in the silicone body of the electrode array with an additional polymeric coating releasing diclofenac or the immunophilin inhibitor MM284, some anti-inflammatory substances not yet tested in the inner ear. Guinea pigs were implanted for four weeks and hearing thresholds were determined before implantation and after the observation time. Impedances were monitored over time and, finally, connective tissue and the survival of spiral ganglion neurons (SGNs) were quantified. Impedances increased in all groups to a similar extent but this increase was delayed in the groups with an additional release of diclofenac or MM284. Using Poly-L-lactide (PLLA)-coated electrodes, the damage caused during insertion was much higher than without the coating. Only in these groups, connective tissue could extend to the apex of the cochlea. Despite this, numbers of SGNs were only reduced in PLLA and PLLA plus diclofenac groups. Even though the polymeric coating was not flexible enough, MM284 seems to especially have potential for further evaluation in connection with cochlear implantation.DOI: 10.3390/pharmaceutics15030726PMCID: PMC10058822",pubmed,36986587,10.3390/pharmaceutics15030726
development of cnnbased cochlear implant and normal hearing sound recognition models using natural and auralized environmental audio,"Restoration of auditory function among hearing impaired individuals using Cochlear Implant (CI) technology has contributed significantly towards an improved quality of life. CI users experience greater challenges in recognizing speech effectively in noisy, reverberant, or time-varying diverse environments. Most CI research efforts focus on enhancing speech perception and environmental sound awareness has received little or no attention. This study focuses on a comparative analysis of normal hearing (NH) vs. CI environmental sound recognition using classifiers trained on learned sound representations using a CNN-based sound event model. Sounds experienced by CI listeners are recreated by auralizing electrical stimuli. CCi-MOBILE is used to generate electrical stimuli and Braecker Vocoder is used for auralization. Natural and auralized sound representations are then applied in order to develop NH and CI sound recognition models. Comparative assessment of environmental sound recognition is carried out by analyzing f1-scores and other performance characteristics. Benefits stemming from this research can help CI researchers improve sound recognition performance, develop novel sound processing algorithms, exclusively for environmental sounds, and identify optimal CI electrical stimulation characteristics to enhance sound perception. Among CI users, improvement in environmental sound awareness contributes to improved quality of life. © 2021 IEEE.",scopus,2-s2.0-85103952046,10.1109/SLT48900.2021.9383550
differences in the temporal course of interaural time difference sensitivity between acoustic and electric hearing in amplitude modulated stimuli,"847. J Acoust Soc Am. 2017 Mar;141(3):1862. doi: 10.1121/1.4977014.Differences in the temporal course of interaural time difference sensitivity between acoustic and electric hearing in amplitude modulated stimuli.Hu H(1), Ewert SD(1), McAlpine D(2), Dietz M(1).Author information:(1)Medizinische Physik and Cluster of Excellence ""Hearing4all,"" Universität Oldenburg, D-26111 Oldenburg, Germany.(2)Department of Linguistics, Australian Hearing Hub, Macquarie University, New South Wales 2109, Australia.Previous studies have shown that normal-hearing (NH) listeners' spatial perception of non-stationary interaural time differences (ITDs) is dominated by the carrier ITD during rising amplitude segments. Here, ITD sensitivity throughout the amplitude-modulation cycle in NH listeners and bilateral cochlear implant (CI) subjects is compared, the latter by means of direct stimulation of a single electrode pair. The data indicate that, while NH listeners are most sensitive to ITDs applied toward the beginning of a modulation cycle at 600 Hz, NH listeners at 200 Hz and especially bilateral CI subjects at 200 pulses per second (pps) are more sensitive to ITDs applied to the modulation maximum. This has implications for spatial-hearing in complex environments: NH listeners' dominant 600-Hz ITD information from the rising amplitude segments comprises direct sound information. The 200-pps low rate required to get ITD sensitivity in CI users results in a higher weight of pulses later in the modulation cycle where the source ITDs are more likely corrupted by reflections. This indirectly indicates that even if future binaural CI processors are able to provide perceptually exploitable ITD information, CI users will likely not get the full benefit from such pulse-based ITD cues in reverberant and other complex environments.DOI: 10.1121/1.4977014",pubmed,28372072,10.1121/1.4977014
audioprofiledirected screening identifies novel mutations in kcnq4 causing hearing loss at the dfna2 locus,"364. Genet Med. 2008 Nov;10(11):797-804. doi: 10.1097/GIM.0b013e318187e106.Audioprofile-directed screening identifies novel mutations in KCNQ4 causing hearing loss at the DFNA2 locus.Hildebrand MS(1), Tack D, McMordie SJ, DeLuca A, Hur IA, Nishimura C, Huygen P, Casavant TL, Smith RJ.Author information:(1)Department of Otolaryngology - Head and Neck Surgery, University of Iowa, Iowa City, IA 52242, USA. michael-hildebrand@uiowa.eduPURPOSE: Gene identification in small families segregating autosomal dominant sensorineural hearing loss presents a significant challenge. To address this challenge, we have developed a machine learning-based software tool, AudioGene v2.0, to prioritize candidate genes for mutation screening based on audioprofiling.METHODS: We analyzed audiometric data from a cohort of American families with high-frequency autosomal dominant sensorineural hearing loss. Those families predicted to have a DFNA2 audioprofile by AudioGene v2.0 were screened for mutations in the KCNQ4 gene.RESULTS: Two novel missense mutations and a stop mutation were detected in three American families predicted to have DFNA2-related deafness for a positive predictive value of 6.3%. The false negative rate was 0%. The missense mutations were located in the channel pore region and the stop mutation was in transmembrane domain S5. The latter is the first DFNA2-causing stop mutation reported in KCNQ4.CONCLUSIONS: Our data suggest that the N-terminal end of the P-loop is crucial in maintaining the integrity of the KCNQ4 channel pore and AudioGene audioprofile analysis can effectively prioritize genes for mutation screening in small families segregating high-frequency autosomal dominant sensorineural hearing loss. AudioGene software will be made freely available to clinicians and researchers once it has been fully validated.DOI: 10.1097/GIM.0b013e318187e106PMCID: PMC3337550",pubmed,18941426,10.1097/GIM.0b013e318187e106
hypnotic induction of deafness to elementary sounds an electroencephalography casestudy and a proposed cognitive and neural scenario,"Hypnosis can be conceived as a unique opportunity to explore how top-down effects can influence various conscious and non-conscious processes. In the field of perception, such modulatory effects have been described in distinct sensory modalities. In the present study we focused on the auditory channel and aimed at creating a radical deafness to elementary sounds by a specific hypnotic suggestion. We report here a single case-study in a highly suggestible healthy volunteer who reported a total hypnotically suggested deafness. We recorded high-density scalp EEG during an auditory odd-ball paradigm before and after hypnotic deafness suggestion. While both early auditory event-related potentials to sounds (P1) and mismatch negativity component were not affected by hypnotic deafness, we observed a total disappearance of the late P3 complex component when the subject reported being deaf. Moreover, a centro-mesial positivity was present exclusively during the hypnotic condition prior to the P3 complex. Interestingly, source localization suggested an anterior cingulate cortex (ACC) origin of this neural event. Multivariate decoding analyses confirmed and specified these findings. Resting state analyses confirmed a similar level of conscious state in both conditions, and suggested a functional disconnection between auditory areas and other cortical areas. Taken together these results suggest the following plausible scenario: (i) preserved early processing of auditory information unaffected by hypnotic suggestion, (ii) conscious setting of an inhibitory process (ACC) preventing conscious access to sounds, (iii) functional disconnection between the modular and unconscious representations of sounds and global neuronal workspace. This single subject study presents several limits that are discussed and remains open to alternative interpretations. This original proof-of-concept paves the way to a larger study that will test the predictions stemming from our theoretical model and from this first report. Copyright © 2022 Munoz Musat, Rohaut, Sangare, Benhaiem and Naccache.",scopus,2-s2.0-85127951199,10.3389/fnins.2022.756651
improving emotion perception in cochlear implant users insights from machine learning analysis of eeg signals,"509. BMC Neurol. 2024 Apr 8;24(1):115. doi: 10.1186/s12883-024-03616-0.Improving emotion perception in cochlear implant users: insights from machine learning analysis of EEG signals.Paquette S(1)(2)(3), Gouin S(4)(5), Lehmann A(6)(4)(5).Author information:(1)Psychology Department, Faculty of Arts and Science, Trent University, Peterborough, ON, Canada. sebastienpaquette@trentu.ca.(2)Research Institute of the McGill University Health Centre (RI-MUHC), Montreal, QC, Canada. sebastienpaquette@trentu.ca.(3)Centre for Research On Brain, Language, and Music (CRBLM), International Laboratory for Brain, Music & Sound Research (BRAMS), Psychology Department, University of Montreal, Montreal, QC, Canada. sebastienpaquette@trentu.ca.(4)Centre for Research On Brain, Language, and Music (CRBLM), International Laboratory for Brain, Music & Sound Research (BRAMS), Psychology Department, University of Montreal, Montreal, QC, Canada.(5)Faculty of Medicine and Health Sciences, Department of Otolaryngology-Head and Neck Surgery, McGill University, Montreal, QC, Canada.(6)Research Institute of the McGill University Health Centre (RI-MUHC), Montreal, QC, Canada.BACKGROUND: Although cochlear implants can restore auditory inputs to deafferented auditory cortices, the quality of the sound signal transmitted to the brain is severely degraded, limiting functional outcomes in terms of speech perception and emotion perception. The latter deficit negatively impacts cochlear implant users' social integration and quality of life; however, emotion perception is not currently part of rehabilitation. Developing rehabilitation programs incorporating emotional cognition requires a deeper understanding of cochlear implant users' residual emotion perception abilities.METHODS: To identify the neural underpinnings of these residual abilities, we investigated whether machine learning techniques could be used to identify emotion-specific patterns of neural activity in cochlear implant users. Using existing electroencephalography data from 22 cochlear implant users, we employed a random forest classifier to establish if we could model and subsequently predict from participants' brain responses the auditory emotions (vocal and musical) presented to them.RESULTS: Our findings suggest that consistent emotion-specific biomarkers exist in cochlear implant users, which could be used to develop effective rehabilitation programs incorporating emotion perception training.CONCLUSIONS: This study highlights the potential of machine learning techniques to improve outcomes for cochlear implant users, particularly in terms of emotion perception.© 2024. The Author(s).DOI: 10.1186/s12883-024-03616-0PMCID: PMC11000345",pubmed,38589815,10.1186/s12883-024-03616-0
analysis of mir376 rna cluster members in the mouse inner ear,"778. Int J Exp Pathol. 2012 Dec;93(6):450-7. doi: 10.1111/j.1365-2613.2012.00840.x.Analysis of miR-376 RNA cluster members in the mouse inner ear.Yan D(1), Xing Y, Ouyang X, Zhu J, Chen ZY, Lang H, Liu XZ.Author information:(1)Department of Otolaryngology, University of Miami, Miami, FL 33136, USA.Mutations in phosphoribosyl pyrophosphate synthetase 1 (PRPS1) are associated with a spectrum of non-syndromic to syndromic hearing loss. PRPS1 transcript levels have been shown to be regulated by the microRNA-376 genes. The long primary RNA transcript of the miR-376 RNA cluster members undergo extensive and simultaneous A → I editing at one or both of two specific sites (+4 and +44) in particular human and mouse tissues. The PRPS1 gene, which contains target sites for the edited version of miR-376a-5p within its 3'UTR, has been shown to be repressed in a tissue-specific manner. To investigate whether the transcription of Prps1 is regulated by miR-376 cluster members in the mouse inner ear, we first quantified the expression of the mature miR-376 RNAs by quantitative real-time-PCR. The spatio-temporal patterns of miR-376 expression were assessed by in situ hybridization. Finally, we examined whether A →I editing of pri-miR-376 RNAs occurs in mouse inner ear by direct sequencing. Our data showed that the miR-376a-3p, b-3p, c-3p are present in mouse embryonic inner ears and intensive expression of miR-376a-3p/b-3p was detected in the sensory epithelia and ganglia of both auditory and vestibular portions of the inner ear. In adult inner ear, the expression of miR-376a-3p/b-3p is restricted within ganglion neurons of auditory and vestibular systems as well as the cells in the stria vascularis. Only unedited pri-miR-376 RNAs were detected in the cochlea suggesting that the activity of PRPS1 in the inner ear may not be regulated through the editing of miR-376 cluster.© 2012 The Authors. International Journal of Experimental Pathology © 2012 International Journal of Experimental Pathology.DOI: 10.1111/j.1365-2613.2012.00840.xPMCID: PMC3521901",pubmed,23136997,10.1111/j.1365-2613.2012.00840.x
a short splice form of xinactin binding repeat containing 2 xirp2 lacking the xin repeats is required for maintenance of stereocilia morphology and hearing function,"615. J Neurosci. 2015 Feb 4;35(5):1999-2014. doi: 10.1523/JNEUROSCI.3449-14.2015.A short splice form of Xin-actin binding repeat containing 2 (XIRP2) lacking the Xin repeats is required for maintenance of stereocilia morphology and hearing function.Francis SP(1), Krey JF(2), Krystofiak ES(3), Cui R(3), Nanda S(1), Xu W(4), Kachar B(3), Barr-Gillespie PG(2), Shin JB(5).Author information:(1)Departments of Neuroscience.(2)Oregon Hearing Research Center & Vollum Institute, Oregon Health & Science University, Portland, Oregon 97239, and.(3)National Institute for Deafness and Communications Disorders, National Institute of Health, Bethesda, Maryland 20892.(4)Gene Targeting and Transgenic Facility, University of Virginia, Charlottesville, Virginia 22908.(5)Departments of Neuroscience, Brain, Immunology, and Glia Center, and js2ee@virginia.edu.Approximately one-third of known deafness genes encode proteins located in the hair bundle, the sensory hair cell's mechanoreceptive organelle. In previous studies, we used mass spectrometry to characterize the hair bundle's proteome, resulting in the discovery of novel bundle proteins. One such protein is Xin-actin binding repeat containing 2 (XIRP2), an actin-cross-linking protein previously reported to be specifically expressed in striated muscle. Because mutations in other actin-cross-linkers result in hearing loss, we investigated the role of XIRP2 in hearing function. In the inner ear, XIRP2 is specifically expressed in hair cells, colocalizing with actin-rich structures in bundles, the underlying cuticular plate, and the circumferential actin belt. Analysis using peptide mass spectrometry revealed that the bundle harbors a previously uncharacterized XIRP2 splice variant, suggesting XIRP2's role in the hair cell differs significantly from that reported in myocytes. To determine the role of XIRP2 in hearing, we applied clustered regularly interspaced short palindromic repeat (CRISPR)/Cas9-mediated genome-editing technology to induce targeted mutations into the mouse Xirp2 gene, resulting in the elimination of XIRP2 protein expression in the inner ear. Functional analysis of hearing in the resulting Xirp2-null mice revealed high-frequency hearing loss, and ultrastructural scanning electron microscopy analyses of hair cells demonstrated stereocilia degeneration in these mice. We thus conclude that XIRP2 is required for long-term maintenance of hair cell stereocilia, and that its dysfunction causes hearing loss in the mouse.Copyright © 2015 the authors 0270-6474/15/351999-16$15.00/0.DOI: 10.1523/JNEUROSCI.3449-14.2015PMCID: PMC4315831",pubmed,25653358,10.1523/JNEUROSCI.3449-14.2015
functional magnetic resonance imaging of enhanced central auditory gain and electrophysiological correlates in a behavioral model of hyperacusis,"791. Hear Res. 2020 Apr;389:107908. doi: 10.1016/j.heares.2020.107908. Epub 2020 Feb 6.Functional magnetic resonance imaging of enhanced central auditory gain and electrophysiological correlates in a behavioral model of hyperacusis.Wong E(1), Radziwon K(2), Chen GD(2), Liu X(2), Manno FA(3), Manno SH(4), Auerbach B(2), Wu EX(5), Salvi R(6), Lau C(7).Author information:(1)Department of Physics, City University of Hong Kong, Hong Kong, China; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, China; Laboratory of Biomedical Imaging and Signal Processing, The University of Hong Kong, Hong Kong, China.(2)Center for Hearing & Deafness, Department of Communicative Disorders and Sciences, SUNY at Buffalo, 137 Cary Hall, Buffalo, NY, 14214, USA.(3)Department of Physics, City University of Hong Kong, Hong Kong, China; School of Biomedical Engineering, University of Sydney, Sydney, New South Wales, Australia.(4)Department of Biomedical Sciences, City University of Hong Kong, Hong Kong, China.(5)Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, China; Laboratory of Biomedical Imaging and Signal Processing, The University of Hong Kong, Hong Kong, China.(6)Center for Hearing & Deafness, Department of Communicative Disorders and Sciences, SUNY at Buffalo, 137 Cary Hall, Buffalo, NY, 14214, USA; Department of Audiology and Speech-Language Pathology, Asia University, Taichung, Taiwan, ROC. Electronic address: salvi@buffalo.edu.(7)Department of Physics, City University of Hong Kong, Hong Kong, China. Electronic address: condon.lau@cityu.edu.hk.Hyperacusis is a debilitating hearing condition in which normal everyday sounds are perceived as exceedingly loud, annoying, aversive or even painful. The prevalence of hyperacusis approaches 10%, making it an important, but understudied medical condition. To noninvasively identify the neural correlates of hyperacusis in an animal model, we used sound-evoked functional magnetic resonance imaging (fMRI) to locate regions of abnormal activity in the central nervous system of rats with behavioral evidence of hyperacusis induced with an ototoxic drug (sodium salicylate, 250 mg/kg, i.p.). Reaction time-intensity measures of loudness-growth revealed behavioral evidence of salicylate-induced hyperacusis at high intensities. fMRI revealed significantly enhanced sound-evoked responses in the auditory cortex (AC) to 80 dB SPL tone bursts presented at 8 and 16 kHz. Sound-evoked responses in the inferior colliculus (IC) were also enhanced, but to a lesser extent. To confirm the main results, electrophysiological recordings of spike discharges from multi-unit clusters were obtained from the central auditory pathway. Salicylate significantly enhanced tone-evoked spike-discharges from multi-unit clusters in the AC from 4 to 30 kHz at intensities ≥60 dB SPL; less enhancement occurred in the medial geniculate body (MGB), and even less in the IC. Our results demonstrate for the first time that non-invasive sound-evoked fMRI can be used to identify regions of neural hyperactivity throughout the brain in an animal model of hyperacusis.Copyright © 2020 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2020.107908PMCID: PMC7080601",pubmed,32062293,10.1016/j.heares.2020.107908
modeling the interaction of phonemic intelligibility and lexical structure in audiovisual word recognitiondp   oct 1998,"Studies of audiovisual perception of spoken language have mostly modeled phoneme identification in nonsense syllables, but it is doubtful that models or theories of phonetic processing can adequately account for audiovisual word recognition. The present study took a computational approach to examine how lexical structure may additionally constrain word recognition, given the phonetic information available under vocoded audio, visual, and audiovisual stimulus conditions. Adults with normal hearing (18-45 yrs old) made phonemic identification judgments on recordings of spoken nonsense syllables. Deaf Ss participated in a consonant identification task. Hierarchical cluster analysis was used 1st to select classes of perceptually equivalent phonemes for each of the stimulus conditions, and then a machine-readable phonemically transcribed lexicon was re-transcribed in terms of these phonemic equivalence classes. For each of the transcriptions computations were made of percent information extracted, percent words unique, and expected class size. The findings suggest that superadditive levels of audiovisual enhancement are more likely for monosyllabic than for multisyllabic words. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc3&DO=10.1016%2fS0167-6393%252898%252900049-1
investigation on noise exposure level and health status of workers in transportation equipment manufacturing industry ,"Objective To explore the noise exposure level and the health status of workers in transportation equipment manufacturing industry, and provide a scientific basis for guidance and implementation of intervention measures. Methods From January to December in 2019, a total of 2088 noise workers from a large enterprise were selected by cluster sampling method in railway transportation equipment manufacturing, automobile manufacturing and aerospace aircraft manufacturing enterprises. The worker's noise exposure level was detected. Occupatio nal health checkups were performed on the noise workers including electrical audiometry, blood pressure and electrocardiogram. χ2 test and trend χ2 test were used to analyze the data. Results The noise exposure level of 66.9%(1396/2088) workers exceeded 85 dB(A), and the median noise level was 87.9(84.3-90.3) dB(A). Among them, workers of railway transportation equipment manufacturing enterprises had the highest noise exposure level[89.9(87.8-91.6) dB(A)]. The detection rate of high鄄frequency hearing loss, abnormal blood pressure and abnormal electrocardiogram of noise workers were 15.7% (327/ 2088),18.1% (378/2088) and 6.1% (128/2088), respectively. The differences in the detection rates of high鄄frequency hearing loss, abnormal blood pressure, and abnormal electrocardiogram in workers of railway transportation equipment manufacturing enterprises, automobile manufacturing enterprises, and aerospace manufacturing enterprises were statistically significant (P<0.05). Workers of railway transportation equipment manufacturing enterprises had higher detection rates of high鄄frequency hearing loss(17.6%, 186/1056). Workers of aerospace manufacturing enterprises had higher detection rates of abnormal blood pressure and abnormal electrocardiogram (26.3%, 169/642;10.0%, 64/642). The differences in the detection rates of high鄄frequency hearing loss, abnormal blood pressure and abnormal electrocardiogram of noise workers were statistically significant in different age and working age groups, and gradually increased with age and working age (P< 0.05). The difference in the detection rate of high鄄frequency hearing loss of noise workers was statistically significant in different noise intensity groups, and the overall trend was increasing(P<0.05). Conclusion The transportation equipment manufacturing industry has serious noise hazards, especially the railway transportation equipment manufacturing industry. Long鄄term occupational noise exposure can adversely affect workers' hearing and cardiovascular system. Enterprises should strengthen occupational health inspections, and at the same time, take personal protective measures to protect the health of workers. © 2019 Zhonghua er ke za zhi / Chinese Journal of Pediatrics.",scopus,2-s2.0-85113729857,10.3760/cma.j.cn121094-20200513-00258
english to indian sign language gloss structure translation using sequence to sequence model,"A significant portion of the deaf and hard of hearing population in India uses Indian Sign Language (ISL) for communication. However, they frequently encounter significant communication and educational challenges as a result of the lack of ISL-related resources and technology. In this work, we aim to create a system for translating English text to ISL gloss structure. It can be viewed as a significant step towards ISL generation from normal text. The proposed method uses a seq2seq BiLSTM encoder-LSTM decoder based approach for performing the grammatical transformation of sentences to convert them from English to ISL gloss structure. The evaluation of the system, using BLEU score and ROUGE-L F1 score yielded good results for different batch sizes.",ieee,,10.1109/ICSCC59169.2023.10335040
learning from modelbased approaches for hearing loss compensation  which speech features are enhanced in machinelearningbased audio algorithms for cochlear synaptopathy compensation,,base,49362aad4b252cf7ba38edc766634a5a6598a416f2feee26704d5db029e17983,
musictofacial expressions emotionbased music visualization for the hearing impaired,"While music is made to convey messages and emotions, auditory music is not equally accessible to everyone. Music visualization is a common approach to augment the listening experiences of the hearing users and to provide music experiences for the hearing-impaired. In this paper, we present a music visualization system that can turn the input of a piece of music into a series of facial expressions representative of the continuously changing sentiments in the music. The resulting facial expressions, recorded as action units, can later animate a static virtual avatar to be emotive synchronously with the music. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",scopus,2-s2.0-85168248690,
reduced resting state functional connectivity with increasing agerelated hearing loss and mcgurk susceptibility,"126. Sci Rep. 2020 Oct 12;10(1):16987. doi: 10.1038/s41598-020-74012-0.Reduced resting state functional connectivity with increasing age-related hearing loss and McGurk susceptibility.Schulte A(1), Thiel CM(2)(3), Gieseler A(4)(5), Tahden M(4)(5), Colonius H(4)(5), Rosemann S(1)(4).Author information:(1)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl-Von-Ossietzky Universität Oldenburg, Ammerländer Heerstraße 114-118, 26111, Oldenburg, Germany.(2)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl-Von-Ossietzky Universität Oldenburg, Ammerländer Heerstraße 114-118, 26111, Oldenburg, Germany. christiane.thiel@uni-oldenburg.de.(3)Cluster of Excellence ""Hearing4all"", Carl Von Ossietzky Universität Oldenburg, Oldenburg, Germany. christiane.thiel@uni-oldenburg.de.(4)Cluster of Excellence ""Hearing4all"", Carl Von Ossietzky Universität Oldenburg, Oldenburg, Germany.(5)Cognitive Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl-Von-Ossietzky Universität Oldenburg, Oldenburg, Germany.Age-related hearing loss has been related to a compensatory increase in audio-visual integration and neural reorganization including alterations in functional resting state connectivity. How these two changes are linked in elderly listeners is unclear. The current study explored modulatory effects of hearing thresholds and audio-visual integration on resting state functional connectivity. We analysed a large set of resting state data of 65 elderly participants with a widely varying degree of untreated hearing loss. Audio-visual integration, as gauged with the McGurk effect, increased with progressing hearing thresholds. On the neural level, McGurk illusions were negatively related to functional coupling between motor and auditory regions. Similarly, connectivity of the dorsal attention network to sensorimotor and primary motor cortices was reduced with increasing hearing loss. The same effect was obtained for connectivity between the salience network and visual cortex. Our findings suggest that with progressing untreated age-related hearing loss, functional coupling at rest declines, affecting connectivity of brain networks and areas associated with attentional, visual, sensorimotor and motor processes. Especially connectivity reductions between auditory and motor areas were related to stronger audio-visual integration found with increasing hearing loss.DOI: 10.1038/s41598-020-74012-0PMCID: PMC7550565",pubmed,33046800,10.1038/s41598-020-74012-0
frequency map variations in squirrel monkey primary auditory cortex,"766. Laryngoscope. 2005 Jul;115(7):1136-44. doi: 10.1097/01.MLG.0000165369.65046.CD.Frequency map variations in squirrel monkey primary auditory cortex.Cheung SW(1).Author information:(1)Coleman Memorial Laboratory and W. M. Keck Center for Integrative Neuroscience, Division of Otology, Neurotology and Skull Base Surgery, University of California, San Francisco, California 94143, USA. scheung@ohns.ucsf.eduOBJECTIVE: The goal of this work is to understand the neural basis for cortical representation of hearing in highly vocal primates to gain insights into the substrates for communication. Variation patterns in frequency representation among animals are incorporated into an explanatory model to reconcile heterogeneous observations.STUDY DESIGN: Prospective.METHODS: Thirty-four squirrel monkeys underwent microelectrode mapping experiments in primary auditory cortex (AI) using tone pip stimuli. Characteristic frequency (CF) was extracted from the excitatory frequency receptive field. Frequency maps were reconstructed using Voronoi-Dirichlet tessellation. The spatial locations (rostral vs. caudal) of highest CF isofrequency contours (minimum length 1 mm) and highest CF neuronal clusters on the temporal gyral surface were analyzed.RESULTS: Isofrequency contours at least 1 mm long with CFs greater than 2.9 kHz (75% cases) are accessible on the temporal gyrus. Variability of the highest CF isofrequency contours accessible on the temporal gyrus has an interquartile range from 2.9 to 5.1 (mean 4.3) kHz. The highest CF isofrequency contours are located mainly in rostral AI, whereas the highest CF neuronal clusters flanking fully expressed isofrequency contours are equally distributed in rostral and caudal locations.CONCLUSIONS: Squirrel monkey AI frequency map variations are sizeable across animals and small within single animals (interhemispheric comparison). AI frequency map variations, modeled as translations and rotations relative to the lateral sulcus, are independent transfers. Caution must be exercised when interpreting nominal frequency map changes that are attributed to hearing loss and auditory learning effects.DOI: 10.1097/01.MLG.0000165369.65046.CD",pubmed,15995498,10.1097/01.MLG.0000165369.65046.CD
acoustic startle modification as a tool for evaluating auditory function of the mouse progress pitfalls and potential,"571. Neurosci Biobehav Rev. 2017 Jun;77:194-208. doi: 10.1016/j.neubiorev.2017.03.009. Epub 2017 Mar 19.Acoustic startle modification as a tool for evaluating auditory function of the mouse: Progress, pitfalls, and potential.Lauer AM(1), Behrens D(2), Klump G(2).Author information:(1)Department of Otolaryngology-Head and Neck Surgery and Center for Hearing and Balance, Johns Hopkins University, 515 Traylor Building, 720 Rutland Ave., Baltimore, MD 21205, USA. Electronic address: alauer2@jhmi.edu.(2)Cluster of Excellence Hearing4all, Animal Physiology & Behavior Group, Department for Neuroscience, School of Medicine and Health Sciences, Carl Von Ossietzky University Oldenburg, Carl Von Ossietzky Str. 9-11, 26111 Oldenburg, Germany.Acoustic startle response (ASR) modification procedures, especially prepulse inhibition (PPI), are increasingly used as behavioral measures of auditory processing and sensorimotor gating in rodents due to their perceived ease of implementation and short testing times. In practice, ASR and PPI procedures are extremely variable across animals, experimental setups, and studies, and the interpretation of results is subject to numerous caveats and confounding influences. We review considerations for modification of the ASR using acoustic stimuli, and we compare the sensitivity of PPI procedures to more traditional operant psychoacoustic techniques. We also discuss non-auditory variables that must be considered. We conclude that ASR and PPI measures cannot substitute for traditional operant techniques due to their low sensitivity. Additionally, a substantial amount of pilot testing must be performed to properly optimize an ASR modification experiment, negating any time benefit over operant conditioning. Nevertheless, there are some circumstances where ASR measures may be the only option for assessing auditory behavior, such as when testing mouse strains with early-onset hearing loss or learning impairments.Copyright © 2017 Elsevier Ltd. All rights reserved.DOI: 10.1016/j.neubiorev.2017.03.009PMCID: PMC5446932",pubmed,28327385,10.1016/j.neubiorev.2017.03.009
early prediction of neonatal jaundice using artificial intelligence techniques,"Jaundice in newborns is a prevalent problem all over the world. This syndrome can induce brain damage and kernicterus, which is characterized by repeated and uncontrollable movements, an upward gaze, and hearing loss. As a result, early detection and treatment can prevent long-term harm. As a result, in this study, we have investigated several researchers' strategies for detecting jaundice among newborn babies using various artificial intelligence-based techniques. We have also drawn some findings based on our analysis of the multiple AI techniques. In addition, the report highlighted their accomplishments and the challenges they have faced in this field.",ieee,,10.1109/ICIPTM54933.2022.9753884
sensorineural hearing loss in nasopharyngeal carcinoma survivors in the modern treatment era  the early and late effects of radiation and cisplatin,"548. Clin Oncol (R Coll Radiol). 2022 Apr;34(4):e160-e167. doi: 10.1016/j.clon.2021.10.013. Epub 2021 Nov 10.Sensorineural Hearing Loss in Nasopharyngeal Carcinoma Survivors in the Modern Treatment Era - The Early and Late Effects of Radiation and Cisplatin.Yip PL(1), Mok KCJ(2), Ho HS(1), Lee WYV(1), Wong ACL(2), Lau CT(1), Wong FCS(1), Yeung KW(2), Lee SF(3).Author information:(1)Department of Clinical Oncology, Tuen Mun Hospital, New Territories West Cluster, Hospital Authority, Hong Kong.(2)Department of Otorhinolaryngology, Tuen Mun Hospital, New Territories West Cluster, Hospital Authority, Hong Kong.(3)Department of Clinical Oncology, Tuen Mun Hospital, New Territories West Cluster, Hospital Authority, Hong Kong; Department of Clinical Oncology, University of Hong Kong, Hong Kong. Electronic address: leesf@hku.hk.AIMS: Hearing loss is a common debilitating complication in nasopharyngeal carcinoma (NPC) survivors. The aim of the present study was to investigate the impact of inner ear/cochlear radiation dose and cisplatin use on early and late sensorineural hearing loss (SNHL) in NPC patients treated with radiotherapy alone, concurrent chemoradiation (cCRT) and induction chemotherapy followed by cCRT (iCRT) in the intensity-modulated radiotherapy era.MATERIALS AND METHODS: The study included 81 NPC patients treated with intensity-modulated radiotherapy between 2014 and 2016. Pure tone audiometry was carried out at baseline and follow-up. The effects of cochlear/inner ear radiation and cisplatin doses on early (<12 months) and late (≥24 months) SNHL were analysed using multivariable regression after adjusting for important predictors.RESULTS: In total, 156 ears were examined. In early SNHL (n = 136), cisplatin use predicted the incidence of early high-frequency SHNL (HF-SNHL) (odds ratio 6.4, 95% confidence interval 1.7-23.9, P = 0.005). Ninety ears were analysed for late SNHL (median follow-up 38 months). Inner ear/cochlear radiation and cisplatin doses and better pre-treatment hearing were independent predictors of threshold change at 4 kHz. Every 10 Gy increase in inner ear/cochlear Dmean resulted in 5-dB and 6-dB threshold changes, respectively (cochlear Dmean: B = 0.005, 95% confidence interval 0.0004-0.009, P = 0.031; inner ear Dmean: B = 0.006, 95% confidence interval 0.001-0.010, P = 0.014). Cisplatin use was associated with late HF-SNHL (odds ratio 3.74, 95% confidence interval 1.1-12.3, P = 0.031). In the cCRT and iCRT subgroups, no cisplatin dose-dependent ototoxicity was observed. Severe (≥30 dB) late HF-SNHL occurred in 14% and 25% of the patients when the cochlear dose constraints were 40 Gy and 44 Gy, respectively. The radiotherapy-alone group did not develop severe late HF-SNHL.CONCLUSION: Cochlear/inner ear radiation dose and cisplatin use showed differential and independent ototoxicity in early and late SNHL. As cochlear/inner ear dose-dependent ototoxicity was demonstrated, the cochlear dose constraint should be as low as reasonably achievable, especially when cisplatin is also administered.Copyright © 2021 The Royal College of Radiologists. Published by Elsevier Ltd. All rights reserved.DOI: 10.1016/j.clon.2021.10.013",pubmed,34772581,10.1016/j.clon.2021.10.013
the generation of zebrafish mariner model using the crisprcas9 system,"Targeted genome editing mediated by clustered, regularly interspaced, short palindromic repeat (CRISPR)/CRISPR-associated nuclease 9 (Cas9) technology has emerged as a powerful tool for gene function studies and has great potential for gene therapy. Although CRISPR/Cas9 has been widely used in many research fields, only a few successful zebrafish models have been established using this technology in hearing research. In this study, we successfully created zebrafish mariner mutants by targeting the motor head domain of Myo7aa using CRISPR/Cas9. The CRISPR/Cas9-generated mutants showed unbalanced swimming behavior and disorganized sterocilia of inner ear hair cells, which resemble the phenotype of the zebrafish mariner mutants. In addition, we found that CRISPR/Cas9-generated mutants have reduced number of stereociliary bundles of inner ear hair cells and have significant hearing loss. Furthermore, phenotypic analysis was performed on F0 larvae within the first week post fertilization, which dramatically shortens data collection period. Therefore, results of this study showed that CRISPR/Cas9 is a quick and effective method to generate zebrafish mutants as a model for studying human genetic deafness. Anat Rec, 303:556–562, 2020. © 2019 American Association for Anatomy. © 2019 Wiley Periodicals, Inc.",scopus,2-s2.0-85069924617,10.1002/ar.24221
morphological development of the human cochlear nucleus,"846. Hear Res. 2019 Oct;382:107784. doi: 10.1016/j.heares.2019.107784. Epub 2019 Aug 20.Morphological development of the human cochlear nucleus.Saini S(1), Kaur C(1), Pal I(1), Kumar P(1), Jacob TG(1), Thakar A(2), Roy KK(3), Roy TS(4).Author information:(1)Department of Anatomy, All India Institute of Medical Sciences, New Delhi, 110029, India.(2)Department of Otorhinolaryngology, All India Institute of Medical Sciences, New Delhi, 110029, India.(3)Department of Obstetrics and Gynecology, All India Institute of Medical Sciences, New Delhi, 110029, India.(4)Department of Anatomy, All India Institute of Medical Sciences, New Delhi, 110029, India. Electronic address: tarasankar@hotmail.com.Morphological studies in developing brain determine critical periods of proliferation, neurogenesis, gliogenesis, and apoptosis. During these periods both intrinsic and extrinsic pathological factors can hamper development. These time points are not available for the human cochlear nucleus (CN). We have used design-based stereology and determined that 18-22 weeks of gestation (WG) are critical in the development of the human CN. Twenty-three fetuses and seven postnatal brainstems were processed for cresyl violet (CV) staining and immunoexpression of NeuN (neurons), GFAP (astrocytes), Ki-67 (proliferation) and TUNEL (apoptosis) and 3-D reconstruction. The volume of CN, total number of neurons selected profiles and the volume of neurons and their nuclei were estimated. Data were grouped (G) into: G1:18-20 WG, G2: 21-24 WG, G3: 25-28 WG and G4 >29 WG. The dimensions of morphologically identified neurons were also measured. The CN primordium was first identifiable at 10WG. Definitive DCN (Dorsal cochlear nucleus) and VCN (ventral cochlear nucleus) were identifiable at 16 WG. There was a sudden growth spurt in total volume of CN, number of neurons and astrocytes from 18 WG. We also observed an increase in proliferation and apoptosis after 22 WG. The number of neurons identifiable by CV was significantly lower than that by NeuN-immunostaining till 25 WG (p = 0.020), after which, both methods were equivalent. Eight morphological types of neurons were identifiable by 26 WG and could be resolved into four clusters by volume and diameter. The CN changed orientation from small, flat and horizontal at 10-16 WG to larger and oblique from 18WG onwards. Prevention of exposure to noxious factors at 18-22 WG may be important in preventing congenital deafness.Copyright © 2019 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2019.107784",pubmed,31522073,10.1016/j.heares.2019.107784
oral antioxidant vitamins and magnesium limit noiseinduced hearing loss by promoting sensory hair cell survival role of antioxidant enzymes and apoptosis genes,"686. Antioxidants (Basel). 2020 Nov 25;9(12):1177. doi: 10.3390/antiox9121177.Oral Antioxidant Vitamins and Magnesium Limit Noise-Induced Hearing Loss by Promoting Sensory Hair Cell Survival: Role of Antioxidant Enzymes and Apoptosis Genes.Alvarado JC(1), Fuentes-Santamaría V(1), Melgar-Rojas P(1), Gabaldón-Ull MC(1), Cabanes-Sanchis JJ(1), Juiz JM(1)(2).Author information:(1)Instituto de Investigación en Discapacidades Neurológicas (IDINE), School of Medicine, Universidad de Castilla-La Mancha, 02008 Albacete, Spain.(2)Department of Otolaryngology, Hannover Medical School, NIFE-VIANNA, Cluster of Excellence Hearing4all-German Research Foundation, 30625 Hannover, Germany.Noise induces oxidative stress in the cochlea followed by sensory cell death and hearing loss. The proof of principle that injections of antioxidant vitamins and Mg2+ prevent noise-induced hearing loss (NIHL) has been established. However, effectiveness of oral administration remains controversial and otoprotection mechanisms are unclear. Using auditory evoked potentials, quantitative PCR, and immunocytochemistry, we explored effects of oral administration of vitamins A, C, E, and Mg2+ (ACEMg) on auditory function and sensory cell survival following NIHL in rats. Oral ACEMg reduced auditory thresholds shifts after NIHL. Improved auditory function correlated with increased survival of sensory outer hair cells. In parallel, oral ACEMg modulated the expression timeline of antioxidant enzymes in the cochlea after NIHL. There was increased expression of glutathione peroxidase-1 and catalase at 1 and 10 days, respectively. Also, pro-apoptotic caspase-3 and Bax levels were diminished in ACEMg-treated rats, at 10 and 30 days, respectively, following noise overstimulation, whereas, at day 10 after noise exposure, the levels of anti-apoptotic Bcl-2, were significantly increased. Therefore, oral ACEMg improves auditory function by limiting sensory hair cell death in the auditory receptor following NIHL. Regulation of the expression of antioxidant enzymes and apoptosis-related proteins in cochlear structures is involved in such an otoprotective mechanism.DOI: 10.3390/antiox9121177PMCID: PMC7761130",pubmed,33255728,10.3390/antiox9121177
innovative artificial intelligence approach for hearingloss symptoms identification model using machine learning techniques,,base,06fe4d8387ec01609152b04cdfc03887f18094b39aeedf0d3f45812e7a44c757,
a novel mitochondrial dna missense mutation at g3421a in a family with maternally inherited diabetes and deafness,"Objective: Mutations in mtDNA are thought to be responsible for the pathogenesis of maternally inherited diabetes. Here, we report a family with maternally inherited diabetes and deafness whose members did not harbour the mtDNA A3243G mutation, the most frequent point mutation in mitochondrial diabetic patients. This study aimed to investigate a possible other mtDNA mutation and its prevalence in type 2 diabetic patients. Methods: Height, body weight, waistline, and hip circumference were measured and serum biochemical marks determined in all members of the family. In addition, a 75 g oral glucose tolerance test and electric listening test were conducted in these members. Genomic DNA was prepared from peripheral leukocytes. Direct sequencing of PCR products was used to detect the mtDNA mutation in this family. The prevalence of mtDNA G3421A nucleotide substitutions was investigated by restriction fragment length polymorphism analysis in 1350 unrelated type 2 diabetic patients recruited by random cluster sampling from the central city area of Shanghai, China. Results: (1) A new missense homoplasmic mutation of mtDNA G3421A was found in a maternally inherited diabetic family and existed neither in 1350 unrelated type 2 diabetic patients nor in 50 non-diabetic individuals. (2) The mode of mutation and diabetes transmission was typical maternal inheritance in this family. (3) All diabetic family members were found to have an onset at 35-42 years of age, accompanied by deafness of varying degrees. Conclusion: mtDNA G3421A (Val39Ile) found in a family with maternally inherited diabetes and deafness is a novel missense mutation. Whether this is a diabetogenic mutation and its effect on mitochondrial function needs to be further studied. © 2006 Elsevier B.V. All rights reserved.",scopus,2-s2.0-33750608223,10.1016/j.mrfmmm.2006.07.006
verbal fluency in adults with postlingually acquired hearing impairment,"This study examined verbal retrieval in participants with acquired moderate-to-severe sensorineural hearing impairment (M age = 63, M education level = 13 years) compared to participants with normal hearing thresholds (M age = 62, M education level = 14 years) using the letter and category fluency tasks. Analyses of number of words produced, clustering, and switching, were conducted. There was no significant difference between the groups in category fluency performance. In letter fluency, however, the participants with hearing impairment produced significantly fewer words than the normal hearing participants and their production was characterized by fewer switches. Regression analyses were conducted to examine the relationship between demographic, auditory, and cognitive variables and letter fluency performance in the two groups. Phonological skills and auditory acuity predicted letter fluency output only in participants with hearing impairment and a hearing-related link between phonological skills, working memory capacity, and letter fluency switching was found. © W. S. Maney & Son Ltd 2014.",scopus,2-s2.0-84928158722,10.1179/205057113X13781290153457
meningeal carcinomatosis secondary to neurenteric cysts with malignant transformation a case report,"Background: Meningeal carcinomatosis is mainly associated with breast cancer, lung cancer, and melanoma. However, meningeal carcinomatosis secondary to a neurenteric cyst with malignant features is extremely rare. Case presentation: We report the case of a 35-year-old woman who was admitted to the hospital with a 10-month history of headache, 6-month history of diplopia, 4-month history of hearing loss, and 1-month history of back pain, suggesting a diagnosis of chronic meningitis. Notably, enhanced brain and spinal cord magnetic resonance imaging (MRI) revealed extensive lesions with enhancement signals in the pia mater of the pons and cervical, thoracic, and lumbar spinal cord. The cerebral spinal fluid profile showed that pressure was significantly elevated, with a slight increase in leukocytes that mostly comprised mononuclear cells and decreased glucose concentration. Cytology evaluation showed a small cluster of atypical nuclei, which were suspected to be tumor cells arising from the epithelium. However, no primary tumor was found through comprehensive body and skin screening. After a histopathological biopsy of subarachnoid meninx of the thoracic spinal canal, the cause of meningeal carcinomatosis of this patient was determined as neurenteric cysts with malignant features, which is extremely rare. Conclusion: This is the first case to ever report neurenteric cysts as a cause of leptomeningeal carcinomatosis and the first ever report of neurenteric cysts presenting as leptomeningeal carcinomatosis without typical cyst visible on brain MRI. This extremely rare case provided a novel view on the pathogenesis of meningeal carcinomatosis and clinical presentation of neurenteric cysts, highlighting the value of meningeal biopsy in chronic meningitis of unknown causes. © 2022, The Author(s).",scopus,2-s2.0-85142038056,10.1186/s12883-022-02978-7
acoustic monitoring of professionally managed marine mammals for health and welfare insights,"Research evaluating marine mammal welfare and opportunities for advancements in the care of species housed in a professional facility have rapidly increased in the past decade. While topics, such as comfortable housing, adequate social opportunities, stimulating enrichment, and a high standard of medical care, have continued to receive attention from managers and scientists, there is a lack of established acoustic consideration for monitoring the welfare of these animals. Marine mammals rely on sound production and reception for navigation and communication. Regulations governing anthropogenic sound production in our oceans have been put in place by many countries around the world, largely based on the results of research with managed and trained animals, due to the potential negative impacts that unrestricted noise can have on marine mammals. However, there has not been an established best practice for the acoustic welfare monitoring of marine mammals in professional care. By monitoring animal hearing and vocal behavior, a more holistic view of animal welfare can be achieved through the early detection of anthropogenic sound sources, the acoustic behavior of the animals, and even the features of the calls. In this review, the practice of monitoring cetacean acoustic welfare through behavioral hearing tests and auditory evoked potentials (AEPs), passive acoustic monitoring, such as the Welfare Acoustic Monitoring System (WAMS), as well as ideas for using advanced technologies for utilizing vocal biomarkers of health are introduced and reviewed as opportunities for integration into marine mammal welfare plans. © 2023 by the authors.",scopus,2-s2.0-85164714220,10.3390/ani13132124
neural preservation underlies speech improvement from auditory deprivation in young cochlear implant recipients,"80. Proc Natl Acad Sci U S A. 2018 Jan 30;115(5):E1022-E1031. doi: 10.1073/pnas.1717603115. Epub 2018 Jan 16.Neural preservation underlies speech improvement from auditory deprivation in young cochlear implant recipients.Feng G(1)(2), Ingvalson EM(3)(4), Grieco-Calub TM(5)(6), Roberts MY(5), Ryan ME(7)(8), Birmingham P(9)(10), Burrowes D(7)(8), Young NM(4)(6)(11), Wong PCM(12)(2)(13).Author information:(1)Department of Linguistics and Modern Languages, The Chinese University of Hong Kong, Hong Kong SAR, China.(2)Brain and Mind Institute, The Chinese University of Hong Kong, Hong Kong SAR, China.(3)School of Communication Science and Disorders, Florida State University, Tallahassee, FL 32301.(4)Department of Otolaryngology - Head and Neck Surgery, Feinberg School of Medicine, Northwestern University, Chicago, IL 60611.(5)Roxelyn and Richard Pepper Department of Communication Sciences and Disorders, Northwestern University, Evanston, IL 60208.(6)Knowles Hearing Center, School of Communication, Northwestern University, Evanston, IL 60208.(7)Department of Radiology, Feinberg School of Medicine, Northwestern University, Chicago, IL 60611.(8)Department of Medical Imaging, Ann & Robert H. Lurie Children's Hospital of Chicago, Chicago, IL 60611.(9)Department of Anesthesiology, Ann & Robert H. Lurie Children's Hospital of Chicago, Chicago, IL 60611.(10)Department of Anesthesiology, Feinberg School of Medicine, Northwestern University, Chicago, IL 60611.(11)Division of Otolaryngology - Head and Neck Surgery, Ann & Robert H. Lurie Children's Hospital of Chicago, Chicago, IL 60611.(12)Department of Linguistics and Modern Languages, The Chinese University of Hong Kong, Hong Kong SAR, China; p.wong@cuhk.edu.hk.(13)Department of Otorhinolaryngology, Head and Neck Surgery, The Chinese University of Hong Kong, Hong Kong SAR, China.Although cochlear implantation enables some children to attain age-appropriate speech and language development, communicative delays persist in others, and outcomes are quite variable and difficult to predict, even for children implanted early in life. To understand the neurobiological basis of this variability, we used presurgical neural morphological data obtained from MRI of individual pediatric cochlear implant (CI) candidates implanted younger than 3.5 years to predict variability of their speech-perception improvement after surgery. We first compared neuroanatomical density and spatial pattern similarity of CI candidates to that of age-matched children with normal hearing, which allowed us to detail neuroanatomical networks that were either affected or unaffected by auditory deprivation. This information enables us to build machine-learning models to predict the individual children's speech development following CI. We found that regions of the brain that were unaffected by auditory deprivation, in particular the auditory association and cognitive brain regions, produced the highest accuracy, specificity, and sensitivity in patient classification and the most precise prediction results. These findings suggest that brain areas unaffected by auditory deprivation are critical to developing closer to typical speech outcomes. Moreover, the findings suggest that determination of the type of neural reorganization caused by auditory deprivation before implantation is valuable for predicting post-CI language outcomes for young children.DOI: 10.1073/pnas.1717603115PMCID: PMC5798370",pubmed,29339512,10.1073/pnas.1717603115
role of cytoskeletal diaphanousrelated formins in hearing loss,"Hearing relies on the proper functioning of auditory hair cells and on actin-based cytoskele-tal structures. Diaphanous-related formins (DRFs) are evolutionarily conserved cytoskeletal proteins that regulate the nucleation of linear unbranched actin filaments. They play key roles during meta-zoan development, and they seem particularly pivotal for the correct physiology of the reproductive and auditory systems. Indeed, in Drosophila melanogaster, a single diaphanous (dia) gene is present, and mutants show sterility and impaired response to sound. Vertebrates, instead, have three orthologs of the diaphanous gene: DIAPH1, DIAPH2, and DIAPH3. In humans, defects in DIAPH1 and DIAPH3 have been associated with different types of hearing loss. In particular, heterozygous mutations in DIAPH1 are responsible for autosomal dominant deafness with or without thrombocytopenia (DFNA1, MIM #124900), whereas regulatory mutations inducing the overexpression of DIAPH3 cause autosomal dominant auditory neuropathy 1 (AUNA1, MIM #609129). Here, we provide an overview of the expression and function of DRFs in normal hearing and deafness. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",scopus,2-s2.0-85130833070,10.3390/cells11111726
use of data mining to predict significant factors and benefits of bilateral cochlear implantation,"203. Eur Arch Otorhinolaryngol. 2015 Nov;272(11):3157-62. doi: 10.1007/s00405-014-3337-3. Epub 2014 Oct 17.Use of data mining to predict significant factors and benefits of bilateral cochlear implantation.Ramos-Miguel A(1)(2), Perez-Zaballos T(3)(4), Perez D(3)(4), Falconb JC(3)(4), Ramosb A(3)(4).Author information:(1)University of Las Palmas of Gran Canaria, Gran Canaria, Spain. aramos.gcc@gmail.com.(2)Hearing Loss Unit, University Hospital Insular De Gran Canaria, Gran Canaria, Spain. aramos.gcc@gmail.com.(3)University of Las Palmas of Gran Canaria, Gran Canaria, Spain.(4)Hearing Loss Unit, University Hospital Insular De Gran Canaria, Gran Canaria, Spain.Data mining (DM) is a technique used to discover pattern and knowledge from a big amount of data. It uses artificial intelligence, automatic learning, statistics, databases, etc. In this study, DM was successfully used as a predictive tool to assess disyllabic speech test performance in bilateral implanted patients with a success rate above 90%. 60 bilateral sequentially implanted adult patients were included in the study. The DM algorithms developed found correlations between unilateral medical records and Audiological test results and bilateral performance by establishing relevant variables based on two DM techniques: the classifier and the estimation. The nearest neighbor algorithm was implemented in the first case, and the linear regression in the second. The results showed that patients with unilateral disyllabic test results below 70% benefited the most from a bilateral implantation. Finally, it was observed that its benefits decrease as the inter-implant time increases.DOI: 10.1007/s00405-014-3337-3",pubmed,25323153,10.1007/s00405-014-3337-3
an integrated model of pitch perception incorporating place and temporal pitch codes with application to cochlear implant research,"535. Hear Res. 2017 Feb;344:135-147. doi: 10.1016/j.heares.2016.11.005. Epub 2016 Nov 12.An integrated model of pitch perception incorporating place and temporal pitch codes with application to cochlear implant research.Erfanian Saeedi N(1), Blamey PJ(2), Burkitt AN(3), Grayden DB(4).Author information:(1)NeuroEngineering Laboratory, Dept. of Electrical & Electronic Engineering, University of Melbourne, Australia; Centre for Neural Engineering, University of Melbourne, Australia. Electronic address: n.erfaniansaeedi@student.unimelb.edu.au.(2)The Bionics Institute, East Melbourne, Australia; Dept. of Medical Bionics, University of Melbourne, Australia.(3)NeuroEngineering Laboratory, Dept. of Electrical & Electronic Engineering, University of Melbourne, Australia; The Bionics Institute, East Melbourne, Australia.(4)NeuroEngineering Laboratory, Dept. of Electrical & Electronic Engineering, University of Melbourne, Australia; Centre for Neural Engineering, University of Melbourne, Australia; The Bionics Institute, East Melbourne, Australia.Although the neural mechanisms underlying pitch perception are not yet fully understood, there is general agreement that place and temporal representations of pitch are both used by the auditory system. This paper describes a neural network model of pitch perception that integrates both codes of pitch and explores the contributions of, and the interactions between, the two representations in simulated pitch ranking trials in normal and cochlear implant hearing. The model can replicate various psychophysical observations including the perception of the missing fundamental pitch and sensitivity to pitch interval sizes. As a case study, the model was used to investigate the efficiency of pitch perception cues in a novel sound processing scheme, Stimulation based on Auditory Modelling (SAM), that aims to improve pitch perception in cochlear implant hearing. Results showed that enhancement of the pitch perception cues would lead to better pitch ranking scores in the integrated model only if the place and temporal pitch cues were consistent.Copyright © 2016 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2016.11.005",pubmed,27845260,10.1016/j.heares.2016.11.005
automatic analysis of auditory nerve electrically evoked compound action potential with an artificial neural network,"322. Artif Intell Med. 2004 Jul;31(3):221-9. doi: 10.1016/j.artmed.2004.03.004.Automatic analysis of auditory nerve electrically evoked compound action potential with an artificial neural network.Charasse B(1), Thai-Van H, Chanal JM, Berger-Vachon C, Collet L.Author information:(1)UMR CNRS 5020, Laboratoire Neurosciences & Systèmes Sensoriels, 50 avenue Tony Garnier, 69366 Lyon Cedex, France. bcharass@olfac.uni-lyon1.frThe auditory nerve's electrically evoked compound action potential is recorded in deaf patients equipped with the Nucleus 24 cochlear implant using a reverse telemetry system (NRT). Since the threshold of the NRT response (NRT-T) is thought to reflect the psychophysics needed for programming cochlear implants, efforts have been made by specialized management teams to develop its use. This study aimed at developing a valid tool, based on artificial neural networks (ANN) technology, for automatic estimation of NRT-T. The ANN used was a single layer perceptron, trained with 120 NRT traces. Learning traces differed from data used for the validation. A total of 550 NRT traces from 11 cochlear implant subjects were analyzed separately by the system and by a group of physicians with expertise in NRT analysis. Both worked to determine 37 NRT-T values, using the response amplitude growth function (AGF) (linear regression of response amplitudes obtained at decreasing stimulus intensity levels). The validity of the system was assessed by comparing the NRT-T values automatically determined by the system with those determined by the physicians. A strong correlation was found between automatic and physician-obtained NRT-T values (Pearson r correlation coefficient >0.9). ANOVA statistics confirmed that automatic NRT-Ts did not differ from physician-obtained values (F = 0.08999, P = 0.03). Moreover, the average error between NRT-Ts predicted by the system and NRT-Ts measured by the physicians (3.6 stimulation units) did not differ significantly from the average error between NRT-Ts measured by each of the three physicians (4.2 stimulation units). In conclusion, the automatic system developed in this study was found to be as efficient as human experts for fitting the amplitude growth function and estimating NRT-T, with the advantage of considerable time-saving.DOI: 10.1016/j.artmed.2004.03.004",pubmed,15302088,10.1016/j.artmed.2004.03.004
consideration of life rhythm for hearingdog robots searching for user,"A hearing dog is a sort of assistance dog for hearing-impaired individuals. The physical touch of the dog can alert the individuals to important sounds such as doorbells, alarm clocks, and fire alarms. Although hearing dogs can assist people, there is an insufficient number of them around the world today. As an alternative, a hearing-dog robot has been developed. This robot can move around autonomously to search for a user and notify him or her of important sounds. In this work, we propose an exploring algorithm for the robot that considers past information about the location of the user. Specifically, this algorithm utilizes the user's life rhythm in order to achieve efficient exploring. In our experiments, proposed algorithm showed a shorter time as compared with the algorithm without the user's life rhythm.",ieee,2376-6824,10.1109/TAAI.2018.00031
peripheral visual localization is degraded by globally incongruent auditoryspatial attention cues,"809. Exp Brain Res. 2019 Sep;237(9):2137-2143. doi: 10.1007/s00221-019-05578-z. Epub 2019 Jun 14.Peripheral visual localization is degraded by globally incongruent auditory-spatial attention cues.Ahveninen J(1), Ingalls G(2), Yildirim F(2), Calabro FJ(2)(3), Vaina LM(4)(2)(5).Author information:(1)Harvard Medical School, Athinoula A. Martinos Center for Biomedical Imaging, Department of Radiology, Massachusetts General Hospital, Charlestown, MA, USA. jyrki@nmr.mgh.harvard.edu.(2)Brain and Vision Research Laboratory, Department of Biomedical Engineering, Boston University, Boston, MA, USA.(3)Department of Psychiatry and Bioengineering, University of Pittsburgh, Pittsburgh, PA, USA.(4)Harvard Medical School, Athinoula A. Martinos Center for Biomedical Imaging, Department of Radiology, Massachusetts General Hospital, Charlestown, MA, USA.(5)Department of Neurology, Harvard Medical School, Massachusetts General Hospital and Brigham and Women's Hospital, Boston, MA, USA.Global auditory-spatial orienting cues help the detection of weak visual stimuli, but it is not clear whether crossmodal attention cues also enhance the resolution of visuospatial discrimination. Here, we hypothesized that if anywhere, crossmodal modulations of visual localization should emerge in the periphery where the receptive fields are large. Subjects were presented with trials where a Visual Target, defined by a cluster of low-luminance dots, was shown for 220 ms at 25°-35° eccentricity in either the left or right hemifield. The Visual Target was either Uncued or it was presented 250 ms after a crossmodal Auditory Cue that was simulated either from the same or the opposite hemifield than the Visual Target location. After a whole-screen visual mask displayed for 800 ms, a pair of vertical Reference Bars was presented ipsilateral to the Visual Target. In a two-alternative forced choice task, subjects were asked to determine which of these two bars was closer to the center of the Visual Target. When the Auditory Cue and Visual Target were hemispatially incongruent, the speed and accuracy of visual localization performance was significantly impaired. However, hemispatially congruent Auditory Cues did not improve the localization of Visual Targets when compared to the Uncued condition. Further analyses suggested that the crossmodal Auditory Cues decreased the sensitivity (d') of the Visual Target localization without affecting post-perceptual decision biases. Our results suggest that in the visual periphery, the detrimental effect of hemispatially incongruent Auditory Cues is far greater than the benefit produced by hemispatially congruent cues. Our working hypothesis for future studies is that auditory-spatial attention cues suppress irrelevant visual locations in a global fashion, without modulating the local visual precision at relevant sites.DOI: 10.1007/s00221-019-05578-zPMCID: PMC6677609",pubmed,31201472,10.1007/s00221-019-05578-z
perframe sign language gloss recognition,"Sign language is a language for communicating among the deaf and hard of hearing people and the hearing people. Sign language has its own grammatical system which is different from spoken or written languages. A sentence of sign language consists of glosses as morphemes, and the meaning of sign language depends on the movements of a body, hands, finger shapes, and facial expressions. Previous methods can translate sign language sentences into a sequence of glosses or written language but are weak for the out-of-vocabulary problem such as variations of word order. This is because the model only considers a sign language sentence as an inseparable sequence, even though the sentence consists of multiple glosses. In this paper, we propose a method that predicting every gloss for each video frame of Korean sign language using transformers. Predicted frame-by-frame gloss information can be used to transform a video of a sign language sentence into a gloss sequence, even if the model has not learned that pattern.",ieee,2162-1233,10.1109/ICTC52510.2021.9621167
real time sign language recognition framework for two way communication,"The need to express oneself clearly and freely is a fundamental desire and must be available to all regardless of any handicap they face. Often, people that endure hearing and speech impairments use sign language as their only way of communication. This leaves a major communication barrier between people who speak sign language and those that communicate otherwise. Not a lot of people who communicate via vocal languages are inclined to learn sign language. The contribution of this paper is to develop a model which converts signs from the American Sign Language (ASL) into English characters using a CNN which segments the signs to their English language alphabets. The model was able to detect 26 alphabets and 3 additional characters with upto 99.78% accuracy. This paper also proposes inclusion of a speech to text module in order to facilitate seamless two way communication.",ieee,,10.1109/ICCICT50803.2021.9510094
a deep learning approach to quantify auditory hair cells,"112. Hear Res. 2021 Sep 15;409:108317. doi: 10.1016/j.heares.2021.108317. Epub 2021 Jul 22.A deep learning approach to quantify auditory hair cells.Cortada M(1), Sauteur L(2), Lanz M(3), Levano S(4), Bodmer D(5).Author information:(1)Department of Biomedicine, University of Basel, Hebelstrasse 20, Basel 4031, Switzerland. Electronic address: maurizio.cortada@unibas.ch.(2)Department of Biomedicine, University of Basel, Hebelstrasse 20, Basel 4031, Switzerland. Electronic address: loic.sauteur@unibas.ch.(3)Department of Biomedicine, University of Basel, Hebelstrasse 20, Basel 4031, Switzerland. Electronic address: michi.lanz@unibas.ch.(4)Department of Biomedicine, University of Basel, Hebelstrasse 20, Basel 4031, Switzerland. Electronic address: s.levano@unibas.ch.(5)Department of Biomedicine, University of Basel, Hebelstrasse 20, Basel 4031, Switzerland; Clinic for Otorhinolaryngology, Head and Neck Surgery, University of Basel Hospital, Petersgraben 4, Basel CH-4031, Switzerland. Electronic address: Daniel.Bodmer@usb.ch.Hearing loss affects millions of people worldwide. Yet, there are still no curative therapies for sensorineural hearing loss. Frequent causes of sensorineural hearing loss are due to damage or loss of the sensory hair cells, the spiral ganglion neurons, or the synapses between them. Culturing the organ of Corti allows the study of all these structures in an experimental model, which is easy to manipulate. Therefore, the in vitro culture of the neonatal mammalian organ of Corti remains a frequently used experimental system, in which hair cell survival is routinely assessed. However, the analysis of the surviving hair cells is commonly performed via manual counting, which is a time-consuming process and the inter-rater reliability can be an issue. Here, we describe a deep learning approach to quantify hair cell survival in the murine organ of Corti explants. We used StarDist, a publicly available platform and plugin for Fiji (Fiji is just ImageJ), to train and apply our own custom deep learning model. We successfully validated our model in untreated, cisplatin, and gentamicin treated organ of Corti explants. Therefore, deep learning is a valuable approach for quantifying hair cell survival in organ of Corti explants. Moreover, we also demonstrate how the publicly available Fiji plugin StarDist can be efficiently used for this purpose.Copyright © 2021. Published by Elsevier B.V.DOI: 10.1016/j.heares.2021.108317",pubmed,34343849,10.1016/j.heares.2021.108317
empowering patients with personalized compression widex mysound is an aibased function that allows the user to personalize their sound,,cinahl,10745734,
statistical learning of transition patterns in the songbird auditory forebrain,"628. Sci Rep. 2020 May 12;10(1):7848. doi: 10.1038/s41598-020-64671-4.Statistical learning of transition patterns in the songbird auditory forebrain.Dong M(1), Vicario DS(2).Author information:(1)Department of Psychology, Rutgers, the State University of New Jersey, New Brunswick, NJ, United States. mingwen.dong7@gmail.com.(2)Department of Psychology, Rutgers, the State University of New Jersey, New Brunswick, NJ, United States.Statistical learning of transition patterns between sounds-a striking capability of the auditory system-plays an essential role in animals' survival (e.g., detect deviant sounds that signal danger). However, the neural mechanisms underlying this capability are still not fully understood. We recorded extracellular multi-unit and single-unit activity in the auditory forebrain of awake male zebra finches while presenting rare repetitions of a single sound in a long sequence of sounds (canary and zebra finch song syllables) patterned in either an alternating or random order at different inter-stimulus intervals (ISI). When preceding stimuli were regularly alternating (alternating condition), a repeated stimulus violated the preceding transition pattern and was a deviant. When preceding stimuli were in random order (control condition), a repeated stimulus did not violate any regularities and was not a deviant. At all ISIs tested (1 s, 3 s, or jittered at 0.8-1.2 s), deviant repetition enhanced neural responses in the alternating condition in a secondary auditory area (caudomedial nidopallium, NCM) but not in the primary auditory area (Field L2); in contrast, repetition suppressed responses in the control condition in both Field L2 and NCM. When stimuli were presented in the classical oddball paradigm at jittered ISI (0.8-1.2 s), neural responses in both NCM and Field L2 were stronger when a stimulus occurred as deviant with low probability than when the same stimulus occurred as standard with high probability. Together, these results demonstrate: (1) classical oddball effect exists even when ISI is jittered and the onset of a stimulus is not fully predictable; (2) neurons in NCM can learn transition patterns between sounds at multiple ISIs and detect violation of these transition patterns; (3) sensitivity to deviant sounds increases from Field L2 to NCM in the songbird auditory forebrain. Further studies using the current paradigms may help us understand the neural substrate of statistical learning and even speech comprehension.DOI: 10.1038/s41598-020-64671-4PMCID: PMC7217825",pubmed,32398864,10.1038/s41598-020-64671-4
sound source localisation on android smartphones a first step to using smartphones as auditory sensors for training ai systems with big data,"The ability to estimate positions of sound sources is one that gives animals a 360° awareness of their acoustic environment. This helps complement the visual scene which is restricted to 180° in humans. Unfortunately, deaf people are left out on this ability. Smart phones are rapidly becoming a common tool amongst mobile users in developed and emerging markets. Their processing ability has more than doubled since their introduction to mass consumer markets by Apple in 2007. Top-end smart phones such as the Samsung Galaxy Series; 3, 4, and 5 models, have two microphones with which one can acquire stereo recordings. The purpose of this research project was to establish a feasible Sound source localization algorithm for current top-end smart phones, and to recommend hardware improvements for future smart phones, to pave way for the use of smart phones as advanced auditory sensory devices capable of acting as avatars for intelligent remote systems to learn about different acoustic scenes with help of human users. © 2015 IEEE.",scopus,2-s2.0-84962509835,10.1109/AFRCON.2015.7331970
dimensions of artefacts caused by cochlear and auditory brainstem implants in magnetic resonance imaging,"541. Cochlear Implants Int. 2020 Mar;21(2):67-74. doi: 10.1080/14670100.2019.1668617. Epub 2019 Sep 25.Dimensions of artefacts caused by cochlear and auditory brainstem implants in magnetic resonance imaging.Majdani E(1), Majdani O(1)(2), Steffens M(1), Warnecke A(1)(2), Lesinski-Schiedat A(1), Lenarz T(1)(2), Götz F(3).Author information:(1)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.(2)Cluster of Excellence of the German Research Foundation (DFG; 'Deutsche Forschungsgemeinschaft') 'Hearing4all', Hannover, Germany.(3)Institute of Neuroradiology, Hannover Medical School, Hannover, Germany.Background: The aim of the study was to investigate the extent of MRI artefacts due to the magnet of selected auditory implants.Study design: Artefacts of the Synchrony cochlear implant at 1.5 T as well as at 3 T MRI devices were examined in cadavers and compared to the artefacts in MRI scans at 1.5 T of 17 patients implanted with CI (n = 12) and auditory brainstem implants (ABI) (n = 5).Results: None of the scanned implants showed any failure after MRI. After removal of the magnet, only a portion of the images in the direct neighbourhood of the implant, especially in the temporal and parietal lobe, contained artefacts. More anatomical substructures were visible without artefacts using the MedEl Synchrony device.Conclusion: Artefacts can be markedly reduced by rotating, self-aligning magnet. Removal of the magnet also results in reduction of artefacts.DOI: 10.1080/14670100.2019.1668617",pubmed,31553273,10.1080/14670100.2019.1668617
hearing impairment and ear pathology in nepal,"790. J Laryngol Otol. 1993 May;107(5):395-400. doi: 10.1017/s0022215100123278.Hearing impairment and ear pathology in Nepal.Little P(1), Bridges A, Guragain R, Friedman D, Prasad R, Weir N.Author information:(1)Aldermoor Health Centre, Southampton.A stratified random cluster sample of 15,845 subjects was performed in two regions of Nepal to determine the prevalence and main causes of hearing impairment (the most common disability) and the prevalence of ear disease. Subjects reporting current ear pain, or ear discharge, or hearing impairment on direct questioning by a Nepali health worker (primary screening failed), had otoscopy and audiometry (using the Liverpool Field Audiometer) performed, and a questionnaire administered relating to past history. In every fifth house subjects who passed the primary screening (1,716 subjects) were examined to assess the false negative rate of screening. An estimated 16.6 per cent of the study population have hearing impairment (either ear worse than 30 dB hearing threshold level (HTL) 1.0-4.0 kHz, or 50 dB HTL 0.5 kHz), and 7.4 per cent ear drum pathology, equivalent to respectively 2.71 and 1.48 million people extrapolated to the whole of Nepal. Most hearing impairment in the school age group (55.2 per cent) is associated with otitis media or its sequelae. Probably at least 14 per cent of sensorineural deafness is preventable (7 per cent infectious disease, 3.9 per cent trauma, 0.8 per cent noise exposure, 1 per cent cretinism, and 1 per cent abnormal pregnancy or labour). Most individuals reporting current ear pathology (61 per cent) had never attended a health post, and of those receiving ear drop treatment, 84 per cent still had serious pathology. Of subjects who reported ear drop treatment at any time, 31 per cent still had serious pathology. The use of traditional remedies was prevalent.(ABSTRACT TRUNCATED AT 250 WORDS)DOI: 10.1017/s0022215100123278",pubmed,8326217,10.1017/s0022215100123278
digital accessibility in the era of artificial intelligencebibliometric analysis and systematic review,"762. Front Artif Intell. 2024 Feb 16;7:1349668. doi: 10.3389/frai.2024.1349668. eCollection 2024.Digital accessibility in the era of artificial intelligence-Bibliometric analysis and systematic review.Chemnad K(1), Othman A(1).Author information:(1)Mada Qatar Assistive Technology Center, Doha, Qatar.INTRODUCTION: Digital accessibility involves designing digital systems and services to enable access for individuals, including those with disabilities, including visual, auditory, motor, or cognitive impairments. Artificial intelligence (AI) has the potential to enhance accessibility for people with disabilities and improve their overall quality of life.METHODS: This systematic review, covering academic articles from 2018 to 2023, focuses on AI applications for digital accessibility. Initially, 3,706 articles were screened from five scholarly databases-ACM Digital Library, IEEE Xplore, ScienceDirect, Scopus, and Springer.RESULTS: The analysis narrowed down to 43 articles, presenting a classification framework based on applications, challenges, AI methodologies, and accessibility standards.DISCUSSION: This research emphasizes the predominant focus on AI-driven digital accessibility for visual impairments, revealing a critical gap in addressing speech and hearing impairments, autism spectrum disorder, neurological disorders, and motor impairments. This highlights the need for a more balanced research distribution to ensure equitable support for all communities with disabilities. The study also pointed out a lack of adherence to accessibility standards in existing systems, stressing the urgency for a fundamental shift in designing solutions for people with disabilities. Overall, this research underscores the vital role of accessible AI in preventing exclusion and discrimination, urging a comprehensive approach to digital accessibility to cater to diverse disability needs.Copyright © 2024 Chemnad and Othman.DOI: 10.3389/frai.2024.1349668PMCID: PMC10905618",pubmed,38435800,10.3389/frai.2024.1349668
automatic hearing screening for better monitoring of hearing health using the example of the german armed forces,"419. HNO. 2023 Jun;71(6):386-395. doi: 10.1007/s00106-023-01288-9. Epub 2023 May 2.[Automatic hearing screening for better monitoring of hearing health using the example of the German armed forces].[Article in German; Abstract available in German from the publisher]Jacob R(1), Zokoll MA(2)(3), Berg D(2)(3), Meis M(4).Author information:(1)HNOplus, Bergstr 63a, 56203, Höhr-Grenzhausen, Deutschland. rolandjacob@online.de.(2)Hörzentrum Oldenburg gGmbH, Oldenburg, Deutschland.(3)Cluster of Excellence ""Hearing4all"", Oldenburg, Deutschland.(4)Cochlear Deutschland GmbH & Co. KG, Hannover, Deutschland.In the present study, the concept of a systematic automated screening of temporary soldiers was evaluated based on the example of the ENT Department of the Bundeswehr Central Hospital Koblenz. From 2014 to 2017, anonymized data of 169 individuals were collected from the setting of the Bundeswehr Central Hospital. Included in the data are results from measurements with automated pure-tone audiometry (APTA; e.g., [3]), from measurements with the digit triple test for determination of the speech discrimination threshold in noise (e.g., [20]), and from interviews with questionnaires (Hearing-Dependent Daily Activities [HDDA], e.g., [14]; HearCom questionnaire, e.g., [15]). There was an initial publication from this project evaluating the questionnaires in terms of their suitability for detecting hearing loss [14]. In the following (from March 2015), only the HDDA, which was described as more sensitive, was used for measurements at the hearing screening measurement station. A complete run with the three procedures took approximately 22 min. Approximately 17% of the examined participants had abnormal findings in at least one of the procedures at the screening station. The results of the respective methods taken together detect more than any method alone and can be assumed to be complementary. Deviations between APTA with level monitor and manual tone audiometry were within the measurement accuracy. In the range between 1 and 4 kHz, hearing thresholds are somewhat underestimated with APTA. The threshold for the HDDA questionnaire with an HDDA sum ≥ 19 was confirmed. Automated hearing screening offers a good opportunity to check hearing ability on a regular basis in a standardized and reliable manner, while keeping personnel requirements low.Publisher: In der vorliegenden Studie wurde das Konzept eines systematischen automatisierten Screenings von Zeitsoldaten am Beispiel der HNO-Abteilung des Bundeswehrzentralkrankenhauses Koblenz evaluiert. In den Jahren 2014 bis 2017 sind Daten von 169 Personen aus dem Umfeld des Bundeswehrzentralkrankenhauses anonymisiert gesammelt worden. In den Daten enthalten sind Ergebnisse aus Messungen mit der automatisierten Reintonaudiometrie (APTA; Automatic Pure Tone Audiometry, z. B. [3]), aus Messungen mit dem Ziffern-Tripel-Test für die Ermittlung einer Sprachverständlichkeitsschwelle im Störgeräusch z. B. [20], sowie aus Befragungen mit Fragebögen (HDDA, Hearing-Dependent Daily Activities, z. B. [14], HearCom-Fragebogen, z. B. [15]). Eine erste Publikation aus diesem Projekt gab es zur Evaluation der Fragebögen hinsichtlich ihrer Eignung zur Detektion von Hörverlusten [20]. Im Folgenden (ab März 2015) wurde nur noch der hierbei als sensitiver beschriebene HDDA für Messungen mit der Hörscreening-Messstation verwendet. Ein kompletter Durchgang mit den drei Verfahren dauerte ca. 22 min. Circa 17 % der untersuchten Teilnehmer zeigten auffällige Ergebnisse in mindestens einem der Verfahren auf der Screening-Station. Die Ergebnisse der jeweiligen Verfahren zusammengenommen detektieren mehr als ein Verfahren allein und können als komplementär angenommen werden. Abweichungen zwischen APTA mit Pegelwächter und manueller Tonaudiometrie lagen innerhalb der Messungenauigkeit. Im Bereich zwischen 1 und 4 kHz werden die Hörschwellen mit APTA etwas unterschätzt. Der Schwellenwert für den HDDA-Fragebogen mit einer HDDA-Summe ≥ 19 konnte bestätigt werden. Ein automatisiertes Hörscreening bietet eine gute Möglichkeit auf regelmäßiger Basis die Hörfähigkeit in standardisierter und zuverlässiger Weise zu überprüfen und dabei gleichzeitig den Personaleinsatz gering zu halten.© 2023. The Author(s), under exclusive licence to Springer Medizin Verlag GmbH, ein Teil von Springer Nature.DOI: 10.1007/s00106-023-01288-9",pubmed,37129641,10.1007/s00106-023-01288-9
conversation success in onetoone and group conversation a group concept mapping study of adults with normal and impaired hearing,"788. Int J Audiol. 2023 Sep;62(9):868-876. doi: 10.1080/14992027.2022.2095538. Epub 2022 Jul 23.Conversation success in one-to-one and group conversation: a group concept mapping study of adults with normal and impaired hearing.Nicoras R(1), Gotowiec S(2), Hadley LV(1), Smeds K(1)(2), Naylor G(1).Author information:(1)Hearing Sciences - Scottish Section, School of Medicine, University of Nottingham, Nottingham, UK.(2)ORCA Europe, WS Audiology, Stockholm, Sweden.OBJECTIVE: The concept of conversation success is undefined, although prior work has variously related it to accurate exchange of information, alignment between interlocutors, and good management of misunderstandings. This study aimed (1) to identify factors of conversation success and (2) to explore the importance of these factors in one-to-one versus group conversations.DESIGN: Group concept mapping method was applied. Participants responded to two brainstorming prompts (""What does 'successful conversation' look like?"" and ""Think about a successful conversation you have taken part in. What aspects of that conversation contributed to its success?""). The resulting statements were sorted into related clusters and rated in importance for one-to-one and group conversation.STUDY SAMPLE: Thirty-five adults with normal and impaired hearing.RESULTS: Seven clusters were identified: (1) Being able to listen easily; (2) Being spoken to in a helpful way; (3) Being engaged and accepted; (4) Sharing information as desired; (5) Perceiving flowing and balanced interaction; (6) Feeling positive emotions; (7) Not having to engage coping mechanisms. Three clusters (1, 2, and 4) were more important in group than in one-to-one conversation. There were no differences by hearing group.CONCLUSIONS: These findings emphasise that conversation success is a multifaceted concept.DOI: 10.1080/14992027.2022.2095538",pubmed,35875851,10.1080/14992027.2022.2095538
water waves to sound waves using zebrafish to explore hair cell biology,"Although perhaps best known for their use in developmental studies, over the last couple of decades, zebrafish have become increasingly popular model organisms for investigating auditory system function and disease. Like mammals, zebrafish possess inner ear mechanosensory hair cells required for hearing, as well as superficial hair cells of the lateral line sensory system, which mediate detection of directional water flow. Complementing mammalian studies, zebrafish have been used to gain significant insights into many facets of hair cell biology, including mechanotransduction and synaptic physiology as well as mechanisms of both hereditary and acquired hair cell dysfunction. Here, we provide an overview of this literature, highlighting some of the particular advantages of using zebrafish to investigate hearing and hearing loss. © 2019, Association for Research in Otolaryngology.",scopus,2-s2.0-85059916767,10.1007/s10162-018-00711-1
sign language translation techniques using artificial intelligence for the hearing impaired community in sri lanka a review,"Hearing Impaired individuals routinely encounter limitations in their involvement in social interactions, access to intriguing information, and participation in everyday activities, among various other aspects. However, the hardest part of their interactions with regular people is communication, because sign language is the primary language of those who are hearing impaired. However, the general public is unaware of sign language. Each country has its own sign language. However, there are some striking similarities between them. In Sri Lanka, hearing impaired people use Sri Lankan Sign Language (SLSL) as their communication language. There is several research done on Sign Language recognition and translation. But no fully functioning system is utilized for Sri Lankan Sign Language translation. To find the gap in this area, we conducted a systematic literature review using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method that analyses 12 studies on Sign Language Translation (SLT). As per the literature review, Image Processing (IP) and Convolutional Neural Networks (CNN) are the most used techniques for Sign Language translation. But these methods have limitations: not enough data, differences in how people use sign language, difficulty in translating in real-time, not capturing cultural aspects, needing specific equipment, and understanding the context of conversations. Recognizing and solving these problems is important, especially for languages like SLSL. Future research should focus on getting more data, making translation work for different cultures, and improving real-time translation. This will help hearing impaired people communicate better with others.",ieee,,10.1109/SLAAI-ICAI59257.2023.10365012
speakerindependent auditory attention decoding without access to clean speech sources,"161. Sci Adv. 2019 May 15;5(5):eaav6134. doi: 10.1126/sciadv.aav6134. eCollection 2019 May.Speaker-independent auditory attention decoding without access to clean speech sources.Han C(1)(2), O'Sullivan J(1)(2), Luo Y(1)(2), Herrero J(3), Mehta AD(3), Mesgarani N(1)(2).Author information:(1)Department of Electrical Engineering, Columbia University, New York, NY, USA.(2)Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA.(3)Department of Neurosurgery, Hofstra-Northwell School of Medicine and Feinstein Institute for Medical Research, Manhasset, New York, NY, USA.Speech perception in crowded environments is challenging for hearing-impaired listeners. Assistive hearing devices cannot lower interfering speakers without knowing which speaker the listener is focusing on. One possible solution is auditory attention decoding in which the brainwaves of listeners are compared with sound sources to determine the attended source, which can then be amplified to facilitate hearing. In realistic situations, however, only mixed audio is available. We utilize a novel speech separation algorithm to automatically separate speakers in mixed audio, with no need for the speakers to have prior training. Our results show that auditory attention decoding with automatically separated speakers is as accurate and fast as using clean speech sounds. The proposed method significantly improves the subjective and objective quality of the attended speaker. Our study addresses a major obstacle in actualization of auditory attention decoding that can assist hearing-impaired listeners and reduce listening effort for normal-hearing subjects.DOI: 10.1126/sciadv.aav6134PMCID: PMC6520028",pubmed,31106271,10.1126/sciadv.aav6134
baep dynamic estimation in case of endocochlear pathologies using a time delay correction method,"499. J Med Eng Technol. 2004 Nov-Dec;28(6):235-41. doi: 10.1080/0309190031000139065.BAEP dynamic estimation in case of endocochlear pathologies using a time delay correction method.Cherrid N(1), Naït-Ali A, Siarry P.Author information:(1)Université Paris 12 LERISS, 61 avenue du Général de Gaulle 94010, Créteil, France. cherrid@univ-paris12.frExtraction of Brainstem Auditory Evoked Potentials (BAEPs) from the electroencephalogram (EEG) is generally difficult when both BAEP and EEG are non-stationary. In this paper we focus on the problem of BAEP non-stationarities, in particular those observed in some endocochlear pathologies assumed causing random delays of BAEPs due to an abnormal behaviour of the cochlea. The technique developed in this paper, called the Time Delay Correction (TDC) method, allows us to estimate the averaged BAEP by an optimal alignment of responses based on a correlation criterion. We demonstrate that the TDC method avoids wave smoothness, generally produced with the classical ensemble averaging method, especially in the case when the hypothesis of the time delay non-stationarity is verified. The TDC method is performed using simulated annealing (SA) algorithm, since the criterion to be optimized is nonlinear. Real signals recorded from pathological subjects are used to validate the model of non-stationarity.DOI: 10.1080/0309190031000139065",pubmed,15513741,10.1080/0309190031000139065
kanamycin and cisplatin ototoxicity differences in patterns of oxidative stress antioxidant enzyme expression and hair cell loss in the cochlea,"832. Antioxidants (Basel). 2022 Sep 6;11(9):1759. doi: 10.3390/antiox11091759.Kanamycin and Cisplatin Ototoxicity: Differences in Patterns of Oxidative Stress, Antioxidant Enzyme Expression and Hair Cell Loss in the Cochlea.Gibaja A(1), Alvarado JC(1), Scheper V(2)(3), Carles L(4), Juiz JM(1)(2)(3)(5).Author information:(1)Instituto de Investigación en Discapacidades Neurológicas (IDINE), School of Medicine, Universidad de Castilla-La Mancha (UCLM), Campus in Albacete, 02008 Albacete, Spain.(2)Department of Otorhinolaryngology, Head and Neck Surgery, Hannover Medical School, 30625 Hannover, Germany.(3)Cluster of Excellence ""Hearing4all"" of the German Research Foundation, DFG, MHH, 30625 Hannover, Germany.(4)Department of Otolaryngology, University Hospital ""Doce de Octubre"", 28041 Madrid, Spain.(5)IDINE/Med School, UCLM-Campus in Albacete, C/Almansa 14, 02008 Albacete, Spain.Kanamycin and cisplatin are ototoxic drugs. The mechanisms are incompletely known. With subcutaneous kanamycin (400 mg/kg, 15 days), auditory threshold shifts were detected at days 12-13 at 16 and 32 kHz, extending to 8 and 4 kHz at days 14-15. The outer hair cell (OHC) loss was concentrated past day 12. The maximum cochlear length showing apoptotic cells, tested with TUNEL, was at day 13. At day 15, 1/5 of the apical cochlea contained preserved OHCs. 3-nitrotyrosine (3-NT) immunolabeling, showing oxidative stress, was found in surviving OHCs and in basal and middle portions of the stria vascularis (SV). The antioxidant Gpx1 gene expression was decreased. The immunocytochemistry showed diminished Gpx1 in OHCs. With intraperitoneal cisplatin (16 mg/kg, single injection), no evoked auditory activity was recorded at the end of treatment, at 72 h. The basal third of the cochlea lacked OHCs. Apoptosis occupied the adjacent 1/3, and the apical third contained preserved OHCs. 3-NT immunolabeling was extensive in OHCs and the SV. Gpx1 and Sod1 gene expression was downregulated. Gpx1 immunostaining diminished in middle and basal SV. More OHCs survived cisplatin than kanamycin towards the apex, despite undetectable evoked activity. Differential regulation of antioxidant enzyme levels suggests differences in the antioxidant response for both drugs.DOI: 10.3390/antiox11091759PMCID: PMC9495324",pubmed,36139833,10.3390/antiox11091759
from manual to artificial intelligence fitting two cochlear implant case studies,"388. Cochlear Implants Int. 2020 Sep;21(5):299-305. doi: 10.1080/14670100.2019.1667574. Epub 2019 Sep 17.From manual to artificial intelligence fitting: Two cochlear implant case studies.Wathour J(1), Govaerts PJ(2), Deggouj N(1).Author information:(1)Cliniques Universitaires Saint-Luc, Avenue Hippocrate 10, Brussels, 1200, Belgium.(2)Eargroup, Antwerpen-Deurne, Belgium.Objective: To assess whether CI programming by means of a software application using artificial intelligence (AI), FOX®, may improve cochlear implant (CI) performance. Patients: Two adult CI recipients who had mixed auditory results with their manual fitting were selected for an AI-assisted fitting. Even after 17 months CI experience and 19 manual fitting sessions, the first subject hadn't developed open set word recognition. The second subject, after 9 months of manual fitting, had developed good open set word recognition, but his scores remained poor at soft and loud presentation levels. Main outcome measure(s): Cochlear implant fitting parameters, pure tone thresholds, bisyllabic word recognition, phonemic discrimination scores and loudness scaling curves. Results: For subject 1, a first approach trying to optimize the home maps by means of AI-proposed adaptations was not successful whereas a second approach based on the use of Automaps (an AI approach based on universal, i.e. population based group statistics) during 3 months allowed the development of open set word recognition. For subject 2, the word recognition scores improved at soft and loud intensities with the AI suggestions. The AI-suggested modifications seem to be atypical. Conclusions: The two case studies illustrate that adults implanted with manual CI fitting may experience an improvement in their auditory results with AI-assisted fitting.DOI: 10.1080/14670100.2019.1667574",pubmed,31530099,10.1080/14670100.2019.1667574
extent of lateralization at large interaural time differences in simulated electric hearing and bilateral cochlear implant users,"463. J Acoust Soc Am. 2017 Apr;141(4):2338. doi: 10.1121/1.4979114.Extent of lateralization at large interaural time differences in simulated electric hearing and bilateral cochlear implant users.Baumgärtel RM(1), Hu H(1), Kollmeier B(1), Dietz M(1).Author information:(1)Medizinische Physik, Carl von Ossietzky Universität Oldenburg and Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany.Normal-hearing (NH) listeners are able to localize sound sources with extraordinary accuracy through interaural cues, most importantly interaural time differences (ITDs) in the temporal fine structure. Bilateral cochlear implant (CI) users are also able to localize sound sources, yet generally at lower accuracy than NH listeners. The gap in performance can in part be attributed to current CI systems not faithfully transmitting interaural cues, especially ITDs. With the introduction of binaurally linked CI systems, the presentation of ITD cues for bilateral CI users is foreseeable. The current study therefore investigated extent-of-lateralization percepts elicited in bilateral CI listeners when presented with single-electrode pulse-trains carrying controlled ITD cues. The results were compared against NH listeners listening to broadband stimuli as well as simulations of CI listening. Broadband stimuli in NH listeners were perceived as fully lateralized within the natural ITD range. Using simulated as well as real CI stimuli, however, only a fraction of the full extent of lateralization range was covered by natural ITDs. The maximum extent of lateralization was reached at ITDs as large as twice the natural limit. The results suggest that ITD-enhancement might be a viable option for improving localization abilities with future binaural CI systems.DOI: 10.1121/1.4979114",pubmed,28464641,10.1121/1.4979114
vocal singing skills by cochlear implanted children without formal musical training familiar versus unfamiliar songs,"Objectives: Vocal singing skills in pediatric CI users are not much known due to the limited number of studies. The principal aim of the present study was to evaluate vocal singing skills in Italian pediatric CI users. A further aim was to investigate factors that may significantly influence their performance. Methods: The participants were twenty-two implanted children and twenty-two hearing peers. Their vocal singing skills for familiar (“Happy Birthday to You”) and unfamiliar songs (“Baton Twirler” from Pam Pam 2- Tribute to Gordon) were evaluated in relation to their music perception (the Gordon test). Acoustic analysis was performed using Praat and MATLAB software. Nonparametric statistical tests and principal component analysis (PCA) were used to analyze the data. Results: Hearing children outperformed implanted peers in both music perception and vocal singing tasks (all measures regarding intonation, vocal range, melody, and memory for the familiar song versus measures regarding intonation and overall melody production for the unfamiliar song). Music perception and vocal singing performances revealed strong correlations. For the familiar and unfamiliar songs, age-appropriate vocal singing was observed in 27.3% versus 45.4% of children, all implanted within 24 months of age. Age at implantation and duration of CI experience were moderately correlated with the total score obtained from the Gordon test. Conclusion: Implanted children show limited vocal singing skills in comparison to their hearing peers. However, some children implanted within 24 months of age seem to achieve vocal singing skills as good as their hearing peers. Future research could be useful to better understand the role of brain plasticity to implement specific training programs for both music perception and vocal singing. © 2023 Elsevier B.V.",scopus,2-s2.0-85160107869,10.1016/j.ijporl.2023.111605
speech development for children with impaired hearingdp   2021,"This chapter provides a brief description on speech development for children with impaired hearing. Daniel Ling's Speech and the Hearing-Impaired Child: Theory and Practice (1976) had a strong influence on speech practices of the last quarter of the 20th century. This replacement chapter honors his legacy with many of his insights that remain relevant today and with revised information from the 21st century. In the area of communication, the authors generally report two areas: speech perception and speech production. Audiologists usually document speech perception and speech-language pathologists (SLPs) usually document speech production. Ling also designed two speech production tests to determine a child's speech performance and speech potential. The Phonetic Level Evaluation is a repeat-after-me test divided into (1) vocal quality or suprasegments, with varied duration, intensity, and pitch patterns assessed with consonant-vowel syllables; and then segments, concentrating on (2) vowel and diphthong productions and (3) with three consonants steps, representing a developmental progression of consonants in three steps; and, finally, two stages of consonant blends or clusters: (4) world-initial consonant blends and (5) word-final consonant blends. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc20&AN=2019-33408-008
gene expression profiling of dba2j mice cochleae treated with lmethionine and valproic acid,"DBA/2J mice, which have homozygous mutations in Cdh23 and Fscn2, are characterized by early onset hearing loss at as early as three-weeks of age (Noben-Trauth et al., 2003 [1]) and are an animal model for progressive hearing loss research. Recently, it has been reported that epigenetic regulatory pathways likely play an important role in hearing loss (Provenzano and Domann, 2007 [2]; Mutai et al., 2009 [3]; Waldhaus et al., 2012 [4]). We previously reported that DBA/2J mice injected subcutaneously with a combination of epigenetic modifying reagents, l-methionine (MET) as methyl donor and valproic acid (VPA) as a pan-histone deacetylases (Hdac) inhibitor, showed a significant attenuation of progressive hearing loss by measuring their auditory brainstem response (ABR) thresholds (Mutai et al., 2015 [5]). Here we present genome wide expression profiling of the DBA/2J mice cochleae, with and without treatment of MET and VPA, to identify the genes involved in the reduction of progressive hearing loss. The raw and normalized data were deposited in NCBI's Gene Expression Omnibus (GEO ID: GSE62173) for ease of reproducibility and reanalysis. © 2015.",scopus,2-s2.0-84937217276,10.1016/j.gdata.2015.06.022
agerelated hearing loss increases fullbrain connectivity while reversing directed signaling within the dorsalventral pathway for speech,"118. Brain Struct Funct. 2019 Nov;224(8):2661-2676. doi: 10.1007/s00429-019-01922-9. Epub 2019 Jul 25.Age-related hearing loss increases full-brain connectivity while reversing directed signaling within the dorsal-ventral pathway for speech.Bidelman GM(1)(2)(3), Mahmud MS(4), Yeasin M(4), Shen D(5), Arnott SR(5), Alain C(5)(6)(7).Author information:(1)Institute for Intelligent Systems, University of Memphis, Memphis, TN, USA. gmbdlman@memphis.edu.(2)School of Communication Sciences and Disorders, University of Memphis, 4055 North Park Loop, Memphis, TN, 38152, USA. gmbdlman@memphis.edu.(3)Department of Anatomy and Neurobiology, University of Tennessee Health Sciences Center, Memphis, TN, USA. gmbdlman@memphis.edu.(4)Department of Electrical and Computer Engineering, University of Memphis, Memphis, TN, USA.(5)Rotman Research Institute-Baycrest Centre for Geriatric Care, Toronto, ON, Canada.(6)Department of Psychology, University of Toronto, Toronto, ON, Canada.(7)Institute of Medical Sciences, University of Toronto, Toronto, ON, Canada.Speech comprehension difficulties are ubiquitous to aging and hearing loss, particularly in noisy environments. Older adults' poorer speech-in-noise (SIN) comprehension has been related to abnormal neural representations within various nodes (regions) of the speech network, but how senescent changes in hearing alter the transmission of brain signals remains unspecified. We measured electroencephalograms in older adults with and without mild hearing loss during a SIN identification task. Using functional connectivity and graph-theoretic analyses, we show that hearing-impaired (HI) listeners have more extended (less integrated) communication pathways and less efficient information exchange among widespread brain regions (larger network eccentricity) than their normal-hearing (NH) peers. Parameter optimized support vector machine classifiers applied to EEG connectivity data showed hearing status could be decoded (> 85% accuracy) solely using network-level descriptions of brain activity, but classification was particularly robust using left hemisphere connections. Notably, we found a reversal in directed neural signaling in left hemisphere dependent on hearing status among specific connections within the dorsal-ventral speech pathways. NH listeners showed an overall net ""bottom-up"" signaling directed from auditory cortex (A1) to inferior frontal gyrus (IFG; Broca's area), whereas the HI group showed the reverse signal (i.e., ""top-down"" Broca's → A1). A similar flow reversal was noted between left IFG and motor cortex. Our full-brain connectivity results demonstrate that even mild forms of hearing loss alter how the brain routes information within the auditory-linguistic-motor loop.DOI: 10.1007/s00429-019-01922-9PMCID: PMC6778722",pubmed,31346715,10.1007/s00429-019-01922-9
identification of novel cholesteatomarelated gene expression signatures using fullgenome microarrays,"634. PLoS One. 2012;7(12):e52718. doi: 10.1371/journal.pone.0052718. Epub 2012 Dec 20.Identification of novel cholesteatoma-related gene expression signatures using full-genome microarrays.Klenke C(1), Janowski S, Borck D, Widera D, Ebmeyer J, Kalinowski J, Leichtle A, Hofestädt R, Upile T, Kaltschmidt C, Kaltschmidt B, Sudhoff H.Author information:(1)Department of Otolaryngology and Head and Neck Surgery, Klinikum Bielefeld, Bielefeld, Germany. christin.zander@klinikumbielefeld.deBACKGROUND: Cholesteatoma is a gradually expanding destructive epithelial lesion within the middle ear. It can cause extensive local tissue destruction in the temporal bone and can initially lead to the development of conductive hearing loss via ossicular erosion. As the disease progresses, sensorineural hearing loss, vertigo or facial palsy may occur. Cholesteatoma may promote the spread of infection through the tegmen of the middle ear and cause meningitis or intracranial infections with abscess formation. It must, therefore, be considered as a potentially life-threatening middle ear disease.METHODS AND FINDINGS: In this study, we investigated differentially expressed genes in human cholesteatomas in comparison to regular auditory canal skin using Whole Human Genome Microarrays containing 19,596 human genes. In addition to already described up-regulated mRNAs in cholesteatoma, such as MMP9, DEFB2 and KRT19, we identified 3558 new cholesteatoma-related transcripts. 811 genes appear to be significantly differentially up-regulated in cholesteatoma. 334 genes were down-regulated more than 2-fold. Significantly regulated genes with protein metabolism activity include matrix metalloproteinases as well as PI3, SERPINB3 and SERPINB4. Genes like SPP1, KRT6B, PRPH, SPRR1B and LAMC2 are known as genes with cell growth and/or maintenance activity. Transport activity genes and signal transduction genes are LCN2, GJB2 and CEACAM6. Three cell communication genes were identified; one CDH19 and two from the S100 family.CONCLUSIONS: This study demonstrates that the expression profile of cholesteatoma is similar to a metastatic tumour and chronically inflamed tissue. Based on the investigated profiles we present novel protein-protein interaction and signal transduction networks, which include cholesteatoma-regulated transcripts and may be of great value for drug targeting and therapy development.DOI: 10.1371/journal.pone.0052718PMCID: PMC3527606",pubmed,23285167,10.1371/journal.pone.0052718
the european gwasidentified risk snp rs457717 within iqgap2 is not associated with agerelated hearing impairment in han male chinese population,"378. Eur Arch Otorhinolaryngol. 2016 Jul;273(7):1677-87. doi: 10.1007/s00405-015-3711-9. Epub 2015 Jul 18.The European GWAS-identified risk SNP rs457717 within IQGAP2 is not associated with age-related hearing impairment in Han male Chinese population.Luo H(1), Wu H(2)(3), Shen H(4), Chen H(5)(6), Yang T(2)(3), Huang Z(2)(3), Jin X(1), Pang X(2)(3), Li L(2)(3), Hu X(2), Jiang X(3)(7), Fan Z(8), Li J(9).Author information:(1)Department of Otolaryngology, Renji Hospital, School of Medicine, Shanghai Jiaotong University, 160 Pujian Road Pudong New Area, Shanghai, 200127, People's Republic of China.(2)Department of Otolaryngology-Head and Neck Surgery, Xinhua Hospital, Shanghai Jiaotong University, Shanghai, People's Republic of China.(3)Ear Institute, School of Medicine, Shanghai Jiaotong University, Shanghai, People's Republic of China.(4)Renji-Med X Clinical Stem Cell Research Center, Renji Hospital, School of Medicine, Shanghai Jiaotong University, Shanghai, People's Republic of China.(5)Shanghai Center for Bioinformation Technology, Shanghai, 200235, People's Republic of China.(6)Department of Bioinformatics and Biostatistics, College of Life Sciences and Biotechnology, Shanghai Jiaotong University, Shanghai, 200240, People's Republic of China.(7)Health Check-up Center, Xinhua Hospital, School of Medicine, Shanghai Jiaotong University, Shanghai, People's Republic of China.(8)Health Check-up Center, Renji Hospital, School of Medicine, Shanghai Jiaotong University, Shanghai, People's Republic of China.(9)Department of Otolaryngology, Renji Hospital, School of Medicine, Shanghai Jiaotong University, 160 Pujian Road Pudong New Area, Shanghai, 200127, People's Republic of China. drlijiping@163.com.This study aimed to test the association between the European GWAS-identified risk IQGAP2 SNP rs457717 (A>G) and age-related hearing impairment (ARHI) in a Han male Chinese (HMC) population. A total of 2420 HMC subjects were divided into two groups [group 70+: >70 years (n = 1306), and group 70-: ≤70 years (n = 1114)]. The participants were categorised into case and control groups according to Z high scores for group 70- and the severity of hearing loss and different audiogram shapes identified by K-means cluster analysis for group 70+. The IQGAP2 tagSNP rs457717 was genotyped in accordance with the different ARHI phenotypes. The genotype distributions of IQGAP2 (AA/AG/GG) were not significantly different between the case and control groups (P = 0.613 for group 70-; P = 0.602 for group 70+). Compared with genotype AA, the ORs of genotypes AG and GG for ARHI were not significantly different following adjustment for other environmental risk factors. We demonstrated that the IQGAP2 TagSNP rs457717 (A/G) was not associated with ARHI in HMC individuals.DOI: 10.1007/s00405-015-3711-9",pubmed,26187738,10.1007/s00405-015-3711-9
case report of a new coupler for round window application of an active middle ear implant,"565. Otol Neurotol. 2018 Dec;39(10):e1060-e1063. doi: 10.1097/MAO.0000000000001996.Case Report of a New Coupler for Round Window Application of an Active Middle Ear Implant.Lenarz T(1)(2), Zimmermann D(1), Maier H(1)(2), Busch S(1)(2).Author information:(1)Department of Otorhinolaryngology, Hannover Medical School.(2)Cluster of Excellence EXC 1077/1 ""Hearing4all"", Hannover, Germany.OBJECTIVES: To evaluate feasibility, surgical handling, audiological outcome, and coupling efficiency of a new coupler (custom-made device) for an active middle ear implant.PATIENT: Revision surgery after implantation of an active middle ear implant in a 66-year-old male patient with mixed hearing loss.INTERVENTION: Prosthetic hearing rehabilitation with a new coupler for round window application.MAIN OUTCOME AND RESULTS: The patient obtained good speech perception in quiet (word recognition scores 80%; Freiburg monosyllables) and noise (-3.3 dB SNR; Oldenburg Sentence Test). The effective gain with the Hannover coupler improved at frequencies > 0.5 kHz compared with the values reported for other round window (RW)-coupling modalities.CONCLUSION: The coupler provides a feasible option for the RW application of the middle ear implant actuator. The spring concept of the coupler needs to be improved to further standardize RW-coupling and improve coupling efficiency at low frequencies (0.5 kHz).DOI: 10.1097/MAO.0000000000001996",pubmed,30239437,10.1097/MAO.0000000000001996
cochlear implant speech intelligibility outcomes with structured and unstructured binary mask errors,"487. J Acoust Soc Am. 2016 Feb;139(2):800-10. doi: 10.1121/1.4941567.Cochlear implant speech intelligibility outcomes with structured and unstructured binary mask errors.Kressner AA(1), Westermann A(1), Buchholz JM(1), Rozell CJ(2).Author information:(1)National Acoustic Laboratories, Australian Hearing, 16 University Avenue, Macquarie University, New South Wales 2109, Australia.(2)School of Electrical and Computer Engineering, 777 Atlantic Drive Northwest, Georgia Institute of Technology, Atlanta, Georgia 30332, USA.It has been shown that intelligibility can be improved for cochlear implant (CI) recipients with the ideal binary mask (IBM). In realistic scenarios where prior information is unavailable, however, the IBM must be estimated, and these estimations will inevitably contain errors. Although the effects of both unstructured and structured binary mask errors have been investigated with normal-hearing (NH) listeners, they have not been investigated with CI recipients. This study assesses these effects with CI recipients using masks that have been generated systematically with a statistical model. The results demonstrate that clustering of mask errors substantially decreases the tolerance of errors, that incorrectly removing target-dominated regions can be as detrimental to intelligibility as incorrectly adding interferer-dominated regions, and that the individual tolerances of the different types of errors can change when both are present. These trends follow those of NH listeners. However, analysis with a mixed effects model suggests that CI recipients tend to be less tolerant than NH listeners to mask errors in most conditions, at least with respect to the testing methods in each of the studies. This study clearly demonstrates that structure influences the tolerance of errors and therefore should be considered when analyzing binary-masking algorithms.DOI: 10.1121/1.4941567",pubmed,26936562,10.1121/1.4941567
segmental and suprasegmental properties in nonword repetition  an explorative study of the associations with nonword decoding in children with normal hearing and children with bilateral cochlear implants,"This study explored nonword repetition (NWR) and nonword decoding in normal-hearing (NH) children and in children with bilateral cochlear implants (CI). Participants were 11 children, with CI, 5:0-7:11 years (M = 6.5 years), and 11 NH children, individually age-matched to the children with CI. This study fills an important gap in research, since it thoroughly describes detailed aspects of NWR and nonword decoding and their possible associations. All children were assessed after having practiced with a computer-assisted reading intervention with a phonics approach during four weeks. Results showed that NH children outperformed children with CI on the majority of aspects of NWR. The analysis of syllable number in NWR revealed that children with CI made more syllable omissions than did the NH children, and predominantly in prestressed positions. In addition, the consonant cluster analysis in NWR showed significantly more consonant omissions and substitutions in children with CI suggesting that reaching fine-grained levels of phonological processing was particularly difficult for these children. No significant difference was found for nonword-decoding accuracy between the groups, as measured by whole words correct and phonemes correct, but differences were observed regarding error patterns. In children with CI phoneme, deletions occurred significantly more often than in children with NH. The correlation analysis revealed that the ability to repeat consonant clusters in NWR had the strongest associations to nonword decoding in both groups. The absence of as frequent significant associations between NWR and nonword decoding in children with CI compared to children with NH suggest that these children partly use other decoding strategies to compensate for less precise phonological knowledge, for example, lexicalizations in nonword decoding, specifically, making a real word of a nonword. © 2015 Informa UK Ltd.",scopus,2-s2.0-84923000043,10.3109/02699206.2014.987926
agerelated hearing loss influences functional connectivity of auditory cortex for the mcgurk illusion,"54. Cortex. 2020 Aug;129:266-280. doi: 10.1016/j.cortex.2020.04.022. Epub 2020 May 16.Age-related hearing loss influences functional connectivity of auditory cortex for the McGurk illusion.Rosemann S(1), Smith D(2), Dewenter M(3), Thiel CM(4).Author information:(1)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany; Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany. Electronic address: Stephanie.rosemann@uni-oldenburg.de.(2)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany. Electronic address: drsmith@bu.edu.(3)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany. Electronic address: mariemhdw@aol.com.(4)Biological Psychology, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany; Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany. Electronic address: Christiane.thiel@uol.de.Age-related hearing loss affects hearing at high frequencies and is associated with difficulties in understanding speech. Increased audio-visual integration has recently been found in age-related hearing impairment, the brain mechanisms that contribute to this effect are however unclear. We used functional magnetic resonance imaging in elderly subjects with normal hearing and mild to moderate uncompensated hearing loss. Audio-visual integration was studied using the McGurk task. In this task, an illusionary fused percept can occur if incongruent auditory and visual syllables are presented. The paradigm included unisensory stimuli (auditory only, visual only), congruent audio-visual and incongruent (McGurk) audio-visual stimuli. An illusionary precept was reported in over 60% of incongruent trials. These McGurk illusion rates were equal in both groups of elderly subjects and correlated positively with speech-in-noise perception and daily listening effort. Normal-hearing participants showed an increased neural response in left pre- and postcentral gyri and right middle frontal gyrus for incongruent stimuli (McGurk) compared to congruent audio-visual stimuli. Activation patterns were however not different between groups. Task-modulated functional connectivity differed between groups showing increased connectivity from auditory cortex to visual, parietal and frontal areas in hard of hearing participants as compared to normal-hearing participants when comparing incongruent stimuli (McGurk) with congruent audio-visual stimuli. These results suggest that changes in functional connectivity of auditory cortex rather than activation strength during processing of audio-visual McGurk stimuli accompany age-related hearing loss.Copyright © 2020 Elsevier Ltd. All rights reserved.DOI: 10.1016/j.cortex.2020.04.022",pubmed,32535378,10.1016/j.cortex.2020.04.022
a cloudbased 3d digital twin for arabic sign language alphabet using machine learning object detection model,"People with hearing loss or hard hearing struggle with daily life activities as sign language is not widely known by the public. There are many attempts to use technology to help assist hearing loss individuals. However, most proposed solutions are standalone applications or require special hardware like a wearable glove. Our goal is to leverage cloud computing and artificial intelligence (AI) to provide a solution that is portable and does not require any special hardware. We created a lightweight 3D model and rendered it on the browser along with another lightweight object detection model for Arabic Sign Language (ArSL) for real-time detection. Our contribution is primarily based on integrating our novel functional lightweight 3D avatar model and a lightweight ArSL alphabet detection model, which is trained on public ArSL21L dataset, that are suitable to be given as a cloud service. Prototypes of the 3D digital twin avatar model and AI model are publicly offered for the research community on GitHub. We will be working on a full-scale real-time cloud-based communication system in ArSL.",ieee,2473-2052,10.1109/IIT59782.2023.10366491
mutations in the wfs1 gene that cause lowfrequency sensorineural hearing loss are small noninactivating mutations,"339. Hum Genet. 2002 May;110(5):389-94. doi: 10.1007/s00439-002-0719-1. Epub 2002 Apr 9.Mutations in the WFS1 gene that cause low-frequency sensorineural hearing loss are small non-inactivating mutations.Cryns K(1), Pfister M, Pennings RJ, Bom SJ, Flothmann K, Caethoven G, Kremer H, Schatteman I, Köln KA, Tóth T, Kupka S, Blin N, Nürnberg P, Thiele H, van de Heyning PH, Reardon W, Stephens D, Cremers CW, Smith RJ, Van Camp G.Author information:(1)Department of Medical Genetics, University of Antwerp, Universiteitsplein 1, B-2610 Antwerp, Belgium.Hereditary hearing impairment is an extremely heterogeneous trait, with more than 70 identified loci. Only two of these loci are associated with an auditory phenotype that predominantly affects the low frequencies (DFNA1 and DFNA6/14). In this study, we have completed mutation screening of the WFS1 gene in eight autosomal dominant families and twelve sporadic cases in which affected persons have low-frequency sensorineural hearing impairment (LFSNHI). Mutations in this gene are known to be responsible for Wolfram syndrome or DIDMOAD (diabetes insipidus, diabetes mellitus, optic atrophy, and deafness), which is an autosomal recessive trait. We have identified seven missense mutations and a single amino acid deletion affecting conserved amino acids in six families and one sporadic case, indicating that mutations in WFS1 are a major cause of inherited but not sporadic low-frequency hearing impairment. Among the ten WFS1 mutations reported in LFSNHI, none is expected to lead to premature protein truncation, and nine cluster in the C-terminal protein domain. In contrast, 64% of the Wolfram syndrome mutations are inactivating. Our results indicate that only non-inactivating mutations in WFS1 are responsible for non-syndromic low-frequency hearing impairment.DOI: 10.1007/s00439-002-0719-1",pubmed,12073007,10.1007/s00439-002-0719-1
intelligent learning systems for inclusive education in ghana towards an effective engagement with hard of hearing students,"This research attempts to address some challenges faced when facilitating an inclusive classroom experience for Deaf/Hard of Hearing students (D/HH) in Ghanaian schools and universities by proposing an intelligent learning system. Hence, this research contributes an Intelligent and Inclusive Learning System (IILS) with two main components: signWithMe subsystem and audio-visual transcription system (AVTS) subsystem.The signWithMe subsystem presents a Ghanaian sign language learning management system (LMS) that is motivated by the limited sign language literacy in Ghana. This subsystem consists of a sign language dictionary, an E-resource, an E-forum, and an E-payment system. It supports inclusive learning of the Ghanaian sign language for both Deaf and Non-Deaf students to interact with the system. This ensures improved diversity and inclusion in our Ghanaian society. Also, the AVTS focuses on improving the teaching and learning experience of Deaf students in Ghanaian schools. The AVTS contributes a novel artificial intelligence approach that uses a counter-checking of Google speech-to-text and lip-reading-to-text transcriptions of the Ghanaian sign language in inclusive university lectures. These two subsystems provide services in line with the mandate of the tenth and fourth Sustainable Development Goal (S.D.G.) of “Reduced Inequality” and “Quality Education” respectively.Finally, the IILS was analysed per the two main components (i.e., signWithMe and AVTS) and are considered to have high level of usability by 20 Deaf and 5 Non-Deaf users.",ieee,,10.1109/IUCE55902.2022.10079372
diagnosing and screening in a minority language a validation study,"159. Am J Audiol. 2017 Oct 12;26(3S):369-372. doi: 10.1044/2017_AJA-16-0138.Diagnosing and Screening in a Minority Language: A Validation Study.Zokoll MA(1)(2)(3), Wagener KC(2)(3), Kollmeier B(1)(2)(3)(4).Author information:(1)Medical Physics, Carl von Ossietzky University Oldenburg, Germany.(2)Hörzentrum Oldenburg GmbH, Germany.(3)Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany.(4)HörTech gGmbH, Oldenburg, Germany.PURPOSE: The Turkish Digit Triplet Test for hearing self-screening purposes and the Turkish Matrix Test (TURMatrix) for follow-up hearing diagnostics offer an automated closed-set response format where patients respond by choosing from response alternatives. Their applicability for testing Turkish-speaking patients in their native language by German audiologists with different Turkish language skills was investigated.METHOD: Tests were composed of spoken numbers (Turkish Digit Triplet Test) or sentences (TURMatrix). For 49 participants differing in hearing ability, speech reception thresholds (SRTs) in noise and quiet were obtained, for the TURMatrix with either the open- or closed-set response format, by audiologists with and without Turkish language skills, respectively.RESULTS: SRTs of both tests correlate closely with each other as well as with hearing ability, but not as closely as individual SRTs in quiet with hearing ability. SRTs in noise of listeners with normal hearing were about 0.7 dB lower for the closed-set than for the open-set response format.CONCLUSIONS: The 2 tests yield comparable results and are applicable to professionals without suitable language skills. For the closed-set response format of the TURMatrix, literacy is crucial and supplemental (visual) cues improve performance. Speech audiometry in noise should assess suprathreshold processing deficits independently from language proficiency in the majority language.DOI: 10.1044/2017_AJA-16-0138",pubmed,29049620,10.1044/2017_AJA-16-0138
novel genetic and neuropathological insights in neurogenic muscle weakness ataxia and retinitis pigmentosa narp,"Introduction: Neurogenic muscle weakness, ataxia, and retinitis pigmentosa (NARP) is caused by m.8993T>G/C mutations in the mitochondrial adenosine triphosphate synthase subunit 6 gene (MT-ATP6). Traditionally, heteroplasmy levels between 70% and 90% lead to NARP, and >90% result in Leigh syndrome. Methods: In this study we report a 30-year-old man with NARP and m.8993T>G in MT-ATP6. Results: Although the patient carried the mutation in homoplasmic state in blood with similarly high levels in urine (94%) and buccal swab (92%), he presented with NARP and not the expected, more severe Leigh phenotype. The mutation could not be detected in any of the 3 analyzed tissues of the mother, indicating a large genetic shift between mother and offspring. Nerve biopsy revealed peculiar endoneurial Schwann cell nuclear accumulations, clusters of concentrically arranged Schwann cells devoid of myelinated axons, and degenerated mitochondria. Conclusions: We emphasize the phenotypic variability of the m.8993T>G MT-ATP6 mutation and the need for caution in predictive counseling in such patients. Muscle Nerve 54: 328–333, 2016. © 2016 Wiley Periodicals, Inc.",scopus,2-s2.0-84978863817,10.1002/mus.25125
peripheral hearing loss and its association with cognition among ethnic chinese older adults,"62. Dement Geriatr Cogn Disord. 2021;50(4):394-400. doi: 10.1159/000519291. Epub 2021 Sep 30.Peripheral Hearing Loss and Its Association with Cognition among Ethnic Chinese Older Adults.Nicholas SO(1), Koh EJ(1), Wee SL(1)(2)(3), Eikelboom RH(4)(5)(6), Jayakody DMP(4)(5), Lin F(7)(8), Ng TP(1)(9), Heywood RL(4)(5)(10)(11).Author information:(1)Geriatric Education and Research Institute (GERI), Singapore, Singapore.(2)Singapore Institute of Technology, Health and Social Sciences Cluster, Singapore, Singapore.(3)Duke-NUS Medical School, Singapore, Singapore.(4)Ear Science Institute Australia, Subiaco, Washington, Australia.(5)Ear Sciences Centre, Medical School, The University of Western Australia, Subiaco, Washington, Australia.(6)Department of Speech Language Pathology and Audiology, University of Pretoria, Pretoria, South Africa.(7)Cochlear Center for Hearing and Public Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, Maryland, USA.(8)Departments of Otolaryngology, Medicine, Mental Health, and Epidemiology, Johns Hopkins University, Baltimore, Maryland, USA.(9)Department of Psychological Medicine, Gerontology Research Program, National University of Singapore (NUS), Singapore, Singapore.(10)Department of Otolaryngology, Ng Teng Fong General Hospital, Singapore, Singapore.(11)Yong Loo Lin School of Medicine, National University of Singapore, Singapore, Singapore.INTRODUCTION: Many studies on hearing loss (HL) and cognition are limited by subjective hearing assessments and verbally administered cognition tests, the majority of the document findings in Western populations. This study aimed to assess the association of HL with cognitive impairment among ethnic Chinese Singaporean older adults using visually presented cognitive tests.METHODS: The hearing of community-dwelling older adults was assessed using pure tone audiometry. Cognitive function was assessed using the Computerized Cambridge Cognitive Test Battery (CANTAB). Multiple regression analyses examined the association between hearing and cognitive function, adjusted for age, education, and gender.RESULTS: HL (pure-tone average [PTA] of thresholds at 0.5, 1, 2, and 4 kHz in the better ear, BE4PTA) was associated with reduced performance in delayed matching and multitasking tasks (β = -0.25, p = 0.019, and β = 0.02, p = 0.023, respectively). Moderate to severe HL was associated with reduced performance in delayed matching and verbal recall memory tasks (β = -10.6, p = 0.019, and β = -0.28, p = 0.042). High-frequency HL was associated with reduced performance in the spatial working memory task (β = 0.004, p = 0.022). All-frequency HL was associated with reduced performance in spatial working memory and multitasking (β = 0.01, p = 0.040, and β = 0.02, p = 0.048).CONCLUSION: Similar to Western populations, HL among tonal language-speaking ethnic Chinese was associated with worse performance in tasks requiring working memory and executive function.© 2021 The Author(s). Published by S. Karger AG, Basel.DOI: 10.1159/000519291",pubmed,34592737,10.1159/000519291
development of a german reading span test with dual task design for application in cognitive hearing research,"418. Int J Audiol. 2015 Feb;54(2):136-41. doi: 10.3109/14992027.2014.952458. Epub 2014 Sep 8.Development of a German reading span test with dual task design for application in cognitive hearing research.Carroll R(1), Meis M, Schulte M, Vormann M, Kießling J, Meister H.Author information:(1)* Cluster of Excellence 'Hearing4all' , Oldenburg , Germany.OBJECTIVE: To report the development of a standardized German version of a reading span test (RST) with a dual task design. Special attention was paid to psycholinguistic control of the test items and time-sensitive scoring. We aim to establish our RST version to use for determining an individual's working memory in the framework of hearing research in German contexts.DESIGN: RST stimuli were controlled and pretested for psycholinguistic factors. The RST task was to read sentences, quickly determine their plausibility, and later recall certain words to determine a listener's individual reading span. RST results were correlated with outcomes of additional sentence-in-noise tests measured in an aided and an unaided listening condition, each at two reception thresholds.STUDY SAMPLE: Item plausibility was pre-determined by 28 native German participants. An additional 62 listeners (45-86 years, M = 69.8) with mild-to-moderate hearing loss were tested for speech intelligibility and reading span in a multicenter study.RESULTS: The reading span test significantly correlated with speech intelligibility at both speech reception thresholds in the aided listening condition.CONCLUSION: Our German RST is standardized with respect to psycholinguistic construction principles of the stimuli, and is a cognitive correlate of intelligibility in a German matrix speech-in-noise test.DOI: 10.3109/14992027.2014.952458",pubmed,25195607,10.3109/14992027.2014.952458
ledbased optical cochlear implants for spectrally selective activation of the auditory nerve,"819. EMBO Mol Med. 2020 Aug 7;12(8):e12387. doi: 10.15252/emmm.202012387. Epub 2020 Jun 29.μLED-based optical cochlear implants for spectrally selective activation of the auditory nerve.Dieter A(#)(1)(2), Klein E(#)(3), Keppeler D(1), Jablonski L(1)(4), Harczos T(1)(4), Hoch G(1)(4), Rankovic V(1)(4)(5), Paul O(3)(6), Jeschke M(1)(4)(7), Ruther P(3)(6), Moser T(1)(2)(4)(8)(9).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(2)Göttingen Graduate School for Neurosciences and Molecular Biosciences, University of Göttingen, Göttingen, Germany.(3)Department of Microsystems Engineering (IMTEK), University of Freiburg, Freiburg, Germany.(4)Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(5)Restorative Cochlear Genomics Group, Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(6)BrainLinks-BrainTools, Cluster of Excellence, University of Freiburg, Freiburg, Germany.(7)Cognitive Hearing in Primates Group, Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(8)Auditory Neuroscience Group, Max Planck Institute for Experimental Medicine, Göttingen, Germany.(9)Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Goettingen, Goettingen, Germany.(#)Contributed equallyElectrical cochlear implants (eCIs) partially restore hearing and enable speech comprehension to more than half a million users, thereby re-connecting deaf patients to the auditory scene surrounding them. Yet, eCIs suffer from limited spectral selectivity, resulting from current spread around each electrode contact and causing poor speech recognition in the presence of background noise. Optogenetic stimulation of the auditory nerve might overcome this limitation as light can be conveniently confined in space. Here, we combined virus-mediated optogenetic manipulation of cochlear spiral ganglion neurons (SGNs) and microsystems engineering to establish acute multi-channel optical cochlear implant (oCI) stimulation in adult Mongolian gerbils. oCIs based on 16 microscale thin-film light-emitting diodes (μLEDs) evoked tonotopic activation of the auditory pathway with high spectral selectivity and modest power requirements in hearing and deaf gerbils. These results prove the feasibility of μLED-based oCIs for spectrally selective activation of the auditory nerve.© 2020 The Authors. Published under the terms of the CC BY 4.0 license.DOI: 10.15252/emmm.202012387PMCID: PMC7411546",pubmed,32596983,10.15252/emmm.202012387
comparing binaural preprocessing strategies i instrumental evaluation,"258. Trends Hear. 2015 Dec 30;19:2331216515617916. doi: 10.1177/2331216515617916.Comparing Binaural Pre-processing Strategies I: Instrumental Evaluation.Baumgärtel RM(1), Krawczyk-Becker M(2), Marquardt D(3), Völker C(4), Hu H(4), Herzke T(5), Coleman G(5), Adiloğlu K(5), Ernst SM(4), Gerkmann T(2), Doclo S(3), Kollmeier B(4), Hohmann V(6), Dietz M(4).Author information:(1)Medical Physics Group, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany Regina.Baumgaertel@uni-oldenburg.de.(2)Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany Speech Signal Processing Group, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany.(3)Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany Signal Processing Group, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany.(4)Medical Physics Group, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany.(5)Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany HörTech gGmbH, Oldenburg, Germany.(6)Medical Physics Group, Carl von Ossietzky Universität Oldenburg, Oldenburg, Germany Cluster of Excellence ""Hearing4all,"" Oldenburg, Germany HörTech gGmbH, Oldenburg, Germany.In a collaborative research project, several monaural and binaural noise reduction algorithms have been comprehensively evaluated. In this article, eight selected noise reduction algorithms were assessed using instrumental measures, with a focus on the instrumental evaluation of speech intelligibility. Four distinct, reverberant scenarios were created to reflect everyday listening situations: a stationary speech-shaped noise, a multitalker babble noise, a single interfering talker, and a realistic cafeteria noise. Three instrumental measures were employed to assess predicted speech intelligibility and predicted sound quality: the intelligibility-weighted signal-to-noise ratio, the short-time objective intelligibility measure, and the perceptual evaluation of speech quality. The results show substantial improvements in predicted speech intelligibility as well as sound quality for the proposed algorithms. The evaluated coherence-based noise reduction algorithm was able to provide improvements in predicted audio signal quality. For the tested single-channel noise reduction algorithm, improvements in intelligibility-weighted signal-to-noise ratio were observed in all but the nonstationary cafeteria ambient noise scenario. Binaural minimum variance distortionless response beamforming algorithms performed particularly well in all noise scenarios.© The Author(s) 2015.DOI: 10.1177/2331216515617916PMCID: PMC4771044",pubmed,26721920,10.1177/2331216515617916
defining interdisciplinary competencies for audiological rehabilitation findings from a modified delphi study,"Objectives: The aim of this study is to derive a consensus on an interdisciplinary competency framework regarding a holistic approach for audiological rehabilitation (AR), which includes disciplines from medicine, engineering, social sciences and humanities. Design: We employed a modified Delphi method. In the first round survey, experts were asked to rate an initial list of 28 generic interdisciplinary competencies and to propose specific knowledge areas for AR. In the second round, experts were asked to reconsider their answers in light of the group answers of the first round. Study sample: An international panel of 27 experts from different disciplines in AR completed the first round. Twenty-two of them completed the second round. Results: We developed a competency framework consisting of 21 generic interdisciplinary competencies grouped in five domains and nine specific competencies (knowledge areas) in three clusters. Suggestions for the implementation of the generic competencies in interdisciplinary programmes were identified. Conclusions: This study reveals insights into the interdisciplinary competencies that are unique for AR. The framework will be useful for educators in developing interdisciplinary programmes as well as for professionals in considering their lifelong training needs in AR. © 2017 British Society of Audiology, International Society of Audiology, and Nordic Audiological Society.",scopus,2-s2.0-85035758909,10.1080/14992027.2017.1406156
scn2a encephalopathy,"Objective: De novo SCN2A mutations have recently been associated with severe infantile-onset epilepsies. Herein, we define the phenotypic spectrum of SCN2A encephalopathy. Methods: Twelve patients with an SCN2A epileptic encephalopathy underwent electroclinical phenotyping. Results: Patients were aged 0.7 to 22 years; 3 were deceased. Seizures commenced on day 1-4 in 8, week 2-6 in 2, and after 1 year in 2. Characteristic features included clusters of brief focal seizures with multiple hourly (9 patients), multiple daily (2), or multiple weekly (1) seizures, peaking at maximal frequency within 3 months of onset. Multifocal interictal epileptiform discharges were seen in all. Three of 12 patients had infantile spasms. The epileptic syndrome at presentation was epilepsy of infancy with migrating focal seizures (EIMFS) in 7 and Ohtahara syndrome in 2. Nine patients had improved seizure control with sodium channel blockers including supratherapeutic or high therapeutic phenytoin levels in 5. Eight had severe to profound developmental impairment. Other features included movement disorders (10), axial hypotonia (11) with intermittent or persistent appendicular spasticity, early handedness, and severe gastrointestinal symptoms. Mutations arose de novo in 11 patients; paternal DNA was unavailable in one. Conclusions: Review of our 12 and 34 other reported cases of SCN2A encephalopathy suggests 3 phenotypes: neonatal-infantile-onset groups with severe and intermediate outcomes, and a childhood-onset group. Here, we show that SCN2A is the second most common cause of EIMFS and, importantly, does not always have a poor developmental outcome. Sodium channel blockers, particularly phenytoin, may improve seizure control. © 2015 American Academy of Neurology.",scopus,2-s2.0-84947230795,10.1212/WNL.0000000000001926
auditory brainstem response latency in noise as a marker of cochlear synaptopathy,"82. J Neurosci. 2016 Mar 30;36(13):3755-64. doi: 10.1523/JNEUROSCI.4460-15.2016.Auditory Brainstem Response Latency in Noise as a Marker of Cochlear Synaptopathy.Mehraei G(1), Hickox AE(2), Bharadwaj HM(3), Goldberg H(4), Verhulst S(5), Liberman MC(6), Shinn-Cunningham BG(7).Author information:(1)Program in Speech and Hearing Bioscience and Technology, Harvard University/Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, Center for Computational Neuroscience and Neural Technology, Boston University, Boston, Massachusetts 02215, gmehraei@mit.edu.(2)Program in Speech and Hearing Bioscience and Technology, Harvard University/Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, Eaton-Peabody Laboratory, Massachusetts Eye and Ear Infirmary, Boston, Massachusetts 02114.(3)Center for Computational Neuroscience and Neural Technology, Boston University, Boston, Massachusetts 02215, Martinos Center for Biomedical Imaging, Department of Neurology, Massachusetts General Hospital/Harvard Medical School, Charlestown, Massachusetts 02129.(4)Program in Speech and Hearing Bioscience and Technology, Harvard University/Massachusetts Institute of Technology, Cambridge, Massachusetts 02139.(5)Center for Computational Neuroscience and Neural Technology, Boston University, Boston, Massachusetts 02215, Cluster of Excellence Hearing4All and Medical Physics, Department of Medical Physics and Acoustics, Oldenburg University, 26129 Oldenburg, Germany.(6)Program in Speech and Hearing Bioscience and Technology, Harvard University/Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, Eaton-Peabody Laboratory, Massachusetts Eye and Ear Infirmary, Boston, Massachusetts 02114, Department of Otology and Laryngology, Harvard Medical School, Boston, Massachusetts 02114.(7)Center for Computational Neuroscience and Neural Technology, Boston University, Boston, Massachusetts 02215, Department of Biomedical Engineering, Boston University, Boston, Massachusetts 02215.Evidence from animal and human studies suggests that moderate acoustic exposure, causing only transient threshold elevation, can nonetheless cause ""hidden hearing loss"" that interferes with coding of suprathreshold sound. Such noise exposure destroys synaptic connections between cochlear hair cells and auditory nerve fibers; however, there is no clinical test of this synaptopathy in humans. In animals, synaptopathy reduces the amplitude of auditory brainstem response (ABR) wave-I. Unfortunately, ABR wave-I is difficult to measure in humans, limiting its clinical use. Here, using analogous measurements in humans and mice, we show that the effect of masking noise on the latency of the more robust ABR wave-V mirrors changes in ABR wave-I amplitude. Furthermore, in our human cohort, the effect of noise on wave-V latency predicts perceptual temporal sensitivity. Our results suggest that measures of the effects of noise on ABR wave-V latency can be used to diagnose cochlear synaptopathy in humans.SIGNIFICANCE STATEMENT: Although there are suspicions that cochlear synaptopathy affects humans with normal hearing thresholds, no one has yet reported a clinical measure that is a reliable marker of such loss. By combining human and animal data, we demonstrate that the latency of auditory brainstem response wave-V in noise reflects auditory nerve loss. This is the first study of human listeners with normal hearing thresholds that links individual differences observed in behavior and auditory brainstem response timing to cochlear synaptopathy. These results can guide development of a clinical test to reveal this previously unknown form of noise-induced hearing loss in humans.Copyright © 2016 the authors 0270-6474/16/363755-10$15.00/0.DOI: 10.1523/JNEUROSCI.4460-15.2016PMCID: PMC4812134",pubmed,27030760,10.1523/JNEUROSCI.4460-15.2016
chronic cluster headache managed by nervus intermedius section,"841. Headache. 1990 Jun;30(7):401-6. doi: 10.1111/j.1526-4610.1990.hed3007401.x.Chronic cluster headache managed by nervus intermedius section.Rowed DW(1).Author information:(1)Sunnybrook Health Sciences Centre, University of Toronto, Ontario, Canada.Cluster headache sufferers who become candidates for surgical treatment are those relatively rare patients who are refractory to all attempts at pharmacological relief. Ablative surgical procedures have been directed against either the trigeminal nerve or the nervus intermedius/greater superficial petrosal (NI/GSP) pathway. Both carry nociceptive impulses from the head and face, and the NI also carries parasympathetic fibres which appear to be responsible for the autonomic concomitants of cluster headache. Trigeminal operative procedures are not consistently helpful in chronic cluster headache, while NI section has been shown to give potentially long lasting relief but carries the potential risks of cerebellopontine angle surgery. In eight selected cases of chronic cluster headache we have demonstrated a high early success rate for pain relief, with few complications, in the performance of NI section, combined, when indicated, with microvascular decompression of the trigeminal main sensory root. We believe that cochlear nerve monitoring helps prevent postoperative hearing impairment. An intimate relationship between the NI and arterial loops of the anterior inferior cerebellar artery (AICA) or the internal auditory artery has been frequently observed in our chronic cluster headache patients.DOI: 10.1111/j.1526-4610.1990.hed3007401.x",pubmed,2401621,10.1111/j.1526-4610.1990.hed3007401.x
a systematic review on the genetic contribution to tinnitus,"596. J Assoc Res Otolaryngol. 2024 Feb;25(1):13-33. doi: 10.1007/s10162-024-00925-6. Epub 2024 Feb 9.A Systematic Review on the Genetic Contribution to Tinnitus.Perez-Carpena P(1)(2)(3), Lopez-Escamez JA(4)(5)(6), Gallego-Martinez Á(7)(8).Author information:(1)Otology and Neurotology Group CTS495, Division of Otolaryngology, Department of Surgery, Instituto de Investigación Biosanitaria, Ibs.GRANADA, Universidad de Granada, Granada, Spain. percarpena@ugr.es.(2)Sensorineural Pathology Programme, Centro de Investigación Biomédica en Red en Enfermedades Raras, CIBERER, Madrid, Spain. percarpena@ugr.es.(3)Department of Otolaryngology, Instituto de Investigación Biosanitaria Ibs.GRANADA, Hospital Universitario Virgen de Las Nieves, Granada, Spain. percarpena@ugr.es.(4)Otology and Neurotology Group CTS495, Division of Otolaryngology, Department of Surgery, Instituto de Investigación Biosanitaria, Ibs.GRANADA, Universidad de Granada, Granada, Spain. jose.lopezescamez@sydney.edu.au.(5)Sensorineural Pathology Programme, Centro de Investigación Biomédica en Red en Enfermedades Raras, CIBERER, Madrid, Spain. jose.lopezescamez@sydney.edu.au.(6)Meniere's Disease Neuroscience Research Program, Faculty of Medicine & Health, School of Medical Sciences, The Kolling Institute, University of Sydney, Sydney, NSW, Australia. jose.lopezescamez@sydney.edu.au.(7)Otology and Neurotology Group CTS495, Division of Otolaryngology, Department of Surgery, Instituto de Investigación Biosanitaria, Ibs.GRANADA, Universidad de Granada, Granada, Spain.(8)Sensorineural Pathology Programme, Centro de Investigación Biomédica en Red en Enfermedades Raras, CIBERER, Madrid, Spain.PURPOSE: To assess the available evidence to support a genetic contribution and define the role of common and rare variants in tinnitus.METHODS: After a systematic search and quality assessment, 31 records including 383,063 patients were selected (14 epidemiological studies and 17 genetic association studies). General information on the sample size, age, sex, tinnitus prevalence, severe tinnitus distribution, and sensorineural hearing loss was retrieved. Studies that did not include data on hearing assessment were excluded. Relative frequencies were used for qualitative variables to compare different studies and to obtain average values. Genetic variants and genes were listed and clustered according to their potential role in tinnitus development.RESULTS: The average prevalence of tinnitus estimated from population-based studies was 26.3% for any tinnitus, and 20% of patients with tinnitus reported it as an annoying symptom. One study has reported population-specific differences in the prevalence of tinnitus, the white ancestry being the population with a higher prevalence. Genome-wide association studies have identified and replicated two common variants in the Chinese population (rs2846071; rs4149577) in the intron of TNFRSF1A, associated with noise-induced tinnitus. Moreover, gene burden analyses in sequencing data from Spanish and Swede patients with severe tinnitus have identified and replicated ANK2, AKAP9, and TSC2 genes.CONCLUSIONS: The genetic contribution to tinnitus is starting to be revealed and it shows population-specific effects in European and Asian populations. The common allelic variants associated with tinnitus that showed replication are associated with noise-induced tinnitus. Although severe tinnitus has been associated with rare variants with large effect, their role on hearing or hyperacusis has not been established.© 2024. The Author(s).DOI: 10.1007/s10162-024-00925-6PMCID: PMC10907330",pubmed,38334885,10.1007/s10162-024-00925-6
a systemslevel approach reveals new gene regulatory modules in the developing ear,"632. Development. 2017 Apr 15;144(8):1531-1543. doi: 10.1242/dev.148494. Epub 2017 Mar 6.A systems-level approach reveals new gene regulatory modules in the developing ear.Chen J(1), Tambalo M(1), Barembaum M(2), Ranganathan R(1), Simões-Costa M(2), Bronner ME(2), Streit A(3).Author information:(1)Department of Craniofacial Development and Stem Cell Biology, King's College London, London SE1 9RT, UK.(2)Division of Biology and Biological Engineering, California Institute of Technology, Pasadena, CA 91125, USA.(3)Department of Craniofacial Development and Stem Cell Biology, King's College London, London SE1 9RT, UK andrea.streit@kcl.ac.uk.The inner ear is a complex vertebrate sense organ, yet it arises from a simple epithelium, the otic placode. Specification towards otic fate requires diverse signals and transcriptional inputs that act sequentially and/or in parallel. Using the chick embryo, we uncover novel genes in the gene regulatory network underlying otic commitment and reveal dynamic changes in gene expression. Functional analysis of selected transcription factors reveals the genetic hierarchy underlying the transition from progenitor to committed precursor, integrating known and novel molecular players. Our results not only characterize the otic transcriptome in unprecedented detail, but also identify new gene interactions responsible for inner ear development and for the segregation of the otic lineage from epibranchial progenitors. By recapitulating the embryonic programme, the genes and genetic sub-circuits discovered here might be useful for reprogramming naïve cells towards otic identity to restore hearing loss.© 2017. Published by The Company of Biologists Ltd.DOI: 10.1242/dev.148494PMCID: PMC5399671",pubmed,28264836,10.1242/dev.148494
ldl receptorrelated protein 1 lrp1 a novel target for opening the bloodlabyrinth barrier blb,"597. Signal Transduct Target Ther. 2022 Jun 10;7(1):175. doi: 10.1038/s41392-022-00995-z.LDL receptor-related protein 1 (LRP1), a novel target for opening the blood-labyrinth barrier (BLB).Shi X(#)(1)(2), Wang Z(#)(3), Ren W(#)(4)(5)(6)(7), Chen L(#)(1)(8), Xu C(4)(5)(6)(7), Li M(2), Fan S(3), Xu Y(3), Chen M(4)(5)(6)(7), Zheng F(4)(5)(6)(7), Zhang W(8), Zhou X(3), Zhang Y(4)(5)(6)(7), Qiu S(2), Wu L(2), Zhou P(8), Lv X(2), Cui T(2), Qiao Y(2), Zhao H(4)(5)(6)(7), Guo W(4)(5)(6)(7), Chen W(4)(5)(6)(7), Li S(3), Zhong W(9), Lin J(10)(11), Yang S(12)(13)(14)(15).Author information:(1)Department of Pharmacy, Peking University Third Hospital, Beijing, China.(2)Artificial Auditory Laboratory of Jiangsu Province, Xuzhou Medical University, Xuzhou, China.(3)National Engineering Research Center for the Emergency Drug, Beijing Institute of Pharmacology and Toxicology, Beijing, China.(4)College of Otolaryngology Head and Neck Surgery, Chinese PLA General Hospital, Beijing, China.(5)National Clinical Research Center for Otolaryngologic Diseases, Beijing, China.(6)Key Lab of Hearing Science, Ministry of Education, Beijing, China.(7)Beijing Key Lab of Hearing Impairment for Prevention and Treatment, Beijing, China.(8)Synthetic and Functional Biomolecules Center, Beijing National Laboratory for Molecular Sciences, Key Laboratory of Bioorganic Chemistry and Molecular Engineering of Ministry of Education, College of Chemistry and Molecular Engineering, Innovation Center for Genomics, Peking University, Beijing, China.(9)National Engineering Research Center for the Emergency Drug, Beijing Institute of Pharmacology and Toxicology, Beijing, China. zhongwu@bmi.ac.cn.(10)Department of Pharmacy, Peking University Third Hospital, Beijing, China. linjian@pku.edu.cn.(11)Synthetic and Functional Biomolecules Center, Beijing National Laboratory for Molecular Sciences, Key Laboratory of Bioorganic Chemistry and Molecular Engineering of Ministry of Education, College of Chemistry and Molecular Engineering, Innovation Center for Genomics, Peking University, Beijing, China. linjian@pku.edu.cn.(12)College of Otolaryngology Head and Neck Surgery, Chinese PLA General Hospital, Beijing, China. shm_yang@163.com.(13)National Clinical Research Center for Otolaryngologic Diseases, Beijing, China. shm_yang@163.com.(14)Key Lab of Hearing Science, Ministry of Education, Beijing, China. shm_yang@163.com.(15)Beijing Key Lab of Hearing Impairment for Prevention and Treatment, Beijing, China. shm_yang@163.com.(#)Contributed equallyInner ear disorders are a cluster of diseases that cause hearing loss in more than 1.5 billion people worldwide. However, the presence of the blood-labyrinth barrier (BLB) on the surface of the inner ear capillaries greatly hinders the effectiveness of systemic drugs for prevention and intervention due to the low permeability, which restricts the entry of most drug compounds from the bloodstream into the inner ear tissue. Here, we report the finding of a novel receptor, low-density lipoprotein receptor-related protein 1 (LRP1), that is expressed on the BLB, as a potential target for shuttling therapeutics across this barrier. As a proof-of-concept, we developed an LRP1-binding peptide, IETP2, and covalently conjugated a series of model small-molecule compounds to it, including potential drugs and imaging agents. All compounds were successfully delivered into the inner ear and inner ear lymph, indicating that targeting the receptor LRP1 is a promising strategy to enhance the permeability of the BLB. The discovery of the receptor LRP1 will illuminate developing strategies for crossing the BLB and for improving systemic drug delivery for inner ear disorders.© 2022. The Author(s).DOI: 10.1038/s41392-022-00995-zPMCID: PMC9184653",pubmed,35680846,10.1038/s41392-022-00995-z
genes involved in the development and physiology of both the peripheral and central auditory systems,"725. Annu Rev Neurosci. 2019 Jul 8;42:67-86. doi: 10.1146/annurev-neuro-070918-050428. Epub 2019 Jan 30.Genes Involved in the Development and Physiology of Both the Peripheral and Central Auditory Systems.Michalski N(1)(2)(3), Petit C(1)(2)(3)(4)(5).Author information:(1)Unité de Génétique et Physiologie de l'Audition, Institut Pasteur, 75015 Paris, France; email: nicolas.michalski@pasteur.fr , christine.petit@pasteur.fr.(2)Institut National de la Santé et de la Recherche Médicale, UMRS 1120, 75015 Paris, France.(3)Sorbonne Universités, 75005 Paris, France.(4)Syndrome de Usher et Autres Atteintes Rétino-Cochléaires, Institut de la Vision, 75012 Paris, France.(5)Collège de France, 75005 Paris, France.The genetic approach, based on the study of inherited forms of deafness, has proven to be particularly effective for deciphering the molecular mechanisms underlying the development of the peripheral auditory system, the cochlea and its afferent auditory neurons, and how this system extracts the physical parameters of sound. Although this genetic dissection has provided little information about the central auditory system, scattered data suggest that some genes may have a critical role in both the peripheral and central auditory systems. Here, we review the genes controlling the development and function of the peripheral and central auditory systems, focusing on those with demonstrated intrinsic roles in both systems and highlighting the current underappreciation of these genes. Their encoded products are diverse, from transcription factors to ion channels, as are their roles in the central auditory system, mostly evaluated in brainstem nuclei. We examine the ontogenetic and evolutionary mechanisms that may underlie their expression at different sites.DOI: 10.1146/annurev-neuro-070918-050428",pubmed,30699050,10.1146/annurev-neuro-070918-050428
modifications of the multi stimulus test with hidden reference and anchor mushra for use in audiology,"235. Int J Audiol. 2018 Jun;57(sup3):S92-S104. doi: 10.1080/14992027.2016.1220680. Epub 2016 Sep 6.Modifications of the MUlti stimulus test with Hidden Reference and Anchor (MUSHRA) for use in audiology.Völker C(1)(2), Bisitz T(3), Huber R(3), Kollmeier B(1)(2), Ernst SMA(1)(2).Author information:(1)a Abteilung Medizinische Physik , Carl von Ossietzky Universität Oldenburg , Oldenburg , Germany.(2)b Cluster of Excellence 'Hearing4all' , Oldenburg , Germany , and.(3)c Centre of Competence HörTech gGmbH , Oldenburg , Germany.OBJECTIVE: Two modifications of the standardised MUlti Stimulus test with Hidden Reference and Anchor (MUSHRA), namely MUSHRA simple and MUSHRA drag&drop, were implemented and evaluated together with the original test method. The modifications were designed to maximise the accessibility of MUSHRA for elderly and technically non-experienced listeners, who constitute the typical target group in hearing aid evaluation.DESIGN: Three MUSHRA variants were assessed based on subjective and objective measures, e.g. test-retest reliability, discrimination ability, time exposure and overall preference. With each method, participants repeated the task to rate the quality of several hearing aid algorithms four times.STUDY SAMPLE: Fifty listeners grouped into five subject classes were tested, including elderly and technically non-experienced participants with normal and impaired hearing. Normal-hearing, technically experienced students served as controls.RESULTS: Both modifications can be used to obtain compatible rating results. Both were preferred over the classical MUSHRA procedure. Technically experienced listeners performed best with the modification MUSHRA drag&drop.CONCLUSIONS: The comprehensive comparison of the MUSHRA variants demonstrates that the intuitive modification MUSHRA drag&drop can be generally recommended. However, considering e.g. specific evaluation demands, we suggest a differentiated and careful application of listening test methods.DOI: 10.1080/14992027.2016.1220680",pubmed,27598985,10.1080/14992027.2016.1220680
preface,[No abstract available],scopus,2-s2.0-85101360730,10.1016/S0079-6123(21)00069-8
research progress of the inferior colliculus from neuron neural circuit to auditory disease,"664. Brain Res. 2024 Apr 1;1828:148775. doi: 10.1016/j.brainres.2024.148775. Epub 2024 Jan 18.Research progress of the inferior colliculus: from Neuron, neural circuit to auditory disease.Liu M(1), Wang Y(1), Jiang L(1), Zhang X(1), Wang C(1), Zhang T(2).Author information:(1)Department of Otolaryngology Head and Neck Surgery, First Affiliated Hospital of Harbin Medical University, Harbin, Heilongjiang 150001, China.(2)Department of Otolaryngology Head and Neck Surgery, First Affiliated Hospital of Harbin Medical University, Harbin, Heilongjiang 150001, China. Electronic address: zth3856@126.com.The auditory midbrain, also known as the inferior colliculus (IC), serves as a crucial hub in the auditory pathway. Comprising diverse cell types, the IC plays a pivotal role in various auditory functions, including sound localization, auditory plasticity, sound detection, and sound-induced behaviors. Notably, the IC is implicated in several auditory central disorders, such as tinnitus, age-related hearing loss, autism and Fragile X syndrome. Accurate classification of IC neurons is vital for comprehending both normal and dysfunctional aspects of IC function. Various parameters, including dendritic morphology, neurotransmitter synthesis, potassium currents, biomarkers, and axonal targets, have been employed to identify distinct neuron types within the IC. However, the challenge persists in effectively classifying IC neurons into functional categories due to the limited clustering capabilities of most parameters. Recent studies utilizing advanced neuroscience technologies have begun to shed light on biomarker-based approaches in the IC, providing insights into specific cellular properties and offering a potential avenue for understanding IC functions. This review focuses on recent advancements in IC research, spanning from neurons and neural circuits to aspects related to auditory diseases.Copyright © 2024 Elsevier B.V. All rights reserved.DOI: 10.1016/j.brainres.2024.148775",pubmed,38244755,10.1016/j.brainres.2024.148775
whole exome sequencing identified a second pathogenic variant in homer2 for autosomal dominant nonsyndromic deafness,"Hearing loss is one of the most common sensory disorders worldwide, and about half of all occurrences are attributable to genetic factors. Here, we have identified a novel pathogenic variant in HOMER2 in a Chinese family with autosomal dominant, non-syndromic hearing loss. This is the second family reported globally with hearing loss caused by a variant in HOMER2. The pathogenic variant c.840_841insC in HOMER2 (NM_199330), segregating with the hearing-loss phenotype in the family, leads to a premature stop codon producing a truncated protein. The coiled-coil domain in the C-terminal of HOMER2 protein is essential for protein multimerization and HOMER2-CDC42 interaction. We compared the phenotypes in the two families and found that hearing impairment in this Chinese family was more severe. Furthermore, we found that the ability of this insertion mutant type HOMER2 (HOMER2MU) to multimerize decreased more significantly than wild-type HOMER2 (HOMER2WT) and the reported c.554G>C (NM_004839) mutant HOMER2. HOMER2MU protein tended to be distributed in a diffuse manner, whereas HOMER2WT and the reported mutant HOMER2 tended to cluster together. Our research provides a validating second family for variants in HOMER2 causing non-syndromic sensorineural hearing loss. HOMER2 homo−/hetero-multimerization might be the first step in exerting its normal function. © 2018 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd",scopus,2-s2.0-85054431014,10.1111/cge.13422
salicylate induced tinnitus behavioral measures and neural activity in auditory cortex of awake rats,"813. Hear Res. 2007 Apr;226(1-2):244-53. doi: 10.1016/j.heares.2006.06.013. Epub 2006 Aug 14.Salicylate induced tinnitus: behavioral measures and neural activity in auditory cortex of awake rats.Yang G(1), Lobarinas E, Zhang L, Turner J, Stolzberg D, Salvi R, Sun W.Author information:(1)Center for Hearing and Deafness, Department of Communicative Disorders and Sciences, University at Buffalo, Buffalo, NY 14214, USA.Neurophysiological studies of salicylate-induced tinnitus have generally been carried out under anesthesia, a condition that abolishes the perception of tinnitus and depresses neural activity. To overcome these limitations, measurement of salicylate induced tinnitus were obtained from rats using schedule induced polydipsia avoidance conditioning (SIPAC) and gap pre-pulse inhibition of acoustic startle (GPIAS). Both behavioral measures indicated that tinnitus was present after treatment with 150 and 250 mg/kg of salicylate; measurements with GPIAS indicated that the pitch of the tinnitus was near 16 kHz. Chronically implanted microwire electrode arrays were used to monitor the local field potentials and spontaneous discharge rate from multiunit clusters in the auditory cortex of awake rats before and after treatment with 150 mg/kg of salicylate. The amplitude of the local field potential elicited with 60 dB SPL tone bursts increased significantly 2h after salicylate treatment particularly at 16-20 kHz; frequencies associated with the tinnitus pitch. Field potential amplitudes had largely recovered 1-2 days post-salicylate when behavioral results showed that tinnitus was absent. The mean spontaneous spike recorded from the same multiunit cluster pre- and post-salicylate decreased from 22 spikes/s before treatment to 14 spikes/s 2h post-salicylate and recovered 1 day post-treatment. These preliminary physiology data suggest that salicylate induced tinnitus is associated with sound evoked hyperactivity in auditory cortex and spontaneous hypoactivity.DOI: 10.1016/j.heares.2006.06.013",pubmed,16904853,10.1016/j.heares.2006.06.013
developmental changes of the mitochondria in the murine anteroventral cochlear nucleus,"838. iScience. 2023 Dec 8;27(1):108700. doi: 10.1016/j.isci.2023.108700. eCollection 2024 Jan 19.Developmental changes of the mitochondria in the murine anteroventral cochlear nucleus.Hintze A(1), Lange F(2)(3), Steyer AM(4)(5), Anstatt J(2)(3), Möbius W(4)(5), Jakobs S(2)(3)(4)(6), Wichmann C(1)(4).Author information:(1)Molecular Architecture of Synapses Group, Institute for Auditory Neuroscience, InnerEarLab and Center for Biostructural Imaging of Neurodegeneration, University Medical Center Göttingen, 37075 Göttingen, Germany.(2)Department of NanoBiophotonics, Max Planck Institute for Multidisciplinary Sciences, 37077 Göttingen, Germany.(3)Clinic of Neurology, University Medical Center Göttingen, 37075 Göttingen, Germany.(4)Multiscale Bioimaging Cluster of Excellence (MBExC), University of Göttingen, 37075 Göttingen, Germany.(5)Electron Microscopy-City Campus, Department of Neurogenetics, Max Planck Institute for Multidisciplinary Sciences, 37075 Göttingen, Germany.(6)Translational Neuroinflammation and Automated Microscopy, Fraunhofer Institute for Translational Medicine and Pharmacology ITMP, Göttingen, Germany.Mitochondria are key organelles to provide ATP for synaptic transmission. This study aims to unravel the structural adaptation of mitochondria to an increase in presynaptic energy demand and upon the functional impairment of the auditory system. We use the anteroventral cochlear nucleus (AVCN) of wild-type and congenital deaf mice before and after hearing onset as a model system for presynaptic states of lower and higher energy demands. We combine focused ion beam scanning electron microscopy and electron tomography to investigate mitochondrial morphology. We found a larger volume of synaptic boutons and mitochondria after hearing onset with a higher crista membrane density. In deaf animals lacking otoferlin, we observed a shallow increase of mitochondrial volumes toward adulthood in endbulbs, while in wild-type animals mitochondria further enlarged. We propose that in the AVCN, presynaptic mitochondria undergo major structural changes likely to serve higher energy demands upon the onset of hearing and further maturation.© 2023 The Author(s).DOI: 10.1016/j.isci.2023.108700PMCID: PMC10783593",pubmed,38213623,10.1016/j.isci.2023.108700
behavioral training enhances cortical temporal processing in neonatally deafened juvenile cats references,"Deaf humans implanted with a cochlear prosthesis depend largely on temporal cues for speech recognition because spectral information processing is severely impaired. Training with a cochlear prosthesis is typically required before speech perception shows improvement, suggesting that relevant experience modifies temporal processing in the central auditory system. We tested this hypothesis in neonatally deafened cats by comparing temporal processing in the primary auditory cortex (AI) of cats that received only chronic passive intracochlear electric stimulation (ICES) with cats that were also trained with ICES to detect temporally challenging trains of electric pulses. After months of chronic passive stimulation and several weeks of detection training in behaviorally trained cats, multineuronal AI responses evoked by temporally modulated ICES were recorded in anesthetized animals. The stimulus repetition rates that produced the maximum number of phase-locked spikes (best repetition rate) and 50% cutoff rate were significantly higher in behaviorally trained cats than the corresponding rates in cats that received only chronic passive ICES. Behavioral training restored neuronal temporal following ability to levels comparable with those recorded in naive prior normal-hearing adult deafened animals. Importantly, best repetitition rates and cutoff rates were highest for neuronal clusters activated by the electrode configuration used in behavioral training. These results suggest that neuroplasticity in the AI is induced by behavioral training and perceptual learning in animals deprived of ordinary auditory experience during development and indicate that behavioral training can ameliorate or restore temporal processing in the AI of profoundly deaf animals. (PsycInfo Database Record (c) 2022 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc10&DO=10.1152%2fjn.00731.2010
a novel biased probability neural network bpnn and regularized extreme learning machine relm based hearing loss prediction system,,base,f37acfe6f1ed1ac0b0a60ac4c59589a9468a38f22fb9aa36684d2ba016ac4ed2,
quantifying hearing difficulty associated with covid19 infection control measures,,cinahl,3038106,
auditory rehabilitation after temporal bone fracture with cochlear implants  a case control study,"508. Cochlear Implants Int. 2023 Jul;24(4):195-204. doi: 10.1080/14670100.2022.2148351. Epub 2023 Jan 8.Auditory rehabilitation after temporal bone fracture with cochlear implants - a case control study.Heine K(1), Timm ME(1)(2), Gärtner L(1), Lenarz T(1), Lesinski-Schiedat A(1).Author information:(1)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.(2)Cluster of Excellence, Hearing4all, Hannover Medical School, Hannover, Germany.OBJECTIVES: Temporal bone fracture can cause posttraumatic deafness. Sequelae like ossification or obliteration of the cochlea can impact the outcome of cochlear implantation. This study highlights the effect of localisation of the fracture to morphologic, electric and functional criteria.METHODS: The study group consists of patients suffering from hearing loss caused by temporal bone fracture (n = 61 ears). Patients were divided into otic capsule sparing (OCS) and otic capsule involving (OCI) fractures. The OCI group was additionally divided into subgroups with or without signs of ossification inside the cochlea. Postoperative imaging, hearing tests and electrode impedances were analysed.RESULTS: The results of postoperative hearing rehabilitation showed lower speech understanding scores for the OCI group, especially for the ossification group. OCI fractures with signs of ossification showed increased impedances. Patients in the OCI group suffered more frequently from facial nerve stimulation (FNS). FNS was most frequently observed within the ossification group.CONCLUSION: Cochlear implantation in patients with temporal bone fracture is adequate therapy for the treatment of fracture-induced deafness. In long-term observation, these patients show comparable results with regular cochlear implant (CI) patients. Implantation should be performed as soon as possible after hearing loss, before obstructing obliteration or ossification of the cochlea start.DOI: 10.1080/14670100.2022.2148351",pubmed,36617461,10.1080/14670100.2022.2148351
product showcase,,cinahl,10745734,
loss of myh14 increases susceptibility to noiseinduced hearing loss in cbacaj mice,"MYH14 is a member of the myosin family, which has been implicated in many motile processes such as ion-channel gating, organelle translocation, and the cytoskeleton rearrangement. Mutations in MYH14 lead to a DFNA4-type hearing impairment. Further evidence also shows that MYH14 is a candidate noise-induced hearing loss (NIHL) susceptible gene. However, the specific roles of MYH14 in auditory function and NIHL are not fully understood. In the present study, we used CRISPR/Cas9 technology to establish a Myh14 knockout mice line in CBA/CaJ background (now referred to as Myh14-/- mice) and clarify the role of MYH14 in the cochlea and NIHL. We found that Myh14-/- mice did not exhibit significant hearing loss until five months of age. In addition, Myh14-/- mice were more vulnerable to high intensity noise compared to control mice. More significant outer hair cell loss was observed in Myh14-/- mice than in wild type controls after acoustic trauma. Our findings suggest that Myh14 may play a beneficial role in the protection of the cochlea after acoustic overstimulation in CBA/CaJ mice. © 2016 Xiaolong Fu et al.",scopus,2-s2.0-85008877292,10.1155/2016/6720420
an overview of auditory and vestibular disorders in alzheimers disease a narrative review,,cinahl,23456167,
adding simultaneous stimulating channels to reduce power consumption in cochlear implants,"450. Hear Res. 2017 Mar;345:96-107. doi: 10.1016/j.heares.2017.01.010. Epub 2017 Jan 16.Adding simultaneous stimulating channels to reduce power consumption in cochlear implants.Langner F(1), Saoji AA(2), Büchner A(3), Nogueira W(3).Author information:(1)Department of Otolaryngology, Medical University Hannover and Cluster of Excellence Hearing4all, Hanover, Germany. Electronic address: langner.florian@mh-hannover.de.(2)Advanced Bionics LLC, Valencia, CA, USA.(3)Department of Otolaryngology, Medical University Hannover and Cluster of Excellence Hearing4all, Hanover, Germany.Sound coding strategies for Cochlear Implant (CI) listeners can be used to control the trade-off between speech performance and power consumption. Most commercial CI strategies use non-simultaneous channel stimulation, stimulating only one electrode at a time. One could add parallel simultaneous stimulating channels such that the electrical interaction between channels is increased. This would produce spectral smearing, because the electrical fields of the simultaneous stimulated channels interact, but also power savings. The parallel channels produce a louder sensation than sequential stimulation. To test this hypothesis we implemented different sound coding strategies using a research interface from Advanced Bionics: the commercial F120 strategy using sequential channel stimulation (one channel equals two electrodes with current steering) and the Paired strategy, consisting of simultaneous stimulation with two channels. Here, the electrical field of both channels will interact, requiring less current on each channel to perceive the same loudness as with F120. However, channel interaction between the independent channels may reduce speech recognition or understanding. This can be diminished by adding an inverse-polarity stimulation channel between both channels. This strategy is termed Paired with Flanks. Additionally, Triplet with three channels and an adjacent Flank style was investigated. For each strategy we measured speech intelligibility with the Hochmair-Schulz-Moser sentence test. Spectral resolution was assessed using a spectral modulation depth detection task. Results show that Paired without Flanks obtains similar performance while reducing the current by 20% on average compared to F120. Triplet with and without Flanks shows overall poorer performance when compared to F120. All strategies inhibit the option to increase the pulse width which would result in even further decreased power consumption.Copyright © 2017 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2017.01.010",pubmed,28104408,10.1016/j.heares.2017.01.010
manipulations of sensory experiences during development reveal mechanisms underlying vocal learning biases in zebra finches,"376. Dev Neurobiol. 2020 Mar;80(3-4):132-146. doi: 10.1002/dneu.22754. Epub 2020 May 20.Manipulations of sensory experiences during development reveal mechanisms underlying vocal learning biases in zebra finches.James LS(1)(2), Davies R Jr(1), Mori C(3), Wada K(3)(4), Sakata JT(1)(2)(5).Author information:(1)Department of Biology, McGill University, Montreal, QC, Canada.(2)Centre for Research in Brain, Language and Music, McGill University, Montreal, Quebec, Canada.(3)Graduate School of Life Science, Hokkaido University, Sapporo, Japan.(4)Faculty of Science, Hokkaido University, Sapporo, Japan.(5)Center for Studies of Behavioral Neurobiology, Concordia University, Montreal, QC, Canada.Biological predispositions in learning can bias and constrain the cultural evolution of social and communicative behaviors (e.g., speech and birdsong), and lead to the emergence of behavioral and cultural ""universals."" For example, surveys of laboratory and wild populations of zebra finches (Taeniopygia guttata) document consistent patterning of vocal elements (""syllables"") with respect to their acoustic properties (e.g., duration, mean frequency). Furthermore, such universal patterns are also produced by birds that are experimentally tutored with songs containing randomly sequenced syllables (""tutored birds""). Despite extensive demonstrations of learning biases, much remains to be uncovered about the nature of biological predispositions that bias song learning and production in songbirds. Here, we examined the degree to which ""innate"" auditory templates and/or biases in vocal motor production contribute to vocal learning biases and production in zebra finches. Such contributions can be revealed by examining acoustic patterns in the songs of birds raised without sensory exposure to song (""untutored birds"") or of birds that are unable to hear from early in development (""early-deafened birds""). We observed that untutored zebra finches and early-deafened zebra finches produce songs with positional variation in some acoustic features (e.g., mean frequency) that resemble universal patterns observed in tutored birds. Similar to tutored birds, early-deafened birds also produced song motifs with alternation in acoustic features across adjacent syllables. That universal acoustic patterns are observed in the songs of both untutored and early-deafened birds highlights the contribution motor production biases to the emergence of universals in culturally transmitted behaviors.© 2020 Wiley Periodicals, LLC.DOI: 10.1002/dneu.22754",pubmed,32330360,10.1002/dneu.22754
electricacoustic interaction measurements in cochlearimplant users with ipsilateral residual hearing using electrocochleography,"427. J Acoust Soc Am. 2020 Jan;147(1):350. doi: 10.1121/10.0000577.Electric-acoustic interaction measurements in cochlear-implant users with ipsilateral residual hearing using electrocochleography.Krüger B(1), Büchner A(1), Lenarz T(1), Nogueira W(1).Author information:(1)Department of Otolaryngology, Hannover Medical School, Cluster of Excellence Hearing4all, Carl-Neuberg-Straße 1, 30625 Hannover, Germany.Cochlear implantation is increasingly being used as a hearing-loss treatment for patients with residual hearing in the low acoustic frequencies. These patients obtain combined electric-acoustic stimulation (EAS). Substantial residual hearing and relatively long electrode arrays can lead to interactions between the electric and acoustic stimulation. This work investigated EAS interaction through psychophysical and electrophysiological measures. Moreover, cone-beam computed-tomography data was used to characterize the interaction along spatial cochlear locations. Psychophysical EAS interaction was estimated based on the threshold of audibility of an acoustic probe stimulus in the presence of a simultaneously presented electric masker stimulus. Intracochlear electrocochleography was used to estimate electrophysiological EAS interaction via the telemetry capability of the cochlear implant. EAS interaction was observed using psychophysical and electrophysiological measurements. While psychoacoustic EAS interaction was most pronounced close to the electrical stimulation site, electrophysiological EAS interaction was observed over a wider range of spatial cochlear locations. Psychophysical EAS interaction was significantly larger than electrophysiological EAS interaction for acoustic probes close to the electrode position.DOI: 10.1121/10.0000577",pubmed,32006967,10.1121/10.0000577
nationwide analysis of the relationships between mental health body mass index and tinnitus in premenopausal female adults in korea 20102012 knhanes,"802. Sci Rep. 2018 May 4;8(1):7028. doi: 10.1038/s41598-018-25576-5.Nationwide analysis of the relationships between mental health, body mass index and tinnitus in premenopausal female adults in Korea: 2010-2012 KNHANES.Lee DH(1)(2), Kim YS(3)(4), Chae HS(3)(4), Han K(5).Author information:(1)Epidemiology Study Cluster of Uijeongbu St. Mary's Hospital, Uijeongbu St. Mary's Hospital, College of Medicine, The Catholic University of Korea, Uijeongbu, Korea. leedh0814@catholic.ac.kr.(2)Department of Otolaryngology-HNS, College of Medicine, The Catholic University of Korea, Seoul, Korea. leedh0814@catholic.ac.kr.(3)Epidemiology Study Cluster of Uijeongbu St. Mary's Hospital, Uijeongbu St. Mary's Hospital, College of Medicine, The Catholic University of Korea, Uijeongbu, Korea.(4)Department of Internal medicine, College of Medicine, The Catholic University of Korea, Seoul, Korea.(5)Department of Biostatistics, College of Medicine, The Catholic University of Korea, Seoul, Korea.Tinnitus is related to serious comorbidities such as suicidal ideation and attempts. Body mass index (BMI) is associated with auditory symptoms including hearing loss. The aim of this nationwide, population-based, cross-sectional study was to evaluate the relationship between mental health, body mass index and tinnitus in a Korean premenopausal female population. This study analyzed data from the Korea National Health and Nutrition Examination Surveys in 2010-2012. Data were collected from 4628 19 years or older, premenopausal women. After adjustments, underweight premenopausal women exhibited a higher odds ratio for tinnitus (odd ratio = 1.54; 95% confidence interval = 1.14-2.08) compared with women of normal weight. Moderate and severe tinnitus was highly prevalent in underweight as well as extremely obese women. The prevalence of perceived stress, melancholy, and suicide ideation was significantly higher in women with tinnitus. The prevalence of perceived stress and suicide ideation was significantly higher in underweight women with tinnitus, but that of melancholy was significantly lower. This study demonstrated that underweight premenopausal Korean women had a higher risk of tinnitus, which has grown in importance as a public health issue. Women with tinnitus experience perceived stress and suicide ideation more frequently, but melancholy less frequently than women without.DOI: 10.1038/s41598-018-25576-5PMCID: PMC5935674",pubmed,29728692,10.1038/s41598-018-25576-5
synaptic release potentiation at aging auditory ribbon synapses,"746. Front Aging Neurosci. 2021 Oct 18;13:756449. doi: 10.3389/fnagi.2021.756449. eCollection 2021.Synaptic Release Potentiation at Aging Auditory Ribbon Synapses.Peineau T(1)(2), Belleudy S(1), Pietropaolo S(3), Bouleau Y(1)(2), Dulon D(1)(2).Author information:(1)Neurophysiologie de la Synapse Auditive, INSERM UMRS 1120, Bordeaux Neurocampus, Université de Bordeaux, Bordeaux, France.(2)Institut de l'Audition, Centre Institut Pasteur/Inserm, Paris, France.(3)INCIA, UMR 5287, CNRS, University of Bordeaux, Bat B2, Pessac, France.Age-related hidden hearing loss is often described as a cochlear synaptopathy that results from a progressive degeneration of the inner hair cell (IHC) ribbon synapses. The functional changes occurring at these synapses during aging are not fully understood. Here, we characterized this aging process in IHCs of C57BL/6J mice, a strain which is known to carry a cadherin-23 mutation and experiences early hearing loss with age. These mice, while displaying a large increase in auditory brainstem thresholds due to 50% loss of IHC synaptic ribbons at middle age (postnatal day 365), paradoxically showed enhanced acoustic startle reflex suggesting a hyperacusis-like response. The auditory defect was associated with a large shrinkage of the IHCs' cell body and a drastic enlargement of their remaining presynaptic ribbons which were facing enlarged postsynaptic AMPAR clusters. Presynaptic Ca2+ microdomains and the capacity of IHCs to sustain high rates of exocytosis were largely increased, while on the contrary the expression of the fast-repolarizing BK channels, known to negatively control transmitter release, was decreased. This age-related synaptic plasticity in IHCs suggested a functional potentiation of synaptic transmission at the surviving synapses, a process that could partially compensate the decrease in synapse number and underlie hyperacusis.Copyright © 2021 Peineau, Belleudy, Pietropaolo, Bouleau and Dulon.DOI: 10.3389/fnagi.2021.756449PMCID: PMC8558230",pubmed,34733152,10.3389/fnagi.2021.756449
mandibulofacial dysostosis in a patient with a de novo 217 translocation that disrupts the hoxd gene cluster,"811. Am J Med Genet A. 2007 May 15;143A(10):1053-9. doi: 10.1002/ajmg.a.31715.Mandibulofacial dysostosis in a patient with a de novo 2;17 translocation that disrupts the HOXD gene cluster.Stevenson DA(1), Bleyl SB, Maxwell T, Brothman AR, South ST.Author information:(1)Division of Medical Genetics, Department of Pediatrics, University of Utah, Salt Lake City, Utah 84132, USA. david.stevenson@hsc.utah.eduTreacher Collins syndrome (TCS) is the prototypical mandibulofacial dysostosis syndrome, but other mandibulofacial dysostosis syndromes have been described. We report an infant with mandibulofacial dysostosis and an apparently balanced de novo 2;17 translocation. She presented with severe lower eyelid colobomas requiring skin grafting, malar and mandibular hypoplasia, bilateral microtia with external auditory canal atreasia, dysplastic ossicles, hearing loss, bilateral choanal stenosis, cleft palate without cleft lip, several oral frenula of the upper lip/gum, and micrognathia requiring tracheostomy. Her limbs were normal. Chromosome analysis at the 600-band level showed a 46,XX,t(2;17)(q24.3;q23) karyotype. Sequencing of the entire TCOF1 coding region did not show evidence of a sequence variation. High-resolution genomic microarray analysis did not identify a cryptic imbalance. FISH mapping refined the breakpoints to 2q31.1 and 17q24.3-25.1 and showed the 2q31.1 breakpoint likely affects the HOXD gene cluster. Several atypical findings and lack of an identifiable TCOF1 mutation suggest that this child has a provisionally unique mandibulofacial dysostosis syndrome. The apparently balanced de novo translocation provides candidate loci for atypical and TCOF1 mutation negative cases of TCS. Based on the agreement of our findings with one previous case of mandibulofacial dysostosis with a 2q31.1 transocation, we hypothesize that misexpression of genes in the HOXD gene cluster produced the described phenotype in this patient.DOI: 10.1002/ajmg.a.31715PMCID: PMC3243067",pubmed,17431905,10.1002/ajmg.a.31715
loudness normalization for cochlear implant using pulserate modulation to convey mandarin tonal information a modelbased study,"497. Conf Proc IEEE Eng Med Biol Soc. 2006;2006:1236-9. doi: 10.1109/IEMBS.2006.259368.Loudness normalization for cochlear implant using pulse-rate modulation to convey Mandarin tonal information: a model-based study.Chen F(1), Zhang YT.Author information:(1)Shun Hing Institute of Advanced Engineering, Chinese University of Hong Kong, Hong Kong.Cochlear implant (CI) devices employ electrical pulsatile stimulation of the auditory nerves (AN) to restore partial hearing to a profoundly deafened person. In order to improve the speech perception for CI users speaking tonal language, such as Mandarin, the pulse-rate has been suggested to be modulated according to the Mandarin tonal patterns to convey the Mandarin tonal information. However, recent psychological experiments have found that the pulse-rate modulation will produce accompanying variation of perceived loudness. The purpose of this paper is to introduce an amplitude compensation scheme to normalize the loudness perception when the pulse-rate is modulated to convey the Mandarin tonal information. Based on an integrate-and-fire AN model, a loudness perception model and a pitch perception were implemented. Result of model-based simulation showed that using the proposed amplitude compensation scheme, the estimated loudness was normalized while the Mandarin tonal information could still be efficiently transmitted. It is believed that, when the proposed electrical pulsatile stimulation incorporating both pulse-rate modulation and amplitude compensation is integrated with present CI devices, it would more efficiently enhance the speech identification for cochlear implantee speaking tonal languages, such as Mandarin.DOI: 10.1109/IEMBS.2006.259368",pubmed,17946451,10.1109/IEMBS.2006.259368
relating hearing loss and executive functions to hearing aid users preference for and speech recognition with different combinations of binaural noise reduction and microphone directionality,"720. Front Neurosci. 2014 Dec 4;8:391. doi: 10.3389/fnins.2014.00391. eCollection 2014.Relating hearing loss and executive functions to hearing aid users' preference for, and speech recognition with, different combinations of binaural noise reduction and microphone directionality.Neher T(1).Author information:(1)Medical Physics and Cluster of Excellence Hearing4all, Oldenburg University Germany.Knowledge of how executive functions relate to preferred hearing aid (HA) processing is sparse and seemingly inconsistent with related knowledge for speech recognition outcomes. This study thus aimed to find out if (1) performance on a measure of reading span (RS) is related to preferred binaural noise reduction (NR) strength, (2) similar relations exist for two different, non-verbal measures of executive function, (3) pure-tone average hearing loss (PTA), signal-to-noise ratio (SNR), and microphone directionality (DIR) also influence preferred NR strength, and (4) preference and speech recognition outcomes are similar. Sixty elderly HA users took part. Six HA conditions consisting of omnidirectional or cardioid microphones followed by inactive, moderate, or strong binaural NR as well as linear amplification were tested. Outcome was assessed at fixed SNRs using headphone simulations of a frontal target talker in a busy cafeteria. Analyses showed positive effects of active NR and DIR on preference, and negative and positive effects of, respectively, strong NR and DIR on speech recognition. Also, while moderate NR was the most preferred NR setting overall, preference for strong NR increased with SNR. No relation between RS and preference was found. However, larger PTA was related to weaker preference for inactive NR and stronger preference for strong NR for both microphone modes. Equivalent (but weaker) relations between worse performance on one non-verbal measure of executive function and the HA conditions without DIR were found. For speech recognition, there were relations between HA condition, PTA, and RS, but their pattern differed from that for preference. Altogether, these results indicate that, while moderate NR works well in general, a notable proportion of HA users prefer stronger NR. Furthermore, PTA and executive functions can account for some of the variability in preference for, and speech recognition with, different binaural NR and DIR settings.DOI: 10.3389/fnins.2014.00391PMCID: PMC4255521",pubmed,25538547,10.3389/fnins.2014.00391
is copd associated with alterations in hearing a systematic review and metaanalysis,"70. Int J Chron Obstruct Pulmon Dis. 2018 Dec 28;14:149-162. doi: 10.2147/COPD.S182730. eCollection 2019.Is COPD associated with alterations in hearing? A systematic review and meta-analysis.Bayat A(1), Saki N(2), Nikakhlagh S(2), Mirmomeni G(3), Raji H(4), Soleimani H(1), Rahim F(5).Author information:(1)Department of Audiology, Hearing Research Center, Imam Khomeini Hospital, Ahvaz Jundishapur University of Medical Sciences, Ahvaz, Iran.(2)Department of Otorhinolaryngology, Hearing Research Center, Ahvaz Jundishapur University of Medical Sciences, Ahvaz Iran.(3)Department of Biostatistics and Epidemiology, School of Health, Ahvaz Jundishapur University of Medical Sciences, Ahvaz, Iran.(4)Department of Internal Medicine, Air Pollution and Respiratory Diseases Research Center, Ahvaz Jundishapur University of Medical Sciences, Ahvaz, Iran.(5)Department of Molecular Medicine, Health Research Institute, Thalassemia and Hemoglobinopathies Research Centre, Ahvaz Jundishapur University of Medical Sciences, Ahvaz, Iran, bioinfo2003@gmail.com.Comment in    Int J Chron Obstruct Pulmon Dis. 2019 Feb 18;14:457-460.BACKGROUND AND AIMS: COPD is an irreversible or persistent airflow obstruction, which affects up to 600 million people globally. The primary purpose of this systematic review was to explore the COPD-based alteration in the auditory system function by conducting a quantitative analysis of presently published data.MATERIALS AND METHODS: We systematically searched seven diverse electronic databases and manual searching of references to identify relevant studies. Data from the selected studies were rated by two investigators independently in a blinded fashion. Meta-analysis was done on pooled data using Cochrane's Review Manager 5.3.RESULTS: Sixteen articles received suitable scores and were thus included for further processes. Hearing loss (HL) was defined as a change in pure tone audiometry (PTA) thresholds, auditory brainstem response (ABR), and auditory P300 parameters. ABR wave was significantly elongated in patients with COPD than in controls (standardized mean difference [SMD]=0.27, 95% CI: 0.05-0.48, P=0.02). PTA was significantly higher in patients with COPD when compared with controls (SMD=1.76, 95% CI: 0.43-3.08, P=0.0004). We found that patients with COPD had a significantly higher latency than controls (SMD=1.30, 95% CI: 0.79-1.80, P=0.0001).CONCLUSION: COPD patients had considerably greater incidence of HL when compared with controls. Interestingly, although the mean PTA thresholds at every frequency for COPD patients were higher than those for controls, these values were still in the slight to mild HL ranges. Prolonged ABR wave latencies in the COPD patients suggest retro-cochlear involvement. Thus, COPD most frequently clusters with HL, but it is worth noting that alteration in hearing is not always recognized by medical experts as a frequent comorbidity associated with COPD.DOI: 10.2147/COPD.S182730PMCID: PMC6312399",pubmed,30643401,10.2147/COPD.S182730
potential consequences of spectral and binaural loudness summation for bilateral hearing aid fitting,"359. Trends Hear. 2018 Jan-Dec;22:2331216518805690. doi: 10.1177/2331216518805690.Potential Consequences of Spectral and Binaural Loudness Summation for Bilateral Hearing Aid Fitting.van Beurden M(1)(2), Boymans M(1)(2), van Geleuken M(1), Oetting D(3)(4), Kollmeier B(5), Dreschler WA(1).Author information:(1)1 Department of Clinical and Experimental Audiology, Amsterdam UMC, Amsterdam, the Netherlands.(2)2 Libra Rehabilitation and Audiology, Eindhoven, the Netherlands.(3)3 HörTech gGmbH, Oldenburg, Germany.(4)4 Cluster of Excellence Hearing4all, Oldenburg, Germany.(5)5 Medizinische Physik, Universität Oldenburg, Oldenburg, Germany.Aversiveness of loud sounds is a frequent complaint by hearing aid users, especially when fitted bilaterally. This study investigates whether loudness summation can be held responsible for this finding. Two aspects of loudness summation should be taken into account: spectral loudness summation for broadband signals and binaural loudness summation for signals that are presented binaurally. In this study, the effect of different symmetrical hearing losses was studied. Measurements were obtained with the widely used technique of Adaptive Categorical Loudness Scaling. For large bandwidths, spectral loudness summation for hearing-impaired listeners was found to be greater than that for normal-hearing listeners, both for monaurally and binaurally presented signals. For binaural loudness summation, the effect of hearing loss was not significant. In all cases, individual differences were substantial.DOI: 10.1177/2331216518805690PMCID: PMC6201175",pubmed,30353784,10.1177/2331216518805690
cabp2gene therapy restores inner hair cell calcium currents and improves hearing in a dfnb93 mouse model,"834. Front Mol Neurosci. 2021 Aug 19;14:689415. doi: 10.3389/fnmol.2021.689415. eCollection 2021.Cabp2-Gene Therapy Restores Inner Hair Cell Calcium Currents and Improves Hearing in a DFNB93 Mouse Model.Oestreicher D(1)(2), Picher MM(3), Rankovic V(3)(4), Moser T(2)(3)(5)(6), Pangrsic T(1)(2)(5)(6).Author information:(1)Experimental Otology Group, InnerEarLab, Department of Otolaryngology, University Medical Center Göttingen, Göttingen, Germany.(2)Auditory Neuroscience Group, Max Planck Institute of Experimental Medicine, Göttingen, Germany.(3)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(4)Restorative Cochlear Genomics Group, Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(5)Collaborative Research Center 889, University of Göttingen, Göttingen, Germany.(6)Multiscale Bioimaging Cluster of Excellence (MBExC), University of Göttingen, Göttingen, Germany.Clinical management of auditory synaptopathies like other genetic hearing disorders is currently limited to the use of hearing aids or cochlear implants. However, future gene therapy promises restoration of hearing in selected forms of monogenic hearing impairment, in which cochlear morphology is preserved over a time window that enables intervention. This includes non-syndromic autosomal recessive hearing impairment DFNB93, caused by defects in the CABP2 gene. Calcium-binding protein 2 (CaBP2) is a potent modulator of inner hair cell (IHC) voltage-gated calcium channels CaV1.3. Based on disease modeling in Cabp2-/- mice, DFNB93 hearing impairment has been ascribed to enhanced steady-state inactivation of IHC CaV1.3 channels, effectively limiting their availability to trigger synaptic transmission. This, however, does not seem to interfere with cochlear development and does not cause early degeneration of hair cells or their synapses. Here, we studied the potential of a gene therapeutic approach for the treatment of DFNB93. We used AAV2/1 and AAV-PHP.eB viral vectors to deliver the Cabp2 coding sequence into IHCs of early postnatal Cabp2-/- mice and assessed the level of restoration of hair cell function and hearing. Combining in vitro and in vivo approaches, we observed high transduction efficiency, and restoration of IHC CaV1.3 function resulting in improved hearing of Cabp2-/- mice. These preclinical results prove the feasibility of DFNB93 gene therapy.Copyright © 2021 Oestreicher, Picher, Rankovic, Moser and Pangrsic.DOI: 10.3389/fnmol.2021.689415PMCID: PMC8417311",pubmed,34489639,10.3389/fnmol.2021.689415
asmaak an emarati sign language translator,"This research highlights the challenges faced by individuals who are deaf in communicating with those who do not understand sign language. Artificial Intelligence (AI) has emerged as a promising solution to this problem, with deep learning enabling machines to process sequences of data and accurately recognize sign language gestures. The Asma'ak sign language recognition system was developed to detect Emirati Sign Language hand gestures and instantly translate them into text, thus promoting greater inclusivity and engagement within society. The system's reliability and validity are demonstrated through testing on various operating systems, genders, and age groups, achieving a high level of accuracy and precision. Overall, Asma’ak holds significant potential for improving communication and breaking down linguistic barriers for individuals with hearing impairments.",ieee,2162-1241,10.1109/ICTC58733.2023.10392859
enhancing digital content accessibility for the hearing impaired through aidriven visual representations,"Ensuring inclusivity for individuals with hearing impairments is paramount in an increasingly digital world. This article explores an innovative solution that leverages artificial intelligence (AI) and machine learning to enhance content comprehension for this demographic. We present a system that utilizes deep learning models and natural language processing algorithms to convert spoken language into visual representations, including emojis and images. Our approach empowers the hearing impaired to access and interpret information more effectively by bridging the gap between audio-based content and visual cues. Extensive experiments and user studies confirm the system's effectiveness, significantly improving content understanding and engagement. This novel technology opens new avenues for independent and comprehensive comprehension of audio-based content by individuals with hearing impairments.",ieee,,10.1109/QICAR61538.2024.10496621
hearing loss in recreational shooters in central queensland a pilot study,,cinahl,10385282,10.1111/j.1440-1584.2006.00796.x
age effects on concurrent speech segregation by onset asynchrony,"146. J Speech Lang Hear Res. 2019 Jan 30;62(1):177-189. doi: 10.1044/2018_JSLHR-H-18-0064.Age Effects on Concurrent Speech Segregation by Onset Asynchrony.Stuckenberg MV(1)(2)(3), Nayak CV(1), Meyer BT(1), Völker C(1), Hohmann V(1), Bendixen A(1)(4).Author information:(1)Cluster of Excellence ""Hearing4all,"" Carl von Ossietzky University of Oldenburg, Germany.(2)Department of Psychology, University of Leipzig, Germany.(3)Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany.(4)Faculty of Natural Sciences, Chemnitz University of Technology, Germany.Purpose For elderly listeners, it is more challenging to listen to 1 voice surrounded by other voices than for young listeners. This could be caused by a reduced ability to use acoustic cues-such as slight differences in onset time-for the segregation of concurrent speech signals. Here, we study whether the ability to benefit from onset asynchrony differs between young (18-33 years) and elderly (55-74 years) listeners. Method We investigated young (normal hearing, N = 20) and elderly (mildly hearing impaired, N = 26) listeners' ability to segregate 2 vowels with onset asynchronies ranging from 20 to 100 ms. Behavioral measures were complemented by a specific event-related brain potential component, the object-related negativity, indicating the perception of 2 distinct auditory objects. Results Elderly listeners' behavioral performance (identification accuracy of the 2 vowels) was considerably poorer than young listeners'. However, both age groups showed the same amount of improvement with increasing onset asynchrony. Object-related negativity amplitude also increased similarly in both age groups. Conclusion Both age groups benefit to a similar extent from onset asynchrony as a cue for concurrent speech segregation during active (behavioral measurement) and during passive (electroencephalographic measurement) listening.DOI: 10.1044/2018_JSLHR-H-18-0064",pubmed,30534994,10.1044/2018_JSLHR-H-18-0064
characterizing the binaural contribution to speechinnoise reception in elderly hearingimpaired listeners,"690. J Acoust Soc Am. 2017 Feb;141(2):EL159. doi: 10.1121/1.4976327.Characterizing the binaural contribution to speech-in-noise reception in elderly hearing-impaired listeners.Neher T(1).Author information:(1)Medizinische Physik and Cluster of Excellence ""Hearing4all,"" Oldenburg University, Oldenburg, Germany tobias.neher@uni-oldenburg.de.To scrutinize the binaural contribution to speech-in-noise reception, four groups of elderly participants with or without audiometric asymmetry <2 kHz and with or without near-normal binaural intelligibility level difference (BILD) completed tests of monaural and binaural phase sensitivity as well as cognitive function. Groups did not differ in age, overall degree of hearing loss, or cognitive function. Analyses revealed an influence of BILD status but not audiometric asymmetry on monaural phase sensitivity, strong correlations between monaural and binaural detection thresholds, and monaural and binaural but not cognitive BILD contributions. Furthermore, the N0Sπ threshold at 500 Hz predicted BILD performance effectively.DOI: 10.1121/1.4976327",pubmed,28253695,10.1121/1.4976327
functional anomaly mapping reveals local and distant dysfunction caused by brain lesions,"662. Neuroimage. 2020 Jul 15;215:116806. doi: 10.1016/j.neuroimage.2020.116806. Epub 2020 Apr 10.Functional anomaly mapping reveals local and distant dysfunction caused by brain lesions.DeMarco AT(1), Turkeltaub PE(2).Author information:(1)Department of Neurology, Georgetown University, Washington, DC, 20057, United States. Electronic address: andrew.demarco@georgetown.edu.(2)Department of Neurology, Georgetown University, Washington, DC, 20057, United States; MedStar National Rehabilitation Hospital, Washington, DC, 20010, United States.The lesion method has been important for understanding brain-behavior relationships in humans, but has previously used maps based on structural damage. Lesion measurement based on structural damage may label partly damaged but functional tissue as abnormal, and moreover, ignores distant dysfunction in structurally intact tissue caused by deafferentation, diaschisis, and other processes. A reliable method to map functional integrity of tissue throughout the brain would provide a valuable new approach to measuring lesions. Here, we use machine learning on four dimensional resting state fMRI data obtained from left-hemisphere stroke survivors in the chronic period of recovery and control subjects to generate graded maps of functional anomaly throughout the brain in individual patients. These functional anomaly maps identify areas of obvious structural lesions and are stable across multiple measurements taken months and even years apart. Moreover, the maps identify functionally anomalous regions in structurally intact tissue, providing a direct measure of remote effects of lesions on the function of distant brain structures. Multivariate lesion-behavior mapping using functional anomaly maps replicates classic behavioral localization, identifying inferior frontal regions related to speech fluency, lateral temporal regions related to auditory comprehension, parietal regions related to phonology, and the hand area of motor cortex and descending corticospinal pathways for hand motor function. Further, this approach identifies relationships between tissue function and behavior distant from the structural lesions, including right premotor dysfunction related to ipsilateral hand movement, and right cerebellar regions known to contribute to speech fluency. Brain-wide maps of the functional effects of focal lesions could have wide implications for lesion-behavior association studies and studies of recovery after brain injury.Copyright © 2020 The Author(s). Published by Elsevier Inc. All rights reserved.DOI: 10.1016/j.neuroimage.2020.116806PMCID: PMC7292795",pubmed,32278896,10.1016/j.neuroimage.2020.116806
sign language translation based on new continuous sign language dataset,"deaf individuals rely heavily on one another’s use of sign language as a means of communication. In this era, the significance of accessibility has become more and more important. However, for most people with normal hearing, learning sign language is a very difficult thing. Many research teams around the world are using Deep Learning to develop translators for sign language recognition. However, there is a lack of larger and more semantically rich sign language datasets. In this article, we propose a new Chinese sign language dataset- 109 unique sentences from 50 sign language signers and 27,250 clips. On the new Chinese sign language dataset, we further propose a sequence-to-sequence deep learning approach in order to demonstrate how deep learning may continually lower the communication barriers that exist between persons who are deaf and those who have normal hearing.",ieee,,10.1109/ICAICA54878.2022.9844468
familial clustering of migraine episodic vertigo and mni res disease,"OBJECTIVE: To evaluate the association between migraine, episodic vertigo, and Ménière's disease in families. STUDY DESIGN: Clinical report. SETTING: University Neurotology Clinic. PATIENTS: Index patients identified with Ménière's disease and migraine and their family members. INTERVENTION: Structured interview to assess a diagnosis of migraine, episodic vertigo, and Ménière's disease in 6 families. Genotyping was performed on 3 sets of twins to analyze monozygosity or dizygosity. MAIN OUTCOME MEASURES: Clinical history of migraine, episodic vertigo, and Ménière's disease. RESULTS: Six index patients and 57 family members were interviewed either by a senior neurologist in person or over the phone by a trained study coordinator. An additional 6 family members completed questionnaires by mail. All 6 index patients had Ménière's disease and migraine. Twenty-six (41%) of the 63 relatives met International Classification of Headache Disorders II criteria for migraine headaches. Thirteen (50%) of these 26 experienced migraine with aura. Three others experienced typical aura without headache. Seventeen (27%) of 63 family members experienced recurrent spells of spontaneous episodic vertigo. There was one twin pair in each of 3 families; 2 pairs were monozygotic and one was dizygotic. In each twin pair, one twin had migraine and Ménière's disease, whereas the other experienced migraine and episodic vertigo without auditory symptoms. CONCLUSION: The frequent association of episodic vertigo, migraine, and Ménière's disease in closely related individuals, including identical twins supports the heritability of a migraine-Ménière's syndrome, with variable expression of the individual features of hearing loss, episodic vertigo, and migraine headaches. © 2008 Otology & Neurotology, Inc.",scopus,2-s2.0-38349027309,10.1097/mao.0b013e31815c2abb
microglialike cells in rat organ of corti following aminoglycoside ototoxicity,"638. Neuroreport. 2000 May 15;11(7):1389-93. doi: 10.1097/00001756-200005150-00008.Microglia-like cells in rat organ of Corti following aminoglycoside ototoxicity.Wang Z(1), Li H.Author information:(1)Department of Otolaryngology, EENT Hospital, Shanghai Medical University, PR China.The repair process of neomycin induced cochlear damage in the postnatal developing rat was investigated in the present study. The results showed that electron dense atypical cells with a cluster of microvilli on their apical surface, resembling early stage of embryonic hair cell, were observed in the former hair cell region. The striking finding was that microglia-like cells appeared and replaced OHCs in the injured auditory sensory epithelium. Using Brdu immunohistochemistry, cell proliferation was found in the area of inner and outer spiral sulcus but not in the hair cells and supporting cells. It is proposed that microglia-like cells play a role in eliminating waste products from the organ of Corti and may participate in direct structural repair.DOI: 10.1097/00001756-200005150-00008",pubmed,10841344,10.1097/00001756-200005150-00008
noiseinduced synaptic loss and its postexposure recovery in cbacaj vs c57bl6j mice,"99. Hear Res. 2024 Apr;445:108996. doi: 10.1016/j.heares.2024.108996. Epub 2024 Mar 23.Noise-induced synaptic loss and its post-exposure recovery in CBA/CaJ vs. C57BL/6J mice.Wu PZ(1), Liberman LD(2), Liberman MC(3).Author information:(1)Eaton-Peabody Laboratories, Massachusetts Eye and Ear, Boston, MA 02114, USA; Department of Otolaryngology, Harvard Medical School, Boston, MA 02115, USA.(2)Eaton-Peabody Laboratories, Massachusetts Eye and Ear, Boston, MA 02114, USA.(3)Eaton-Peabody Laboratories, Massachusetts Eye and Ear, Boston, MA 02114, USA; Department of Otolaryngology, Harvard Medical School, Boston, MA 02115, USA. Electronic address: Charles_Liberman@meei.harvard.edu.Acute noise-induced loss of synapses between inner hair cells (IHCs) and auditory nerve fibers (ANFs) has been documented in several strains of mice, but the extent of post-exposure recovery reportedly varies dramatically. If such inter-strain heterogeneity is real, it could be exploited to probe molecular pathways mediating neural remodeling in the adult cochlea. Here, we compared synaptopathy repair in CBA/CaJ vs. C57BL/6J, which are at opposite ends of the reported recovery spectrum. We evaluated C57BL/6J mice 0 h, 24 h, 2 wks or 8 wks after exposure for 2 h to octave-band noise (8-16 kHz) at either 90, 94 or 98 dB SPL, to compare with analogous post-exposure results in CBA/CaJ at 98 or 101 dB. We counted pre- and post-synaptic puncta in immunostained cochleas, using machine learning to classify paired (GluA2 and CtBP2) vs. orphan (CtBP2 only) puncta, and batch-processing to quantify immunostaining intensity. At 98 dB, both strains show ongoing loss of ribbons and synapses between 0 and 24 h, followed by partial recovery, however the extent and degree of these changes were greater in C57BL/6J. Much of the synaptic recovery is due to transient reduction in GluA2 intensity in synaptopathic regions. In contrast, CtBP2 intensity showed only transient increases (at 2 wks). Neurofilament staining revealed transient extension of ANF terminals in C57BL/6J, but not in CBA/CaJ, peaking at 24 h and reverting by 2 wks. Thus, although interstrain differences in synapse recovery are dominated by reversible changes in GluA2 receptor levels, the neurite extension seen in C57BL/6J suggests a qualitative difference in regenerative capacity.Copyright © 2024 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2024.108996PMCID: PMC11024800",pubmed,38547565,10.1016/j.heares.2024.108996
brain responses to musical feature changes in adolescent cochlear implant users,"831. Front Hum Neurosci. 2015 Feb 6;9:7. doi: 10.3389/fnhum.2015.00007. eCollection 2015.Brain responses to musical feature changes in adolescent cochlear implant users.Petersen B(1), Weed E(2), Sandmann P(3), Brattico E(4), Hansen M(5), Sørensen SD(6), Vuust P(1).Author information:(1)Center for Functionally Integrative Neuroscience, Aarhus University Hospital , Aarhus , Denmark ; Royal Academy of Music , Aarhus , Denmark.(2)Center for Functionally Integrative Neuroscience, Aarhus University Hospital , Aarhus , Denmark ; Department of Aesthetics and Communication - Linguistics, Aarhus University , Aarhus , Denmark.(3)Central Auditory Diagnostics Lab, Department of Neurology, Cluster of Excellence ""Hearing4all"", Hannover Medical School , Hannover , Germany.(4)Brain and Mind Laboratory, Department of Biomedical Engineering and Computational Science, Aalto University , Aalto , Finland ; Cognitive Brain Research Unit, Institute of Behavioral Sciences, University of Helsinki , Helsinki , Finland.(5)Center for Functionally Integrative Neuroscience, Aarhus University Hospital , Aarhus , Denmark ; Department of Psychology and Behavioural Sciences, Aarhus University , Aarhus , Denmark.(6)Department of Aesthetics and Communication - Linguistics, Aarhus University , Aarhus , Denmark.Cochlear implants (CIs) are primarily designed to assist deaf individuals in perception of speech, although possibilities for music fruition have also been documented. Previous studies have indicated the existence of neural correlates of residual music skills in postlingually deaf adults and children. However, little is known about the behavioral and neural correlates of music perception in the new generation of prelingually deaf adolescents who grew up with CIs. With electroencephalography (EEG), we recorded the mismatch negativity (MMN) of the auditory event-related potential to changes in musical features in adolescent CI users and in normal-hearing (NH) age mates. EEG recordings and behavioral testing were carried out before (T1) and after (T2) a 2-week music training program for the CI users and in two sessions equally separated in time for NH controls. We found significant MMNs in adolescent CI users for deviations in timbre, intensity, and rhythm, indicating residual neural prerequisites for musical feature processing. By contrast, only one of the two pitch deviants elicited an MMN in CI users. This pitch discrimination deficit was supported by behavioral measures, in which CI users scored significantly below the NH level. Overall, MMN amplitudes were significantly smaller in CI users than in NH controls, suggesting poorer music discrimination ability. Despite compliance from the CI participants, we found no effect of the music training, likely resulting from the brevity of the program. This is the first study showing significant brain responses to musical feature changes in prelingually deaf adolescent CI users and their associations with behavioral measures, implying neural predispositions for at least some aspects of music processing. Future studies should test any beneficial effects of a longer lasting music intervention in adolescent CI users.DOI: 10.3389/fnhum.2015.00007PMCID: PMC4319402",pubmed,25705185,10.3389/fnhum.2015.00007
tlr24 double knockout attenuates the degeneration of primary auditory neurons potential mechanisms from transcriptomic perspectives,"757. Front Cell Dev Biol. 2021 Oct 25;9:750271. doi: 10.3389/fcell.2021.750271. eCollection 2021.Tlr2/4 Double Knockout Attenuates the Degeneration of Primary Auditory Neurons: Potential Mechanisms From Transcriptomic Perspectives.Wang Q(1)(2), Shen Y(1)(2), Pan Y(1)(2), Chen K(1)(2), Ding R(1)(2), Zou T(1)(2), Zhang A(1)(2), Guo D(1)(2), Ji P(1)(2), Fan C(1)(2), Mei L(2), Hu H(1)(2), Ye B(1)(2), Xiang M(1)(2).Author information:(1)Department of Otolaryngology and Head and Neck Surgery, Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, China.(2)Ear Institute, Shanghai Jiao Tong University School of Medicine, Shanghai, China.The transcriptomic landscape of mice with primary auditory neurons degeneration (PAND) indicates key pathways in its pathogenesis, including complement cascades, immune responses, tumor necrosis factor (TNF) signaling pathway, and cytokine-cytokine receptor interaction. Toll-like receptors (TLRs) are important immune and inflammatory molecules that have been shown to disrupt the disease network of PAND. In a PAND model involving administration of kanamycin combined with furosemide to destroy cochlear hair cells, Tlr 2/4 double knockout (DKO) mice had auditory preservation advantages, which were mainly manifested at 4-16 kHz. DKO mice and wild type (WT) mice had completely damaged cochlear hair cells on the 30th day, but the density of spiral ganglion neurons (SGN) in the Rosenthal canal was significantly higher in the DKO group than in the WT group. The results of immunohistochemistry for p38 and p65 showed that the attenuation of SGN degeneration in DKO mice may not be mediated by canonical Tlr signaling pathways. The SGN transcriptome of DKO and WT mice indicated that there was an inverted gene set enrichment relationship between their different transcriptomes and the SGN degeneration transcriptome, which is consistent with the morphology results. Core module analysis suggested that DKO mice may modulate SGN degeneration by activating two clusters, and the involved molecules include EGF, STAT3, CALB2, LOX, SNAP25, CAV2, SDC4, MYL1, NCS1, PVALB, TPM4, and TMOD4.Copyright © 2021 Wang, Shen, Pan, Chen, Ding, Zou, Zhang, Guo, Ji, Fan, Mei, Hu, Ye and Xiang.DOI: 10.3389/fcell.2021.750271PMCID: PMC8573328",pubmed,34760891,10.3389/fcell.2021.750271
common audiological functional parameters cafpas statistical and compact representation of rehabilitative audiological classification based on expert knowledge,"355. Int J Audiol. 2019 Apr;58(4):231-245. doi: 10.1080/14992027.2018.1554912.Common Audiological Functional Parameters (CAFPAs): statistical and compact representation of rehabilitative audiological classification based on expert knowledge.Buhl M(1)(2), Warzybok A(1)(2), Schädler MR(1)(2), Lenarz T(2)(3), Majdani O(2)(3)(4), Kollmeier B(1)(2)(5)(6).Author information:(1)a Medizinische Physik , Universität Oldenburg , Oldenburg , Germany.(2)b Cluster of Excellence Hearing4all, Universität Oldenburg , Oldenburg , Germany.(3)c Clinic and Policlinic for Otolaryngology , Hanover Medical School , Hannover , Germany.(4)d Clinic for Otolaryngology , Städt. Klinikum Wolfsburg , Wolfsburg , Germany.(5)e HörTech gGmbH , Oldenburg , Germany.(6)f Hearing Speech and Audio Technology , Fraunhofer IDMT , Oldenburg , Germany.OBJECTIVE: As a step towards objectifying audiological rehabilitation and providing comparability between different test batteries and clinics, the Common Audiological Functional Parameters (CAFPAs) were introduced as a common and abstract representation of audiological knowledge obtained from diagnostic tests.DESIGN: Relationships between CAFPAs as an intermediate representation between diagnostic tests and audiological findings, diagnoses and treatment recommendations (summarised as ""diagnostic cases"") were established by means of an expert survey. Expert knowledge was collected for 14 given categories covering different diagnostic cases. For each case, the experts were asked to indicate expected ranges of diagnostic test outcomes, as well as traffic light-encoded CAFPAs.STUDY SAMPLE: Eleven German experts in the field of audiological rehabilitation from Hanover and Oldenburg participated in the survey.RESULTS: Audiological findings or treatment recommendations could be distinguished by a statistical model derived from the experts' answers for CAFPAs as well as audiological tests.CONCLUSIONS: The CAFPAs serve as an abstract, comprehensive representation of audiological knowledge. If more detailed information on certain functional aspects of the auditory system is required, the CAFPAs indicate which information is missing. The statistical graphical representations for CAFPAs and audiological tests are suitable for audiological teaching material; they are universally applicable for real clinical databases.DOI: 10.1080/14992027.2018.1554912",pubmed,30900518,10.1080/14992027.2018.1554912
investigating differences in preferred noise reduction strength among hearing aid users,"116. Trends Hear. 2016 Sep 7;20:2331216516655794. doi: 10.1177/2331216516655794.Investigating Differences in Preferred Noise Reduction Strength Among Hearing Aid Users.Neher T(1), Wagener KC(2).Author information:(1)Medizinische Physik, Oldenburg University, Oldenburg, Germany Cluster of Excellence Hearing4all, Oldenburg, Germany tobias.neher@uni-oldenburg.de.(2)Cluster of Excellence Hearing4all, Oldenburg, Germany Hörzentrum Oldenburg GmbH, Oldenburg, Germany.Even though hearing aid (HA) users can respond very differently to noise reduction (NR) processing, knowledge about possible drivers of this variability (and thus ways of addressing it in HA fittings) is sparse. The current study investigated differences in preferred NR strength among HA users. Participants were groups of experienced users with clear preferences (""NR lovers""; N = 14) or dislikes (""NR haters""; N = 13) for strong NR processing, as determined in two earlier studies. Maximally acceptable background noise levels, detection thresholds for speech distortions caused by NR processing, and self-reported ""sound personality"" traits were considered as candidate measures for explaining group membership. Participants also adjusted the strength of the (binaural coherence-based) NR algorithm to their preferred level. Consistent with previous findings, NR lovers favored stronger processing than NR haters, although there also was some overlap. While maximally acceptable noise levels and detection thresholds for speech distortions tended to be higher for NR lovers than for NR haters, group differences were only marginally significant. No clear group differences were observed in the self-report data. Taken together, these results indicate that preferred NR strength is an individual trait that is fairly stable across time and that is not easily captured by psychoacoustic, audiological, or self-report measures aimed at indexing susceptibility to background noise and processing artifacts. To achieve more personalized NR processing, an effective approach may be to let HA users determine the optimal setting themselves during the fitting process.© The Author(s) 2016.DOI: 10.1177/2331216516655794PMCID: PMC5017568",pubmed,27604781,10.1177/2331216516655794
expression of ctype lectin receptor mrna in chronic otitis media with cholesteatoma,"Conclusions: The levels of expression of various C-type lectin receptors (CLRs) messenger ribo nucleic acids (mRNAs) were significantly higher in cholesteatomas than in normal skin, suggesting that these CLRs may be involved in the pathogenesis of cholesteatoma. Objectives: Altered expression of pattern recognition receptors may be associated with immune responses in patients with cholesteatoma. This study assessed the levels of expression of CLR mRNAs in normal skin and in cholesteatoma. Methods: Cholesteatoma specimens were obtained from 38 patients with acquired cholesteatoma. The levels of expression of various CLR mRNAs were assessed quantitatively using real-time RT-PCR (Reverse transcription polymerase chain reaction) and correlated with age, sex, the presence of bacteria, hearing level, frequency of surgery, and degree of ossicle destruction. Results: The levels of CD206 (cluster of differentiation 206), DEC-205 (Dendritic and epithelial cell-205), MGL (monoacylglycerol lipase), CLEC5A (C-type lectin domain family 5 member A), Dectin-2 (dendrite cell-associated C-type lectin-2), BDCA2 (Blood dendritic cell antigen 2), Mincle, DCIR (dendritic cell immunoreceptor), Dectin-1, MICL (Myeloid inhibitory C type-like lectin), and CLEC12B (C-type lectin domain family 12, member B) mRNAs were significantly higher in cholesteatoma than in control skin samples (p < 0.05). The levels of CLEC5A (C-type lectin domain family 5 member) and Dectin-1 mRNAs were significantly higher in cholesteatomas with ≥2 than ≤1 destroyed ossicles (p < 0.05), and the levels of MGL, Mincle, Dectin-1, and CLEC12B mRNAs were significantly higher in recurrent than initial cholesteatoma specimens (p < 0.05). The level of CLEC5A mRNAs was significantly higher in patients with severe than mild-to-moderate hearing loss (p < 0.05). © 2017 Acta Oto-Laryngologica AB (Ltd).",scopus,2-s2.0-85013743012,10.1080/00016489.2016.1269196
mobile health school screening and telemedicine referral to improve access to specialty care in rural alaska a cluster randomised controlled trial,"Background: School-based programmes, including hearing screening, provide essential preventive services for rural children. However, minimal evidence on screening methodologies, loss to follow-up, and scarcity of specialists for subsequent care compound rural health disparities. We hypothesised telemedicine specialty referral would improve time to follow-up for school hearing screening compared with standard primary care referral. Methods: In this cluster-randomised controlled trial conducted in 15 rural Alaskan communities, USA, we randomised communities to telemedicine specialty referral (intervention) or standard primary care referral (control) for school hearing screening. All children (K–12; aged 4–21 years) enrolled in Bering Straight School District were eligible. Community randomisation occurred within four strata using location and school size. Participants were masked to group allocation until screening day, and assessors were masked throughout data collection. Screening occurred annually, and children who screened positive for possible hearing loss or ear disease were monitored for 9 months from the screening date for follow-up. Primary outcome was the time to follow-up after a positive hearing screen; analysis was by intention to treat. The trial was registered with ClinicalTrials.gov, NCT03309553. Findings: We recruited participants between Oct 10, 2017, and March 28, 2019. 15 communities were randomised: eight (750 children) to telemedicine referral and seven (731 children) to primary care referral. 790 (53·3%) of 1481 children screened positive in at least one study year: 391 (52∤1%) in the telemedicine referral communities and 399 (50∤4%) in the primary care referral communities. Of children referred, 268 (68·5%) in the telemedicine referral communities and 128 (32·1%) in primary care referral communities received follow-up within 9 months. Among children who received follow-up, mean time to follow-up was 41·5 days (SD 55·7) in the telemedicine referral communities and 92·0 days (75·8) in the primary care referral communities (adjusted event-time ratio 17·6 [95% CI 6·8–45·3] for all referred children). There were no adverse events. Interpretation: Telemedicine specialty referral significantly improved the time to follow-up after hearing screening in Alaska. Telemedicine might apply to other preventive school-based services to improve access to specialty care for rural children. Funding: Patient-Centered Outcomes Research Institute. © 2022 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license",scopus,2-s2.0-85131935466,10.1016/S2214-109X(22)00184-X
automated highthroughput damage scoring of zebrafish lateral line hair cells after ototoxin exposure,"713. Zebrafish. 2018 Apr;15(2):145-155. doi: 10.1089/zeb.2017.1451. Epub 2018 Jan 30.Automated High-Throughput Damage Scoring of Zebrafish Lateral Line Hair Cells After Ototoxin Exposure.Philip RC(1), Rodriguez JJ(1), Niihori M(2)(3), Francis RH(2)(4), Mudery JA(2)(4), Caskey JS(2)(4), Krupinski E(5), Jacob A(2)(3)(6)(7).Author information:(1)1 Department of Electrical and Computer Engineering, The University of Arizona , Tucson, Arizona.(2)2 Department of Otolaryngology, The University of Arizona , Tucson, Arizona.(3)3 The University of Arizona Cancer Center , Tucson, Arizona.(4)4 College of Medicine, The University of Arizona , Tucson, Arizona.(5)5 Department of Radiology and Imaging Sciences, Emory University , Atlanta, Georgia .(6)6 BIO5 Institute, The University of Arizona , Tucson, Arizona.(7)7 Ear & Hearing, Center for Neurosciences , Tucson, Arizona.Zebrafish have emerged as a powerful biological system for drug development against hearing loss. Zebrafish hair cells, contained within neuromasts along the lateral line, can be damaged with exposure to ototoxins, and therefore, pre-exposure to potentially otoprotective compounds can be a means of identifying promising new drug candidates. Unfortunately, anatomical assays of hair cell damage are typically low-throughput and labor intensive, requiring trained experts to manually score hair cell damage in fluorescence or confocal images. To enhance throughput and consistency, our group has developed an automated damage-scoring algorithm based on machine-learning techniques that produce accurate damage scores, eliminate potential operator bias, provide more fidelity in determining damage scores that are between two levels, and deliver consistent results in a fraction of the time required for manual analysis. The system has been validated against trained experts using linear regression, hypothesis testing, and the Pearson's correlation coefficient. Furthermore, performance has been quantified by measuring mean absolute error for each image and the time taken to automatically compute damage scores. Coupling automated analysis of zebrafish hair cell damage to behavioral assays for ototoxicity produces a novel drug discovery platform for rapid translation of candidate drugs into preclinical mammalian models of hearing loss.DOI: 10.1089/zeb.2017.1451",pubmed,29381431,10.1089/zeb.2017.1451
longterm feeding of a highfat diet ameliorated agerelated phenotypes in samp8 mice,"High-fat diets (HFD) have been thought to increase the risk of obesity and metabolic syndrome, as well as shorten lifespan. On the other hand, chrono-nutritional studies have shown that time-restricted feeding during active phase significantly suppresses the induction of HFDinduced obesity in mouse model. However, the long-term effects of time-restricted HFD feeding on aging are unknown. Therefore, in this study, we set up a total of four groups: mutual combination of ad libitum feeding or night-time-restricted feeding (NtRF) and an HFD or a control diet. We examined their long-term effects in a senescence-accelerated mouse strain, SAMP8, for over a year. Hearing ability, cognitive function, and other behavioral and physiological indexes were evaluated during the study. Unexpectedly, SAMP8 mice did not show early onset of death caused by the prolonged HFD intake, and both HFD and NtRF retarded age-related hearing loss (AHL). NtRF improved grip strength and cognitive memory scores, while HFD weakly suppressed age-related worsening of the appearance scores associated with the eyes. Notably, the HFD also retarded the progression of AHL in both DBA/2J and C57BL/6J mice. These results suggest that HFD prevents aging unless metabolic disorders occur and that HFD and NtRF are independently effective in retarding aging; thus, the combination of HFD and chrono-nutritional feeding may be an effective anti-aging strategy. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",scopus,2-s2.0-85084787725,10.3390/nu12051416
altered verbal fluency processes in older adults with agerelated hearing loss,"88. Exp Gerontol. 2020 Feb;130:110794. doi: 10.1016/j.exger.2019.110794. Epub 2019 Nov 30.Altered verbal fluency processes in older adults with age-related hearing loss.Loughrey DG(1), Pakhomov SVS(2), Lawlor BA(3).Author information:(1)Global Brain Health Institute, Trinity College Dublin, Ireland; Global Brain Health Institute, University of California, San Francisco, USA; Trinity College Institute of Neuroscience, Trinity College Dublin. Electronic address: loughred@tcd.ie.(2)Institute for Health Informatics, University of Minnesota, Minneapolis, USA.(3)Global Brain Health Institute, Trinity College Dublin, Ireland; Global Brain Health Institute, University of California, San Francisco, USA; Mercer's Institute for Successful Ageing, St James Hospital, Dublin, Ireland.Epidemiological studies have linked age-related hearing loss (ARHL) with an increased risk of neurocognitive decline. Difficulties in speech perception with subsequent changes in brain morphometry, including regions important for lexical-semantic memory, are thought to be a possible mechanism for this relationship. This study investigated differences in automatic and executive lexical-semantic processes on verbal fluency tasks in individuals with acquired hearing loss. The primary outcomes were indices of automatic (clustering/word retrieval at start of task) and executive (switching/word retrieval after start of the task) processes from semantic and phonemic fluency tasks. To extract indices of clustering and switching, we used both manual and computerised methods. There were no differences between groups on indices of executive fluency processes or on any indices from the semantic fluency task. The hearing loss group demonstrated weaker automatic processes on the phonemic fluency task. Further research into differences in lexical-semantic processes with ARHL is warranted.Copyright © 2019 Elsevier Inc. All rights reserved.DOI: 10.1016/j.exger.2019.110794",pubmed,31790801,10.1016/j.exger.2019.110794
iv1 spectrin stabilizes the nodes of ranvier and axon initial segments,"Saltatory electric conduction requires clustered voltage-gated sodium channels (VGSCs) at axon initial segments (AIS) and nodes of Ranvier (NR). A dense membrane undercoat is present at these sites, which is thought to be key for the focal accumulation of channels. Here, we prove that βIVΣ1 spectrin, the only βIV spectrin with an actin-binding domain, is an essential component of this coat. Specifically, βIVΣ1 coexists with βIVΣ6 at both AIS and NR, being the predominant spectrin at AIS. Removal of βIVΣ1 alone causes the disappearance of the nodal coat, an increased diameter of the NR, and the presence of dilations filled with organelles. Moreover, in myelinated cochlear afferent fibers, VGSC and ankyrin G clusters appear fragmented. These ultrastructural changes can explain the motor and auditory neuropathies present in βIVΣ1 -/- mice and point to the βIVΣ1 spectrin isoform as a master-stabilizing factor of AIS/NR membranes.",scopus,2-s2.0-4644286228,10.1083/jcb.200408007
protection from cisplatininduced hearing loss with lentiviral vectormediated ectopic expression of the antiapoptotic protein bclxl,"718. Mol Ther Nucleic Acids. 2024 Feb 19;35(1):102157. doi: 10.1016/j.omtn.2024.102157. eCollection 2024 Mar 12.Protection from cisplatin-induced hearing loss with lentiviral vector-mediated ectopic expression of the anti-apoptotic protein BCL-XL.Nassauer L(1), Staecker H(2), Huang P(2), Renslo B(2), Goblet M(3)(4), Harre J(3)(4), Warnecke A(3)(4), Schott JW(1), Morgan M(1), Galla M(1), Schambach A(1)(5).Author information:(1)Institute of Experimental Hematology, Hannover Medical School, 30625 Hannover, Germany.(2)Department of Otolaryngology-Head and Neck Surgery, University of Kansas School of Medicine, Kansas City, KS 66160, USA.(3)Department of Otorhinolaryngology, Head and Neck Surgery, Hannover Medical School, 30625 Hannover, Germany.(4)Cluster of Excellence ""Hearing4all"", Hannover Medical School, 30625 Hannover, Germany.(5)Division of Hematology/Oncology, Boston Children's Hospital, Harvard Medical School, Boston, MA 02115, USA.Cisplatin is a highly effective chemotherapeutic agent, but it can cause sensorineural hearing loss (SNHL) in patients. Cisplatin-induced ototoxicity is closely related to the accumulation of reactive oxygen species (ROS) and subsequent death of hair cells (HCs) and spiral ganglion neurons (SGNs). Despite various strategies to combat ototoxicity, only one therapeutic agent has thus far been clinically approved. Therefore, we have developed a gene therapy concept to protect cochlear cells from cisplatin-induced toxicity. Self-inactivating lentiviral (LV) vectors were used to ectopically express various antioxidant enzymes or anti-apoptotic proteins to enhance the cellular ROS scavenging or prevent apoptosis in affected cell types. In direct comparison, anti-apoptotic proteins mediated a stronger reduction in cytotoxicity than antioxidant enzymes. Importantly, overexpression of the most promising candidate, Bcl-xl, achieved an up to 2.5-fold reduction in cisplatin-induced cytotoxicity in HEI-OC1 cells, phoenix auditory neurons, and primary SGN cultures. BCL-XL protected against cisplatin-mediated tissue destruction in cochlear explants. Strikingly, in vivo application of the LV BCL-XL vector improved hearing and increased HC survival in cisplatin-treated mice. In conclusion, we have established a preclinical gene therapy approach to protect mice from cisplatin-induced ototoxicity that has the potential to be translated to clinical use in cancer patients.© 2024 The Author(s).DOI: 10.1016/j.omtn.2024.102157PMCID: PMC10915631",pubmed,38450280,10.1016/j.omtn.2024.102157
hearing loops and induction coils improving snr in public spaces,,cinahl,10745734,
prevalence of unilateral and bilateral deafness in border collies and association with phenotype,"279. J Vet Intern Med. 2006 Nov-Dec;20(6):1355-62. doi: 10.1892/0891-6640(2006)20[1355:pouabd]2.0.co;2.Prevalence of unilateral and bilateral deafness in border collies and association with phenotype.Platt S(1), Freeman J, di Stefani A, Wieczorek L, Henley W.Author information:(1)Centre for Small Animal Studies, Animal Health Trust, Lanwades Park, Kentford, Newmar- ket, Suffolk CB8 7UU, UK. Simon.platt@aht.org.ukBACKGROUND: Congenital sensorineural deafness (CSD) occurs in Border Collies, but its prevalence and inheritance are unknown. This study estimated the prevalence of CSD in Border Collies and investigated its association with phenotypic attributes linked to the merle gene, including coat pigmentation and iris color.HYPOTHESIS: Deafness in Border Collies is associated with pigmentation patterns linked to the merle gene.ANIMALS: A total of 2597 Border Collies from the United Kingdom.METHODS: A retrospective study of Border Collies tested, during 1994-2002, by using brainstem auditory evoked responses. Associations between deafness and phenotypic attributes were assessed by using generalized logistic regression.RESULTS: The prevalence of CSD in puppies was estimated as 2.8%. The corresponding rates of unilateral and bilateral CSD were 2.3 and 0.5%, respectively. Adjustment for clustering of hearing status by litter reduced the overall prevalence estimate to 1.6%. There was no association between CSD and sex (P = .2). Deaf Border Collies had higher rates of merle coat pigmentation, blue iris pigment, and excess white on the head than normal hearing Border Collies (all P < .001). The odds of deafness were increased by a factor of 14 for Border Collies with deaf dams, relative to the odds for dogs with normal dams (P = .007), after adjustment for phenotypic attributes.CONCLUSIONS AND CLINICAL IMPORTANCE: Associations between CSD and pigmentation patterns linked to the merle gene were demonstrated for Border Collies. Evidence for an inherited component to CSD in Border Collies supports selective breeding from only tested and normal parents to reduce the prevalence of this disease.DOI: 10.1892/0891-6640(2006)20[1355:pouabd]2.0.co;2",pubmed,17186850,10.1892/0891-6640(2006)20[1355:pouabd]2.0.co;2
in reply association of patient frailty with vestibular schwannoma resection outcomes and machine learning development of a vestibular schwannoma risk stratification score,[No abstract available],scopus,2-s2.0-85140143866,10.1227/neu.0000000000002155
crossmodal connectivity effects in agerelated hearing loss,"5. Neurobiol Aging. 2022 Mar;111:1-13. doi: 10.1016/j.neurobiolaging.2021.09.024. Epub 2021 Nov 14.Cross-modal connectivity effects in age-related hearing loss.Ponticorvo S(1), Manara R(2), Cassandro E(3), Canna A(4), Scarpa A(3), Troisi D(3), Cassandro C(5), Cuoco S(3), Cappiello A(3), Pellecchia MT(3), Salle FD(3), Esposito F(6).Author information:(1)Department of Medicine, Surgery and Dentistry, Scuola Medica Salernitana, University of Salerno, Baronissi, Italy.(2)Department of Medicine, Surgery and Dentistry, Scuola Medica Salernitana, University of Salerno, Baronissi, Italy; Department of Neuroscience, University of Padova, Padova, Italy.(3)Department of Medicine, Surgery and Dentistry, Scuola Medica Salernitana, University of Salerno, Baronissi, Italy; University Hospital ""San Giovanni di Dio e Ruggi D'Aragona"", Scuola Medica Salernitana, Salerno, Italy.(4)Department of Advanced Medical and Surgical Sciences, University of Campania ""Luigi Vanvitelli"", Napoli, Italy.(5)University Hospital ""San Giovanni di Dio e Ruggi D'Aragona"", Scuola Medica Salernitana, Salerno, Italy.(6)Department of Advanced Medical and Surgical Sciences, University of Campania ""Luigi Vanvitelli"", Napoli, Italy. Electronic address: fabrizio.esposito@unicampania.it.Age-related sensorineural hearing loss (HL) leads to localized brain changes in the primary auditory cortex, long-range functional alterations, and is considered a risk factor for dementia. Nonhuman studies have repeatedly highlighted cross-modal brain plasticity in sensorial brain networks other than those primarily involved in the peripheral damage, thus in this study, the possible cortical alterations associated with HL have been analyzed using a whole-brain multimodal connectomic approach. Fifty-two HL and 30 normal hearing participants were examined in a 3T MRI study along with audiological and neurological assessments. Between-regions functional connectivity and whole-brain probabilistic tractography were calculated in a connectome-based manner and graph theory was used to obtain low-dimensional features for the analysis of brain connectivity at global and local levels. The HL condition was associated with a different functional organization of the visual subnetwork as revealed by a significant increase in global efficiency, density, and clustering coefficient. These functional effects were mirrored by similar (but more subtle) structural effects suggesting that a functional repurposing of visual cortical centers occurs to compensate for age-related loss of hearing abilities.Copyright © 2021 Elsevier Inc. All rights reserved.DOI: 10.1016/j.neurobiolaging.2021.09.024",pubmed,34915240,10.1016/j.neurobiolaging.2021.09.024
mechanosensitive hair celllike cells from embryonic and induced pluripotent stem cells,"776. Cell. 2010 May 14;141(4):704-16. doi: 10.1016/j.cell.2010.03.035.Mechanosensitive hair cell-like cells from embryonic and induced pluripotent stem cells.Oshima K(1), Shin K, Diensthuber M, Peng AW, Ricci AJ, Heller S.Author information:(1)Department of Otolaryngology-Head and Neck Surgery, Stanford University School of Medicine, Stanford, CA 94305, USA.Mechanosensitive sensory hair cells are the linchpin of our senses of hearing and balance. The inability of the mammalian inner ear to regenerate lost hair cells is the major reason for the permanence of hearing loss and certain balance disorders. Here, we present a stepwise guidance protocol starting with mouse embryonic stem and induced pluripotent stem cells, which were directed toward becoming ectoderm capable of responding to otic-inducing growth factors. The resulting otic progenitor cells were subjected to varying differentiation conditions, one of which promoted the organization of the cells into epithelial clusters displaying hair cell-like cells with stereociliary bundles. Bundle-bearing cells in these clusters responded to mechanical stimulation with currents that were reminiscent of immature hair cell transduction currents.Copyright (c) 2010 Elsevier Inc. All rights reserved.DOI: 10.1016/j.cell.2010.03.035PMCID: PMC2873974",pubmed,20478259,10.1016/j.cell.2010.03.035
first report of mexican patients with pacs1 related neurodevelopmental disorder and review of the pacs1  pacs2  and wdr37related ophthalmological manifestations,"Introduction: PACS1-related neurodevelopmental disorder (PACS1-related NDD) is caused by pathogenic variants in the PACS1 gene and is characterized by a distinctive facial appearance, intellectual disability, speech delay, seizures, feeding difficulties, cryptorchidism, hernias, and structural anomalies of the brain, heart, eye, and kidney. There is a marked facial resemblance and a common multisystem affectation with patients carrying pathogenic variants in the WDR37 and PACS2 genes, although they vary in terms of severity and eye involvement. Case Presentation: Here, we describe 4 individuals with PACS1-related NDD from Mexico, all of them carrying a de novo PACS1 variant c.607C>T; p.(Arg203Trp) identified by exome sequencing. In addition to eye colobomata, this report identified corneal leukoma, cataracts, and tortuosity of retinal vessels as ophthalmic manifestations not previously reported in patients with PACS1-related NDD. Discussion: We reviewed the ocular phenotypes reported in 74 individuals with PACS1-related NDD and the overlaps with WDR37- and PACS2-related syndromes. We found that the 3 syndromes have in common the presence of colobomata, ptosis, nystagmus, strabismus, and refractive errors, whereas microphthalmia, microcornea, and Peters anomaly are found only among individuals with PACS1-related NDD and WDR37 syndrome, being more severe in the latter. This supports the previous statement that the so-called WDR37-PACS1-PACS2 axis might have an important role in ocular development and also that the specific ocular findings could be useful in the clinical differentiation between these related syndromes. © 2022 S. Karger AG, Basel. All rights reserved.",scopus,2-s2.0-85145289036,10.1159/000526975
proteostasis is essential during cochlear development for neuron survival and hair cell polarity,"759. EMBO Rep. 2019 Sep;20(9):e47097. doi: 10.15252/embr.201847097. Epub 2019 Jul 19.Proteostasis is essential during cochlear development for neuron survival and hair cell polarity.Freeman S(1), Mateo Sánchez S(1), Pouyo R(1), Van Lerberghe PB(1), Hanon K(1), Thelen N(1), Thiry M(1), Morelli G(1)(2), Van Hees L(1), Laguesse S(1), Chariot A(1)(3)(4), Nguyen L(1), Delacroix L(1), Malgrange B(1).Author information:(1)GIGA-Neurosciences, Interdisciplinary Cluster for Applied Genoproteomics (GIGA-R), C.H.U. Sart Tilman, University of Liège, Liège, Belgium.(2)UHasselt, BIOMED, Hasselt, Belgium.(3)GIGA-Molecular Biology of Diseases, Interdisciplinary Cluster for Applied Genoproteomics (GIGA-R), C.H.U. Sart Tilman, University of Liège, Liège, Belgium.(4)Walloon Excellence in Life Sciences and Biotechnology (WELBIO), Wavre, Belgium.Protein homeostasis is essential to cell function, and a compromised ability to reduce the load of misfolded and aggregated proteins is linked to numerous age-related diseases, including hearing loss. Here, we show that altered proteostasis consequent to Elongator complex deficiency also impacts the proper development of the cochlea and results in deafness. In the absence of the catalytic subunit Elp3, differentiating spiral ganglion neurons display large aggresome-like structures and undergo apoptosis before birth. The cochlear mechanosensory cells are able to survive proteostasis disruption but suffer defects in polarity and stereociliary bundle morphogenesis. We demonstrate that protein aggregates accumulate at the apical surface of hair cells, where they cause a local slowdown of microtubular trafficking, altering the distribution of intrinsic polarity proteins and affecting kinocilium position and length. Alleviation of protein misfolding using the chemical chaperone 4-phenylbutyric acid during embryonic development ameliorates hair cell polarity in Elp3-deficient animals. Our study highlights the importance of developmental proteostasis in the cochlea and unveils an unexpected link between proteome integrity and polarized organization of cellular components.© 2019 The Authors.DOI: 10.15252/embr.201847097PMCID: PMC6726910",pubmed,31321879,10.15252/embr.201847097
ent from afar opportunities for remote patient assessment clinical management teaching and learning,"Remote communication in ENT has been expanding, spurred by the COVID-19 pandemic. Conferences and teaching have moved online, enabling easier participation and reducing financial and environmental costs. Online multi-disciplinary meetings have recently been instigated in Africa to discuss management of cases in head and neck cancer, or cochlear implantation, expanding access and enhancing patient care. Remote patient consultation has also seen an explosion, but existing literature suggests some caution, particularly because many patients in ENT need an examination to enable definitive diagnosis. Ongoing experience will help us to better understand how remote communication will fit into our future working lives, and also where face-to-face interaction may still be preferable. © 2021 John Wiley & Sons Ltd",scopus,2-s2.0-85105045625,10.1111/coa.13784
hearingimpaired listeners show increased audiovisual benefit when listening to speech in noise,"113. Neuroimage. 2019 Aug 1;196:261-268. doi: 10.1016/j.neuroimage.2019.04.017. Epub 2019 Apr 9.Hearing-impaired listeners show increased audiovisual benefit when listening to speech in noise.Puschmann S(1), Daeglau M(2), Stropahl M(3), Mirkovic B(4), Rosemann S(5), Thiel CM(6), Debener S(7).Author information:(1)Montreal Neurological Institute, McGill University, Montreal, Quebec, H3A 2B4, Canada; Biological Psychology Lab, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany; Cluster of Excellence Hearing4All, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany. Electronic address: sebastian.puschmann@mail.mcgill.ca.(2)Neuropsychology Lab, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany; Neurocognition and Functional Neurorehabilitation Group, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany.(3)Neuropsychology Lab, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany.(4)Cluster of Excellence Hearing4All, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany; Neuropsychology Lab, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany.(5)Biological Psychology Lab, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany; Cluster of Excellence Hearing4All, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany.(6)Biological Psychology Lab, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany; Cluster of Excellence Hearing4All, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany; Research Center Neurosensory Science, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany.(7)Cluster of Excellence Hearing4All, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany; Neuropsychology Lab, Department of Psychology, School of Medicine and Health Sciences, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany; Research Center Neurosensory Science, Carl von Ossietzky Universität Oldenburg, 26111, Oldenburg, Germany.Recent studies provide evidence for changes in audiovisual perception as well as for adaptive cross-modal auditory cortex plasticity in older individuals with high-frequency hearing impairments (presbycusis). We here investigated whether these changes facilitate the use of visual information, leading to an increased audiovisual benefit of hearing-impaired individuals when listening to speech in noise. We used a naturalistic design in which older participants with a varying degree of high-frequency hearing loss attended to running auditory or audiovisual speech in noise and detected rare target words. Passages containing only visual speech served as a control condition. Simultaneously acquired scalp electroencephalography (EEG) data were used to study cortical speech tracking. Target word detection accuracy was significantly increased in the audiovisual as compared to the auditory listening condition. The degree of this audiovisual enhancement was positively related to individual high-frequency hearing loss and subjectively reported listening effort in challenging daily life situations, which served as a subjective marker of hearing problems. On the neural level, the early cortical tracking of the speech envelope was enhanced in the audiovisual condition. Similar to the behavioral findings, individual differences in the magnitude of the enhancement were positively associated with listening effort ratings. Our results therefore suggest that hearing-impaired older individuals make increased use of congruent visual information to compensate for the degraded auditory input.Copyright © 2019 Elsevier Inc. All rights reserved.DOI: 10.1016/j.neuroimage.2019.04.017",pubmed,30978494,10.1016/j.neuroimage.2019.04.017
familial clustering of migraine episodic vertigo and mnires disease,"517. Otol Neurotol. 2008 Jan;29(1):93-6. doi: 10.1097/mao.0b013e31815c2abb.Familial clustering of migraine, episodic vertigo, and Ménière's disease.Cha YH(1), Kane MJ, Baloh RW.Author information:(1)Department of Neurology, University of California-Los Angeles, Los Angeles, California 90095, USA. yhcha@mednet.ucla.eduOBJECTIVE: To evaluate the association between migraine, episodic vertigo, and Ménière's disease in families.STUDY DESIGN: Clinical report.SETTING: University Neurotology Clinic.PATIENTS: Index patients identified with Ménière's disease and migraine and their family members.INTERVENTION: Structured interview to assess a diagnosis of migraine, episodic vertigo, and Ménière's disease in 6 families. Genotyping was performed on 3 sets of twins to analyze monozygosity or dizygosity.MAIN OUTCOME MEASURES: Clinical history of migraine, episodic vertigo, and Ménière's disease.RESULTS: Six index patients and 57 family members were interviewed either by a senior neurologist in person or over the phone by a trained study coordinator. An additional 6 family members completed questionnaires by mail. All 6 index patients had Ménière's disease and migraine. Twenty-six (41%) of the 63 relatives met International Classification of Headache Disorders II criteria for migraine headaches. Thirteen (50%) of these 26 experienced migraine with aura. Three others experienced typical aura without headache. Seventeen (27%) of 63 family members experienced recurrent spells of spontaneous episodic vertigo. There was one twin pair in each of 3 families; 2 pairs were monozygotic and one was dizygotic. In each twin pair, one twin had migraine and Ménière's disease, whereas the other experienced migraine and episodic vertigo without auditory symptoms.CONCLUSION: The frequent association of episodic vertigo, migraine, and Ménière's disease in closely related individuals, including identical twins supports the heritability of a migraine-Ménière's syndrome, with variable expression of the individual features of hearing loss, episodic vertigo, and migraine headaches.DOI: 10.1097/mao.0b013e31815c2abbPMCID: PMC2820370",pubmed,18046258,10.1097/mao.0b013e31815c2abb
a diagnostic marker for speech delay associated with otitis media with effusion the intelligibilityspeech gap,"485. Clin Linguist Phon. 2003 Oct-Nov;17(7):507-28. doi: 10.1080/0269920031000138169.A diagnostic marker for speech delay associated with otitis media with effusion: the intelligibility-speech gap.Shriberg LD(1), Flipsen P Jr, Kwiatkowski J, McSweeny JL.Author information:(1)Phonology Project, Waisman Center, University of Wisconsin-Madison, 1500 Highland Avenue, Madison, WI 53705, USA. shriberg@waisman.wisc.eduThe goal of this study was to determine if notably reduced intelligibility is a potential diagnostic marker for children with speech delay and histories of early recurrent otitis media with effusion (SD-OME). Intelligibility was assessed in one 5-10 minute conversational speech sample from each of 281 speakers. The OME histories of 148 of these children with normal speech acquisition were described in two prior reports. OME histories of 85 additional children with speech delay were obtained from case history reports. For both groups, the children with positive OME (OME+) histories had significantly lower intelligibility scores but significantly higher speech production scores than children with negative OME (OME-) histories. Findings for a diagnostic marker to discriminate speech delayed children with OME+ versus OME- histories were promising, considering that the data were obtained retrospectively and did not include audiological information characterizing children's concurrent fluctuant hearing loss. The formula for the diagnostic marker, termed the Intelligibility-Speech Gap, was identified by a machine learning routine. Diagnostic accuracy findings for the marker were as follows: positive predictive value = 74%, negative predictive value = 86%, sensitivity = 79%, specificity = 83%, positive likelihood ratio = 4.6 and negative likelihood ratio = 0.25. Discussion considers speech processing perspectives on the source of the intelligibility-speech gap in children with suspected SD-OME, and methodological perspectives on its development as a diagnostic marker of one etiological subtype of speech delay.DOI: 10.1080/0269920031000138169",pubmed,14608797,10.1080/0269920031000138169
earlyonset cerebellar ataxia in a patient with cmt2a2,"801. Cold Spring Harb Mol Case Stud. 2020 Jun 12;6(3):a005108. doi: 10.1101/mcs.a005108. Print 2020 Jun.Early-onset cerebellar ataxia in a patient with CMT2A2.Madrid R(1), Guariglia SR(1), Haworth A(2), Korosh W(1), Gavin M(1), Lyon GJ(1).Author information:(1)Jervis Clinic, NYS Institute for Basic Research in Developmental Disabilities (IBR), Staten Island, New York 10314, USA.(2)Congenica Ltd, Biodata Innovation Centre, Wellcome Genome Campus, Hinxton, Cambridge CB10 1SA, United Kingdom.A 9-yr 8-mo-old right-handed female presented with a history of gait difficulties, which first became apparent at age 9 mo of age, along with slurred speech and hand tremors while holding a tray. Her past medical history was significant for global developmental delay, and she was attending fourth grade special education classes. On examination, she had an ataxic gait, dysarthria, absent deep tendon reflexes, and flexor plantar responses. There were no signs of optic atrophy or hearing loss. Nerve conduction studies were consistent with an axonal neuropathy. A fascicular sural nerve biopsy showed a marked decrease of myelinated fibers larger than 6 µm in diameter as compared with an age-matched control. By electron microscopy, clusters of degenerating axonal mitochondria in both myelinated and unmyelinated fibers were frequently found. Whole-exome sequencing revealed a heterozygous c.314C > T (p.Thr105Met) missense variant in MFN2 in the patient but not in her mother. The father was unavailable for testing. The phenotypes with MFN2 variants can be quite variable, including intellectual disability, optic atrophy, auditory impairment, spinal atrophy with or without hydromyelia, and hydrocephalus. We report here that early onset ataxia with intellectual disability can also be associated with MFN2-related Charcot-Marie-Tooth, Type 2A2A diagnosis, the most common type of autosomal dominant axonal neuropathy.© 2020 Madrid et al.; Published by Cold Spring Harbor Laboratory Press.DOI: 10.1101/mcs.a005108PMCID: PMC7304361",pubmed,32532879,10.1101/mcs.a005108
identifying the mechanisms underlying the protective effect of tetramethylpyrazine against cisplatininduced in vitro ototoxicity in heioc1 auditory cells using gene expression profiling,"Sensorineural hearing loss is prevalent in patients receiving cisplatin therapy. Tetramethylpyrazine (Tet) and tanshinone IIA (Tan IIA ) have protective roles against hearing impairment or ototoxicity. The present study aimed to investigate the molecular mechanisms underlying cisplatin-induced ototoxicity and the protective effect of Tet and Tan IIA against it. House Ear Institute-O rgan of Corti 1 auditory cells were treated with titrating doses of Tan IIA , Tet, and cisplatin. In a cell viability assay, cisplatin, Tan IIA and Tet had IC 50 values of 42.89 μM, 151.80 and 1.04x103 mg/l, respectively. Tan IIA augmented cisplatin-induced cytotoxicity. However, Tet concentrations <75 mg/l attenuated cisplatin-induced cytotoxicity and apoptosis. Moreover, RNA sequencing analysis was carried out on auditory cells treated for 30 h with 30 μM cisplatin alone for 48 h or combined with 37.5 mg/l Tet for 30 h. Differentially expressed genes (DE Gs) induced in these conditions were identified and examined using Gene Ontology and Kyoto Encyclopedia of Genes and Genomes pathway analysis. Cisplatin increased the expression of genes related to the p53 and FoxO pathways, such as Fas, p21/CD KN1A, and Bcl-2 binding component 3, but decreased the expression of insulin-like growth factor 1 (IGF1), as well as genes in the histone (Hist)1 and Hist2 clusters. Treatment with Tet downregulated FOXO3 and Bcl-2 binding component 3, and increased the expression of IGF1. Moreover, Tet upregulated genes associated with Wnt signaling, but not p53-related genes. Thus, the otoprotective properties of Tet might be mediated by activation of Wnt and IGF1 signaling, and inhibition of FoxO signaling. © 2020 Spandidos Publications. All rights reserved.",scopus,2-s2.0-85095686460,10.3892/mmr.2020.11631
medicalgrade silicone rubberhydrogelcomposites for modiolar hugging cochlear implants,"692. Polymers (Basel). 2022 Apr 26;14(9):1766. doi: 10.3390/polym14091766.Medical-Grade Silicone Rubber-Hydrogel-Composites for Modiolar Hugging Cochlear Implants.Yilmaz-Bayraktar S(1)(2), Foremny K(1)(2), Kreienmeyer M(1)(2), Warnecke A(1)(2), Doll T(1)(2)(3).Author information:(1)Department of Otolaryngology, Hannover Medical School, Carl-Neuberg-Straße 1, 30625 Hannover, Germany.(2)Cluster of Excellence Hearing4All, Carl-Neuberg-Straße 1, 30625 Hannover, Germany.(3)Fraunhofer Institute for Toxicology and Experimental Medicine (ITEM), Nikolai-Fuchs-Straße 1, 30625 Hannover, Germany.The gold standard for the partial restoration of sensorineural hearing loss is cochlear implant surgery, which restores patients' speech comprehension. The remaining limitations, e.g., music perception, are partly due to a gap between cochlear implant electrodes and the auditory nerve cells in the modiolus of the inner ear. Reducing this gap will most likely lead to improved cochlear implant performance. To achieve this, a bending or curling mechanism in the electrode array is discussed. We propose a silicone rubber-hydrogel actuator where the hydrogel forms a percolating network in the dorsal silicone rubber compartment of the electrode array to exert bending forces at low volume swelling ratios. A material study of suitable polymers (medical-grade PDMS and hydrogels), including parametrized bending curvature measurements, is presented. The curvature radii measured meet the anatomical needs for positioning electrodes very closely to the modiolus. Besides stage-one biocompatibility according to ISO 10993-5, we also developed and validated a simplified mathematical model for designing hydrogel-actuated CI with modiolar hugging functionality.DOI: 10.3390/polym14091766PMCID: PMC9103165",pubmed,35566935,10.3390/polym14091766
improved tactile speech robustness to background noise with a dualpath recurrent neural network noisereduction method,"259. Sci Rep. 2024 Mar 28;14(1):7357. doi: 10.1038/s41598-024-57312-7.Improved tactile speech robustness to background noise with a dual-path recurrent neural network noise-reduction method.Fletcher MD(1)(2), Perry SW(3)(4), Thoidis I(5), Verschuur CA(3), Goehring T(6).Author information:(1)University of Southampton Auditory Implant Service, University of Southampton, University Road, Southampton, SO17 1BJ, UK. M.D.Fletcher@soton.ac.uk.(2)Institute of Sound and Vibration Research, University of Southampton, University Road, Southampton, SO17 1BJ, UK. M.D.Fletcher@soton.ac.uk.(3)University of Southampton Auditory Implant Service, University of Southampton, University Road, Southampton, SO17 1BJ, UK.(4)Institute of Sound and Vibration Research, University of Southampton, University Road, Southampton, SO17 1BJ, UK.(5)School of Electrical and Computer Engineering, Aristotle University of Thessaloniki, 54124, Thessaloniki, Greece.(6)MRC Cognition and Brain Sciences Unit, University of Cambridge, 15 Chaucer Road, Cambridge, CB2 7EF, UK.Many people with hearing loss struggle to understand speech in noisy environments, making noise robustness critical for hearing-assistive devices. Recently developed haptic hearing aids, which convert audio to vibration, can improve speech-in-noise performance for cochlear implant (CI) users and assist those unable to access hearing-assistive devices. They are typically body-worn rather than head-mounted, allowing additional space for batteries and microprocessors, and so can deploy more sophisticated noise-reduction techniques. The current study assessed whether a real-time-feasible dual-path recurrent neural network (DPRNN) can improve tactile speech-in-noise performance. Audio was converted to vibration on the wrist using a vocoder method, either with or without noise reduction. Performance was tested for speech in a multi-talker noise (recorded at a party) with a 2.5-dB signal-to-noise ratio. An objective assessment showed the DPRNN improved the scale-invariant signal-to-distortion ratio by 8.6 dB and substantially outperformed traditional noise-reduction (log-MMSE). A behavioural assessment in 16 participants showed the DPRNN improved tactile-only sentence identification in noise by 8.2%. This suggests that advanced techniques like the DPRNN could substantially improve outcomes with haptic hearing aids. Low-cost haptic devices could soon be an important supplement to hearing-assistive devices such as CIs or offer an alternative for people who cannot access CI technology.© 2024. The Author(s).DOI: 10.1038/s41598-024-57312-7PMCID: PMC10978864",pubmed,38548750,10.1038/s41598-024-57312-7
a neuralbased vocoder implementation for evaluating cochlear implant coding strategies,"272. Hear Res. 2016 Mar;333:136-149. doi: 10.1016/j.heares.2016.01.005. Epub 2016 Jan 14.A neural-based vocoder implementation for evaluating cochlear implant coding strategies.El Boghdady N(1), Kegel A(2), Lai WK(2), Dillier N(2).Author information:(1)Institute for Neuroinformatics (INI), Universität Zürich (UZH)/ ETH Zürich (ETHZ), Zürich, Switzerland. Electronic address: n.el.boghdady@umcg.nl.(2)Laboratory of Experimental Audiology, ENT Department, Universitätsspital Zürich (USZ), Zürich, Switzerland.Most simulations of cochlear implant (CI) coding strategies rely on standard vocoders that are based on purely signal processing techniques. However, these models neither account for various biophysical phenomena, such as neural stochasticity and refractoriness, nor for effects of electrical stimulation, such as spectral smearing as a function of stimulus intensity. In this paper, a neural model that accounts for stochastic firing, parasitic spread of excitation across neuron populations, and neuronal refractoriness, was developed and augmented as a preprocessing stage for a standard 22-channel noise-band vocoder. This model was used to subjectively and objectively assess consonant discrimination in commercial and experimental coding strategies. Stimuli consisting of consonant-vowel (CV) and vowel-consonant-vowel (VCV) tokens were processed by either the Advanced Combination Encoder (ACE) or the Excitability Controlled Coding (ECC) strategies, and later resynthesized to audio using the aforementioned vocoder model. Baseline performance was measured using unprocessed versions of the speech tokens. Behavioural responses were collected from seven normal hearing (NH) volunteers, while EEG data were recorded from five NH participants. Psychophysical results indicate that while there may be a difference in consonant perception between the two tested coding strategies, mismatch negativity (MMN) waveforms do not show any marked trends in CV or VCV contrast discrimination.Copyright © 2016 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2016.01.005",pubmed,26775182,10.1016/j.heares.2016.01.005
case report cochlear implantation in mondini dysplasia with congenital footplate defect  implications for meningitis risks during implantation,"Following the reports of a cluster of meningitis cases in recently implanted patients the FDA issued cautionary advice relating to the risk of meningitis after cochlear implantation (US Food and Drug Administration, 2002). Similar advice and a national reporting call has been issued by the Department of Health in the UK (Medical Devices Agency, 2002) and universal prophylactic pneumococcal vaccination started. We present a case of bilateral Mondini-type dysplasia associated with a defective stapes footplate and highlight the need for surgical vigilance to reduce the risks of meningitis from undiagnosed anatomical defects. 2003 © Whurr Publishers Ltd.",scopus,2-s2.0-1342311672,10.1002/cii.81
encapsulated cell device approach for combined electrical stimulation and neurotrophic treatment of the deaf cochlea,"511. Hear Res. 2017 Jul;350:110-121. doi: 10.1016/j.heares.2017.04.013. Epub 2017 Apr 25.Encapsulated cell device approach for combined electrical stimulation and neurotrophic treatment of the deaf cochlea.Konerding WS(1), Janssen H(2), Hubka P(3), Tornøe J(4), Mistrik P(5), Wahlberg L(6), Lenarz T(7), Kral A(8), Scheper V(9).Author information:(1)Institute of AudioNeuroTechnology, Department of Experimental Otology, Hannover Medical School, Hannover, Germany; Department of Otolaryngology, Hannover Medical School, Hannover, Germany. Electronic address: Konerding.wiebke@mh-hannover.de.(2)Institute of AudioNeuroTechnology, Department of Experimental Otology, Hannover Medical School, Hannover, Germany; Department of Otolaryngology, Hannover Medical School, Hannover, Germany; Institute of Zoology, University of Veterinary Medicine Hannover, Foundation, Hannover, Germany. Electronic address: Janssen.Heike@mh-hannover.de.(3)Institute of AudioNeuroTechnology, Department of Experimental Otology, Hannover Medical School, Hannover, Germany. Electronic address: Hubka.Peter@mh-hannover.de.(4)NsGene A/S, Ballerup, Denmark. Electronic address: jt@nsgene.com.(5)MED-EL, Innsbruck, Austria. Electronic address: Pavel.Mistrik@medel.com.(6)NsGene A/S, Ballerup, Denmark. Electronic address: luw@nsgene.com.(7)Department of Otolaryngology, Hannover Medical School, Hannover, Germany; Cluster of Excellence Hearing4all, German Research Foundation, Hannover, Germany. Electronic address: Lenarz.Thomas@mh-hannover.de.(8)Institute of AudioNeuroTechnology, Department of Experimental Otology, Hannover Medical School, Hannover, Germany; Cluster of Excellence Hearing4all, German Research Foundation, Hannover, Germany. Electronic address: Kral.Andrej@mh-hannover.de.(9)Department of Otolaryngology, Hannover Medical School, Hannover, Germany; Cluster of Excellence Hearing4all, German Research Foundation, Hannover, Germany. Electronic address: Scheper.Verena@mh-hannover.de.Profound hearing impairment can be overcome by electrical stimulation (ES) of spiral ganglion neurons (SGNs) via a cochlear implant (CI). Thus, SGN survival is critical for CI efficacy. Application of glial cell line-derived neurotrophic factor (GDNF) has been shown to reduce SGN degeneration following deafness. We tested a novel method for local, continuous GDNF-delivery in combination with ES via a CI. The encapsulated cell (EC) device contained a human ARPE-19 cell-line, genetically engineered for secretion of GDNF. In vitro, GDNF delivery was stable during ES delivered via a CI. In the chronic in vivo part, cats were systemically deafened and unilaterally implanted into the scala tympani with a CI and an EC device, which they wore for six months. The implantation of control devices (same cell-line not producing GDNF) had no negative effect on SGN survival. GDNF application without ES led to an unexpected reduction in SGN survival, however, the combination of GDNF with initial, short-term ES resulted in a significant protection of SGNs. A tight fibrous tissue formation in the scala tympani of the GDNF-only group is thought to be responsible for the increased SGN degeneration, due to mechanisms related to an aggravated foreign body response. Furthermore, the fibrotic encapsulation of the EC device led to cell death or cessation of GDNF release within the EC device during the six months in vivo. In both in vitro and in vivo, fibrosis was reduced by CI stimulation, enabling the neuroprotective effect of the combined treatment. Thus, fibrous tissue growth limits treatment possibilities with an EC device. For a stable and successful long-term neurotrophic treatment of the SGN via EC devices in human CI users, it would be necessary to make changes in the treatment approach (provision of anti-inflammatories), the EC device surface (reduced cell adhesion) and the ES (initiation prior to fibrosis formation).Copyright © 2017 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2017.04.013",pubmed,28463804,10.1016/j.heares.2017.04.013
overloaded adenoassociated virus as a novel gene therapeutic tool for otoferlinrelated deafness,"731. Front Mol Neurosci. 2021 Jan 7;13:600051. doi: 10.3389/fnmol.2020.600051. eCollection 2020.Overloaded Adeno-Associated Virus as a Novel Gene Therapeutic Tool for Otoferlin-Related Deafness.Rankovic V(1)(2), Vogl C(1)(3)(4), Dörje NM(1)(3), Bahader I(4)(5), Duque-Afonso CJ(1)(6)(7), Thirumalai A(1), Weber T(1), Kusch K(1)(2), Strenzke N(4)(5), Moser T(1)(4)(6)(7)(8)(9).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(2)Restorative Cochlear Genomics Group, Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(3)Presynaptogenesis and Intracellular Transport in Hair Cells Group, Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(4)Collaborative Research Center 889, University of Göttingen, Göttingen, Germany.(5)Auditory Systems Physiology Group, Institute for Auditory Neuroscience and Department of Otolaryngology, University Medical Center Göttingen, Göttingen, Germany.(6)Auditory Neuroscience Group, Max Planck Institute of Experimental Medicine, Göttingen, Germany.(7)Multiscale Bioimaging Cluster of Excellence (MBExC), University of Göttingen, Göttingen, Germany.(8)Synaptic Nanophysiology Group, Max Planck Institute of Biophysical Chemistry, Göttingen, Germany.(9)Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.Hearing impairment is the most common sensory disorder in humans. So far, rehabilitation of profoundly deaf subjects relies on direct stimulation of the auditory nerve through cochlear implants. However, in some forms of genetic hearing impairment, the organ of Corti is structurally intact and therapeutic replacement of the mutated gene could potentially restore near natural hearing. In the case of defects of the otoferlin gene (OTOF), such gene therapy is hindered by the size of the coding sequence (~6 kb) exceeding the cargo capacity (<5 kb) of the preferred viral vector, adeno-associated virus (AAV). Recently, a dual-AAV approach was used to partially restore hearing in deaf otoferlin knock-out (Otof-KO) mice. Here, we employed in vitro and in vivo approaches to assess the gene-therapeutic potential of naturally-occurring and newly-developed synthetic AAVs overloaded with the full-length Otof coding sequence. Upon early postnatal injection into the cochlea of Otof-KO mice, overloaded AAVs drove specific expression of otoferlin in ~30% of all IHCs, as demonstrated by immunofluorescence labeling and polymerase chain reaction. Recordings of auditory brainstem responses and a behavioral assay demonstrated partial restoration of hearing. Together, our results suggest that viral gene therapy of DFNB9-using a single overloaded AAV vector-is indeed feasible, reducing the complexity of gene transfer compared to dual-AAV approaches.Copyright © 2021 Rankovic, Vogl, Dörje, Bahader, Duque-Afonso, Thirumalai, Weber, Kusch, Strenzke and Moser.DOI: 10.3389/fnmol.2020.600051PMCID: PMC7817888",pubmed,33488357,10.3389/fnmol.2020.600051
neural correlates of statistical learning in developmental dyslexia an electroencephalography study,"The human brain extracts statistical regularities from the surrounding environment in a process called statistical learning. Behavioural evidence suggests that developmental dyslexia affects statistical learning. However, surprisingly few studies have assessed how developmental dyslexia affects the neural processing underlying this type of learning. We used electroencephalography to explore the neural correlates of an important aspect of statistical learning – sensitivity to transitional probabilities – in individuals with developmental dyslexia. Adults diagnosed with developmental dyslexia (n = 17) and controls (n = 19) were exposed to a continuous stream of sound triplets. Every so often, a triplet ending had a low transitional probability given the triplet's first two sounds (“statistical deviants”). Furthermore, every so often a triplet ending was presented from a deviant location (“acoustic deviants”). We examined mismatch negativity elicited by statistical deviants (sMMN), and MMN elicited by location deviants (i.e., acoustic changes). Acoustic deviants elicited a MMN which was larger in the control group than in the developmental dyslexia group. Statistical deviants elicited a small, yet significant, sMMN in the control group, but not in the developmental dyslexia group. However, the difference between the groups was not significant. Our findings indicate that the neural mechanisms underlying pre-attentive acoustic change detection and implicit statistical auditory learning are both affected in developmental dyslexia. © 2023 The Authors",scopus,2-s2.0-85161654769,10.1016/j.biopsycho.2023.108592
objective eyegaze behaviour during facetoface communication with proficient alaryngeal speakers a preliminary study,"581. Int J Lang Commun Disord. 2011 Sep-Oct;46(5):535-49. doi: 10.1111/j.1460-6984.2011.00005.x. Epub 2011 Mar 7.Objective eye-gaze behaviour during face-to-face communication with proficient alaryngeal speakers: a preliminary study.Evitts P(1), Gallop R.Author information:(1)Department of Audiology, Speech-Language Pathology & Deaf Studies, Towson University, MD, USA. pevitts@towson.eduBACKGROUND: There is a large body of research demonstrating the impact of visual information on speaker intelligibility in both normal and disordered speaker populations. However, there is minimal information on which specific visual features listeners find salient during conversational discourse.AIMS: To investigate listeners' eye-gaze behaviour during face-to-face conversation with normal, laryngeal and proficient alaryngeal speakers.METHODS & PROCEDURES: Sixty participants individually participated in a 10-min conversation with one of four speakers (typical laryngeal, tracheoesophageal, oesophageal, electrolaryngeal; 15 participants randomly assigned to one mode of speech). All speakers were > 85% intelligible and were judged to be 'proficient' by two certified speech-language pathologists. Participants were fitted with a head-mounted eye-gaze tracking device (Mobile Eye, ASL) that calculated the region of interest and mean duration of eye-gaze. Self-reported gaze behaviour was also obtained following the conversation using a 10 cm visual analogue scale.OUTCOMES & RESULTS: While listening, participants viewed the lower facial region of the oesophageal speaker more than the normal or tracheoesophageal speaker. Results of non-hierarchical cluster analyses showed that while listening, the pattern of eye-gaze was predominantly directed at the lower face of the oesophageal and electrolaryngeal speaker and more evenly dispersed among the background, lower face, and eyes of the normal and tracheoesophageal speakers. Finally, results show a low correlation between self-reported eye-gaze behaviour and objective regions of interest data.CONCLUSIONS & IMPLICATIONS: Overall, results suggest similar eye-gaze behaviour when healthy controls converse with normal and tracheoesophageal speakers and that participants had significantly different eye-gaze patterns when conversing with an oesophageal speaker. Results are discussed in terms of existing eye-gaze data and its potential implications on auditory-visual speech perception.© 2011 Royal College of Speech & Language Therapists.DOI: 10.1111/j.1460-6984.2011.00005.x",pubmed,21899671,10.1111/j.1460-6984.2011.00005.x
genotypephenotype analysis of 18q121q122 copy number variation in autism,"Autism Spectrum Disorders (ASD) are complex neurodevelopmental conditions characterized by delays in social interactions and communication as well as displays of restrictive/repetitive interests. DNA copy number variants have been identified as a genomic susceptibility factor in ASDs and imply significant genetic heterogeneity. We report a 7-year-old female with ADOS-G and ADI-R confirmed autistic disorder harbouring a de novo 4Mb duplication (18q12.1). Our subject displays severely deficient expressive language, stereotypic and repetitive behaviours, mild intellectual disability (ID), focal epilepsy, short stature and absence of significant dysmorphic features. Search of the PubMed literature and DECIPHER database identified 4 additional cases involving 18q12.1 associated with autism and/or ID that overlap our case: one duplication, two deletions and one balanced translocation. Notably, autism and ID are seen with genomic gain or loss at 18q12.1, plus epilepsy and short stature in duplication cases, and hypotonia and tall stature in deletion cases. No consistent dysmorphic features were noted amongst the reviewed cases. We review prospective ASD/ID candidate genes integral to 18q12.1, including those coding for the desmocollin/desmoglein cluster, ring finger proteins 125 and 138, trafficking protein particle complex 8 and dystrobrevin-alpha. The collective clinical and molecular features common to microduplication 18q12.1 suggest that dosage-sensitive, position or contiguous gene effects may be associated in the etiopathogenesis of this autism-ID-epilepsy syndrome. © 2013 Elsevier Masson SAS.",scopus,2-s2.0-84881546916,10.1016/j.ejmg.2013.05.006
soundscape awareness intervention reduced neuropsychiatric symptoms in nursing home residents with dementia a clusterrandomized trial with mosart,"856. J Am Med Dir Assoc. 2023 Feb;24(2):192-198.e5. doi: 10.1016/j.jamda.2022.11.010. Epub 2022 Dec 14.Soundscape Awareness Intervention Reduced Neuropsychiatric Symptoms in Nursing Home Residents With Dementia: A Cluster-Randomized Trial With MoSART.Kosters J(1), Janus SIM(1), van den Bosch KA(2), Andringa TC(3), Hoop EO(4), de Boer MR(1), Elburg RAJ(5), Warmelink S(5), Zuidema SU(1), Luijendijk HJ(6).Author information:(1)Department of General Practice and Elderly Care Medicine, University of Groningen, University Medical Center Groningen, Groningen, the Netherlands.(2)Department of Youth Studies, University of Groningen, Groningen, the Netherlands.(3)Department of General Practice and Elderly Care Medicine, University of Groningen, University Medical Center Groningen, Groningen, the Netherlands; SoundAppraisal Ltd., Groningen, the Netherlands.(4)Department of Medical Oncology, Erasmus MC Cancer Institute, Rotterdam, the Netherlands.(5)SoundAppraisal Ltd., Groningen, the Netherlands.(6)Department of General Practice and Elderly Care Medicine, University of Groningen, University Medical Center Groningen, Groningen, the Netherlands. Electronic address: h.j.luijendijk@umcg.nl.OBJECTIVES: Auditory environments as perceived by an individual, also called soundscapes, are often suboptimal for nursing home residents. Poor soundscapes have been associated with neuropsychiatric symptoms (NPS). We evaluated the effect of the Mobile Soundscape Appraisal and Recording Technology sound awareness intervention (MoSART+) on NPS in nursing home residents with dementia.DESIGN: A 15-month, stepped-wedge, cluster-randomized trial. Every 3 months, a nursing home switched from care as usual to the use of the intervention.INTERVENTION: The 3-month MoSART+ intervention involved ambassador training, staff performing sound measurements with the MoSART application, meetings, and implementation of microinterventions. The goal was to raise awareness about soundscapes and their influence on residents.SETTING AND PARTICIPANTS: We included 110 residents with dementia in 5 Dutch nursing homes. Exclusion criteria were palliative sedation and deafness.METHODS: The primary outcome was NPS severity measured with the Neuropsychiatric Inventory-Nursing Home version (NPI-NH) by the resident's primary nurse. Secondary outcomes were quality of life (QUALIDEM), psychotropic drug use (ATC), staff workload (workload questionnaire), and staff job satisfaction (Maastricht Questionnaire of Job Satisfaction).RESULTS: The mean age of the residents (n = 97) at enrollment was 86.5 ± 6.7 years, and 76 were female (76.8%). The mean NPI-NH score was 17.5 ± 17.3. One nursing home did not implement the intervention because of staff shortages. Intention-to-treat analysis showed a clinically relevant reduction in NPS between the study groups (-8.0, 95% CI -11.7, -2.6). There was no clear effect on quality of life [odds ratio (OR) 2.8, 95% CI -0.7, 6.3], psychotropic drug use (1.2, 95% CI 0.9, 1.7), staff workload (-0.3, 95% CI -0.3, 0.8), or staff job satisfaction (-0.2, 95% CI -1.2, 0.7).CONCLUSIONS AND IMPLICATIONS: MoSART+ empowered staff to adapt the local soundscape, and the intervention effectively reduced staff-reported levels of NPS in nursing home residents with dementia. Nursing homes should consider implementing interventions to improve the soundscape.Copyright © 2022 The Authors. Published by Elsevier Inc. All rights reserved.DOI: 10.1016/j.jamda.2022.11.010",pubmed,36528077,10.1016/j.jamda.2022.11.010
multichannel optogenetic stimulation of the auditory pathway using microfabricated led cochlear implants in rodents,"823. Sci Transl Med. 2020 Jul 22;12(553):eabb8086. doi: 10.1126/scitranslmed.abb8086.Multichannel optogenetic stimulation of the auditory pathway using microfabricated LED cochlear implants in rodents.Keppeler D(1)(2), Schwaerzle M(3)(4), Harczos T(1)(5), Jablonski L(1)(5), Dieter A(1)(2), Wolf B(1)(5), Ayub S(3)(4), Vogl C(1)(6), Wrobel C(1)(7), Hoch G(1)(5), Abdellatif K(1)(5), Jeschke M(1)(5), Rankovic V(1)(5), Paul O(3)(4), Ruther P(8)(4), Moser T(9)(2)(5)(6)(10)(11).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany.(2)Göttingen Graduate Center for Neurosciences and Molecular Biosciences, University of Göttingen, 37075 Göttingen, Germany.(3)University of Freiburg, Department of Microsystems Engineering (IMTEK), 79110 Freiburg, Germany.(4)Cluster of Excellence BrainLinks-BrainTools, University of Freiburg, 79110 Freiburg, Germany.(5)Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Kellnerweg 4, 37077 Göttingen, Germany.(6)Collaborative Research Center 889, University of Göttingen, 37075 Göttingen, Germany.(7)Department of Otorhinolaryngology, Head and Neck Surgery, University Medical Center Göttingen, 37099 Göttingen, Germany.(8)University of Freiburg, Department of Microsystems Engineering (IMTEK), 79110 Freiburg, Germany. ruther@imtek.de tmoser@gwdg.de.(9)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany. ruther@imtek.de tmoser@gwdg.de.(10)Multiscale Bioimaging Cluster of Excellence, University Medical Center Göttingen, 37075 Göttingen, Germany.(11)MPI for Biophysical Chemistry, 37077 Göttingen, Germany.When hearing fails, electrical cochlear implants (eCIs) provide the brain with auditory information. One important bottleneck of CIs is the poor spectral selectivity that results from the wide current spread from each of the electrode contacts. Optical CIs (oCIs) promise to make better use of the tonotopic order of spiral ganglion neurons (SGNs) inside the cochlea by spatially confined stimulation. Here, we established multichannel oCIs based on light-emitting diode (LED) arrays and used them for optical stimulation of channelrhodopsin (ChR)-expressing SGNs in rodents. Power-efficient blue LED chips were integrated onto microfabricated 15-μm-thin polyimide-based carriers comprising interconnecting lines to address individual LEDs by a stationary or mobile driver circuitry. We extensively characterized the optoelectronic, thermal, and mechanical properties of the oCIs and demonstrated stability over weeks in vitro. We then implanted the oCIs into ChR-expressing rats and gerbils, and characterized multichannel optogenetic SGN stimulation by electrophysiological and behavioral experiments. Improved spectral selectivity was directly demonstrated by recordings from the auditory midbrain. Long-term experiments in deafened ChR-expressing rats and in nontreated control animals demonstrated specificity of optogenetic stimulation. Behavioral studies on animals carrying a wireless oCI sound processor revealed auditory percepts. This study demonstrates hearing restoration with improved spectral selectivity by an LED-based multichannel oCI system.Copyright © 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works.DOI: 10.1126/scitranslmed.abb8086PMCID: PMC7611895",pubmed,32718992,10.1126/scitranslmed.abb8086
enhanced speech perception capabilities in a blind listener are associated with activation of fusiform gyrus and primary visual cortex,"Blind individuals may learn to understand ultra-fast synthetic speech at a rate of up to about 25 syllables per second (syl)/s, an accomplishment by far exceeding the maximum performance level of normal-sighted listeners (8-10 syl/s). The present study indicates that this exceptional skill engages distinct regions of the central-visual system. Hemodynamic brain activation during listening to moderately- (8 syl/s) and ultra-fast speech (16 syl/s) was measured in a blind individual and six normal-sighted controls. Moderately-fast speech activated posterior and anterior 'language zones' in all subjects. Regarding ultra-fast tokens, the controls showed exclusive activation of supratemporal regions whereas the blind participant exhibited enhanced left inferior frontal and temporoparietal responses as well as significant hemodynamic activation of left fusiform gyrus (FG) and right primary visual cortex. Since left FG is known to be involved in phonological processing, this structure, presumably, provides the functional link between the central-auditory and -visual systems.",cinahl,13554794,10.1080/13554790802709054
report of a case with ferredoxin reductase fdxr gene variants in a chinese boy exhibiting hearing loss visual impairment and motor retardation,"554. Int J Dev Neurosci. 2021 Jun;81(4):364-369. doi: 10.1002/jdn.10104. Epub 2021 Mar 26.Report of a case with ferredoxin reductase (FDXR) gene variants in a Chinese boy exhibiting hearing loss, visual impairment, and motor retardation.Yang C(1), Zhang Y(1), Li J(2), Song Z(1), Yi Z(1), Li F(1), Xue J(1), Zhang W(3)(4), Wang C(4).Author information:(1)Department of Pediatric, the Affiliated Hospital of Qingdao University, Shandong, China.(2)Department of Neurology, Beijing Children's Hospital, Capital Medical University, National Center for Children's Health, Beijing, China.(3)Department of Molecular and Human Genetics, Baylor College of Medicine, Houston, TX, USA.(4)AmCare Genomics Lab, GuangZhou, China.Ferredoxin reductase (FDXR), located in 17q25.1, encodes for a mitochondrial NADPH: adrenodoxin oxidoreductase or ferredoxin reductase, the sole human ferredoxin reductase involved in the biosynthesis of iron-sulfur (Fe-S) clusters and heme formation. Iron-sulfur (Fe-S) clusters are involved in enzymatic catalysis, gene expression, and DNA replication and repair. Variants in FDXR lead to sensorial neuropathies, damage optic, and auditory neurons. Here, we report a Chinese boy with hearing loss, visual impairment, and motor retardation, with two novel compound heterozygous variants in FDXR (NM_004110), namely, c.250C > T (p.P84S) and c.634G > C (p.D212H), identified by whole-exome sequencing. Compared with the reported cases, except hearing loss and visual impairment, the clinical manifestations of this boy were more serious, who also had motor retardation and died in infancy after infection. The present study expands our knowledge of FDXR variants and related phenotypes, and provides new information on the genetic defects associated with this disease for clinical diagnosis.© 2021 International Society for Developmental Neuroscience.DOI: 10.1002/jdn.10104",pubmed,33742450,10.1002/jdn.10104
pitchresponsive cortical regions in congenital amusia,"815. J Neurosci. 2016 Mar 9;36(10):2986-94. doi: 10.1523/JNEUROSCI.2705-15.2016.Pitch-Responsive Cortical Regions in Congenital Amusia.Norman-Haignere SV(1), Albouy P(2), Caclin A(3), McDermott JH(4), Kanwisher NG(5), Tillmann B(3).Author information:(1)Department of Brain and Cognitive Sciences and snormanhaignere@gmail.com.(2)Lyon Neuroscience Research Centre, Centre National de la Recherche Scientifique UMR5292, INSERM U1028, Lyon 1 University, F-69000 Lyon, France, and Montreal Neurological Institute, McGill University, Quebec H3A 0G4, Canada.(3)Lyon Neuroscience Research Centre, Centre National de la Recherche Scientifique UMR5292, INSERM U1028, Lyon 1 University, F-69000 Lyon, France, and.(4)Department of Brain and Cognitive Sciences and.(5)Department of Brain and Cognitive Sciences and McGovern Institute for Brain Science, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139.Congenital amusia is a lifelong deficit in music perception thought to reflect an underlying impairment in the perception and memory of pitch. The neural basis of amusic impairments is actively debated. Some prior studies have suggested that amusia stems from impaired connectivity between auditory and frontal cortex. However, it remains possible that impairments in pitch coding within auditory cortex also contribute to the disorder, in part because prior studies have not measured responses from the cortical regions most implicated in pitch perception in normal individuals. We addressed this question by measuring fMRI responses in 11 subjects with amusia and 11 age- and education-matched controls to a stimulus contrast that reliably identifies pitch-responsive regions in normal individuals: harmonic tones versus frequency-matched noise. Our findings demonstrate that amusic individuals with a substantial pitch perception deficit exhibit clusters of pitch-responsive voxels that are comparable in extent, selectivity, and anatomical location to those of control participants. We discuss possible explanations for why amusics might be impaired at perceiving pitch relations despite exhibiting normal fMRI responses to pitch in their auditory cortex: (1) individual neurons within the pitch-responsive region might exhibit abnormal tuning or temporal coding not detectable with fMRI, (2) anatomical tracts that link pitch-responsive regions to other brain areas (e.g., frontal cortex) might be altered, and (3) cortical regions outside of pitch-responsive cortex might be abnormal. The ability to identify pitch-responsive regions in individual amusic subjects will make it possible to ask more precise questions about their role in amusia in future work.Copyright © 2016 the authors 0270-6474/16/362986-09$15.00/0.DOI: 10.1523/JNEUROSCI.2705-15.2016PMCID: PMC6601753",pubmed,26961952,10.1523/JNEUROSCI.2705-15.2016
signtific translator the automated sign language recognition,"People with hearing loss or other limitations frequently communicate using sign language. Sign languages have been used by humans since the beginning of time. The learning curve for sign language is rather low. Signs and symbols were used by early people to communicate. Our goal is to develop a sign language translator with voice help that can translate sign language into text. By using our detection model, anybody who is deaf and has no knowledge of sign language may easily communicate with them. In this essay, we've covered key approaches, strategies, and associated details concerning our model's operation.",ieee,,10.1109/ICCST55948.2022.10040280
remixing preferences for western instrumental classical music of bilateral cochlear implant users,"760. Trends Hear. 2024 Jan-Dec;28:23312165241245219. doi: 10.1177/23312165241245219.Remixing Preferences for Western Instrumental Classical Music of Bilateral Cochlear Implant Users.Althoff J(1)(2), Gajecki T(1)(2), Nogueira W(1)(2).Author information:(1)Department of Otolaryngology, Hannover Medical School, Hannover, Germany.(2)Cluster of Excellence Hearing4all, Hanover, Germany.For people with profound hearing loss, a cochlear implant (CI) is able to provide access to sounds that support speech perception. With current technology, most CI users obtain very good speech understanding in quiet listening environments. However, many CI users still struggle when listening to music. Efforts have been made to preprocess music for CI users and improve their music enjoyment. This work investigates potential modifications of instrumental music to make it more accessible for CI users. For this purpose, we used two datasets with varying complexity and containing individual tracks of instrumental music. The first dataset contained trios and it was newly created and synthesized for this study. The second dataset contained orchestral music with a large number of instruments. Bilateral CI users and normal hearing listeners were asked to remix the multitracks grouped into melody, bass, accompaniment, and percussion. Remixes could be performed in the amplitude, spatial, and spectral domains. Results showed that CI users preferred tracks being panned toward the right side, especially the percussion component. When CI users were grouped into frequent or occasional music listeners, significant differences in remixing preferences in all domains were observed.DOI: 10.1177/23312165241245219",pubmed,38613359,10.1177/23312165241245219
automatic determination of optimal linear drilling trajectories for cochlear access accounting for drillpositioning error,"579. Int J Med Robot. 2010 Sep;6(3):281-90. doi: 10.1002/rcs.330.Automatic determination of optimal linear drilling trajectories for cochlear access accounting for drill-positioning error.Noble JH(1), Majdani O, Labadie RF, Dawant B, Fitzpatrick JM.Author information:(1)Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN 37235, USA. jack.h.noble@vanderbilt.eduBACKGROUND: Cochlear implantation is a surgical procedure in which an electrode array is permanently implanted into the cochlea to stimulate the auditory nerve and allow deaf people to hear. Percutaneous cochlear access, a new minimally invasive implantation approach, requires drilling a single linear channel from the skull surface to the cochlea. The focus of this paper addresses a major challenge with this approach, which is the ability to determine, in a pre-operative CT, a safe and effective drilling trajectory.METHODS: A measure of the safety and effectiveness of a given trajectory relative to sensitive structures is derived using a Monte Carlo approach. The drilling trajectory that maximizes this measure is found using an optimization algorithm.RESULTS: In tests on 13 ears, the technique was shown to find approximately twice as many acceptable trajectories as those found manually by an experienced surgeon.CONCLUSIONS: Using this method, safe trajectories can be automatically determined quickly and consistently.Copyright 2010 John Wiley & Sons, Ltd.DOI: 10.1002/rcs.330PMCID: PMC2933923",pubmed,20812268,10.1002/rcs.330
transplantation of mouse embryonic stem cells into the cochlea of an auditoryneuropathy animal model effects of timing after injury,"567. J Assoc Res Otolaryngol. 2008 Jun;9(2):225-40. doi: 10.1007/s10162-008-0119-x. Epub 2008 May 1.Transplantation of mouse embryonic stem cells into the cochlea of an auditory-neuropathy animal model: effects of timing after injury.Lang H(1), Schulte BA, Goddard JC, Hedrick M, Schulte JB, Wei L, Schmiedt RA.Author information:(1)Department of Pathology and Laboratory Medicine, Medical University of South Carolina, 165 Ashley Avenue, P.O. Box 250908, Charleston, SC 29425, USA. langh@musc.eduApplication of ouabain to the round window membrane of the gerbil selectively induces the death of most spiral ganglion neurons and thus provides an excellent model for investigating the survival and differentiation of embryonic stem cells (ESCs) introduced into the inner ear. In this study, mouse ESCs were pretreated with a neural-induction protocol and transplanted into Rosenthal's canal (RC), perilymph, or endolymph of Mongolian gerbils either 1-3 days (early post-injury transplant group) or 7 days or longer (late post-injury transplant group) after ouabain injury. Overall, ESC survival in RC and perilymphatic spaces was significantly greater in the early post-injury microenvironment as compared to the later post-injury condition. Viable clusters of ESCs within RC and perilymphatic spaces appeared to be associated with neovascularization in the early post-injury group. A small number of ESCs transplanted within RC stained for mature neuronal or glial cell markers. ESCs introduced into perilymph survived in several locations, but most differentiated into glia-like cells. ESCs transplanted into endolymph survived poorly if at all. These experiments demonstrate that there is an optimal time window for engraftment and survival of ESCs that occurs in the early post-injury period.DOI: 10.1007/s10162-008-0119-xPMCID: PMC2504604",pubmed,18449604,10.1007/s10162-008-0119-x
subjective hearing ability physical and mental comorbidities in individuals with bothersome tinnitus in a swedish population sample,"854. Prog Brain Res. 2021;260:51-78. doi: 10.1016/bs.pbr.2020.10.001. Epub 2021 Feb 4.Subjective hearing ability, physical and mental comorbidities in individuals with bothersome tinnitus in a Swedish population sample.Basso L(1), Boecking B(1), Brueggemann P(1), Pedersen NL(2), Canlon B(3), Cederroth CR(4), Mazurek B(5).Author information:(1)Tinnitus Center, Charité-Universitätsmedizin Berlin, Berlin, Germany.(2)Department of Medical Epidemiology and Biostatistics, Karolinska Institutet, Stockholm, Sweden.(3)Laboratory of Experimental Audiology, Department of Physiology and Pharmacology, Karolinska Institutet, Stockholm, Sweden.(4)Laboratory of Experimental Audiology, Department of Physiology and Pharmacology, Karolinska Institutet, Stockholm, Sweden; National Institute for Health Research (NIHR) Nottingham Biomedical Research Centre, Nottingham University Hospitals NHS Trust, Nottingham, United Kingdom; Hearing Sciences, Division of Clinical Neuroscience, School of Medicine, University of Nottingham, Nottingham, United Kingdom.(5)Tinnitus Center, Charité-Universitätsmedizin Berlin, Berlin, Germany. Electronic address: birgit.mazurek@charite.de.OBJECTIVE: This study investigates associations of subjective hearing ability, physical comorbidities, and mental comorbidities with bothersome (vs. non-bothersome) tinnitus and mediating effects between these influences.METHODS: The Swedish LifeGene cohort was used to sample cross-sectional survey data (collected 2009-2016) of 7615 participants with tinnitus, 697 (9.2%) of whom rated their tinnitus as bothersome. Associations between bothersome tinnitus and subjective hearing ability, physical and mental comorbidities were investigated by separate age- and gender-adjusted multiple logistic regression models. Interrelationships between these associations were investigated by logistic mediation models.RESULTS: Compared to non-bothersome tinnitus, bothersome tinnitus was associated with higher age, reduced subjective hearing ability, hearing-related difficulties in social situations, cardiovascular disease, chronic shoulder pain, thyroid disease, Ménière's disease, depression, anxiety syndrome, and social anxiety. Subjective hearing impairment or hearing-related difficulties mediated 13-36% of the effects of mental comorbidities on bothersome tinnitus. Depression or anxiety syndrome mediated 5-8% of most relationships between physical comorbidities and bothersome tinnitus. Depression, anxiety syndrome, or social anxiety mediated 2-4% of the effects of subjective hearing impairment or hearing-related difficulties on bothersome tinnitus.CONCLUSION: Psychological factors, subjective hearing impairment, and hearing-related difficulties in social situations play key roles in predicting bothersome (vs. non-bothersome) tinnitus in a large population sample. Psychological factors contribute to explaining the impact of physical comorbidities and hearing-related effects on bothersome tinnitus. This highlights their transdiagnostic importance for aggravating varied physical symptom clusters. Interventions to improve or prevent high tinnitus burden should be interdisciplinary/multimodal and target auditory, physical, and psychological factors.© 2021 Elsevier B.V. All rights reserved.DOI: 10.1016/bs.pbr.2020.10.001",pubmed,33637232,10.1016/bs.pbr.2020.10.001
acoustic shock,"585. J Laryngol Otol. 2007 Apr;121(4):301-5. doi: 10.1017/S0022215107006111. Epub 2007 Feb 19.Acoustic shock.McFerran DJ(1), Baguley DM.Author information:(1)Department of Otolaryngology and Head and Neck Surgery, Essex County Hospital, Colchester, UK. donald.mcferran@essexrivers.nhs.ukAcoustic shock is a recently recognised clinical entity: following an abrupt, intense and unanticipated acoustic stimulus, usually delivered by a telephone handset or headset, some individuals report a symptom cluster that includes otalgia, altered hearing, aural fullness, imbalance, tinnitus, dislike or even fear of loud noises, and anxiety and/or depression. Symptoms start shortly after the triggering acoustic incident and can be short-lived or can last for a considerable time. If persistent, the condition can lead to significant disability. Proposed mechanisms include involvement of the tensor tympani muscle, hyperexcitability of central auditory pathways, and a precursive state of raised anxiety or arousal. A formal treatment programme has not yet been proposed, but the potential utility of modern therapeutic techniques for tinnitus and hyperacusis are considered. Given the large number of UK residents working in telephone call centres, this condition is of considerable clinical importance.DOI: 10.1017/S0022215107006111",pubmed,17306048,10.1017/S0022215107006111
recognize vietnamese sign language using deep neural network,"World Health Organization published an article called `Deafness and hearing loss' in March 2020, it said that more than 466 million people in the world lost their hearing ability, and 34 million of them were children. Sign Language has been born and developed for a long time, but its application to communicate has met with many inadequacies and difficulties. Many methods of Computer Vision-based approach gave good results on Sign Language Alphabet Recognition but all of them require the perfect result from background removing step. However, when it comes to real life, removing a complex background is too difficult for any simple background removing algorithms. In this work, our main purpose is to build a model based on deep learning that can recognize Vietnamese Sign Language Alphabet in a complex environment. Results obtained show a robust accuracy of this model in recognizing Vietnamese Sign Language Alphabet.",ieee,,10.1109/NICS51282.2020.9335904
mapping quantitative trait loci for hearing loss in black swiss mice,"271. Hear Res. 2006 Feb;212(1-2):128-39. doi: 10.1016/j.heares.2005.11.006. Epub 2006 Jan 19.Mapping quantitative trait loci for hearing loss in Black Swiss mice.Drayton M(1), Noben-Trauth K.Author information:(1)Section on Neurogenetics, Laboratory of Molecular Biology, National Institute on Deafness and Other Communication Disorders, National Institutes of Health, 5 Research Court, Rockville, MD 20850, USA.In common inbred mouse strains, hearing loss is a highly prevalent quantitative trait, which is mainly controlled by the Cdh23(753A) variant and alleles at numerous other strain-specific loci. Here, we investigated the genetic basis of hearing loss in non-inbred strains. Mice of Swiss Webster, CF-1, NIH Swiss, ICR, and Black Swiss strains exhibited hearing profiles characteristic of progressive, sensorineural hearing impairment. In particular, CF-1, Black Swiss, and NIH Swiss mice showed early-onset hearing impairment, ICR and Swiss Webster mice expressed a delayed-onset hearing loss, and NMRI mice had normal hearing. By quantitative trait locus (QTL) mapping, two significant QTLs were identified underlying hearing loss in Black Swiss mice: one QTL mapped to chromosome (chr) 10 (named ahl5, LOD 8.9, peak association 35-42 cM) and a second QTL localized to chr 18 (ahl6, LOD 3.8, 38-44 cM). Ahl5 and ahl6 account for 61% and 32% of the variation in the backcross, respectively. Cadherin 23 (Cdh23) and protocadherin 15 (Pcdh15), mapping within the 95% confidence interval of ahl5, bear nucleotide polymorphisms in coding exons, but these appear to be unrelated to the hearing phenotype. Haplotype analyses across the Cdh23 locus demonstrated the phylogenetic relationship between Black Swiss and common inbred strains.DOI: 10.1016/j.heares.2005.11.006",pubmed,16426780,10.1016/j.heares.2005.11.006
toward the pathogenicity of the slc26a4 pc565y variant using a genetically driven mouse model,"Recessive variants of the SLC26A4 gene are globally a common cause of hearing impair-ment. In the past, cell lines and transgenic mice were widely used to investigate the pathogenicity associated with SLC26A4 variants. However, discrepancies in pathogenicity between humans and cell lines or transgenic mice were documented for some SLC26A4 variants. For instance, the p.C565Y variant, which was reported to be pathogenic in humans, did not exhibit functional pathogenic con-sequences in cell lines. To address the pathogenicity of p.C565Y, we used a genotype-based approach in which we generated knock-in mice that were heterozygous (Slc26a4+/C565Y), homozygous (Slc26a4C565Y/C565Y), and compound heterozygous (Slc26a4919-2A>G/C565Y) for this variant. Subsequent phenotypic characterization revealed that mice with these genotypes demonstrated normal auditory and vestibular functions, and normal inner-ear morphology and pendrin expression. These findings indicate that the p.C565Y variant is nonpathogenic for mice, and that a single p.C565Y allele is sufficient to maintain normal inner-ear physiology in mice. Our results highlight the differences in pathogenicity associated with certain SLC26A4 variants between transgenic mice and hu-mans, which should be considered when interpreting the results of animal studies for SLC26A4-related deafness. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",scopus,2-s2.0-85102113936,10.3390/ijms22062789
a case of h syndrome with a novel mutation in slc29a3,"H syndrome is a hereditary disease transmitted in an autosomal recessive pattern.It consists of two major clusters of cutaneous and systemic manifestations [1,2].The symptoms include short stature, hyperpigmentation of skin, hypertrichosis, deafness, hypogonadism, anomalies of heart, hyperglycemia(insulin dependent diabetes mellitus) and hepatosplenomegaly [1]. H syndrome is the result of mutations in theSLC29A3 gene that can be homozygous or compound heterozygous. The SLC29A3 gene is located on chromosome 10q22 and encodes a transporter protein that causes passive transportation of nucleosides which is critical for several functions such as DNA and RNA repair [3–5]. Several different mutations withinSLC29A3 can cause H syndrome. In this report, we present a 4.5 year old girl diagnosed with H syndrome based on genetic investigation. © 2019",scopus,2-s2.0-85068449069,10.1016/j.mgene.2019.100599
a missense mutation in the previously undescribed gene tmhs underlies deafness in hurryscurry hscy mice,"624. Proc Natl Acad Sci U S A. 2005 May 31;102(22):7894-9. doi: 10.1073/pnas.0500760102. Epub 2005 May 19.A missense mutation in the previously undescribed gene Tmhs underlies deafness in hurry-scurry (hscy) mice.Longo-Guess CM(1), Gagnon LH, Cook SA, Wu J, Zheng QY, Johnson KR.Author information:(1)The Jackson Laboratory, Bar Harbor, ME 04609, USA.Mouse deafness mutations provide valuable models of human hearing disorders and entry points into molecular pathways important to the hearing process. A newly discovered mouse mutation named hurry-scurry (hscy) causes deafness and vestibular dysfunction. Scanning electron microscopy of cochleae from 8-day-old mutants revealed disorganized hair bundles, and by 50 days of age, many hair cells are missing. To positionally clone hscy, 1,160 F(2) mice were produced from an intercross of (C57BL/6-hscy x CAST/EiJ) F(1) hybrids, and the mutation was localized to a 182-kb region of chromosome 17. A missense mutation causing a critical cysteine to phenylalanine codon change was discovered in a previously undescribed gene within this candidate interval. The gene is predicted to encode an integral membrane protein with four transmembrane helices. A synthetic peptide designed from the predicted protein was used to produce specific polyclonal antibodies, and strong immunoreactivity was observed on hair bundles of both inner and outer hair cells in cochleae of newborn +/+ controls and +/hscy heterozygotes but was absent in hscy/hscy mutants. Accordingly, the gene was given the name ""tetraspan membrane protein of hair cell stereocilia,"" symbol Tmhs. Two related proteins (>60% amino acid identity) are encoded by genes on mouse chromosomes 5 and 6 and, together with the Tmhs-encoded protein (TMHS), comprise a distinct tetraspan subfamily. Our localization of TMHS to the apical membrane of inner ear hair cells during the period of stereocilia formation suggests a function in hair bundle morphogenesis.DOI: 10.1073/pnas.0500760102PMCID: PMC1142366",pubmed,15905332,10.1073/pnas.0500760102
prevalence of congenital hereditary sensorineural deafness in australian cattle dogs and associations with coat characteristics and sex,"381. BMC Vet Res. 2012 Oct 29;8:202. doi: 10.1186/1746-6148-8-202.Prevalence of congenital hereditary sensorineural deafness in Australian Cattle Dogs and associations with coat characteristics and sex.Sommerlad SF(1), Morton JM, Haile-Mariam M, Johnstone I, Seddon JM, O'Leary CA.Author information:(1)School of Veterinary Science, The University of Queensland, Gatton, Queensland, 4343, Australia. s.sommerlad@uq.edu.auBACKGROUND: Congenital hereditary sensorineural deafness (CHSD) occurs in many dog breeds, including Australian Cattle Dogs. In some breeds, CHSD is associated with a lack of cochlear melanocytes in the stria vascularis, certain coat characteristics, and potentially, abnormalities in neuroepithelial pigment production. This study investigates phenotypic markers for CHSD in 899 Australian Cattle Dogs.RESULTS: Auditory function was tested in 899 Australian Cattle Dogs in family groups using brainstem auditory evoked response testing. Coat colour and patterns, facial and body markings, gender and parental hearing status were recorded.Deafness prevalence among all 899 dogs was 10.8% with 7.5% unilaterally deaf, and 3.3% bilaterally deaf, and amongst pups from completely tested litters (n = 696) was 11.1%, with 7.5% unilaterally deaf, and 3.6% bilaterally deaf.Univariable and multivariable analyses revealed a negative association between deafness and bilateral facial masks (odds ratio 0.2; P ≤ 0.001). Using multivariable logistic animal modelling, the risk of deafness was lower in dogs with pigmented body spots (odds ratio 0.4; P = 0.050).No significant associations were found between deafness and coat colour.Within unilaterally deaf dogs with unilateral facial masks, no association was observed between the side of deafness and side of mask. The side of unilateral deafness was not significantly clustered amongst unilaterally deaf dogs from the same litter. Females were at increased risk of deafness (odds ratio from a logistic animal model 1.9; P = 0.034) after adjusting for any confounding by mask type and pigmented body spots.CONCLUSIONS: Australian Cattle Dogs suffer from CHSD, and this disease is more common in dogs with mask-free faces, and in those without pigmented body patches. In unilaterally deaf dogs with unilateral masks, the lack of observed association between side of deafness and side of mask suggests that if CHSD is due to defects in molecular pigment pathways, the molecular control of embryonic melanoblast migration from ectoderm to skin differs from control of migration from ectoderm to cochlea. In Australian Cattle Dogs, CHSD may be more common in females.DOI: 10.1186/1746-6148-8-202PMCID: PMC3489614",pubmed,23107143,10.1186/1746-6148-8-202
a novel mutation in prps1 causes xlinked charcotmarietooth disease5,"753. Neuropathology. 2019 Oct;39(5):342-347. doi: 10.1111/neup.12589. Epub 2019 Aug 21.A novel mutation in PRPS1 causes X-linked Charcot-Marie-Tooth disease-5.Meng L(1), Wang K(2), Lv H(1), Wang Z(1), Zhang W(1), Yuan Y(1).Author information:(1)Department of Neurology, Peking University First Hospital, Beijing, China.(2)Peking University-Tsinghua University-National Institute of Biological Sciences Joint Graduate Program, School of Life Sciences, Tsinghua University, Beijing, China.X-linked Charcot-Marie-Tooth disease-5 (CMTX5) is a rare hereditary disorder caused by mutations in the gene for phosphoribosyl pyrophosphate synthetase-1 (PRPS1). We investigated a boy with a novel PRPS1 mutation (c.334G>C, p.V112L) via genetic, neuropathological and enzymatic tests. The proband was a 13-year-old boy with congenital non-syndromic sensorineural deafness. At 3 year old, he developed progressive distal weakness of all limbs with muscle atrophy of both hands and shanks. Nerve conduction study revealed the loss of sensory nerve action potentials, and slowing down of motor nerve conduction velocities with a decrease of amplitudes of compound motor action potentials. Visual evoked potentials and brainstem auditory evoked potentials were not bilaterally evocable. Sural biopsy proved the loss of myelinated nerve fibers, with axonal degeneration, regenerating clusters and onion bulbs. Enzymatically, PRPS1 activity was close to zero in the proband and mildly reduced in his mother, compared with controls. To our knowledge, this is the first report of CMTX5 in a Chinese population. The genetic finding has expanded the genotypic spectrum of PRPS1 mutations.© 2019 Japanese Society of Neuropathology.DOI: 10.1111/neup.12589",pubmed,31434166,10.1111/neup.12589
decoding taskrelated functional brain imaging data to identify developmental disorders the case of congenital amusia,"810. Front Neurosci. 2019 Oct 30;13:1165. doi: 10.3389/fnins.2019.01165. eCollection 2019.Decoding Task-Related Functional Brain Imaging Data to Identify Developmental Disorders: The Case of Congenital Amusia.Albouy P(1)(2), Caclin A(3)(4), Norman-Haignere SV(5)(6), Lévêque Y(4)(7), Peretz I(2), Tillmann B(4)(7), Zatorre RJ(1)(2).Author information:(1)Cognitive Neuroscience Unit, Montreal Neurological Institute, McGill University, Montreal, QC, Canada.(2)International Laboratory for Brain, Music and Sound Research, Montreal, QC, Canada.(3)INSERM, U1028, CNRS, UMR 5292, Lyon Neuroscience Research Center, Brain Dynamics and Cognition Team, Lyon, France.(4)University Lyon 1, Lyon, France.(5)Zuckerman Institute of Mind, Brain and Behavior, Columbia University, New York, NY, United States.(6)CNRS, Laboratoire des Sytèmes Perceptifs, Département d'Études Cognitives, ENS, PSL University, Paris, France.(7)CNRS, UMR 5292, INSERM, U1028, Lyon Neuroscience Research Center, Auditory Cognition and Psychoacoustics Team, Lyon, France.Machine learning classification techniques are frequently applied to structural and resting-state fMRI data to identify brain-based biomarkers for developmental disorders. However, task-related fMRI has rarely been used as a diagnostic tool. Here, we used structural MRI, resting-state connectivity and task-based fMRI data to detect congenital amusia, a pitch-specific developmental disorder. All approaches discriminated amusics from controls in meaningful brain networks at similar levels of accuracy. Interestingly, the classifier outcome was specific to deficit-related neural circuits, as the group classification failed for fMRI data acquired during a verbal task for which amusics were unimpaired. Most importantly, classifier outputs of task-related fMRI data predicted individual behavioral performance on an independent pitch-based task, while this relationship was not observed for structural or resting-state data. These results suggest that task-related imaging data can potentially be used as a powerful diagnostic tool to identify developmental disorders as they allow for the prediction of symptom severity.Copyright © 2019 Albouy, Caclin, Norman-Haignere, Lévêque, Peretz, Tillmann and Zatorre.DOI: 10.3389/fnins.2019.01165PMCID: PMC6831619",pubmed,31736698,10.3389/fnins.2019.01165
nterminus of grxcr2 interacts with clic5 and is essential for auditory perception,"747. Front Cell Dev Biol. 2021 May 5;9:671364. doi: 10.3389/fcell.2021.671364. eCollection 2021.N-Terminus of GRXCR2 Interacts With CLIC5 and Is Essential for Auditory Perception.Li J(1), Liu C(1), Zhao B(1).Author information:(1)Department of Otolaryngology-Head and Neck Surgery, Indiana University School of Medicine, Indianapolis, IN, United States.Stereocilia of cochlear hair cells are specialized mechanosensing organelles that convert sound-induced vibration to electrical signals. Glutaredoxin domain-containing cysteine-rich protein 2 (GRXCR2) is localized at the base of stereocilia and is necessary for stereocilia morphogenesis and auditory perception. However, the detailed functions of GRXCR2 in hair cells are still largely unknown. Here, we report that GRXCR2 interacts with chloride intracellular channel protein 5 (CLIC5) which is also localized at the base of stereocilia and required for normal hearing in human and mouse. Immunolocalization analyses suggest that GRXCR2 is not required for the localization of CLIC5 to the stereociliary base during development, or vice versa. Using clustered regularly interspaced short palindromic repeats (CRISPR)/Cas9 system, we deleted 60 amino acids near the N-terminus of GRXCR2 essential for its interaction with CLIC5. Interestingly, mice harboring this in-frame deletion in Grxcr2 exhibit moderate hearing loss at lower frequencies and severe hearing loss at higher frequencies although the morphogenesis of stereocilia is minimally affected. Thus, our findings reveal that the interaction between GRXCR2 and CLIC5 is crucial for normal hearing.Copyright © 2021 Li, Liu and Zhao.DOI: 10.3389/fcell.2021.671364PMCID: PMC8131845",pubmed,34026762,10.3389/fcell.2021.671364
gabaa receptors in the mongolian gerbil a pet study using 18fflumazenil to determine receptor binding in young and old animals,"685. Mol Imaging Biol. 2020 Apr;22(2):335-347. doi: 10.1007/s11307-019-01371-0.GABA(A) Receptors in the Mongolian Gerbil: a PET Study Using [(18)F]Flumazenil to Determine Receptor Binding in Young and Old Animals.Kessler M(1)(2), Mamach M(3)(4)(5), Beutelmann R(6), Lukacevic M(3), Eilert S(3), Bascuñana P(3), Fasel A(7), Bengel FM(3), Bankstahl JP(3), Ross TL(3), Klump GM(4)(6), Berding G(3)(4).Author information:(1)Department of Nuclear Medicine, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany. Kessler.Mariella@mh-hannover.de.(2)Cluster of Excellence Hearing4all, Hannover Medical School and University of Oldenburg, Hannover, Germany. Kessler.Mariella@mh-hannover.de.(3)Department of Nuclear Medicine, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany.(4)Cluster of Excellence Hearing4all, Hannover Medical School and University of Oldenburg, Hannover, Germany.(5)Department of Medical Physics and Radiation Protection, Hannover Medical School, Carl-Neuberg-Str. 1, 30625, Hannover, Germany.(6)Division of Animal Physiology and Behaviour, Department for Neuroscience, School of Medicine and Health Sciences, University of Oldenburg, Carl von Ossietzky Str. 9-11, 26129, Oldenburg, Germany.(7)ABX Advanced Biochemical Compounds GmbH, Heinrich-Glaeser-Strasse 10-14, 01454, Radeberg, Germany.PURPOSE: Plastic changes in the central auditory system involving the GABAergic system accompany age-related hearing loss. Such processes can be investigated with positron emission tomography (PET) imaging using [18F]flumazenil ([18F]FMZ). Here, [18F]FMZ PET-based modeling approaches allow a simple and reliable quantification of GABAA receptor binding capacity revealing regional differences and age-related changes.PROCEDURES: Sixty-minute list-mode PET acquisitions were performed in 9 young (range 5-6 months) and 11 old (range 39-42 months) gerbils, starting simultaneously with the injection of [18F]FMZ via femoral vein. Non-displaceable binding potentials (BPnd) with pons as reference region were calculated for auditory cortex (AC), inferior colliculus (IC), medial geniculate body (MGB), somatosensory cortex (SC), and cerebellum (CB) using (i) a two-tissue compartment model (2TCM), (ii) the Logan plot with image-derived blood-input (Logan (BI)), (iii) a simplified reference tissue model (SRTM), and (iv) the Logan reference model (Logan (RT)). Statistical parametric mapping analysis (SPM) comparing young and old gerbils was performed using 3D parametric images for BPnd based on SRTM. Results were verified with in vitro autoradiography from five additional young gerbils. Model assessment included the Akaike information criterion (AIC). Hearing was evaluated using auditory brainstem responses.RESULTS: BPnd differed significantly between models (p < 0.0005), showing the smallest mean difference between 2TCM as reference and SRTM as simplified procedure. SRTM revealed the lowest AIC values. Both volume of distribution (r2 = 0.8793, p = 0.018) and BPnd (r2 = 0.8216, p = 0.034) correlated with in vitro autoradiography data. A significant age-related decrease of receptor binding was observed in auditory (AC, IC, MGB) and other brain regions (SC and CB) (p < 0.0001, unpaired t test) being confirmed by SPM using pons as reference (p < 0.0001, uncorrected).CONCLUSION: Imaging of GABAA receptor binding capacity in gerbils using [18F]FMZ PET revealed SRTM as a simple and robust quantification method of GABAA receptors. Comparison of BPnd in young and old gerbils demonstrated an age-related decrease of GABAA receptor binding.DOI: 10.1007/s11307-019-01371-0",pubmed,31102039,10.1007/s11307-019-01371-0
enlargement of ribbons in zebrafish hair cells increases calcium currents but disrupts afferent spontaneous activity and timing of stimulus onset,"849. J Neurosci. 2017 Jun 28;37(26):6299-6313. doi: 10.1523/JNEUROSCI.2878-16.2017. Epub 2017 May 25.Enlargement of Ribbons in Zebrafish Hair Cells Increases Calcium Currents But Disrupts Afferent Spontaneous Activity and Timing of Stimulus Onset.Sheets L(1)(2), He XJ(3), Olt J(4), Schreck M(3), Petralia RS(5), Wang YX(5), Zhang Q(3), Beirl A(3), Nicolson T(6), Marcotti W(4), Trapani JG(7), Kindt KS(8).Author information:(1)Department of Otolaryngology, Harvard Medical School, Boston, Massachusetts 02115.(2)Eaton-Peabody Laboratory, Massachusetts Eye and Ear, Boston, Massachusetts 02114.(3)Section on Sensory Cell Development and Function, National Institute on Deafness and Other Communication Disorders/National Institutes of Health, Bethesda, Maryland 20892.(4)Department of Biomedical Science, University of Sheffield, Sheffield S10 2TN, United Kingdom, and.(5)Advanced Imaging Core, National Institute on Deafness and Other Communication Disorders/National Institutes of Health, Bethesda, Maryland 20892.(6)Oregon Hearing Research Center and Vollum Institute, Oregon Health and Science University, Portland, Oregon 97239.(7)Department of Biology and Neuroscience Program, Amherst College, Amherst, Massachusetts 01002.(8)Section on Sensory Cell Development and Function, National Institute on Deafness and Other Communication Disorders/National Institutes of Health, Bethesda, Maryland 20892, katie.kindt@nih.gov.In sensory hair cells of auditory and vestibular organs, the ribbon synapse is required for the precise encoding of a wide range of complex stimuli. Hair cells have a unique presynaptic structure, the synaptic ribbon, which organizes both synaptic vesicles and calcium channels at the active zone. Previous work has shown that hair-cell ribbon size is correlated with differences in postsynaptic activity. However, additional variability in postsynapse size presents a challenge to determining the specific role of ribbon size in sensory encoding. To selectively assess the impact of ribbon size on synapse function, we examined hair cells in transgenic zebrafish that have enlarged ribbons, without postsynaptic alterations. Morphologically, we found that enlarged ribbons had more associated vesicles and reduced presynaptic calcium-channel clustering. Functionally, hair cells with enlarged ribbons had larger global and ribbon-localized calcium currents. Afferent neuron recordings revealed that hair cells with enlarged ribbons resulted in reduced spontaneous spike rates. Additionally, despite larger presynaptic calcium signals, we observed fewer evoked spikes with longer latencies from stimulus onset. Together, our work indicates that hair-cell ribbon size influences the spontaneous spiking and the precise encoding of stimulus onset in afferent neurons.SIGNIFICANCE STATEMENT Numerous studies support that hair-cell ribbon size corresponds with functional sensitivity differences in afferent neurons and, in the case of inner hair cells of the cochlea, vulnerability to damage from noise trauma. Yet it is unclear whether ribbon size directly influences sensory encoding. Our study reveals that ribbon enlargement results in increased ribbon-localized calcium signals, yet reduces afferent spontaneous activity and disrupts the timing of stimulus onset, a distinct aspect of auditory and vestibular encoding. These observations suggest that varying ribbon size alone can influence sensory encoding, and give further insight into how hair cells transduce signals that cover a wide dynamic range of stimuli.Copyright © 2017 Sheets et al.DOI: 10.1523/JNEUROSCI.2878-16.2017PMCID: PMC5490065",pubmed,28546313,10.1523/JNEUROSCI.2878-16.2017
editorial otitis media,[No abstract available],scopus,2-s2.0-85143825689,10.3389/fcimb.2022.1063153
deep learning reinvents the hearing aid,"My mother began to lose her hearing while I was away at college. I would return home to share what I'd learned, and she would lean in to hear. Soon it became difficult for her to hold a conversation if more than one person spoke at a time. Now, even with a hearing aid, she struggles to distinguish the sounds of each voice. When my family visits for dinner, she still pleads with us to speak in turn. My mother's hardship reflects a classic problem for hearing aid manufacturers. The human auditory system can naturally pick out a voice in a crowded room, but creating a hearing aid that mimics that ability has stumped signal processing specialists, artificial intelligence experts, and audiologists for decades. British cognitive scientist Colin Cherry first dubbed this the ""cocktail party problem"" in 1953.",ieee,1939-9340,10.1109/MSPEC.2017.7864754
induction of differentiation of human embryonic stem cells into functional haircelllike cells in the absence of stromal cells,"715. Int J Biochem Cell Biol. 2016 Dec;81(Pt A):208-222. doi: 10.1016/j.biocel.2015.11.012. Epub 2015 Nov 23.Induction of differentiation of human embryonic stem cells into functional hair-cell-like cells in the absence of stromal cells.Ding J(1), Tang Z(2), Chen J(2), Shi H(3), Chen J(2), Wang C(2), Zhang C(2), Li L(2), Chen P(4), Wang J(5).Author information:(1)Institute of Cell and Development, College of Life Sciences, Zhejiang University, Hangzhou 310058, PR China; College of Life Sciences, Guizhou University, Guiyang 550025, PR China.(2)Institute of Cell and Development, College of Life Sciences, Zhejiang University, Hangzhou 310058, PR China.(3)Department of Otorhinolaryngology, The Sixth People's Hospital, Shanghai Jiaotong University, Shanghai 200233, PR China.(4)Institute of Cell and Development, College of Life Sciences, Zhejiang University, Hangzhou 310058, PR China; Departments of Cell Biology and Otolaryngology, Emory University School of Medicine, Atlanta, GA 30322, USA.(5)Institute of Cell and Development, College of Life Sciences, Zhejiang University, Hangzhou 310058, PR China. Electronic address: wjfu@zju.edu.cn.Sensorineural hearing loss and vestibular dysfunction have become the most common forms of sensory defects. Stem cell-based therapeutic strategies for curing hearing loss are being developed. Several attempts to develop hair cells by using chicken utricle stromal cells as feeder cells have resulted in phenotypic conversion of stem cells into inner ear hair-cell-like cells. Here, we induced the differentiation of human embryonic stem cells (hESCs) into otic epithelial progenitors (OEPs), and further induced the differentiation of OEPs into hair-cell-like cells using different substrates. Our results showed that OEPs cultured on the chicken utricle stromal cells with the induction medium could differentiate into hair-cell-like cells with stereociliary bundles. Co-culture with stromal cells, however, may be problematic for subsequent examination of the induced hair-cell-like cells. In order to avoid the interference from stromal cells, we cultured OEPs on laminin with different induction media and examined the effects of the induction medium on the differentiation potentials of OEPs into hair-cell-like cells. The results revealed that the culture of OEPs on laminin with the conditioned medium from chicken utricle stromal cells supplemented with EGF and all-trans retinoic acid (RA) could promote the organization of cells into epithelial clusters displaying hair-cell-like cells with stereociliary bundles. These cells also displayed the expected electrophysiological properties.Copyright © 2015 Elsevier Ltd. All rights reserved.DOI: 10.1016/j.biocel.2015.11.012",pubmed,26615761,10.1016/j.biocel.2015.11.012
the surgical management of facial nerve tumor,"Purpose To improve the diagnosis and management of facial nerve tumor. Methods Eighteen patients with facial nerve tumor were retrospectively analyzed during 1993-2004. All patients were undergone either CT scan or MRI, or both. The period of follow-up varies from 6 months to 5 years (average 40 months). Facial nerve function was evaluated with House-Brackman (H-B) grading system. Results Totally 14 cases complained of facial paralysis, 9 presented with hearing loss, 2 seeked for medical care due to mass in external auditory canal, and 2 had mass in parotid gland. Image studies revealed mass located along the facial nerve course from the parotid gland to internal auditory meatus. Intraoperative findings showed mass involved in internal auditory meatus to parotid gland. Furthermore, mass involved in multiple segment of facial nerve in 12 cases. In 2 cases tumor was pearl-ball cluster along the facial nerve from the internal auditory canal to parotid gland. Postoperative pathological diagnosis demonstrated 14 Schwannoma, 3 neurinoma and 1 case hemangioma. Seven patients were undergone facial nerve graft, 2 recovered to H-B III and the other 5 reached H-B IV, postoperatively. Facial nerve anastomosis was performed on 1 patient, and postoperative outcome was H-B IV. Two patients underwent tumor resection, while the continuity of facial nerve was preserved; their facial nerve function was H-B II and H-B III, respectively. One patient operated with facial nerve decompression recovered postoperative facial nerve function to H-B I. No facial nerve reconstruction was done on other 7 patients. Conclusions The presentation of facial nerve tumor was various, multiple origins of facial nerve tumor were noted. Postoperative facial nerve function was associated with the surgical procedure and preoperative facial nerve function.",scopus,2-s2.0-34248557644,
speechdriven facial animations improve speechinnoise comprehension of humans,"671. Front Neurosci. 2022 Jan 5;15:781196. doi: 10.3389/fnins.2021.781196. eCollection 2021.Speech-Driven Facial Animations Improve Speech-in-Noise Comprehension of Humans.Varano E(1), Vougioukas K(2), Ma P(2), Petridis S(2), Pantic M(2), Reichenbach T(3).Author information:(1)Department of Bioengineering and Centre for Neurotechnology, Imperial College London, London, United Kingdom.(2)Department of Computing, Imperial College London, London, United Kingdom.(3)Department of Artificial Intelligence in Biomedical Engineering, Friedrich-Alexander-University Erlangen-Nuremberg, Erlangen, Germany.Understanding speech becomes a demanding task when the environment is noisy. Comprehension of speech in noise can be substantially improved by looking at the speaker's face, and this audiovisual benefit is even more pronounced in people with hearing impairment. Recent advances in AI have allowed to synthesize photorealistic talking faces from a speech recording and a still image of a person's face in an end-to-end manner. However, it has remained unknown whether such facial animations improve speech-in-noise comprehension. Here we consider facial animations produced by a recently introduced generative adversarial network (GAN), and show that humans cannot distinguish between the synthesized and the natural videos. Importantly, we then show that the end-to-end synthesized videos significantly aid humans in understanding speech in noise, although the natural facial motions yield a yet higher audiovisual benefit. We further find that an audiovisual speech recognizer (AVSR) benefits from the synthesized facial animations as well. Our results suggest that synthesizing facial motions from speech can be used to aid speech comprehension in difficult listening environments.Copyright © 2022 Varano, Vougioukas, Ma, Petridis, Pantic and Reichenbach.DOI: 10.3389/fnins.2021.781196PMCID: PMC8766421",pubmed,35069100,10.3389/fnins.2021.781196
auditory cortex hypoperfusion a metabolic hallmark in beta thalassemia,"119. Orphanet J Rare Dis. 2021 Aug 5;16(1):349. doi: 10.1186/s13023-021-01969-0.Auditory cortex hypoperfusion: a metabolic hallmark in Beta Thalassemia.Manara R(#)(1), Ponticorvo S(#)(2), Perrotta S(3), Barillari MR(4), Costa G(4), Brotto D(5), Di Concilio R(6), Ciancio A(7), De Michele E(8), Carafa PA(9), Canna A(2), Russo AG(2), Troisi D(2), Caiazza M(10), Ammendola F(10), Roberti D(10), Santoro C(10)(11), Picariello S(10), Valentino MS(10), Inserra E(10), Carfora R(10), Cirillo M(12), Raimo S(13), Santangelo G(13), di Salle F(2), Esposito F(2)(12), Tartaglione I(10).Author information:(1)Neuroradiology, Department of Neuroscience, University of Padova, Padua, Italy.(2)Dipartimento di Medicina e Chirurgia, Scuola Medica Salernitana, Università di Salerno, Fisciano, Italy.(3)Dipartimento della Donna, del Bambino e della Chirurgia Generale e Specialistica, Università degli Studi della Campania ""Luigi Vanvitelli"", Via L. De Crecchio 4, 80138, Naples, Italy. silverio.perrotta@unicampania.it.(4)Università degli Studi della Campania, Naples, Italy.(5)Università di Padova, Padua, Italy.(6)Dipartimento di Pediatria, Ospedale ""Umberto I"", Nocera Inferiore, Italy.(7)Unità Operativa Ematologia - Day Hospital di Talassemia, Ospedale ""Madonna Delle Grazie"", Matera, Italy.(8)Medicina Trasfusionale AUO ""San Giovanni di Dio e Ruggi D'Aragona"", Salerno, Italy.(9)Università di Salerno, Fisciano, Italy.(10)Dipartimento della Donna, del Bambino e della Chirurgia Generale e Specialistica, Università degli Studi della Campania ""Luigi Vanvitelli"", Via L. De Crecchio 4, 80138, Naples, Italy.(11)Clinic of Child and Adolescent Neuropsychiatry, Department of Mental Health, Physical and Preventive Medicine, University of Campania ""Luigi Vanvitelli"", Naples, Italy.(12)Department of Advanced Medical and Surgical Sciences, University of Campania ""Luigi Vanvitelli"", Naples, Italy.(13)Department of Psychology, University of Campania 'Luigi Vanvitelli', Caserta, Italy.(#)Contributed equallyBACKGROUND: Sensorineural hearing loss in beta-thalassemia is common and it is generally associated with iron chelation therapy. However, data are scarce, especially on adult populations, and a possible involvement of the central auditory areas has not been investigated yet. We performed a multicenter cross-sectional audiological and single-center 3Tesla brain perfusion MRI study enrolling 77 transfusion-dependent/non transfusion-dependent adult patients and 56 healthy controls. Pure tone audiometry, demographics, clinical/laboratory and cognitive functioning data were recorded.RESULTS: Half of patients (52%) presented with high-frequency hearing deficit, with overt hypoacusia (Pure Tone Average (PTA) > 25 dB) in 35%, irrespective of iron chelation or clinical phenotype. Bilateral voxel clusters of significant relative hypoperfusion were found in the auditory cortex of beta-thalassemia patients, regardless of clinical phenotype. In controls and transfusion-dependent (but not in non-transfusion-dependent) patients, the relative auditory cortex perfusion values increased linearly with age (p < 0.04). Relative auditory cortex perfusion values showed a significant U-shaped correlation with PTA values among hearing loss patients, and a linear correlation with the full scale intelligence quotient (right side p = 0.01, left side p = 0.02) with its domain related to communication skills (right side p = 0.04, left side p = 0.07) in controls but not in beta-thalassemia patients. Audiometric test results did not correlate to cognitive test scores in any subgroup.CONCLUSIONS: In conclusion, primary auditory cortex perfusion changes are a metabolic hallmark of adult beta-thalassemia, thus suggesting complex remodeling of the hearing function, that occurs regardless of chelation therapy and before clinically manifest hearing loss. The cognitive impact of perfusion changes is intriguing but requires further investigations.© 2021. The Author(s).DOI: 10.1186/s13023-021-01969-0PMCID: PMC8340544",pubmed,34353346,10.1186/s13023-021-01969-0
endolymphatic hydrops in patients with vestibular migraine and concurrent menieres disease,"714. Front Neurol. 2021 Mar 11;12:594481. doi: 10.3389/fneur.2021.594481. eCollection 2021.Endolymphatic Hydrops in Patients With Vestibular Migraine and Concurrent Meniere's Disease.Oh SY(1)(2), Dieterich M(3)(4)(5), Lee BN(1), Boegle R(3)(4), Kang JJ(1), Lee NR(2)(6), Gerb J(3)(4), Hwang SB(2)(7), Kirsch V(3)(4).Author information:(1)Department of Neurology, School of Medicine, Jeonbuk National University, Jeonju, South Korea.(2)Research Institute of Clinical Medicine, Jeonbuk National University Hospital-Biomedical Research Institute, Jeonbuk National University, Jeonju, South Korea.(3)Department of Neurology, University Hospital, Ludwig-Maximilians-Universität, Munich, Germany.(4)German Center for Vertigo and Balance Disorders-IFB, University Hospital, Ludwig-Maximilians-Universität, Munich, Germany.(5)Munich Cluster for Systems Neurology (SyNergy), Munich, Germany.(6)Division of Oncology and Hematology, Department of Internal Medicine, Jeonbuk National University Hospital and School of Medicine, Jeonju, South Korea.(7)Department of Radiology, Jeonbuk National University Hospital and School of Medicine, Jeonju, South Korea.Objective: Intravenous contrast agent enhanced, high-resolution magnetic resonance imaging of the inner ear (iMRI) confirmed that patients with Menière's disease (MD) and vestibular migraine (VM) could present with endolymphatic hydrops (EH). The present study aimed to investigate EH characteristics and their interrelation to neurotologic testing in patients with VM, MD, or VM with concurrent MD (VM-MD). Methods: Sixty-two patients (45 females, aged 23-81 years) with definite or probable VM (n = 25, 19 definite), MD (n = 29, 17 definite), or showing characteristics of both diseases (n = 8) were included in this study. Diagnostic workup included neurotologic assessments including video-oculography (VOG) during caloric stimulation and head-impulse test (HIT), ocular and cervical vestibular evoked myogenic potentials (o/cVEMP), pure tone audiometry (PTA), as well as iMRI. EH's degree was assessed visually and via volumetric quantification using a probabilistic atlas-based segmentation of the bony labyrinth and volumetric local thresholding (VOLT). Results: Although a relevant number of VM patients reported varying auditory symptoms (13 of 25, 52.0%), EH in VM was only observed twice. In contrast, EH in VM-MD was prevalent (2/8, 25%) and in MD frequent [23/29, 79.3%; χ2(2) = 29.1, p < 0.001, φ = 0.7]. Location and laterality of EH and neurophysiological testing classifications were highly associated (Fisher exact test, p < 0.005). In MD, visual semi-quantitative grading and volumetric quantification correlated highly to each other (r S = 0.8, p < 0.005, two-sided) and to side differences in VOG during caloric irrigation (vestibular EH ipsilateral: r S = 0.6, p < 0.05, two-sided). In VM, correlations were less pronounced. VM-MD assumed an intermediate position between VM and MD. Conclusion: Cochlear and vestibular hydrops can occur in MD and VM patients with auditory symptoms; this suggests inner ear damage irrespective of the diagnosis of MD or VM. The EH grades often correlated with auditory symptoms such as hearing impairment and tinnitus. Further research is required to uncover whether migraine is one causative factor of EH or whether EH in VM patients with auditory symptoms suggests an additional pathology due to MD.Copyright © 2021 Oh, Dieterich, Lee, Boegle, Kang, Lee, Gerb, Hwang and Kirsch.DOI: 10.3389/fneur.2021.594481PMCID: PMC7991602",pubmed,33776877,10.3389/fneur.2021.594481
risk for acute confusion in sensoryimpaired rural longtermcare elders,"Acute confusion is a common geriatric syndrome in long-term care (LTC) elders with prevalence rates of 10% to 39%. Sensory impairment, specifically vision and hearing impairment, is even more common in LTC, with prevalence rates of 40% to 90%. The purpose of this study was to investigate the risk relationship between sensory impairment and the development of acute confusion in LTC elders. Each resident (N = 114) underwent sensory screening and then was followed for 28 days to monitor for the onset of acute confusion. Twenty residents (17.5%) developed acute confusion, 60 residents (52.6%) were found to be visually impaired, 49 (44.1%) were hearing impaired, and 28 (24.6%) were found to be dually impaired. Significant relationships between vision impairment, odds ratio (OR) = 3.67, confidence interval (CI) (1.13, 11.92), and dual sensory impairment, OR = 2.88, CI (1.04, 8.26), with the development of acute confusion were identified.",cinahl,10547738,10.1177/1054773803253917
is there an unmet medical need for improved hearing restoration,"32. EMBO Mol Med. 2022 Aug 8;14(8):e15798. doi: 10.15252/emmm.202215798. Epub 2022 Jul 14.Is there an unmet medical need for improved hearing restoration?Wolf BJ(1)(2)(3)(4), Kusch K(1)(5), Hunniford V(1)(6), Vona B(1)(7), Kühler R(8), Keppeler D(1)(3), Strenzke N(1)(8)(9), Moser T(1)(2)(3)(4)(9).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(2)Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(3)Auditory Neuroscience & Synaptic Nanophysiology Group, Max-Planck-Institute for Multidisciplinary Sciences, Göttingen, Germany.(4)Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, Göttingen, Germany.(5)Functional Auditory Genomics Group, Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(6)Sensory and Motor Neuroscience PhD Program, Göttingen Graduate Center for Neurosciences, Biophysics, and Molecular Biosciences, Göttingen, Germany.(7)Institute of Human Genetics, University Medical Center Göttingen, Göttingen, Germany.(8)Department of Otolaryngology, University Medical Center Göttingen, Göttingen, Germany.(9)Collaborative Research Center 889, University of Göttingen, Göttingen, Germany.Hearing impairment, the most prevalent sensory deficit, affects more than 466 million people worldwide (WHO). We presently lack causative treatment for the most common form, sensorineural hearing impairment; hearing aids and cochlear implants (CI) remain the only means of hearing restoration. We engaged with CI users to learn about their expectations and their willingness to collaborate with health care professionals on establishing novel therapies. We summarize upcoming CI innovations, gene therapies, and regenerative approaches and evaluate the chances for clinical translation of these novel strategies. We conclude that there remains an unmet medical need for improving hearing restoration and that we are likely to witness the clinical translation of gene therapy and major CI innovations within this decade.© 2022 The Authors. Published under the terms of the CC BY 4.0 license.DOI: 10.15252/emmm.202215798PMCID: PMC9358394",pubmed,35833443,10.15252/emmm.202215798
simple2in1 a simple method for fusing two sequences from different captioning systems into one sequence for a smallscale thai dataset,"The increasing number of Deaf and Hard of Hearing (DHH) individuals has amplified the need for quality captions, especially in real-time. Previous studies have successfully explored various methods to enhance caption quality, including alignment-based and neural network approaches. While alignment-based methods represent a conventional approach, neural network models are inherently more complex yet offer superior performance. However, these neural models demand large datasets, posing challenges for languages such as Thai with limited datasets available. In this paper, we propose a simple but effective method to improve the quality of Thai captions on a small-scale Thai dataset. Our method utilizes a pre-trained mT5 model to generate a single sequence from two sequences from two different captioning systems. Despite a small dataset and limited input sequences, our method shows potential in improving six evaluation metrics, surpassing all baseline models.",ieee,2831-4565,10.1109/iSAI-NLP60301.2023.10355030
oromandibular limb hypogenesis syndrome overlap of moebius and ankyloglossia superior with severe limb defects,"The oromandibular limb hypogenesis syndromes (OLHS) represent a group of rare conditions characterized by congenital malformations involving the tongue, mandible, and limbs. In this report, we describe a newborn girl with paralysis of abducens and facial nerves, transverse agenesis of the distal segments of the limbs, micrognathia, cleft lip and palate, and ankyloglossia superior. This observation confirms an overlap between Moebius syndrome and ankyloglossia superior syndrome with severe limb defects. The etiology of the OLHS is not clearly understood. The intriguing link between facial and limb anomalies can result from their simultaneous development from the fourth to eighth week of gestation, making both areas susceptible to the same teratogenic stimuli. There is an overlap between OLHS conditions, supporting a clustering, rather than a divided nosology and requiring an appropriate classification of these conditions. Patients with OLHS can be successfully managed using a multidisciplinary approach. © 2020, American Cleft Palate-Craniofacial Association.",scopus,2-s2.0-85090774243,10.1177/1055665620954736
congenital cholesteatoma in identical twins,"Congenital cholesteatoma in identical twins has only been described once in Otolaryngology literature thus far. This report describes a case of monozygotic twins with a history of recurrent acute otitis media and bilateral middle ear effusions without tympanic membrane perforation. Upon myringotomy with pressure equalization tube insertion, both were found to have right-sided cholesteatoma with nearly identical location and pattern of progression. In the context of previous case series demonstrating familial clustering and reports of possible genetic associations of this condition, the authors present an important addition to the current understanding of congenital cholesteatoma disorder. © 2022 Elsevier B.V.",scopus,2-s2.0-85139721952,10.1016/j.ijporl.2022.111330
a spondee recognition test for young hearingimpaired childrendp   aug 1974,"10 spondaic words recorded on Language Master cards were presented monaurally through insert receivers to 58 5-9 yr old hearing-impaired children to evaluate their ability to recognize familiar speech material. Ss were tested individually until their performance stabilized-which required from 4 to 10 sessions. They indicated their responses by pointing to labeled picture cards. Spondee recognition scores were bimodally distributed, with clusters of scores of 0-65% and 66-100%, respectively. In general, pure-tone averages better than 93 db HTL (hearing-threshold level) were associated with spondee scores from 66 to 100%, while pure-tone averages poorer than 103 db HTL corresponded to spondee scores from 0 to 65%. No close relation between pure-tone thresholds and spondee recognition scores was found for average hearing levels between 93 and 103 db. Recognition scores varied as a function of repeated testing in 3 general ways: stable performance, steadily improving performance, or inconsistent performance. (PsycInfo Database Record (c) 2021 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc2&DO=10.1044%2fjshd.3903.304
consecutive treatment with brainderived neurotrophic factor and electrical stimulation has a protective effect on primary auditory neurons,"835. Brain Sci. 2020 Aug 15;10(8):559. doi: 10.3390/brainsci10080559.Consecutive Treatment with Brain-Derived Neurotrophic Factor and Electrical Stimulation Has a Protective Effect on Primary Auditory Neurons.Scheper V(1)(2), Seidel-Effenberg I(1), Lenarz T(1)(2), Stöver T(1)(3), Paasche G(1)(2).Author information:(1)Department of Otorhinolaryngology, Hannover Medical School, Carl-Neuberg-Str. 1, 30625 Hannover, Germany.(2)Hearing4all Cluster of Excellence, Hannover Medical School, Carl-Neuberg-Str. 1, 30625 Hannover, Germany.(3)Department of Otorhinolaryngology, Johann Wolfgang Goethe-University, Theodor-Stern-Kai 7, 60590 Frankfurt am Main, Germany.Degeneration of neurons, such as the inner ear spiral ganglion neurons (SGN), may be decelerated or even stopped by neurotrophic factor treatment, such as brain-derived neurotrophic factor (BDNF), as well as electrical stimulation (ES). In a clinical setting, drug treatment of the SGN could start directly during implantation of a cochlear implant, whereas electrical stimulation begins days to weeks later. The present study was conducted to determine the effects of consecutive BDNF and ES treatments on SGN density and electrical responsiveness. An electrode drug delivery device was implanted in guinea pigs 3 weeks after deafening and five experimental groups were established: two groups received intracochlear infusion of artificial perilymph (AP) or BDNF; two groups were treated with AP respectively BDNF in addition to ES (AP + ES, BDNF + ES); and one group received BDNF from the day of implantation until day 34 followed by ES (BDNF ⇨ ES). Electrically evoked auditory brainstem responses were recorded. After one month of treatment, the tissue was harvested and the SGN density was assessed. The results show that consecutive treatment with BDNF and ES was as successful as the simultaneous combined treatment in terms of enhanced SGN density compared to the untreated contralateral side but not in regard to the numbers of protected cells.DOI: 10.3390/brainsci10080559PMCID: PMC7464901",pubmed,32824176,10.3390/brainsci10080559
ethanol inhibits the sensory responses of cerebellar granule cells in anesthetized cats,"793. Alcohol Clin Exp Res. 2007 Feb;31(2):336-44. doi: 10.1111/j.1530-0277.2006.00309.x.Ethanol inhibits the sensory responses of cerebellar granule cells in anesthetized cats.Huang CM(1), Huang RH.Author information:(1)Division of Molecular Biology and Biochemistry, School of Biological Sciences, University of Missouri, Kansas City 64110-2499, USA. huangc@umkc.eduBACKGROUND: Granule cells occupy a strategic position in the transmission of afferent information to the cerebellar cortex. They are also the most abundant type of neurons in the cerebellum. The functions of the cerebellum are thought to be sensitive to acute alcohol intoxication. The effects of acute alcohol intoxication on the in vivo physiology of cerebellar granule cells are, however, not completely known.METHODS: We studied chloralose-anesthetized cats at ethanol doses relevant to human drinking (0.3-1.2 g/kg). We recorded the electrophysiological responses of granule cell clusters to auditory and visual stimulation, and simultaneously monitored the concentration of ethanol in the cerebrospinal fluid (CSF).RESULTS: At an intravenous ethanol dose of 0.3 g/kg, CSF ethanol concentration peaked in 10 minutes at 17 mM, equivalent to a blood alcohol concentration (BAC) of about 0.08 g/dL. Ethanol quickly and almost completely abolished both auditory and visual responses from granule cells. Complete or near-complete inhibition lasted 15 to 20 minutes; approximately 50% recovery required an additional 15 minutes, and a full recovery yet another 15 minutes. A higher ethanol dose at 1.2 g/kg resulted in a more severe inhibition and required longer time for recovery. The relationship between ethanol dose, CSF ethanol concentration, and granule cell responses was dynamic and nonlinear, critically depending upon the elapsed time.CONCLUSIONS: Cerebellar granule cell sensory responses are highly sensitive to ethanol inhibition. A rapid development of acute tolerance appears to be a major factor contributing to the dynamic and nonlinear relationship among ethanol dosage, CSF ethanol concentration, and granule cell responses. It is likely that a generalized de-afferentation of the cerebellum from its mossy fiber afferents, followed by the subsequent development of acute tolerance may play major roles by which alcohol intoxication affects cerebellar functions.DOI: 10.1111/j.1530-0277.2006.00309.x",pubmed,17250627,10.1111/j.1530-0277.2006.00309.x
mmp9 plasma level as biomarker of cochlear implantation outcome in cohort study of deaf children,"625. Eur Arch Otorhinolaryngol. 2023 Oct;280(10):4361-4369. doi: 10.1007/s00405-023-07924-y. Epub 2023 Apr 1.MMP-9 plasma level as biomarker of cochlear implantation outcome in cohort study of deaf children.Matusiak M(1)(2), Oziębło D(3)(4), Ołdak M(3)(4), Rejmak E(5), Kaczmarek L(5), Dobek D(6), Skarżyński H(7)(3).Author information:(1)Oto-Rhino-Laryngosurgery Clinic, Institute of Physiology and Pathology of Hearing, M Mochnackiego 10, 02-042, Warsaw, Poland. m.matusiak@ifps.org.pl.(2)World Hearing Centre, Mokra 17, 05-830, Nadarzyn, Poland. m.matusiak@ifps.org.pl.(3)World Hearing Centre, Mokra 17, 05-830, Nadarzyn, Poland.(4)Department of Genetics, Institute of Physiology and Pathology of Hearing, M Mochnackiego 10, 02-042, Warsaw, Poland.(5)BRAINCITY, Nencki Institute of Experimental Biology, L Pasteura 3, 02-093, Warsaw, Poland.(6)Transition Technologies Science, Pawia 55, 01-030, Warsaw, Poland.(7)Oto-Rhino-Laryngosurgery Clinic, Institute of Physiology and Pathology of Hearing, M Mochnackiego 10, 02-042, Warsaw, Poland.Comment in    Laryngorhinootologie. 2024 Jan;103(1):8.PURPOSE: If before cochlear implantation it was possible to assay biomarkers of neuroplasticity, we might be able to identify those children with congenital deafness who, later on, were at risk of poor speech and language rehabilitation outcomes.METHODS: A group of 40 children aged up to 2 years with DFNB1-related congenital deafness was observed in this prospective cohort study over three follow-up intervals (0, 8, and 18 months) after cochlear implant (CI) activation. Children were assessed for auditory development using the LittlEARS Questionnaire (LEAQ) score, and at the same time, measurements were made of matrix metalloproteinase-9 (MMP-9) plasma levels.RESULTS: There were significant negative correlations between plasma levels of MMP-9 at 8-month follow-up and LEAQ score at cochlear implantation (p = 0.04) and LEAQ score at 18-month follow-up (p = 0.02) and between MMP-9 plasma levels at 18-month follow-up and LEAQ score at cochlear implantation (p = 0.04). As already reported, we confirmed a significant negative correlation between MMP-9 plasma level at cochlear implantation and LEAQ score at 18-month follow-up (p = 0.005). Based on this latter correlation, two clusters of good and poor CI performers could be isolated.CONCLUSIONS: The study shows that children born deaf who have an MMP-9 plasma level of less than 150 ng/ml at cochlear implantation have a good chance of attaining a high LEAQ score after 18 months of speech and language rehabilitation. This indicates that MMP-9 plasma level at cochlear implantation is a good prognostic marker for CI outcome.© 2023. The Author(s).DOI: 10.1007/s00405-023-07924-yPMCID: PMC10497633",pubmed,37004521,10.1007/s00405-023-07924-y
p50 n100 and p200 auditory sensory gating deficits in schizophrenia patients,"Background: Sensory gating describes neurological processes of filtering out redundant or unnecessary stimuli during information processing, and sensory gating deficits may contribute to the symptoms of schizophrenia. Among the three components of auditory event-related potentials reflecting sensory gating, P50 implies pre-attentional filtering of sensory information and N100/P200 reflects attention triggering and allocation processes. Although diminished P50 gating has been extensively documented in patients with schizophrenia, previous studies on N100 were inconclusive, and P200 has been rarely examined. This study aimed to investigate whether patients with schizophrenia have P50, N100, and P200 gating deficits compared with control subjects. Methods: Control subjects and clinically stable schizophrenia patients were recruited. The mid-latency auditory evoked responses, comprising P50, N100, and P200, were measured using the auditory-paired click paradigm without manipulation of attention. Sensory gating parameters included S1 amplitude, S2 amplitude, amplitude difference (S1-S2), and gating ratio (S2/S1). We also evaluated schizophrenia patients with PANSS to be correlated with sensory gating indices. Results: One hundred four patients and 102 control subjects were examined. Compared to the control group, schizophrenia patients had significant sensory gating deficits in P50, N100, and P200, reflected by larger gating ratios and smaller amplitude differences. Further analysis revealed that the S2 amplitude of P50 was larger, while the S1 amplitude of N100/P200 was smaller, in schizophrenia patients than in the controls. We found no correlations between sensory gating indices and schizophrenia positive or negative symptom clusters. However, we found a negative correlation between the P200 S2 amplitude and Bell’s emotional discomfort factor/Wallwork’s depressed factor. Conclusion: Till date, this study has the largest sample size to analyze P50, N100, and P200 collectively by adopting the passive auditory paired-click paradigm without distractors. With covariates controlled for possible confounds, such as age, education, smoking amount and retained pairs, we found that schizophrenia patients had significant sensory gating deficits in P50-N100-P200. The schizophrenia patients had demonstrated a unique pattern of sensory gating deficits, including repetition suppression deficits in P50 and stimulus registration deficits in N100/200. These results suggest that sensory gating is a pervasive cognitive abnormality in schizophrenia patients that is not limited to the pre-attentive phase of information processing. Since P200 exhibited a large effect size and did not require additional time during recruitment, future studies of P50-N100-P200 collectively are highly recommended. © Copyright © 2020 Shen, Chou, Lai, Hsieh, Liu, Liu and Hwu.",scopus,2-s2.0-85091371162,10.3389/fpsyt.2020.00868
the future of hearing aid technology can technology turn us into superheroes references,"Background: Hearing aid technology has proven to be successful in the rehabilitation of hearing loss, but its performance is still limited in difficult everyday conditions characterized by noise and reverberation. Objective: Introduction to the current state of hearing aid technology and presentation of the current state of research and future developments. Methods: The current literature was analyzed and several specific new developments are presented. Results: Both objective and subjective data from empirical studies show the limitations of the current technology. Examples of current research show the potential of machine learning-based algorithms and multimodal signal processing for improving speech processing and perception, of using virtual reality for improving hearing device fitting and of mobile health technology for improving hearing health services. Conclusion: Hearing device technology will remain a key factor in the rehabilitation of hearing impairments. New technology, such as machine learning and multimodal signal processing, virtual reality and mobile health technology, will improve speech enhancement, individual fitting and communication training, thus providing better support for all hearing-impaired patients, including older patients with disabilities or declining cognitive skills. (PsycInfo Database Record (c) 2023 APA, all rights reserved)
Abstract (German)
Hintergrund: Die Horgeratetechnologie hat sich bei der Rehabilitation von Horverlusten als erfolgreich erwiesen, aber ihre Leistung ist unter schwierigen Alltagsbedingungen, die durch Larm und Nachhall gekennzeichnet sind, immer noch begrenzt. Zielsetzung: Einfuhrung in den aktuellen Stand der Horgeratetechnologie und Darstellung des aktuellen Forschungsstandes und der zukunftigen Entwicklung. Methoden: Die aktuelle Literatur wird analysiert und mehrere spezifische Neuentwicklungen werden vorgestellt. Ergebnisse: Sowohl objektive als auch subjektive Daten aus empirischen Studien zeigen die Grenzen der derzeitigen Technologie auf. Beispiele aus der aktuellen Forschung belegen das Potenzial von auf maschinellem Lernen basierenden Algorithmen und multimodaler Signalverarbeitung zur Verbesserung der Sprachverarbeitung und -wahrnehmung, der Nutzung von virtueller Realitat zur Verbesserung der Horgerateanpassung und von mobiler Gesundheitstechnologie zur Verbesserung der Horgesundheitsdienste. Schlussfolgerung: Die Horgeratetechnologie wird ein Schlusselfaktor bei der Rehabilitation von Horschaden bleiben. Neue Technologien wie maschinelles Lernen und multimodale Signalverarbeitung, virtuelle Realitat und mobile Gesundheitstechnologien werden die Sprachverbesserung, die individuelle Anpassung und das Kommunikationstraining verbessern. (PsycInfo Database Record (c) 2023 APA, all rights reserved)",psychinfo,,https://ovidsp.ovid.com/ovidweb.cgi?T=JS&CSC=Y&NEWS=N&PAGE=fulltext&D=psyc22&DO=10.1007%2fs00391-023-02179-y
multiscale photonic imaging of the native and implanted cochlea,"196. Proc Natl Acad Sci U S A. 2021 May 4;118(18):e2014472118. doi: 10.1073/pnas.2014472118.Multiscale photonic imaging of the native and implanted cochlea.Keppeler D(1)(2), Kampshoff CA(1)(2)(3), Thirumalai A(1)(2), Duque-Afonso CJ(1)(2), Schaeper JJ(4), Quilitz T(1)(2), Töpperwien M(4), Vogl C(1)(2), Hessler R(5), Meyer A(2)(3), Salditt T(4)(6), Moser T(7)(2)(3)(6)(8).Author information:(1)Institute for Auditory Neuroscience, University Medical Center Göttingen, 37075 Göttingen, Germany.(2)InnerEarLab, University Medical Center Göttingen, 37075 Göttingen, Germany.(3)Department of Otolaryngology, University Medical Center Göttingen, 37075 Göttingen, Germany.(4)Institute for X-ray Physics, University of Göttingen, 37075 Göttingen, Germany.(5)MED-EL, 6020 Innsbruck, Austria.(6)Cluster of Excellence ""Multiscale Bioimaging: From Molecular Machines to Networks of Excitable Cells,"" University of Göttingen, 37075 Göttingen, Germany.(7)Institute for Auditory Neuroscience, University Medical Center Göttingen, 37075 Göttingen, Germany; tmoser@gwdg.de.(8)Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, 37075 Göttingen, Germany.The cochlea of our auditory system is an intricate structure deeply embedded in the temporal bone. Compared with other sensory organs such as the eye, the cochlea has remained poorly accessible for investigation, for example, by imaging. This limitation also concerns the further development of technology for restoring hearing in the case of cochlear dysfunction, which requires quantitative information on spatial dimensions and the sensorineural status of the cochlea. Here, we employed X-ray phase-contrast tomography and light-sheet fluorescence microscopy and their combination for multiscale and multimodal imaging of cochlear morphology in species that serve as established animal models for auditory research. We provide a systematic reference for morphological parameters relevant for cochlear implant development for rodent and nonhuman primate models. We simulate the spread of light from the emitters of the optical implants within the reconstructed nonhuman primate cochlea, which indicates a spatially narrow optogenetic excitation of spiral ganglion neurons.DOI: 10.1073/pnas.2014472118PMCID: PMC8106341",pubmed,33903231,10.1073/pnas.2014472118
en route to sound coding strategies for optical cochlear implants,"701. iScience. 2023 Aug 25;26(10):107725. doi: 10.1016/j.isci.2023.107725. eCollection 2023 Oct 20.En route to sound coding strategies for optical cochlear implants.Khurana L(1)(2)(3)(4)(5)(6), Harczos T(1)(2), Moser T(1)(2)(3)(6)(7), Jablonski L(1)(2)(4)(6).Author information:(1)Institute for Auditory Neuroscience, University Medical Center Göttingen, Göttingen, Germany.(2)Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(3)Auditory Neuroscience and Synaptic Nanophysiology Group, Max-Planck-Institute for Multidisciplinary Sciences, Göttingen, Germany.(4)Junior Research Group ""Computational Neuroscience and Neuroengineering"", Göttingen, Germany.(5)The Doctoral Program ""Sensory and Motor Neuroscience"", Göttingen Graduate Center for Neurosciences, Biophysics, and Molecular Biosciences (GGNB), Göttingen, Germany.(6)InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(7)Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, Göttingen, Germany.Hearing loss is the most common human sensory deficit. Severe-to-complete sensorineural hearing loss is often treated by electrical cochlear implants (eCIs) bypassing dysfunctional or lost hair cells by direct stimulation of the auditory nerve. The wide current spread from each intracochlear electrode array contact activates large sets of tonotopically organized neurons limiting spectral selectivity of sound coding. Despite many efforts, an increase in the number of independent eCI stimulation channels seems impossible to achieve. Light, which can be better confined in space than electric current may help optical cochlear implants (oCIs) to overcome eCI shortcomings. In this review, we present the current state of the optogenetic sound encoding. We highlight optical sound coding strategy development capitalizing on the optical stimulation that requires fine-grained, fast, and power-efficient real-time sound processing controlling dozens of microscale optical emitters as an emerging research area.© 2023 The Author(s).DOI: 10.1016/j.isci.2023.107725PMCID: PMC10502376",pubmed,37720089,10.1016/j.isci.2023.107725
morphological brain network assessed using graph theory and network filtration in deaf adults,"648. Hear Res. 2014 Sep;315:88-98. doi: 10.1016/j.heares.2014.06.007. Epub 2014 Jul 10.Morphological brain network assessed using graph theory and network filtration in deaf adults.Kim E(1), Kang H(2), Lee H(3), Lee HJ(4), Suh MW(5), Song JJ(6), Oh SH(7), Lee DS(8).Author information:(1)Department of Nuclear Medicine, Seoul National University College of Medicine, Seoul, Republic of Korea; Institute of Radiation Medicine, Medical Research Center, Seoul National University, Seoul, Republic of Korea; Interdisciplinary Program in Cognitive Science, Seoul National University, Seoul, Republic of Korea.(2)Department of Nuclear Medicine, Seoul National University College of Medicine, Seoul, Republic of Korea; Data Science for Knowledge Creation Research Center, Seoul National University, Seoul, Republic of Korea.(3)Department of Nuclear Medicine, Seoul National University College of Medicine, Seoul, Republic of Korea; Institute of Radiation Medicine, Medical Research Center, Seoul National University, Seoul, Republic of Korea.(4)Department of Otorhinolaryngology-Head and Neck Surgery, Hallym University College of Medicine, Chuncheon, Republic of Korea.(5)Sensory Organ Research Institute, Seoul National University Medical Research Center, Seoul, Republic of Korea.(6)Department of Otorhinolaryngology-Head and Neck Surgery, Seoul National University Bundang Hospital, Seongnam, Republic of Korea.(7)Department of Otorhinolaryngology-Head and Neck Surgery, Seoul National University College of Medicine, Seoul, Republic of Korea; Sensory Organ Research Institute, Seoul National University Medical Research Center, Seoul, Republic of Korea. Electronic address: shaoh@snu.ac.kr.(8)Department of Nuclear Medicine, Seoul National University College of Medicine, Seoul, Republic of Korea; Institute of Radiation Medicine, Medical Research Center, Seoul National University, Seoul, Republic of Korea; Interdisciplinary Program in Cognitive Science, Seoul National University, Seoul, Republic of Korea; Department of Molecular Medicine and Biopharmaceutical Sciences, Graduate School of Convergence Science and Technology, and College of Medicine or College of Pharmacy, Seoul National University, Seoul, Republic of Korea. Electronic address: dsl@plaza.snu.ac.kr.Prolonged deprivation of auditory input can change brain networks in pre- and postlingual deaf adults by brain-wide reorganization. To investigate morphological changes in these brains voxel-based morphometry, voxel-wise correlation with the primary auditory cortex, and whole brain network analyses using morphological covariance were performed in eight prelingual deaf, eleven postlingual deaf, and eleven hearing adults. Network characteristics based on graph theory and network filtration based on persistent homology were examined. Gray matter density in the primary auditor cortex was preserved in prelingual deafness, while it tended to decrease in postlingual deafness. Unlike postlingual, prelingual deafness showed increased bilateral temporal connectivity of the primary auditory cortex compared to the hearing adults. Of the graph theory-based characteristics, clustering coefficient, betweenness centrality, and nodal efficiency all increased in prelingual deafness, while all the parameters of postlingual deafness were similar to the hearing adults. Patterns of connected components changing during network filtration were different between prelingual deafness and hearing adults according to the barcode, dendrogram, and single linkage matrix representations, while these were the same in postlingual deafness. Nodes in fronto-limbic and left temporal components were closely coupled, and nodes in the temporo-parietal component were loosely coupled, in prelingual deafness. Patterns of connected components changing in postlingual deafness were the same as hearing adults. We propose that the preserved density of auditory cortex associated with increased connectivity in prelingual deafness, and closer coupling between certain brain areas, represent distinctive reorganization of auditory and related cortices compared with hearing or postlingual deaf adults. The differential network reorganization in the prelingual deaf adults could be related to the absence of auditory speech experience.Copyright © 2014 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2014.06.007",pubmed,25016143,10.1016/j.heares.2014.06.007
differences in glycinergic mipscs in the auditory brain stem of normal and congenitally deaf neonatal mice,"641. J Neurophysiol. 2004 Feb;91(2):1006-12. doi: 10.1152/jn.00771.2003. Epub 2003 Oct 15.Differences in glycinergic mIPSCs in the auditory brain stem of normal and congenitally deaf neonatal mice.Leao RN(1), Oleskevich S, Sun H, Bautista M, Fyffe RE, Walmsley B.Author information:(1)Synaptic Structure and Function Group, Division of Neuroscience, The John Curtin School of Medical Research, The Australian National University, Canberra ACT 0200, Australia.We have investigated the fundamental properties of central auditory glycinergic synapses in early postnatal development in normal and congenitally deaf (dn/dn) mice. Glycinergic miniature inhibitory postsynaptic currents (mIPSCs) were recorded using patch-clamp methods in neurons from a brain slice preparation of the medial nucleus of the trapezoid body (MNTB), at 12-14 days postnatal age. Our results show a number of significant differences between normal and deaf mice. The frequency of mIPSCs is greater (50%) in deaf versus normal mice. Mean mIPSC amplitude is smaller in deaf mice than in normal mice (mean mIPSC amplitude: deaf, 64 pA; normal, 106 pA). Peak-scaled fluctuation analysis of mIPSCs showed that mean single channel conductance is greater in the deaf mice (deaf, 64 pS; normal, 45 pS). The mean decay time course of mIPSCs is slower in MNTB neurons from deaf mice (mean half-width: deaf, 2.9 ms; normal, 2.3 ms). Light- and electron-microscopic immunolabeling results showed that MNTB neurons from deaf mice have more (30%) inhibitory synaptic sites (postsynaptic gephyrin clusters) than MNTB neurons in normal mice. Our results demonstrate substantial differences in glycinergic transmission in normal and congenitally deaf mice, supporting a role for activity during development in regulating both synaptic structure (connectivity) and the fundamental (quantal) properties of mIPSCs at central glycinergic synapses.DOI: 10.1152/jn.00771.2003",pubmed,14561690,10.1152/jn.00771.2003
comparison of continuous sampling with active noise cancelation and sparse sampling for cortical and subcortical auditory functional mri,"812. Magn Reson Med. 2021 Nov;86(5):2577-2588. doi: 10.1002/mrm.28902. Epub 2021 Jul 1.Comparison of continuous sampling with active noise cancelation and sparse sampling for cortical and subcortical auditory functional MRI.Dewey RS(1)(2)(3), Hall DA(2)(3)(4), Plack CJ(5)(6)(7), Francis ST(1).Author information:(1)Sir Peter Mansfield Imaging Centre, School of Physics and Astronomy, University of Nottingham, Nottingham, United Kingdom.(2)National Institute for Health Research (NIHR) Nottingham Biomedical Research Centre, Nottingham, United Kingdom.(3)Hearing Sciences, Division of Mental Health and Clinical Neurosciences, School of Medicine, University of Nottingham, Nottingham, United Kingdom.(4)Heriot-Watt University Malaysia, Putrajaya, Malaysia.(5)Manchester Centre for Audiology and Deafness, University of Manchester, Manchester Academic Health Science Centre, Manchester, United Kingdom.(6)National Institute for Health Research Manchester Biomedical Research Centre, Central Manchester University Hospitals NHS Foundation Trust, Manchester, United Kingdom.(7)Department of Psychology, Lancaster University, Lancaster, United Kingdom.PURPOSE: Detecting sound-related activity using functional MRI requires the auditory stimulus to be more salient than the intense background scanner acoustic noise. Various strategies can reduce the impact of scanner acoustic noise, including ""sparse"" temporal sampling with single/clustered acquisitions providing intervals without any background scanner acoustic noise, or active noise cancelation (ANC) during ""continuous"" temporal sampling, which generates an acoustic signal that adds destructively to the scanner acoustic noise, substantially reducing the acoustic energy at the participant's eardrum. Furthermore, multiband functional MRI allows multiple slices to be collected simultaneously, thereby reducing scanner acoustic noise in a given sampling period.METHODS: Isotropic multiband functional MRI (1.5 mm) with sparse sampling (effective TR = 9000 ms, acquisition duration = 1962 ms) and continuous sampling (TR = 2000 ms) with ANC were compared in 15 normally hearing participants. A sustained broadband noise stimulus was presented to drive activation of both sustained and transient auditory responses within subcortical and cortical auditory regions.RESULTS: Robust broadband noise-related activity was detected throughout the auditory pathways. Continuous sampling with ANC was found to give a statistically significant advantage over sparse sampling for the detection of the transient (onset) stimulus responses, particularly in the auditory cortex (P < .001) and inferior colliculus (P < .001), whereas gains provided by sparse over continuous ANC for detecting offset and sustained responses were marginal (p ~ 0.05 in superior olivary complex, inferior colliculus, medial geniculate body, and auditory cortex).CONCLUSIONS: Sparse and continuous ANC multiband functional MRI protocols provide differing advantages for observing the transient (onset and offset) and sustained stimulus responses.© 2021 The Authors. Magnetic Resonance in Medicine published by Wiley Periodicals LLC on behalf of International Society for Magnetic Resonance in Medicine.DOI: 10.1002/mrm.28902",pubmed,34196020,10.1002/mrm.28902
osbpl2 encodes a protein of inner and outer hair cell stereocilia and is mutated in autosomal dominant hearing loss dfna67,"652. Orphanet J Rare Dis. 2015 Feb 10;10:15. doi: 10.1186/s13023-015-0238-5.OSBPL2 encodes a protein of inner and outer hair cell stereocilia and is mutated in autosomal dominant hearing loss (DFNA67).Thoenes M(1), Zimmermann U(2), Ebermann I(3), Ptok M(4), Lewis MA(5), Thiele H(6), Morlot S(7), Hess MM(8), Gal A(9), Eisenberger T(10), Bergmann C(11)(12), Nürnberg G(13), Nürnberg P(14)(15), Steel KP(16), Knipper M(17), Bolz HJ(18)(19).Author information:(1)Institute of Human Genetics, University Hospital of Cologne, Cologne, Germany. michaela.thoenes@uk-koeln.de.(2)Molecular Physiology of Hearing, Hearing Research Centre Tübingen (THRC), Department of Otolaryngology, University of Tübingen, Tübingen, Germany. ulrike.zimmermann@uni-tuebingen.de.(3)Institute of Human Genetics, University Hospital of Cologne, Cologne, Germany. ingaebermann@yahoo.de.(4)Department of Phoniatrics and Pediatric Audiology, Hannover Medical School, Hannover, Germany. Ptok.Martin@mh-hannover.de.(5)Wolfson Centre for Age-Related Diseases, King's College London, London, UK. morag.lewis@kcl.ac.uk.(6)Cologne Center for Genomics (CCG) and Center for Molecular Medicine Cologne (CMMC), University of Cologne, Cologne, Germany. holger.thiele@uni-koeln.de.(7)Institute for Human Genetics, Hannover Medical School, Hannover, Germany. Morlot.Susanne@mh-hannover.de.(8)Department of Voice, Speech and Hearing Disorders, University Medical Center Hamburg-Eppendorf, Hamburg, Germany. hess@uke.de.(9)Department of Human Genetics, University Medical Center Hamburg-Eppendorf, Hamburg, Germany. gal@uke.de.(10)Center for Human Genetics, Bioscientia, Ingelheim, Germany. tobias.eisenberger@bioscientia.de.(11)Center for Human Genetics, Bioscientia, Ingelheim, Germany. carsten.bergmann@bioscientia.de.(12)Renal Division, Department of Medicine, University Medical Center Freiburg, Freiburg, Germany. carsten.bergmann@bioscientia.de.(13)Cologne Center for Genomics (CCG) and Center for Molecular Medicine Cologne (CMMC), University of Cologne, Cologne, Germany. nuernberg@uni-koeln.de.(14)Cologne Center for Genomics (CCG) and Center for Molecular Medicine Cologne (CMMC), University of Cologne, Cologne, Germany. gudrun.nuernberg@uni-koeln.de.(15)Cologne Excellence Cluster on Cellular Stress Responses in Aging-Associated Diseases (CECAD), University of Cologne, Cologne, Germany. gudrun.nuernberg@uni-koeln.de.(16)Wolfson Centre for Age-Related Diseases, King's College London, London, UK. karen.steel@kcl.ac.uk.(17)Molecular Physiology of Hearing, Hearing Research Centre Tübingen (THRC), Department of Otolaryngology, University of Tübingen, Tübingen, Germany. marlies.knipper@uni-tuebingen.de.(18)Institute of Human Genetics, University Hospital of Cologne, Cologne, Germany. hanno.bolz@bioscientia.de.(19)Center for Human Genetics, Bioscientia, Ingelheim, Germany. hanno.bolz@bioscientia.de.BACKGROUND: Early-onset hearing loss is mostly of genetic origin. The complexity of the hearing process is reflected by its extensive genetic heterogeneity, with probably many causative genes remaining to be identified. Here, we aimed at identifying the genetic basis for autosomal dominant non-syndromic hearing loss (ADNSHL) in a large German family.METHODS: A panel of 66 known deafness genes was analyzed for mutations by next-generation sequencing (NGS) in the index patient. We then conducted genome-wide linkage analysis, and whole-exome sequencing was carried out with samples of two patients. Expression of Osbpl2 in the mouse cochlea was determined by immunohistochemistry. Because Osbpl2 has been proposed as a target of miR-96, we investigated homozygous Mir96 mutant mice for its upregulation.RESULTS: Onset of hearing loss in the investigated ADNSHL family is in childhood, initially affecting the high frequencies and progressing to profound deafness in adulthood. However, there is considerable intrafamilial variability. We mapped a novel ADNSHL locus, DFNA67, to chromosome 20q13.2-q13.33, and subsequently identified a co-segregating heterozygous frameshift mutation, c.141_142delTG (p.Arg50Alafs*103), in OSBPL2, encoding a protein known to interact with the DFNA1 protein, DIAPH1. In mice, Osbpl2 was prominently expressed in stereocilia of cochlear outer and inner hair cells. We found no significant Osbpl2 upregulation at the mRNA level in homozygous Mir96 mutant mice.CONCLUSION: The function of OSBPL2 in the hearing process remains to be determined. Our study and the recent description of another frameshift mutation in a Chinese ADNSHL family identify OSBPL2 as a novel gene for progressive deafness.DOI: 10.1186/s13023-015-0238-5PMCID: PMC4334766",pubmed,25759012,10.1186/s13023-015-0238-5
verbal fluency in prelingually deaf early implanted children and adolescents with cochlear implants,"45. J Speech Lang Hear Res. 2023 Apr 12;66(4):1394-1409. doi: 10.1044/2022_JSLHR-22-00383. Epub 2023 Mar 1.Verbal Fluency in Prelingually Deaf, Early Implanted Children and Adolescents With Cochlear Implants.Hasnain F(1), Herran RM(1), Henning SC(1), Ditmars AM(1), Pisoni DB(1)(2), Sehgal ST(1), Kronenberger WG(1)(3).Author information:(1)Department of Otolaryngology - Head and Neck Surgery, Indiana University School of Medicine, Indianapolis.(2)Department of Psychological and Brain Sciences, Indiana University, Bloomington.(3)Department of Psychiatry, Indiana University School of Medicine, Indianapolis.PURPOSE: Verbal fluency tasks assess the ability to quickly and efficiently retrieve words from the mental lexicon by requiring subjects to rapidly generate words within a phonological or semantic category. This study investigated differences between cochlear implant users and normal-hearing peers in the clustering and time course of word retrieval during phonological and semantic verbal fluency tasks.METHOD: Twenty-eight children and adolescents (aged 9-17 years) with cochlear implants and 33 normal-hearing peers completed measures of verbal fluency, nonverbal intelligence, speech perception, and verbal short-term/working memory. Phonological and semantic verbal fluency tests were scored for total words generated, words generated in each 10-s interval of the 1-min task, latency to first word generated, number of word clusters, average cluster size, and number of word/cluster switches.RESULTS: Children and adolescents with cochlear implants generated fewer words than normal-hearing peers throughout the entire 60-s time interval of the phonological and semantic fluency tasks. Cochlear implant users also had slower start latency times and produced fewer clusters and switches than normal-hearing peers during the phonological fluency task. Speech perception and verbal working memory scores were more strongly associated with verbal fluency scores in children and adolescents with cochlear implants than in normal-hearing peers.CONCLUSIONS: Cochlear implant users show poorer phonological and semantic verbal fluency than normal-hearing peers, and their verbal fluency is significantly associated with speech perception and verbal working memory. These findings suggest deficits in fluent retrieval of phonological and semantic information from long-term lexical memory in cochlear implant users.DOI: 10.1044/2022_JSLHR-22-00383PMCID: PMC10457083",pubmed,36857026,10.1044/2022_JSLHR-22-00383
phytosterols reverse antiretroviralinduced hearing loss with potential implications for cochlear aging,"549. PLoS Biol. 2023 Aug 24;21(8):e3002257. doi: 10.1371/journal.pbio.3002257. eCollection 2023 Aug.Phytosterols reverse antiretroviral-induced hearing loss, with potential implications for cochlear aging.Sodero AO(1), Castagna VC(2)(3), Elorza SD(4), Gonzalez-Rodulfo SM(1), Paulazo MA(1), Ballestero JA(2), Martin MG(4), Gomez-Casati ME(2).Author information:(1)Instituto de Investigaciones Biomédicas, Pontificia Universidad Católica Argentina, Consejo Nacional de Investigaciones Científicas y Técnicas (BIOMED, UCA-CONICET), Buenos Aires, Argentina.(2)Instituto de Farmacología, Facultad de Medicina, Universidad de Buenos Aires, Consejo Nacional de Investigaciones Científicas y Técnicas (CONICET), Buenos Aires, Argentina.(3)Instituto de Investigaciones en Ingeniería Genética y Biología Molecular, Dr. Héctor N. Torres, Consejo Nacional de Investigaciones Científicas y Técnicas (INGEBI-CONICET), Buenos Aires, Argentina.(4)Laboratorio de Neurobiología, Instituto de Investigaciones Médicas Mercedes y Martín Ferreyra, Consejo Nacional de Investigaciones Científicas y Técnicas (INIMEC-CONICET-UNC), Universidad Nacional de Córdoba, Córdoba, Argentina.Cholesterol contributes to neuronal membrane integrity, supports membrane protein clustering and function, and facilitates proper signal transduction. Extensive evidence has shown that cholesterol imbalances in the central nervous system occur in aging and in the development of neurodegenerative diseases. In this work, we characterize cholesterol homeostasis in the inner ear of young and aged mice as a new unexplored possibility for the prevention and treatment of hearing loss. Our results show that cholesterol levels in the inner ear are reduced during aging, an effect that is associated with an increased expression of the cholesterol 24-hydroxylase (CYP46A1), the main enzyme responsible for cholesterol turnover in the brain. In addition, we show that pharmacological activation of CYP46A1 with the antiretroviral drug efavirenz reduces the cholesterol content in outer hair cells (OHCs), leading to a decrease in prestin immunolabeling and resulting in an increase in the distortion product otoacoustic emissions (DPOAEs) thresholds. Moreover, dietary supplementation with phytosterols, plant sterols with structure and function similar to cholesterol, was able to rescue the effect of efavirenz administration on the auditory function. Altogether, our findings point towards the importance of cholesterol homeostasis in the inner ear as an innovative therapeutic strategy in preventing and/or delaying hearing loss.Copyright: © 2023 Sodero et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.DOI: 10.1371/journal.pbio.3002257PMCID: PMC10449472",pubmed,37619212,10.1371/journal.pbio.3002257
garland chrysanthemum consumption ameliorates agerelated hearing loss in c57bl6 mouse model system to explore hearing loss prevention foods in a short period,"227. Biosci Biotechnol Biochem. 2022 Jul 22;86(8):1085-1094. doi: 10.1093/bbb/zbac092.Garland chrysanthemum consumption ameliorates age-related hearing loss in C57BL/6 mouse; model system to explore hearing loss prevention foods in a short period.Oike H(1)(2), Tomita S(1), Koyano H(2), Azami K(1).Author information:(1)Food Research Institute, National Agriculture and Food Research Organization (NARO), 2-1-12 Kannondai, Tsukuba, Ibaraki, Japan.(2)Research Center for Agricultural Information Technology, National Agriculture and Food Research Organization (NARO), 3-1-1 Kannondai, Tsukuba, Ibaraki, Japan.Garland chrysanthemum (Glebionis coronaria L.) is an antioxidant-rich leafy vegetable. We found that garland chrysanthemum consumption ameliorated age-related hearing loss (AHL) in C57BL/6J mice, an early onset model. We also found that AHL progression was significantly ameliorated by three of ten products. Metabolome analysis of the 10 products using nuclear magnetic resonance (NMR) spectroscopy indicated that phytosterols may be involved in the amelioration of AHL. However, the direct inhibitory effect of phytosterol mixture on mouse AHL progression was not identified. These results suggest that garland chrysanthemum consumption delays AHL development in mice and its efficiency varies depending on the source of the product. Our findings also suggest that phytosterol content in garland chrysanthemum functions as an evaluation marker for the efficiency. Furthermore, to accelerate the search for foods that prevent AHL, we have used these data to develop an automatic threshold determination method for auditory brainstem response using machine learning.© The Author(s) 2022. Published by Oxford University Press on behalf of Japan Society for Bioscience, Biotechnology, and Agrochemistry.DOI: 10.1093/bbb/zbac092",pubmed,35687003,10.1093/bbb/zbac092
intrinsic noise improves speech recognition in a computational model of the auditory pathway,"711. Front Neurosci. 2022 Jun 8;16:908330. doi: 10.3389/fnins.2022.908330. eCollection 2022.Intrinsic Noise Improves Speech Recognition in a Computational Model of the Auditory Pathway.Schilling A(1)(2)(3), Gerum R(4), Metzner C(2)(5), Maier A(6), Krauss P(2)(3)(6)(7).Author information:(1)Laboratory of Sensory and Cognitive Neuroscience, Aix-Marseille University, Marseille, France.(2)Neuroscience Lab, University Hospital Erlangen, Erlangen, Germany.(3)Cognitive Computational Neuroscience Group, Friedrich-Alexander-University Erlangen-Nuremberg (FAU), Erlangen, Germany.(4)Department of Physics and Center for Vision Research, York University, Toronto, ON, Canada.(5)Friedrich-Alexander-University Erlangen-Nuremberg (FAU), Erlangen, Germany.(6)Pattern Recognition Lab, Friedrich-Alexander-University Erlangen-Nuremberg (FAU), Erlangen, Germany.(7)Linguistics Lab, Friedrich-Alexander-University Erlangen-Nuremberg (FAU), Erlangen, Germany.Noise is generally considered to harm information processing performance. However, in the context of stochastic resonance, noise has been shown to improve signal detection of weak sub- threshold signals, and it has been proposed that the brain might actively exploit this phenomenon. Especially within the auditory system, recent studies suggest that intrinsic noise plays a key role in signal processing and might even correspond to increased spontaneous neuronal firing rates observed in early processing stages of the auditory brain stem and cortex after hearing loss. Here we present a computational model of the auditory pathway based on a deep neural network, trained on speech recognition. We simulate different levels of hearing loss and investigate the effect of intrinsic noise. Remarkably, speech recognition after hearing loss actually improves with additional intrinsic noise. This surprising result indicates that intrinsic noise might not only play a crucial role in human auditory processing, but might even be beneficial for contemporary machine learning approaches.Copyright © 2022 Schilling, Gerum, Metzner, Maier and Krauss.DOI: 10.3389/fnins.2022.908330PMCID: PMC9215117",pubmed,35757533,10.3389/fnins.2022.908330
extracellular vesicles for developing targeted hearing loss therapy,"52. J Control Release. 2024 Feb;366:460-478. doi: 10.1016/j.jconrel.2023.12.050. Epub 2024 Jan 11.Extracellular vesicles for developing targeted hearing loss therapy.Pan X(1), Li Y(2), Huang P(3), Staecker H(4), He M(5).Author information:(1)Department of Pharmaceutics, College of Pharmacy, University of Florida, Gainesville, Florida 32610, United States.(2)Department of Medicinal Chemistry, Center for Natural Products, Drug Discovery and Development, University of Florida, Gainesville, Florida 32610, United States.(3)Department of Otolaryngology, Head and Neck Surgery, University of Kansas School of Medicine, Kansas City, Kansas 66160, United States.(4)Department of Otolaryngology, Head and Neck Surgery, University of Kansas School of Medicine, Kansas City, Kansas 66160, United States. Electronic address: hstaecker@kumc.edu.(5)Department of Pharmaceutics, College of Pharmacy, University of Florida, Gainesville, Florida 32610, United States. Electronic address: mhe@cop.ufl.edu.Substantial efforts have been made for local administration of small molecules or biologics in treating hearing loss diseases caused by either trauma, genetic mutations, or drug ototoxicity. Recently, extracellular vesicles (EVs) naturally secreted from cells have drawn increasing attention on attenuating hearing impairment from both preclinical studies and clinical studies. Highly emerging field utilizing diverse bioengineering technologies for developing EVs as the bioderived therapeutic materials, along with artificial intelligence (AI)-based targeting toolkits, shed the light on the unique properties of EVs specific to inner ear delivery. This review will illuminate such exciting research field from fundamentals of hearing protective functions of EVs to biotechnology advancement and potential clinical translation of functionalized EVs. Specifically, the advancements in assessing targeting ligands using AI algorithms are systematically discussed. The overall translational potential of EVs is reviewed in the context of auditory sensing system for developing next generation gene therapy.Copyright © 2023. Published by Elsevier B.V.DOI: 10.1016/j.jconrel.2023.12.050",pubmed,38182057,10.1016/j.jconrel.2023.12.050
an update on clinical pathological diagnostic and therapeutic perspectives of childhood leukodystrophies,"Introduction: Leukodystrophies constitute heterogenous group of rare heritable disorders primarily affecting the white matter of central nervous system. These conditions are often under-appreciated among physicians. The first clinical manifestations of leukodystrophies are often nonspecific and can occur in different ages from neonatal to late adulthood periods. The diagnosis is, therefore, challenging in most cases. Area covered: Herein, the authors discuss different aspects of leukodystrophies. The authors used MEDLINE, EMBASE, and GOOGLE SCHOLAR to provide an extensive update about epidemiology, classifications, pathology, clinical findings, diagnostic tools, and treatments of leukodystrophies. Comprehensive evaluation of clinical findings, brain magnetic resonance imaging, and genetic studies play the key roles in the early diagnosis of individuals with leukodystrophies. No cure is available for most heritable white matter disorders but symptomatic treatments can significantly decrease the burden of events. New genetic methods and stem cell transplantation are also under investigation to further increase the quality and duration of life in affected population. Expert opinion: The improvements in molecular diagnostic tools allow us to identify the meticulous underlying etiology of leukodystrophies and result in higher diagnostic rates, new classifications of leukodystrophies based on genetic information, and replacement of symptomatic managements with more specific targeted therapies. Abbreviations: 4H: Hypomyelination, hypogonadotropic hypogonadism and hypodontia; AAV: Adeno-associated virus; AD: autosomal dominant; AGS: Aicardi-Goutieres syndrome; ALSP: Axonal spheroids and pigmented glia; APGBD: Adult polyglucosan body disease; AR: autosomal recessive; ASO: Antisense oligonucleotide therapy; AxD: Alexander disease; BAEP: Brainstem auditory evoked potentials; CAA: Cerebral amyloid angiopathy; CADASIL: Cerebral autosomal dominant arteriopathy with subcortical infarcts and leukoencephalopathy; CARASAL: Cathepsin A–related arteriopathy with strokes and leukoencephalopathy; CARASIL: Cerebral autosomal recessive arteriopathy with subcortical infarcts and leukoencephalopathy; CGH: Comparative genomic hybridization; ClC2: Chloride Ion Channel 2; CMTX: Charcot-Marie-Tooth disease, X-linked; CMV: Cytomegalovirus; CNS: central nervous system; CRISP/Cas9: Clustered regularly interspaced short palindromic repeat/CRISPR-associated 9; gRNA: Guide RNA; CTX: Cerebrotendinous xanthomatosis; DNA: Deoxyribonucleic acid; DSB: Double strand breaks; DTI: Diffusion tensor imaging; FLAIR: Fluid attenuated inversion recovery; GAN: Giant axonal neuropathy; H-ABC: Hypomyelination with atrophy of basal ganglia and cerebellum; HBSL: Hypomyelination with brainstem and spinal cord involvement and leg spasticity; HCC: Hypomyelination with congenital cataracts; HEMS: Hypomyelination of early myelinated structures; HMG CoA: Hydroxy methylglutaryl CoA; HSCT: Hematopoietic stem cell transplant; iPSC: Induced pluripotent stem cells; KSS: Kearns-Sayre syndrome; L-2-HGA: L-2-hydroxy glutaric aciduria; LBSL: Leukoencephalopathy with brainstem and spinal cord involvement and elevated lactate; LCC: Leukoencephalopathy with calcifications and cysts; LTBL: Leukoencephalopathy with thalamus and brainstem involvement and high lactate; MELAS: Mitochondrial myopathy, encephalopathy, lactic acidosis, and stroke; MERRF: Myoclonic epilepsy with ragged red fibers; MLC: Megalencephalic leukoencephalopathy with subcortical cysts; MLD: metachromatic leukodystrophy; MRI: magnetic resonance imaging; NCL: Neuronal ceroid lipofuscinosis; NGS: Next generation sequencing; ODDD: Oculodentodigital dysplasia; PCWH: Peripheral demyelinating neuropathy-central-dysmyelinating leukodystrophy-Waardenburg syndrome-Hirschprung disease; PMD: Pelizaeus‐Merzbacher disease; PMDL: Pelizaeus-Merzbacher-like disease; RNA: Ribonucleic acid; TW: T-weighted; VWM: Vanishing white matter; WES: whole exome sequencing; WGS: whole genome sequencing; X-ALD: X-linked adrenoleukodystrophy; XLD: X-linked dominant; XLR: X-linked recessive. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",scopus,2-s2.0-85076367424,10.1080/14737175.2020.1699060
proceedings of the 2013 ieee symposium on computational intelligence in rehabilitation and assistive technologies cirat 2013  2013 ieee symposium series on computational intelligence ssci 2013,The proceedings contain 8 papers. The topics discussed include: a novel hand strength assessment method integrated into haptic knob for stroke rehabilitation; classification of silent speech using adaptive collection; real-time upper-body detection and orientation estimation via depth cues for assistive technology; handheld device based personal auditory training system to hearing loss; control of a wheelchair using an adaptive k-means clustering of head poses; a biomimetic similarity index for prosthetic hands; effects of behavior network as a suggestion system to assist BCI users; and gesture recognition system for wheelchair control using a depth sensor.,scopus,2-s2.0-84886665581,
probing the role of the c2f domain of otoferlin,"828. Front Mol Neurosci. 2023 Dec 12;16:1299509. doi: 10.3389/fnmol.2023.1299509. eCollection 2023.Probing the role of the C(2)F domain of otoferlin.Chen H(1)(2)(3)(4), Fang Q(1)(2)(3), Benseler F(5), Brose N(2)(5)(6), Moser T(1)(2)(3)(6).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(2)Collaborative Research Center 889, University of Göttingen, Göttingen, Germany.(3)Auditory Neuroscience and Synaptic Nanophysiology Group, Max Planck Institute for Multidisciplinary Sciences, Göttingen, Germany.(4)Göttingen Graduate Center for Neurosciences, Biophysics and Molecular Biosciences, University of Göttingen, Göttingen, Germany.(5)Department of Molecular Neurobiology, Max Planck Institute for Multidisciplinary Sciences, Göttingen, Germany.(6)Multiscale Bioimaging Cluster of Excellence (MBExC), University of Göttingen, Göttingen, Germany.Afferent synapses of cochlear inner hair cells (IHCs) employ a unique molecular machinery. Otoferlin is a key player in this machinery, and its genetic defects cause human auditory synaptopathy. We employed site-directed mutagenesis in mice to investigate the role of Ca2+ binding to the C2F domain of otoferlin. Substituting two aspartate residues of the C2F top loops, which are thought to coordinate Ca2+-ions, by alanines (OtofD1841/1842A) abolished Ca2+-influx-triggered IHC exocytosis and synchronous signaling in the auditory pathway despite substantial expression (~60%) of the mutant otoferlin in the basolateral IHC pole. Ca2+ influx of IHCs and their resting membrane capacitance, reflecting IHC size, as well as the number of IHC synapses were maintained. The mutant otoferlin showed a strong apex-to-base abundance gradient in IHCs, suggesting impaired protein targeting. Our results indicate a role of the C2F domain in otoferlin targeting and of Ca2+ binding by the C2F domain for IHC exocytosis and hearing.Copyright © 2023 Chen, Fang, Benseler, Brose and Moser.DOI: 10.3389/fnmol.2023.1299509PMCID: PMC10751786",pubmed,38152587,10.3389/fnmol.2023.1299509
behavioral nudges to improve audit and feedback report opening among antibiotic prescribers a randomized controlled trial,"Background: Peer comparison audit and feedback has demonstrated effectiveness in improving antibiotic prescribing practices, but only a minority of prescribers view their reports. We rigorously tested 3 behavioral nudging techniques delivered by email to improve report opening. Methods: We conducted a pragmatic randomized controlled trial among Ontario long-Term care prescribers enrolled in an ongoing peer comparison audit and feedback program which includes data on their antibiotic prescribing patterns. Physicians were randomized to 1 of 8 possible sequences of intervention/control allocation to 3 different behavioral email nudges: A social peer comparison nudge (January 2020), a maintenance of professional certification incentive nudge (October 2020), and a prior participation nudge (January 2021). The primary outcome was feedback report opening; the primary analysis pooled the effects of all 3 nudging interventions. Results: The trial included 421 physicians caring for >28 000 residents at 450 facilities. In the pooled analysis, physicians opened only 29.6% of intervention and 23.9% of control reports (odds ratio [OR], 1.51 [95% confidence interval {CI}, 1.10-2.07], P=.011); this difference remained significant after accounting for physician characteristics and clustering (adjusted OR [aOR], 1.74 [95% CI, 1.24-2.45], P=.0014). Of individual nudging techniques, the prior participation nudge was associated with a significant increase in report opening (OR, 1.62 [95% CI, 1.06-2.47], P=.026; aOR, 2.16 [95% CI, 1.33-3.50], P=.0018). In the pooled analysis, nudges were also associated with accessing more report pages (aOR, 1.28 [95% CI, 1.14-1.43], P<.001). Conclusions: Enhanced nudging strategies modestly improved report opening, but more work is needed to optimize physician engagement with audit and feedback. Clinical Trials Registration: NCT04187742.  © 2022 The Author(s) 2022. Published by Oxford University Press on behalf of Infectious Diseases Society of America.",scopus,2-s2.0-85128466436,10.1093/ofid/ofac111
proteomic analysis reveals the composition of glutamatergic organelles of auditory inner hair cells,"792. Mol Cell Proteomics. 2024 Feb;23(2):100704. doi: 10.1016/j.mcpro.2023.100704. Epub 2023 Dec 20.Proteomic Analysis Reveals the Composition of Glutamatergic Organelles of Auditory Inner Hair Cells.Cepeda AP(1), Ninov M(2), Neef J(3), Parfentev I(1), Kusch K(4), Reisinger E(5), Jahn R(6), Moser T(7), Urlaub H(8).Author information:(1)Bioanalytical Mass Spectrometry Group, Max Planck Institute for Multidisciplinary Sciences, Göttingen, Germany.(2)Bioanalytical Mass Spectrometry Group, Max Planck Institute for Multidisciplinary Sciences, Göttingen, Germany; Department of Clinical Chemistry, University Medical Center Göttingen, Göttingen, Germany.(3)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany; Auditory Neuroscience & Synaptic Nanophysiology Group Max-Planck-Institute for Multidisciplinary Sciences, Göttingen, Germany.(4)Functional Auditory Genomics Group, Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(5)Gene Therapy for Hearing Impairment and Deafness, Department for Otolaryngology, Head & Neck Surgery, University Hospital Tübingen, Tübingen, Germany.(6)Laboratory of Neurobiology, Max Planck Institute for Multidisciplinary Sciences, Göttingen, Germany; Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, Göttingen, Germany. Electronic address: reinhard.jahn@mpinat.mpg.de.(7)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany; Auditory Neuroscience & Synaptic Nanophysiology Group Max-Planck-Institute for Multidisciplinary Sciences, Göttingen, Germany; Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, Göttingen, Germany. Electronic address: tmoser@gwdg.de.(8)Bioanalytical Mass Spectrometry Group, Max Planck Institute for Multidisciplinary Sciences, Göttingen, Germany; Department of Clinical Chemistry, University Medical Center Göttingen, Göttingen, Germany; Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, Göttingen, Germany. Electronic address: henning.urlaub@mpinat.mpg.de.In the ear, inner hair cells (IHCs) employ sophisticated glutamatergic ribbon synapses with afferent neurons to transmit auditory information to the brain. The presynaptic machinery responsible for neurotransmitter release in IHC synapses includes proteins such as the multi-C2-domain protein otoferlin and the vesicular glutamate transporter 3 (VGluT3). Yet, much of this likely unique molecular machinery remains to be deciphered. The scarcity of material has so far hampered biochemical studies which require large amounts of purified samples. We developed a subcellular fractionation workflow combined with immunoisolation of VGluT3-containing membrane vesicles, allowing for the enrichment of glutamatergic organelles that are likely dominated by synaptic vesicles (SVs) of IHCs. We have characterized their protein composition in mice before and after hearing onset using mass spectrometry and confocal imaging and provide a fully annotated proteome with hitherto unidentified proteins. Despite the prevalence of IHC marker proteins across IHC maturation, the profiles of trafficking proteins differed markedly before and after hearing onset. Among the proteins enriched after hearing onset were VAMP-7, syntaxin-7, syntaxin-8, syntaxin-12/13, SCAMP1, V-ATPase, SV2, and PKCα. Our study provides an inventory of the machinery associated with synaptic vesicle-mediated trafficking and presynaptic activity at IHC ribbon synapses and serves as a foundation for future functional studies.Copyright © 2023 The Authors. Published by Elsevier Inc. All rights reserved.DOI: 10.1016/j.mcpro.2023.100704PMCID: PMC10832297",pubmed,38128648,10.1016/j.mcpro.2023.100704
virtual acoustic environments for comprehensive evaluation of modelbased hearing devicessup,"430. Int J Audiol. 2018 Jun;57(sup3):S112-S117. doi: 10.1080/14992027.2016.1247501. Epub 2016 Nov 4.Virtual acoustic environments for comprehensive evaluation of model-based hearing devices<sup/>.Grimm G(1)(2), Luberadzka J(1), Hohmann V(1)(2).Author information:(1)a Medizinische Physik and Cluster of Excellence Hearing4all , Universität Oldenburg , Oldenburg , Germany and.(2)b HörTech gGmbH , Oldenburg , Germany.OBJECTIVE: Create virtual acoustic environments (VAEs) with interactive dynamic rendering for applications in audiology.DESIGN: A toolbox for creation and rendering of dynamic virtual acoustic environments (TASCAR) that allows direct user interaction was developed for application in hearing aid research and audiology. The software architecture and the simulation methods used to produce VAEs are outlined. Example environments are described and analysed.CONCLUSION: With the proposed software, a tool for simulation of VAEs is available. A set of VAEs rendered with the proposed software was described.DOI: 10.1080/14992027.2016.1247501",pubmed,27813439,10.1080/14992027.2016.1247501
middle and inner ear malformations in two siblings exposed to valproic acid during pregnancy a case report,"Valproic acid (VPA) is a known teratogenic drug. Exposure to VPA during the pregnancy can lead to a distinct facial appearance, a cluster of major and minor anomalies and developmental delay. In this case report, two siblings with fetal valproate syndrome and a mild conductive hearing loss were investigated. Radiologic evaluation showed middle and inner ear malformations in both children. Audiologic, vestibular and motor examination was performed. This is the first case report to describe middle and inner ear malformations in children exposed to VPA. © 2014 Elsevier Ireland Ltd.",scopus,2-s2.0-84907958769,10.1016/j.ijporl.2014.08.030
is there any relationship between hearing threshold levels and cd4 cell count of human immunodeficiency virus infected adults,"358. Afr J Med Med Sci. 2016 May;45(1):51-60.Is there any relationship between hearing threshold levels and CD4 cell count of human immunodeficiency virus infected adults?Fasunla AJ, Ijitola JO, Akpa OM, Nwaorgu OGB, Taiwo B, Olaleye DO, Murphy RL, Adewole IF, Akinyinka OO.Background The role of viral load level and/or CD4 (Cluster of differentiation 4) cell count in the aetiopathogenesis of hearing loss in HIV infection is unclear. Therefore, we investigated the relationship between CD4 cell counts, viral load and hearing threshold of HIV (Human immunodeficiency virus) infected adults.METHODS: This cohort audiometric study involved consecutive HIV-infected and HIV-uninfected adults as controls. Clinical data relating to hearing loss, HIV status, and highly -active antiretroviral therapy (HAART) were obtained. Audiornetric evaluation was performed. The most recent CD4 cell counts and RNA viral load-of HIV-infected participants were obtained from clinic records.RESULTS: There were 299(66.7%) HIV-infected adults and 149(33.3%) controls with mean age of 39.64± 12.45 years and 39.60±12.45 years respectively (p=0.98). In both groups, there were more participants with left hearing loss. Mild to profound hearing loss was found in 65.9% HIV- infected participants and 53.7% controls. Majority (86.3%) of the HIV-infected participants were on HAART. The mean CD4 cell count was 654.58±289.15 in 41 HIV-infected participants not on HAART and 523.95±300.17 in 258 participants on HAART (p=0.01). Majority,- 197 (62%) HIV- infected participants with hearing loss had CD4 cell count ≤200 cells/mm3. Higher viral load significantly correlated with low CD4 cell counts (p<0.0 1; r=0. 18) and low CD4 cell count significantly correlated with high hearing threshold (p<O.01; r=0. 17).CONCLUSION: There was a trend towards more hearing loss among the HIV-infected adults. The higher hearing threshold in those with low CD4 cell counts of <200 cells/mm3 suggests possible relationship between hearing status and severity of HIV disease.",pubmed,28686827,
metabolic changes in the brain and blood of rats following acoustic trauma tinnitus and hyperacusis,"It has been increasingly recognized that tinnitus is likely to be generated by complex network changes. Acoustic trauma that causes tinnitus induces significant changes in multiple metabolic pathways in the brain. However, it is not clear whether those metabolic changes in the brain could also be reflected in blood samples and whether metabolic changes could discriminate acoustic trauma, hyperacusis and tinnitus. We analyzed brain and serum metabolic changes in rats following acoustic trauma or a sham procedure using metabolomics. Hearing levels were recorded before and after acoustic trauma and behavioral measures to quantify tinnitus and hyperacusis were conducted at 4 weeks following acoustic trauma. Tissues from 11 different brain regions and serum samples were collected at about 3 months following acoustic trauma. Among the acoustic trauma animals, eight exhibited hyperacusis-like behavior and three exhibited tinnitus-like behavior. Using Gas chromatography–mass spectrometry and multivariate statistical analysis, significant metabolic changes were found in acoustic trauma animals in both the brain and serum samples with a number of metabolic pathways significantly perturbated. Furthermore, metabolic changes in the serum were able to differentiate sham from acoustic trauma animals, as well as sham from hyperacusis animals, with high accuracy. Our results suggest that serum metabolic profiling in combination with machine learning analysis may be a promising approach for identifying biomarkers for acoustic trauma, hyperacusis and potentially, tinnitus. © 2021 Elsevier B.V.",scopus,2-s2.0-85099623576,10.1016/bs.pbr.2020.09.002
somatosensory basis of speech production,"681. Nature. 2003 Jun 19;423(6942):866-9. doi: 10.1038/nature01710.Somatosensory basis of speech production.Tremblay S(1), Shiller DM, Ostry DJ.Author information:(1)Department of Psychology, McGill University, Montreal, Quebec H3A 1B1, Canada.The hypothesis that speech goals are defined acoustically and maintained by auditory feedback is a central idea in speech production research. An alternative proposal is that speech production is organized in terms of control signals that subserve movements and associated vocal-tract configurations. Indeed, the capacity for intelligible speech by deaf speakers suggests that somatosensory inputs related to movement play a role in speech production-but studies that might have documented a somatosensory component have been equivocal. For example, mechanical perturbations that have altered somatosensory feedback have simultaneously altered acoustics. Hence, any adaptation observed under these conditions may have been a consequence of acoustic change. Here we show that somatosensory information on its own is fundamental to the achievement of speech movements. This demonstration involves a dissociation of somatosensory and auditory feedback during speech production. Over time, subjects correct for the effects of a complex mechanical load that alters jaw movements (and hence somatosensory feedback), but which has no measurable or perceptible effect on acoustic output. The findings indicate that the positions of speech articulators and associated somatosensory inputs constitute a goal of speech movements that is wholly separate from the sounds produced.DOI: 10.1038/nature01710",pubmed,12815431,10.1038/nature01710
gene discovery in the auditory system characterization of additional cochlearexpressed sequences,"647. J Assoc Res Otolaryngol. 2002 Mar;3(1):45-53. doi: 10.1007/s101620020005.Gene discovery in the auditory system: characterization of additional cochlear-expressed sequences.Resendes BL(1), Robertson NG, Szustakowski JD, Resendes RJ, Weng Z, Morton CC.Author information:(1)Department of Obstetrics, Gynecology and Reproductive Biology, Brigham and Women's Hospital, Boston, MA 02115, USA.To identify genes involved in hearing, 8494 expressed sequence tags (ESTs) were generated from a human fetal cochlear cDNA library in two distinct sequencing projects. Analysis of the first set of 4304 ESTs revealed clones representing 517 known human genes, 41 mammalian genes not previously detected in human tissues, 487 ESTs from other human tissues, and 541 cochlear-specific ESTs (http://hearing.bwh.harvard.edu). We now report results of a DNA sequence similarity (BLAST) analysis of an additional 4190 cochlear ESTs and a comparison to the first set. Among the 4190 new cochlear ESTs, 959 known human genes were identified; 594 were found only among the new ESTs and 365 were found among ESTs from both sequencing projects. COL1A2 was the most abundant transcript among both sets of ESTs, followed in order by COL3A1, SPARC, EEFY1A1, and TPTI. An additional 22 human homologs of known nonhuman mammalian genes and 1595 clusters of ESTs, of which 333 are cochlear-specific, were identified among the new cochlear ESTs. Map positions were determined for 373 of the new cochlear ESTs and revealed 318 additional loci. Forty-nine of the mapped ESTs are located within the genetic interval of 23 deafness loci. Reanalysis of unassigned ESTs from the prior study revealed 338 additional known human genes. The total number of known human genes identified from 8494 cochlear ESTs is 1449 and is represented by 4040 ESTs. Among the known human genes are 14 deafness-associated genes, including GJB2 (connexin 26) and KVLQT1. The total number of nonhuman mammalian genes identified is 43 and is represented by 58 ESTs. The total number of ESTs without sequence similarity to known genes is 4055. Of these, 778 also do not have sequence similarity to any other ESTs, are categorized into 700 clusters, and may represent genes uniquely or preferentially expressed in the cochlea. Identification of additional known genes, ESTs, and cochlear-specific ESTs provides new candidate genes for both syndromic and nonsyndromic deafness disorders.DOI: 10.1007/s101620020005PMCID: PMC3202364",pubmed,12083723,10.1007/s101620020005
individual variability in delayed auditory feedback effects on speech fluency and rate in normally fluent adults,"Purpose: Delayed auditory feedback (DAF) is known to induce stuttering-like disfluencies (SLDs) and cause speech rate reductions in normally fluent adults, but the reason for speech disruptions is not fully known, and individual variation has not been well characterized. Studying individual variation in susceptibility to DAF may identify factors that predispose an individual to be more or less dependent on auditory feedback. Method: Participants were 62 normally fluent adults. Each participant performed a spontaneous speech task in 250-ms DAF and amplified nondelayed auditory feedback (NAF) conditions. SLDs, other disfluencies (ODs), speech errors (SEs), and articulation rate (AR) were measured under each condition. Results: In the DAF condition, SLDs and SEs significantly increased, and AR decreased. Sex had a limited effect in that mean exhibited higher rates of ODs and faster AR than women. More important, parametric cluster analysis identified that 2-and 3-subgroup solutions reveal important variation that differentiates tendencies toward disfluency changes and rate reduction under DAF, which are theoretically and empirically preferred to a single-group solution. Conclusion: Individual variability in response to DAF may be accounted for by subgroups of individuals. This suggests that certain normally fluent individuals could be more dependent on intact feedback to maintain fluency. © American Speech-Language-Hearing Association.",scopus,2-s2.0-84878023847,10.1044/1092-4388(2012/11-0303)
the impact of alerting designs on air traffic controllers eye movement patterns and situation awareness,"This research investigated controller’ situation awareness by comparing COOPANS’s acoustic alerts with newly designed semantic alerts. The results demonstrate that ATCOs’ visual scan patterns had significant differences between acoustic and semantic designs. ATCOs established different eye movement patterns on fixations number, fixation duration and saccade velocity. Effective decision support systems require human-centered design with effective stimuli to direct ATCO’s attention to critical events. It is necessary to provide ATCOs with specific alerting information to reflect the nature of the critical situation in order to minimise the side effects of startle and inattentional deafness. Consequently, the design of a semantic alert can significantly reduce ATCOs’ response time, therefore providing valuable extra time in a time-limited situation to formulate and execute resolution strategies in critical air safety events. The findings of this research indicate that the context-specified design of semantic alerts could improve ATCO’s situational awareness and significantly reduce response time in the event of Short Term Conflict Alert (STCA) activation which alerts to two aircraft having less than the required lateral or vertical separation. Practitioner Summary: Eye movements are closely linked with visual attention and can be analysed to explore shifting attention whilst performing monitoring tasks. This research has found that context-specific designed semantic alerts facilitated improved ATCO cognitive processing by integrating visual and auditory resources. Semantic designs have been demonstrated to be superior to acoustic design by directing the operator’s attention more quickly to critical situations.Abbreviations: APW: area proximity warning; ASRS: aviation safety reporting system; ATC: air traffic control; ATCO: air traffic controller; ATM: air traffic management; COOPANS: cooperation between air navigation service providers; HCI: human-computer interaction; IAA: irish aviation authority; MSAW: minimum safe altitude warning; MTCD: medium-term conflict detection; SA: situation awareness; STCA: short term conflict alert; TP: trajectory prediction. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",scopus,2-s2.0-85053067654,10.1080/00140139.2018.1493151
aifm1 variants associated with auditory neuropathy spectrum disorder cause apoptosis due to impaired apoptosisinducing factor dimerization  aifm1 ,"Auditory neuropathy spectrum disorder (ANSD) represents a variety of sensorineural deafness conditions characterized by abnormal inner hair cells and/or auditory nerve function, but with the preservation of outer hair cell function. ANSD represents up to 15% of individuals with hearing impairments. Through mutation screening, bioinformatic analysis and expression studies, we have previously identified several apoptosis-inducing factor (AIF) mitochondria-associated 1 (AIFM1) variants in ANSD families and in some other sporadic cases. Here, to elucidate the pathogenic mechanisms underlying each AIFM1 variant, we generated AIF-null cells using the clustered regularly interspersed short palindromic repeats (CRISPR)/CRISPR-associated protein 9 (Cas9) system and constructed AIF-wild type (WT) and AIF-mutant (mut) (p.T260A, p.R422W, and p.R451Q) stable transfection cell lines. We then analyzed AIF structure, coenzyme-binding affinity, apoptosis, and other aspects. Results revealed that these variants resulted in impaired dimerization, compromising AIF function. The reduction reaction of AIF variants had proceeded slower than that of AIF-WT. The average levels of AIF dimerization in AIF variant cells were only 34.5%–49.7% of that of AIF-WT cells, resulting in caspase-independent apoptosis. The average percentage of apoptotic cells in the variants was 12.3%–17.9%, which was significantly higher than that (6.9%–7.4%) in controls. However, nicotinamide adenine dinucleotide (NADH) treatment promoted the reduction of apoptosis by rescuing AIF dimerization in AIF variant cells. Our findings show that the impairment of AIF dimerization by AIFM1 variants causes apoptosis contributing to ANSD, and introduce NADH as a potential drug for ANSD treatment. Our results help elucidate the mechanisms of ANSD and may lead to the provision of novel therapies. © 2023, Zhejiang University Press.",scopus,2-s2.0-85147608927,10.1631/jzus.B2200081
tprn is essential for the integrity of stereociliary rootlet in cochlear hair cells in mice,"332. Front Med. 2019 Dec;13(6):690-704. doi: 10.1007/s11684-018-0638-8. Epub 2018 Aug 30.Tprn is essential for the integrity of stereociliary rootlet in cochlear hair cells in mice.Men Y(1), Li X(2), Tu H(1), Zhang A(1), Fu X(1), Wang Z(1), Jin Y(1), Hou C(3), Zhang T(1), Zhang S(1), Zhou Y(1), Li B(4)(5), Li J(6), Sun X(7), Wang H(8), Gao J(9).Author information:(1)School of Life Science, Shandong University, Jinan, 250100, China.(2)Rizhao Polytechnic, Rizhao, 276826, China.(3)The Second Hospital of Shandong University, Jinan, 250033, China.(4)Electron Microscopy Laboratory, Shandong Institute of Otolaryngology, Jinan, 250022, China.(5)Laboratory of Electron Microscopy, Jinan WEI-YA Biotech Company, Jinan, 250100, China.(6)Department of Otolaryngology-Head and Neck Surgery, Provincial Hospital Affiliated to Shandong University, Jinan, 250021, China.(7)School of Life Science, Shandong University, Jinan, 250100, China. sunxy70@sdu.edu.cn.(8)Department of Otolaryngology-Head and Neck Surgery, Provincial Hospital Affiliated to Shandong University, Jinan, 250021, China. wang.hb7585@hotmail.com.(9)School of Life Science, Shandong University, Jinan, 250100, China. jggao@sdu.edu.cn.Tprn encodes the taperin protein, which is concentrated in the tapered region of hair cell stereocilia in the inner ear. In humans, TPRN mutations cause autosomal recessive nonsyndromic deafness (DFNB79) by an unknown mechanism. To determine the role of Tprn in hearing, we generated Tprn-null mice by clustered regularly interspaced short palindromic repeat/Cas9 genome-editing technology from a CBA/CaJ background. We observed significant hearing loss and progressive degeneration of stereocilia in the outer hair cells of Tprn-null mice starting from postnatal day 30. Transmission electron microscopy images of stereociliary bundles in the mutant mice showed some stereociliary rootlets with curved shafts. The central cores of the stereociliary rootlets possessed hollow structures with surrounding loose peripheral dense rings. Radixin, a protein expressed at stereocilia tapering, was abnormally dispersed along the stereocilia shafts in Tprn-null mice. The expression levels of radixin and β-actin significantly decreased.We propose that Tprn is critical to the retention of the integrity of the stereociliary rootlet. Loss of Tprn in Tprn-null mice caused the disruption of the stereociliary rootlet, which resulted in damage to stereociliary bundles and hearing impairments. The generated Tprn-null mice are ideal models of human hereditary deafness DFNB79.DOI: 10.1007/s11684-018-0638-8",pubmed,30159668,10.1007/s11684-018-0638-8
speech motor learning in profoundly deaf adults,"252. Nat Neurosci. 2008 Oct;11(10):1217-22. doi: 10.1038/nn.2193. Epub 2008 Sep 14.Speech motor learning in profoundly deaf adults.Nasir SM(1), Ostry DJ.Author information:(1)Department of Psychology, McGill University, 1205 Dr. Penfield Avenue, Montreal, Quebec H3A1B1, Canada.Speech production, like other sensorimotor behaviors, relies on multiple sensory inputs--audition, proprioceptive inputs from muscle spindles and cutaneous inputs from mechanoreceptors in the skin and soft tissues of the vocal tract. However, the capacity for intelligible speech by deaf speakers suggests that somatosensory input alone may contribute to speech motor control and perhaps even to speech learning. We assessed speech motor learning in cochlear implant recipients who were tested with their implants turned off. A robotic device was used to alter somatosensory feedback by displacing the jaw during speech. We found that implant subjects progressively adapted to the mechanical perturbation with training. Moreover, the corrections that we observed were for movement deviations that were exceedingly small, on the order of millimeters, indicating that speakers have precise somatosensory expectations. Speech motor learning is substantially dependent on somatosensory input.DOI: 10.1038/nn.2193PMCID: PMC2601702",pubmed,18794839,10.1038/nn.2193
gene editing in a myo6 semidominant mouse model rescues auditory function,"Myosin VI（MYO6） is an unconventional myosin that is vital for auditory and vestibular function. Pathogenic variants in the human MYO6 gene cause autosomal-dominant or -recessive forms of hearing loss. Effective treatments for Myo6 mutation causing hearing loss are limited. We studied whether adeno-associated virus (AAV)-PHP.eB vector-mediated in vivo delivery of Staphylococcus aureus Cas9 (SaCas9-KKH)-single-guide RNA (sgRNA) complexes could ameliorate hearing loss in a Myo6WT/C442Y mouse model that recapitulated the phenotypes of human patients. The in vivo editing efficiency of the AAV-SaCas9-KKH-Myo6-g2 system on Myo6C442Y is 4.05% on average in Myo6WT/C442Y mice, which was ∼17-fold greater than editing efficiency of Myo6WT alleles. Rescue of auditory function was observed up to 5 months post AAV-SaCas9-KKH-Myo6-g2 injection in Myo6WT/C442Y mice. Meanwhile, shorter latencies of auditory brainstem response (ABR) wave I, lower distortion product otoacoustic emission (DPOAE) thresholds, increased cell survival rates, more regular hair bundle morphology, and recovery of inward calcium levels were also observed in the AAV-SaCas9-KKH-Myo6-g2-treated ears compared to untreated ears. These findings provide further reference for in vivo genome editing as a therapeutic treatment for various semi-dominant forms of hearing loss and other semi-dominant diseases. © 2021 The American Society of Gene and Cell Therapy",scopus,2-s2.0-85114673060,10.1016/j.ymthe.2021.06.015
gene therapy for deafness are we there now,"543. EMBO Mol Med. 2024 Apr;16(4):675-677. doi: 10.1038/s44321-024-00058-6. Epub 2024 Mar 25.Gene therapy for deafness: are we there now?Moser T(1)(2)(3)(4), Chen H(5)(6), Kusch K(5)(6)(7)(8), Behr R(9), Vona B(5)(10).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099, Göttingen, Germany. tmoser@gwdg.de.(2)Auditory Neuroscience and Synaptic Nanophysiology Group, Max-Planck-Institute for Multidisciplinary Sciences, Göttingen, Germany. tmoser@gwdg.de.(3)Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany. tmoser@gwdg.de.(4)Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, Göttingen, Germany. tmoser@gwdg.de.(5)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099, Göttingen, Germany.(6)Auditory Neuroscience and Synaptic Nanophysiology Group, Max-Planck-Institute for Multidisciplinary Sciences, Göttingen, Germany.(7)Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(8)Functional Auditory Genomics group, Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, Göttingen, Germany.(9)Platform Degenerative Diseases, German Primate Center, Göttingen, Germany.(10)Institute of Human Genetics, University Medical Center Göttingen, 37099, Göttingen, Germany.Approximately half a billion people—5% of the world’s population—suffer from disabling hearing impairment (HI) according to the WHO (http://www.who.int/features/factfiles/deafness/en/). HI commonly hampers speech acquisition, leads to social isolation and increases risk for depression and cognitive decline. One to two per thousand children are born with disabling HI and over 50% of sensorineural HI is caused by defects in individual genes (deafness genes) of which roughly 150 have been identified so far (http://hereditaryhearingloss.org/). In case of profound hearing impairment or deafness, cochlear implants that bypass the dysfunctional or lost sensory hair cells and electrically stimulate the auditory nerve partially restore hearing. However, hearing with cochlear implants has shortcomings such as limited understanding of speech in background noise. So, there remains a major unmet medical need for improved hearing restoration (Wolf et al, 2022). Yet, despite major research efforts, a causal treatment based on pharmacology, gene therapy, or stem cells had, so far, not been clinically available. Now, this is finally changing at least for some patients: first in human trials prove the concept for inner ear gene therapy of otoferlin-related synaptic deafness.This Commentary discusses the successes of the recent first in human trials for gene therapy of otoferlin-deficient hearing impairment. [Image: see text]DOI: 10.1038/s44321-024-00058-6PMCID: PMC11018804",pubmed,38528140,10.1038/s44321-024-00058-6
devising a framework of optogenetic coding in the auditory pathway insights from auditory midbrain recordings,"804. Brain Stimul. 2023 Sep-Oct;16(5):1486-1500. doi: 10.1016/j.brs.2023.09.018. Epub 2023 Sep 29.Devising a framework of optogenetic coding in the auditory pathway: Insights from auditory midbrain recordings.Michael M(1), Wolf BJ(2), Klinge-Strahl A(3), Jeschke M(4), Moser T(5), Dieter A(6).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37075, Göttingen, Germany.(2)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37075, Göttingen, Germany; Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, 37077, Göttingen, Germany; Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, 37075, Göttingen, Germany.(3)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37075, Göttingen, Germany; Department of Otolaryngology, University Medical Center Göttingen, 37075, Göttingen, Germany.(4)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37075, Göttingen, Germany; Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, 37077, Göttingen, Germany; Cognitive Hearing in Primates (CHiP) Group, German Primate Center, 37077, Göttingen, Germany.(5)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37075, Göttingen, Germany; Auditory Neuroscience and Optogenetics Laboratory, German Primate Center, 37077, Göttingen, Germany; Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, 37075, Göttingen, Germany; Auditory Neuroscience and Synaptic Nanophysiology Group, Max Planck Institute for Multidisciplinary Science, Göttingen, Germany. Electronic address: tmoser@gwdg.de.(6)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37075, Göttingen, Germany; Göttingen Graduate Center for Neurosciences, Biophysic, and Molecular Biosciences, 37077, Göttingen, Germany; Department of Neurophysiology, MCTN, Medical Faculty Mannheim, Heidelberg University, 68167, Mannheim, Germany. Electronic address: alexander.dieter@medma.uni-heidelberg.de.Cochlear implants (CIs) restore activity in the deafened auditory system via electrical stimulation of the auditory nerve. As the spread of electric current in biological tissues is rather broad, the spectral information provided by electrical CIs is limited. Optogenetic stimulation of the auditory nerve has been suggested for artificial sound coding with improved spectral selectivity, as light can be conveniently confined in space. Yet, the foundations for optogenetic sound coding strategies remain to be established. Here, we parametrized stimulus-response-relationships of the auditory pathway in gerbils for optogenetic stimulation. Upon activation of the auditory pathway by waveguide-based optogenetic stimulation of the spiral ganglion, we recorded neuronal activity of the auditory midbrain, in which neural representations of spectral, temporal, and intensity information can be found. Screening a wide range of optical stimuli and taking the properties of optical CI emitters into account, we aimed to optimize stimulus paradigms for potent and energy-efficient activation of the auditory pathway. We report that efficient optogenetic coding builds on neural integration of millisecond stimuli built from microsecond light pulses, which optimally accommodate power-efficient laser diode operation. Moreover, we performed an activity-level-dependent comparison of optogenetic and acoustic stimulation in order to estimate the dynamic range and the maximal stimulation intensity amenable to single channel optogenetic sound encoding, and indicate that it complies well with speech comprehension in a typical conversation (65 dB). Our results provide a first framework for the development of coding strategies for future optogenetic hearing restoration.Copyright © 2023 The Authors. Published by Elsevier Inc. All rights reserved.DOI: 10.1016/j.brs.2023.09.018",pubmed,37778456,10.1016/j.brs.2023.09.018
teleaudiology current state and future directions,"The importance of tele-audiology has been heightened by the current COVID-19 pandemic. The present article reviews the current state of tele-audiology practice while presenting its limitations and opportunities. Specifically, this review addresses: (1) barriers to hearing healthcare, (2) tele-audiology services, and (3) tele-audiology key issues, challenges, and future directions. Accumulating evidence suggests that tele-audiology is a viable service delivery model, as remote hearing screening, diagnostic testing, intervention, and rehabilitation can each be completed reliably and effectively. The benefits of tele-audiology include improved access to care, increased follow-up rates, and reduced travel time and costs. Still, significant logistical and technical challenges remain from ensuring a secure and robust internet connection to controlling ambient noise and meeting all state and federal licensure and reimbursement regulations. Future research and development, especially advancements in artificial intelligence, will continue to increase tele-audiology acceptance, expand remote care, and ultimately improve patient satisfaction. Copyright © 2022 D'Onofrio and Zeng.",scopus,2-s2.0-85131266969,10.3389/fdgth.2021.788103
gemini encodes a zebrafish ltype calcium channel that localizes at sensory hair cell ribbon synapses,"663. J Neurosci. 2004 Apr 28;24(17):4213-23. doi: 10.1523/JNEUROSCI.0223-04.2004.gemini encodes a zebrafish L-type calcium channel that localizes at sensory hair cell ribbon synapses.Sidi S(1), Busch-Nentwich E, Friedrich R, Schoenberger U, Nicolson T.Author information:(1)Max-Planck-Institut für Entwicklungsbiologie, 72076 Tübingen, Germany.L-type Ca2+ channels (LTCCs) drive the bulk of voltage-gated Ca2+ entry in vertebrate inner ear hair cells (HCs) and are essential for mammalian auditory processing. LTCC currents have been implicated in neurotransmitter release at the HC afferent active zone, the ribbon synapse. It is likely that LTCCs play a direct role in vesicle fusion; however, the subcellular localization of the channels in HCs has not been fully resolved. Via positional cloning, we show that mutations in a zebrafish LTCC encoding gene, cav1.3a, underlie the auditory-vestibular defects of gemini (gem) circler mutants. gem homozygous receptor mutant HCs display normal cell viability, afferent synaptogenesis, and peripheral innervation, yet exhibit strongly reduced extracellular potentials (approximately 50% of wild-type potentials). Apical FM1-43 uptake, however, is unaffected in gem mutant HCs, suggesting that mechanotransduction channels are functional. Using a Gem-specific antibody, we show that the bulk of Gem/Ca(v)1.3a immunoreactivity in HCs is restricted to basally located focal spots. The number and location of focal spots relative to nerve terminals, and their remarkable ring-shaped structure, which is reminiscent of synaptic dense bodies, are consistent with Gem/Ca(v)1.3a channels clustering at HC ribbon synapses.DOI: 10.1523/JNEUROSCI.0223-04.2004PMCID: PMC6729292",pubmed,15115817,10.1523/JNEUROSCI.0223-04.2004
biallelic variants in slc4a10 encoding a sodiumdependent bicarbonate transporter lead to a neurodevelopmental disorder,"859. Genet Med. 2024 Mar;26(3):101034. doi: 10.1016/j.gim.2023.101034. Epub 2023 Dec 3.Biallelic variants in SLC4A10 encoding a sodium-dependent bicarbonate transporter lead to a neurodevelopmental disorder.Maroofian R(1), Zamani M(2), Kaiyrzhanov R(3), Liebmann L(4), Karimiani EG(5), Vona B(6), Huebner AK(4), Calame DG(7), Misra VK(8), Sadeghian S(9), Azizimalamiri R(9), Mohammadi MH(10), Zeighami J(11), Heydaran S(12), Toosi MB(13), Akhondian J(14), Babaei M(15), Hashemi N(16), Schnur RE(17), Suri M(18), Setzke J(6), Wagner M(19), Brunet T(20), Grochowski CM(21), Emrick L(7), Chung WK(22), Hellmich UA(23), Schmidts M(24), Lupski JR(25), Galehdari H(12), Severino M(26), Houlden H(3), Hübner CA(27).Author information:(1)Department of Neuromuscular Diseases, UCL Queen Square Institute of Neurology, London, United Kingdom. Electronic address: r.maroofian@ucl.ac.uk.(2)Department of Biology, Faculty of Science, Shahid Chamran University of Ahvaz, Ahvaz, Iran; Narges Medical Genetics and Prenatal Diagnosis Laboratory, Kianpars, Ahvaz, Iran.(3)Department of Neuromuscular Diseases, UCL Queen Square Institute of Neurology, London, United Kingdom.(4)Institute of Human Genetics, Jena University Hospital, Friedrich Schiller Universität, Am Klinikum 1, Jena, Germany.(5)Molecular and Clinical Sciences Institute, St. George's, University of London, Cranmer Terrace, London, United Kingdom.(6)Institute of Human Genetics, University Medical Center Göttingen, Göttingen, Germany; Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(7)Division of Neurology and Developmental Neuroscience, Department of Pediatrics, Baylor College of Medicine, Houston, TX; Texas Children's Hospital, Houston, TX; Department of Molecular and Human Genetics, Baylor College of Medicine, Houston, TX.(8)Division of Genetic, Genomic & Metabolic Disorders, Discipline of Pediatrics, College of Medicine, Central Michigan University, Mount Pleasant, MI.(9)Department of Pediatric Neurology, Golestan Medical, Educational, and Research Center, Ahvaz Jundishapur University of Medical Sciences, Ahvaz, Iran.(10)Department of Pediatrics, Zabol University of Medical Sciences, Zabol, Iran.(11)Narges Medical Genetics and Prenatal Diagnosis Laboratory, Kianpars, Ahvaz, Iran.(12)Department of Biology, Faculty of Science, Shahid Chamran University of Ahvaz, Ahvaz, Iran.(13)Pediatric Neurology Department, Ghaem Hospital, Mashhad University of Medical Sciences, Mashhad, Iran; Neuroscience Research Center, Mashhad University of Medical Science, Mashhad, Iran.(14)Pediatric Neurology Department, Ghaem Hospital, Mashhad University of Medical Sciences, Mashhad, Iran.(15)Department of Pediatrics, North Khorasan University of Medical Sciences, Bojnurd, Iran.(16)Department of Pediatrics, School of Medicine, Mashhad University of Medical Sciences, Mashhad, Iran.(17)GeneDx, Gaithersburg, MD.(18)Clinical Genetics Service, Nottingham University Hospitals NHS Trust, Nottingham, United Kingdom.(19)Institute of Human Genetics, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany; Institute of Neurogenomics, Helmholtz Zentrum München, Neuherberg, Germany; Department of Pediatric Neurology and Developmental Medicine and LMU Center for Children with Medical Complexity, Dr. von Hauner Children's Hospital, LMU Hospital, Ludwig-Maximilians-University, Munich, Germany.(20)Institute of Human Genetics, Klinikum rechts der Isar, School of Medicine, Technical University of Munich, Munich, Germany; Institute of Neurogenomics, Helmholtz Zentrum München, Neuherberg, Germany.(21)Department of Molecular and Human Genetics, Baylor College of Medicine, Houston, TX.(22)Department of Pediatrics, Boston Children's Hospital, Harvard Medical School, Boston, MA.(23)Friedrich Schiller University Jena, Faculty of Chemistry and Earth Sciences, Institute of Organic Chemistry and Macromolecular Chemistry, Jena, Germany; Center for Biomolecular Magnetic Resonance (BMRZ), Goethe University, Frankfurt, Germany; Cluster of Excellence Balance of the Microverse, Friedrich Schiller University Jena, Jena, Germany.(24)Pediatrics Genetics Division, Center for Pediatrics and Adolescent Medicine, Faculty of Medicine, Freiburg University, Freiburg, Germany; Genome Research Division, Human Genetics Department, Radboud University Medical Center, Nijmegen, The Netherlands; CIBSS-Centre for Integrative Biological Signalling Studies, University of Freiburg, Freiburg, Germany.(25)Texas Children's Hospital, Houston, TX; Department of Molecular and Human Genetics, Baylor College of Medicine, Houston, TX; Human Genome Sequencing Center, Baylor College of Medicine, Houston, TX.(26)Neuroradiology Unit, IRCCS Istituto Giannina Gaslini, Genoa, Italy.(27)Institute of Human Genetics, Jena University Hospital, Friedrich Schiller Universität, Am Klinikum 1, Jena, Germany; Center for Rare Diseases, Jena University Hospital, Jena, Germany.PURPOSE: SLC4A10 encodes a plasma membrane-bound transporter, which mediates Na+-dependent HCO3- import, thus mediating net acid extrusion. Slc4a10 knockout mice show collapsed brain ventricles, an increased seizure threshold, mild behavioral abnormalities, impaired vision, and deafness.METHODS: Utilizing exome/genome sequencing in families with undiagnosed neurodevelopmental disorders and international data sharing, 11 patients from 6 independent families with biallelic variants in SLC4A10 were identified. Clinico-radiological and dysmorphology assessments were conducted. A minigene assay, localization studies, intracellular pH recordings, and protein modeling were performed to study the possible functional consequences of the variant alleles.RESULTS: The families harbor 8 segregating ultra-rare biallelic SLC4A10 variants (7 missense and 1 splicing). Phenotypically, patients present with global developmental delay/intellectual disability and central hypotonia, accompanied by variable speech delay, microcephaly, cerebellar ataxia, facial dysmorphism, and infrequently, epilepsy. Neuroimaging features range from some non-specific to distinct neuroradiological findings, including slit ventricles and a peculiar form of bilateral curvilinear nodular heterotopia. In silico analyses showed 6 of 7 missense variants affect evolutionarily conserved residues. Functional analyses supported the pathogenicity of 4 of 7 missense variants.CONCLUSION: We provide evidence that pathogenic biallelic SLC4A10 variants can lead to neurodevelopmental disorders characterized by variable abnormalities of the central nervous system, including altered brain ventricles, thus resembling several features observed in knockout mice.Copyright © 2023 The Authors. Published by Elsevier Inc. All rights reserved.DOI: 10.1016/j.gim.2023.101034",pubmed,38054405,10.1016/j.gim.2023.101034
sinhala sign language learning system for hearing impaired community,"This research aims to develop a comprehensive system for Sinhala Sign Language (SSL) that includes a learning system, dynamic sign detection, audio/video to sign conversion, and vocal training. SSL plays a crucial role in facilitating communication for individuals who are deaf or hard of hearing in Sri Lanka. The learning system provides a platform for learning SSL and includes a text-To-sign language interpreter. The dynamic sign detection system uses computer vision techniques to identify and interpret dynamic signs accurately. The audio/video to sign conversion system bridges the gap between spoken language and SSL by converting auditory information into visual representations. The vocal training system focuses on enhancing the vocal skills of cochlear implanted children. This research contributes to the development of effective communication and language skills for SSL users.  © 2023 IEEE.",scopus,2-s2.0-85184659714,10.1109/IISEC59749.2023.10391004
the peroxisome an update on mysteries 20,"Peroxisomes are key metabolic organelles, which contribute to cellular lipid metabolism, e.g. the β-oxidation of fatty acids and the synthesis of myelin sheath lipids, as well as cellular redox balance. Peroxisomal dysfunction has been linked to severe metabolic disorders in man, but peroxisomes are now also recognized as protective organelles with a wider significance in human health and potential impact on a large number of globally important human diseases such as neurodegeneration, obesity, cancer, and age-related disorders. Therefore, the interest in peroxisomes and their physiological functions has significantly increased in recent years. In this review, we intend to highlight recent discoveries, advancements and trends in peroxisome research, and present an update as well as a continuation of two former review articles addressing the unsolved mysteries of this astonishing organelle. We summarize novel findings on the biological functions of peroxisomes, their biogenesis, formation, membrane dynamics and division, as well as on peroxisome–organelle contacts and cooperation. Furthermore, novel peroxisomal proteins and machineries at the peroxisomal membrane are discussed. Finally, we address recent findings on the role of peroxisomes in the brain, in neurological disorders, and in the development of cancer. © 2018, The Author(s).",scopus,2-s2.0-85053618828,10.1007/s00418-018-1722-5
formal idiographic inference in medicine,[No abstract available],scopus,2-s2.0-85049412412,10.1001/jamaoto.2018.0254
human capital development for progress 2009 iti 7th international conference on information and communications technology icict 2009,The proceedings contain 11 papers. The topics discussed include: creating a strategic competitive advantage for an offshoring outsourcing industry; competency management and assessment - the inseparable elements of a quality framework for human resources development; industry institute interaction for capability building in engineering education in India: a study on the Indian information technology companies; e-learning for healthcare professionals towards his in Egypt; distance education degree programs for the Malaysian working adults: acceptability and negotiability perspectives; characterizing leadership and role management for resource sharing; computer mediated collaborative learning; clustering of content supporting computer mediated courseware development; human capital development: the importance of constructing auditory processing intervention instrument for underachiever students; and accessibility system for deaf Arab students.,scopus,2-s2.0-77949316500,
visual speech recognition improving speech perception in noise through artificial intelligence,"48. Otolaryngol Head Neck Surg. 2020 Oct;163(4):771-777. doi: 10.1177/0194599820924331. Epub 2020 May 26.Visual Speech Recognition: Improving Speech Perception in Noise through Artificial Intelligence.Raghavan AM(1), Lipschitz N(2), Breen JT(2), Samy RN(2), Kohlberg GD(2)(3).Author information:(1)University of Cincinnati College of Medicine, Cincinnati, Ohio, USA.(2)Department of Otolaryngology-Head and Neck Surgery, University of Cincinnati/Cincinnati Children's Hospital Medical Center, Cincinnati, Ohio, USA.(3)Department of Otolaryngology-Head and Neck Surgery, University of Washington, Seattle, Washington, USA.OBJECTIVES: To compare speech perception (SP) in noise for normal-hearing (NH) individuals and individuals with hearing loss (IWHL) and to demonstrate improvements in SP with use of a visual speech recognition program (VSRP).STUDY DESIGN: Single-institution prospective study.SETTING: Tertiary referral center.SUBJECTS AND METHODS: Eleven NH and 9 IWHL participants in a sound-isolated booth facing a speaker through a window. In non-VSRP conditions, SP was evaluated on 40 Bamford-Kowal-Bench speech-in-noise test (BKB-SIN) sentences presented by the speaker at 50 A-weighted decibels (dBA) with multiperson babble noise presented from 50 to 75 dBA. SP was defined as the percentage of words correctly identified. In VSRP conditions, an infrared camera was used to track 35 points around the speaker's lips during speech in real time. Lip movement data were translated into speech-text via an in-house developed neural network-based VSRP. SP was evaluated similarly in the non-VSRP condition on 42 BKB-SIN sentences, with the addition of the VSRP output presented on a screen to the listener.RESULTS: In high-noise conditions (70-75 dBA) without VSRP, NH listeners achieved significantly higher speech perception than IWHL listeners (38.7% vs 25.0%, P = .02). NH listeners were significantly more accurate with VSRP than without VSRP (75.5% vs 38.7%, P < .0001), as were IWHL listeners (70.4% vs 25.0% P < .0001). With VSRP, no significant difference in SP was observed between NH and IWHL listeners (75.5% vs 70.4%, P = .15).CONCLUSIONS: The VSRP significantly increased speech perception in high-noise conditions for NH and IWHL participants and eliminated the difference in SP accuracy between NH and IWHL listeners.DOI: 10.1177/0194599820924331",pubmed,32453650,10.1177/0194599820924331
loss of cib2 causes profound hearing loss and abolishes mechanoelectrical transduction in mice,"733. Front Mol Neurosci. 2017 Dec 4;10:401. doi: 10.3389/fnmol.2017.00401. eCollection 2017.Loss of CIB2 Causes Profound Hearing Loss and Abolishes Mechanoelectrical Transduction in Mice.Wang Y(1)(2), Li J(3), Yao X(1)(2), Li W(1)(2), Du H(1)(2), Tang M(4)(5), Xiong W(3), Chai R(4)(5)(6), Xu Z(1)(2).Author information:(1)Shandong Provincial Key Laboratory of Animal Cells and Developmental Biology, School of Life Sciences, Shandong University, Jinan, China.(2)Shandong Provincial Collaborative Innovation Center of Cell Biology, Shandong Normal University, Jinan, China.(3)School of Life Sciences, IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China.(4)Key Laboratory for Developmental Genes and Human Disease, Ministry of Education, Institute of Life Sciences, Southeast University, Nanjing, China.(5)Co-Innovation Center of Neuroregeneration, Nantong University, Nantong, China.(6)Jiangsu Province High-Tech Key Laboratory for Bio-Medical Research, Southeast University, Nanjing, China.Calcium and integrin-binding protein 2 (CIB2) belongs to a protein family with four known members, CIB1 through CIB4, which are characterized by multiple calcium-binding EF-hand domains. Among the family members, the Cib1 and Cib2 genes are expressed in mouse cochlear hair cells, and mutations in the human CIB2 gene have been associated with nonsyndromic deafness DFNB48 and syndromic deafness USH1J. To further explore the function of CIB1 and CIB2 in hearing, we established Cib1 and Cib2 knockout mice using the clustered regularly interspaced short palindromic repeat (CRISPR)-associated Cas9 nuclease (CRISPR/Cas9) genome editing technique. We found that loss of CIB1 protein does not affect auditory function, whereas loss of CIB2 protein causes profound hearing loss in mice. Further investigation revealed that hair cell stereocilia development is affected in Cib2 knockout mice. Noticeably, loss of CIB2 abolishes mechanoelectrical transduction (MET) currents in auditory hair cells. In conclusion, we show here that although both CIB1 and CIB2 are readily detected in the cochlea, only loss of CIB2 results in profound hearing loss, and that CIB2 is essential for auditory hair cell MET.DOI: 10.3389/fnmol.2017.00401PMCID: PMC5722843",pubmed,29255404,10.3389/fnmol.2017.00401
analysis of serum microrna expression in male workers with occupational noiseinduced hearing loss,"557. Braz J Med Biol Res. 2018 Jan 11;51(3):e6426. doi: 10.1590/1414-431X20176426.Analysis of serum microRNA expression in male workers with occupational noise-induced hearing loss.Li YH(1)(2), Yang Y(1), Yan YT(1), Xu LW(1), Ma HY(1), Shao YX(3), Cao CJ(3), Wu X(1), Qi MJ(1), Wu YY(1), Chen R(1), Hong Y(1), Tan XH(1), Yang L(1)(2).Author information:(1)School of Medicine, Hangzhou Normal University, Hangzhou, Zhejiang, China.(2)College of Life Sciences, Shihezi University, Shihezi, Xinjiang, China.(3)Hangzhou Hospital for the Prevention and Treatment of Occupational Diseases, Hangzhou, Zhejiang, China.Occupational noise-induced hearing loss (ONIHL) is a prevalent occupational disorder that impairs auditory function in workers exposed to prolonged noise. However, serum microRNA expression in ONIHL subjects has not yet been studied. We aimed to compare the serum microRNA expression profiles in male workers of ONIHL subjects and controls. MicroRNA microarray analysis revealed that four serum microRNAs were differentially expressed between controls (n=3) and ONIHL subjects (n=3). Among these microRNAs, three were upregulated (hsa-miR-3162-5p, hsa-miR-4484, hsa-miR-1229-5p) and one was downregulated (hsa-miR-4652-3p) in the ONIHL group (fold change >1.5 and Pbon value <0.05). Real time quantitative PCR was conducted for validation of the microRNA expression. Significantly increased serum levels of miR-1229-5p were found in ONIHL subjects compared to controls (n=10 for each group; P<0.05). A total of 659 (27.0%) genes were predicted as the target genes of miR-1229-5p. These genes were involved in various pathways, such as mitogen-activated protein kinase (MAPK) signaling pathway. Overexpression of miR-1229-5p dramatically inhibited the luciferase activity of 3' UTR segment of MAPK1 (P<0.01). Compared to the negative control, HEK293T cells expressing miR-1229-5p mimics showed a significant decline in mRNA levels of MAPK1 (P<0.05). This preliminary study indicated that serum miR-1229-5p was significantly elevated in ONIHL subjects. Increased miR-1229-5p may participate in the pathogenesis of ONIHL through repressing MAPK1 signaling.DOI: 10.1590/1414-431X20176426PMCID: PMC5769754",pubmed,29340520,10.1590/1414-431X20176426
structural insight into kcnq kv7 channel assembly and channelopathy,"851. Neuron. 2007 Mar 1;53(5):663-75. doi: 10.1016/j.neuron.2007.02.010.Structural insight into KCNQ (Kv7) channel assembly and channelopathy.Howard RJ(1), Clark KA, Holton JM, Minor DL Jr.Author information:(1)Chemistry and Chemical Biology Graduate Program, University of California, San Francisco, CA 94158-2330, USA.Kv7.x (KCNQ) voltage-gated potassium channels form the cardiac and auditory I(Ks) current and the neuronal M-current. The five Kv7 subtypes have distinct assembly preferences encoded by a C-terminal cytoplasmic assembly domain, the A-domain Tail. Here, we present the high-resolution structure of the Kv7.4 A-domain Tail together with biochemical experiments that show that the domain is a self-assembling, parallel, four-stranded coiled coil. Structural analysis and biochemical studies indicate conservation of the coiled coil in all Kv7 subtypes and that a limited set of interactions encode assembly specificity determinants. Kv7 mutations have prominent roles in arrhythmias, deafness, and epilepsy. The structure together with biochemical data indicate that A-domain Tail arrhythmia mutations cluster on the solvent-accessible surface of the subunit interface at a likely site of action for modulatory proteins. Together, the data provide a framework for understanding Kv7 assembly specificity and the molecular basis of a distinct set of Kv7 channelopathies.DOI: 10.1016/j.neuron.2007.02.010PMCID: PMC3011230",pubmed,17329207,10.1016/j.neuron.2007.02.010
charge syndrome and cochlear implantation difficulties and outcomes in the paediatric population,"150. Int J Pediatr Otorhinolaryngol. 2015 Apr;79(4):487-92. doi: 10.1016/j.ijporl.2015.01.004. Epub 2015 Jan 19.CHARGE syndrome and Cochlear implantation: difficulties and outcomes in the paediatric population.Birman CS(1), Brew JA(2), Gibson WP(3), Elliott EJ(4).Author information:(1)Discipline of Paediatrics and Child Health, Sydney Medical School, University of Sydney, Sydney, Australia; Sydney Children's Hospital Network (Children's Hospital at Westmead), Hawkesbury Road, Westmead 2145, NSW, Australia; The Sydney Cochlear Implant Centre, Royal Institute for Deaf and Blind Children, PO Box 188, Gladesville 1675, NSW, Australia; Department of Linguistics, Faculty of Human Sciences, Macquarie University, North Ryde, Australia. Electronic address: Catherine.birman@gmail.com.(2)The Sydney Cochlear Implant Centre, Royal Institute for Deaf and Blind Children, PO Box 188, Gladesville 1675, NSW, Australia.(3)Sydney Children's Hospital Network (Children's Hospital at Westmead), Hawkesbury Road, Westmead 2145, NSW, Australia; The Sydney Cochlear Implant Centre, Royal Institute for Deaf and Blind Children, PO Box 188, Gladesville 1675, NSW, Australia; Emeritus Professor, Sydney Medical School, University of Sydney, Sydney, Australia.(4)Discipline of Paediatrics and Child Health, Sydney Medical School, University of Sydney, Sydney, Australia; Sydney Children's Hospital Network (Children's Hospital at Westmead), Hawkesbury Road, Westmead 2145, NSW, Australia.OBJECTIVES: CHARGE syndrome is a complex cluster of congenital abnormalities, these children may have absent or hypoplastic auditory nerves. Our objective was to assess preoperative factors and outcomes for paediatric cochlear implant recipients with CHARGE syndrome, to enable better surgical preparation and family counselling.METHODS: The Sydney Cochlear Implant Centre database was searched for children with CHARGE syndrome who had received a cochlear implant at ages 16 and less. Data were collected regarding clinical history; hearing assessments; MRI and CT scan findings; preoperative transtympanic electrical Auditory Brainstem Response (ABR); intraoperative findings and intraoperative electrical ABR and Neural Response Telemetry; and language outcomes in terms of main language used and Categories of Auditory Performance scores (0-7 ranking).RESULTS: Ten children were identified. All seven prelingual profoundly deaf children with CHARGE syndrome had hypoplastic or absent auditory nerves bilaterally on MRI scans. Middle ear anatomy was often abnormal, affecting surgical landmarks and making identification of the cochlea very difficult in some cases. Three cases required repeated surgery to obtain successful cochlear implant insertion, one under CT scan image guided technique. All seven children used sign language, or simpler gestures, as their main mode of communication. Two children of of these children, who were implanted early, also attained some spoken language. CAP scores ranged from 0 to 6. The three children with CHARGE syndrome and progressive sensorineural hearing loss had a normal auditory nerve in at least one ear on MRI scans. All had preoperative verbal language, with CAP scores of 6, and continued with CAP scores of 6 following receipt of the cochlear implant.CONCLUSION: Children with CHARGE and congenital profound hearing loss all had hypoplasia or absent auditory nerves, affecting their outcomes with cochlear implants. They often had markedly abnormal middle ear anatomy and CT image guided surgery can be helpful. These children should be offered a bilingual early intervention approach, using sign language and verbal language, to ensure best language outcomes. Children with CHARGE syndrome and progressive profound hearing loss did well with cochlear implants and continue to be able to use verbal language.Copyright © 2015 Elsevier Ireland Ltd. All rights reserved.DOI: 10.1016/j.ijporl.2015.01.004",pubmed,25649713,10.1016/j.ijporl.2015.01.004
the russianlanguage version of the matrix test rumatrix in free field in patients after cochlear implantation in the long term,"448. Vestn Otorinolaringol. 2016;81(6):42-46. doi: 10.17116/otorino201681642-46.[The Russian-language version of the matrix test (RUMatrix) in free field in patients after cochlear implantation in the long term].[Article in Russian; Abstract available in Russian from the publisher]Goykhburg MV(1), Bakhshinyan VV(2), Petrova IP(3), Wazybok A(4), Kollmeier B(4), Tavartkiladze GA(2).Author information:(1)National Research Center for Audiology and Hearing Rehabilitation, Russian Medico-Biological Agency, Moscow, Russia, 117513.(2)National Research Center for Audiology and Hearing Rehabilitation, Russian Medico-Biological Agency, Moscow, Russia, 117513; Russian Medical Academy for Post-Graduate Education, Moscow, Russia, 125993.(3)National Research Center for Audiology and Hearing Rehabilitation, Russian Medico-Biological Agency, Moscow, Russia, 117513; Voronezh Children's Clinical Hospital No 1, Voronezh, Russia, 394024.(4)Medical Physics and Cluster of Excellence Hearing4all, Carl-von-Ossietzky, University Oldenburg, 26129, Oldenburg Germany.The deterioration of speech intelligibility in the patients using cochlear implantation (CI) systems is especially well apparent in the noisy environment. It explains why phrasal speech tests, such as a Matrix sentence test, have become increasingly more popular in the speech audiometry during rehabilitation after CI. The Matrix test allows to estimate speech perception by the patients in a real life situation. The objective of this study was to assess the effectiveness of audiological rehabilitation of CI patients using the Russian-language version of the matrix test (RUMatrix) in free field in the noisy environment. 33 patients aged from 5 to 40 years with a more than 3 year experience of using cochlear implants inserted at the National Research Center for Audiology and Hearing Rehabilitation were included in our study. Five of these patients were implanted bilaterally. The results of our study showed a statistically significant improvement of speech intelligibility in the noisy environment after the speech processor adjustment; dynamics of the signal-to-noise ratio changes was -1.7 dB (p<0.001).CONCLUSION: The RUMatrix test is a highly efficient method for the estimation of speech intelligibility in the patients undergoing clinical investigations in the noisy environment. The high degree of comparability of the RUMatrix test with the Matrix tests in other languages makes possible its application in international multicenter studies.Publisher: У пациентов, использующих системы кохлеарной имплантации (КИ), разборчивость речи ухудшается именно в шумной обстановке, в связи с чем в настоящее время в мире все большую популярность приобретают фразовые тесты в шуме для оценки восприятия речи пациентом в реальной акустической обстановке, такие как матриксный фразовый тест RUMatrix. Цель исследования - оценка эффективности слухоречевой реабилитации у пациентов после КИ c использованием фразового теста RUMatrix в свободном звуковом поле в шуме. Обследованы 33 пациента в возрасте от 5 до 40 лет после КИ, выполненной в Российском научно-практическом центре аудиологии и слухопротезирования, с опытом использования системы КИ более 3 лет, из них 5 пациентов - после билатеральной КИ. Выявлено статистически значимое улучшение разборчивости речи в шуме после проведения корректирующей настройки речевого процессора; динамика изменений составила: –1,7 дБ соотношения сигнал/шум (ОСШ) (р<0,001). RUMatrix является эффективным методом обследования для определения разборчивости речи у пациентов после КИ в шумной обстановке. Учитывая высокую сопоставимость RUMatrix с тестами на других языках, возможно его использование для проведения международных мультицентровых исследований.Publisher: У пациентов, использующих системы кохлеарной имплантации (КИ), разборчивость речи ухудшается именно в шумной обстановке, в связи с чем в настоящее время в мире все большую популярность приобретают фразовые тесты в шуме для оценки восприятия речи пациентом в реальной акустической обстановке, такие как матриксный фразовый тест RUMatrix. Цель исследования - оценка эффективности слухоречевой реабилитации у пациентов после КИ c использованием фразового теста RUMatrix в свободном звуковом поле в шуме. Обследованы 33 пациента в возрасте от 5 до 40 лет после КИ, выполненной в Российском научно-практическом центре аудиологии и слухопротезирования, с опытом использования системы КИ более 3 лет, из них 5 пациентов - после билатеральной КИ. Выявлено статистически значимое улучшение разборчивости речи в шуме после проведения корректирующей настройки речевого процессора; динамика изменений составила: –1,7 дБ соотношения сигнал/шум (ОСШ) (р<0,001). RUMatrix является эффективным методом обследования для определения разборчивости речи у пациентов после КИ в шумной обстановке. Учитывая высокую сопоставимость RUMatrix с тестами на других языках, возможно его использование для проведения международных мультицентровых исследований.DOI: 10.17116/otorino201681642-46",pubmed,28091475,10.17116/otorino201681642-46
the perception of the stereo effect in bilateral and bimodal cochlear implant users and its contribution to music enjoyment,"653. PLoS One. 2020 Jul 6;15(7):e0235435. doi: 10.1371/journal.pone.0235435. eCollection 2020.The perception of the stereo effect in bilateral and bimodal cochlear implant users and its contribution to music enjoyment.Buechner A(1)(2), Krueger B(1), Klawitter S(1), Zimmermann D(1), Fredelake S(3), Holube I(2)(4).Author information:(1)Medical University of Hanover, Hanover, Germany.(2)Cluster of Excellence Hearing4all, Germany.(3)Advanced Bionics GmbH, European Research Center, Hanover, Germany.(4)Institute of Hearing Technology and Audiology, Jade University of Applied Sciences, Oldenburg, Germany.OBJECTIVES: In this clinical study, stereo perception of music samples and its contribution to music enjoyment in CI users is investigated. It is studied in free field as well as direct audio presentation.METHODS: 20 bilateral and 9 bimodal CI users performed stereo detection tests and music enjoyment ratings. Music was presented either in mono or in stereo in free field or with direct audio presentation. Stereo detection was assessed with a 3-AFC paradigm. Music enjoyment was studied with scale ratings.RESULTS: For bilateral CI users, stereo detection increased from 52% correct in free field to 86% with direct audio presentation. Increased music enjoyment with improved stereo detection was obtained. Bimodal CI users could not identify stereo sounds. Music enjoyment did not increase for stereo presentations in bimodal subjects.DISCUSSION: For bilateral CI users, improved stereo detection might increase music enjoyment with direct audio presentation, which is likely due to bypassing the room acoustics. In bimodal CI users, no clear improvement was found, which is likely attributed due to the different hearing losses and therefore individually different interaural frequency overlaps between the hearing aid and the cochlear implant.CONCLUSION: Direct audio presentation is an efficient method to improve music enjoyment in bilateral CI users.DOI: 10.1371/journal.pone.0235435PMCID: PMC7337296",pubmed,32628690,10.1371/journal.pone.0235435
robotassisted cochlear implant electrode array insertion in adults a comparative study with manual insertion,"678. Otol Neurotol. 2021 Apr 1;42(4):e438-e444. doi: 10.1097/MAO.0000000000003002.Robot-assisted Cochlear Implant Electrode Array Insertion in Adults: A Comparative Study With Manual Insertion.Daoudi H(1)(2), Lahlou G(1)(2), Torres R(3)(2), Sterkers O(1)(2), Lefeuvre V(1)(2), Ferrary E(1)(2), Mosnier I(1)(2), Nguyen Y(1)(2).Author information:(1)Sorbonne University/AP-HP, GHU Pitié-Salpêtrière, DMU ChIR, Department of Oto-Rhino-Laryngology, Unit of Audiology, Auditory Implants and skull base surgery.(2)Inserm/Pasteur Institute, Hearing Institute, Technology and Gene therapy for Deafness, Paris, France.(3)Departamento de Ciencias Fisiologicas, Facultad de Medicina, Universidad Nacional de San Agustıon de Arequipa, Arequipa, Perou.OBJECTIVE: To describe the first cochlear array insertions using a robot-assisted technique, with different types of straight or precurved electrode arrays, compared with arrays manually inserted into the cochlea.STUDY DESIGN: Retrospective review.SETTING: Tertiary otologic center.PATIENTS: Twenty cochlear implantations in the robot-assisted group and 40 in the manually inserted group.INTERVENTIONS: Cochlear implantations using a robot-assisted technique (RobOtol) with straight (eight Cochlear CI522/622, and eight Advanced Bionics Hifocus Slim J) or precurved (four Advanced Bionics Hifocus Mid-Scala) matched to manual cochlear implantations. Three-dimensional reconstruction images of the basilar membrane and the electrode array were obtained from pre- and postimplantation computed tomography.MAIN OUTCOME MEASURES: Rate and localization of scalar translocations.RESULTS: For straight electrode arrays, scalar translocations occurred in 19% (3/16) of the robot-assisted group and 31% (10/32) of the manually inserted group. Considering the number of translocated electrodes, this was lower in the robot-assisted group (7%) than in the manually inserted group (16%) (p < 0.0001, χ2 test). For precurved electrode arrays, scalar translocations occurred in 50% (2/4) of the robot-assisted group and 38% (3/8) of the manually inserted group.CONCLUSION: This study showed a safe and reliable insertion of different electrode array types with a robot-assisted technique, with a less traumatic robotic insertion of straight electrode arrays when compared with manual insertion.Copyright © 2020, Otology & Neurotology, Inc.DOI: 10.1097/MAO.0000000000003002",pubmed,33306661,10.1097/MAO.0000000000003002
restoring speech intelligibility for hearing aid users with deep learning,"25. Sci Rep. 2023 Feb 15;13(1):2719. doi: 10.1038/s41598-023-29871-8.Restoring speech intelligibility for hearing aid users with deep learning.Diehl PU(1)(2), Singer Y(3), Zilly H(3), Schönfeld U(4), Meyer-Rachner P(3), Berry M(3), Sprekeler H(5)(6)(7), Sprengel E(3), Pudszuhn A(4), Hofmann VM(4).Author information:(1)Audatic, Berlin, Friedrichstr. 210, 10117, Berlin, Germany. peter.u.diehl@gmail.com.(2)Department of Otorhinolaryngology, Head and Neck Surgery, Charité-Universitätsmedizin Berlin, Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health, Campus Benjamin Franklin, Berlin, Germany. peter.u.diehl@gmail.com.(3)Audatic, Berlin, Friedrichstr. 210, 10117, Berlin, Germany.(4)Department of Otorhinolaryngology, Head and Neck Surgery, Charité-Universitätsmedizin Berlin, Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health, Campus Benjamin Franklin, Berlin, Germany.(5)Department for Electrical Engineering and Computer Science, Technische Universität Berlin, Berlin, Germany.(6)Bernstein Center for Computational Neuroscience Berlin, Philippstr. 13, 10115, Berlin, Germany.(7)Exzellenzcluster Science of Intelligence, Technische Universität Berlin, Marchstr. 23, 10587, Berlin, Germany.Almost half a billion people world-wide suffer from disabling hearing loss. While hearing aids can partially compensate for this, a large proportion of users struggle to understand speech in situations with background noise. Here, we present a deep learning-based algorithm that selectively suppresses noise while maintaining speech signals. The algorithm restores speech intelligibility for hearing aid users to the level of control subjects with normal hearing. It consists of a deep network that is trained on a large custom database of noisy speech signals and is further optimized by a neural architecture search, using a novel deep learning-based metric for speech intelligibility. The network achieves state-of-the-art denoising on a range of human-graded assessments, generalizes across different noise categories and-in contrast to classic beamforming approaches-operates on a single microphone. The system runs in real time on a laptop, suggesting that large-scale deployment on hearing aid chips could be achieved within a few years. Deep learning-based denoising therefore holds the potential to improve the quality of life of millions of hearing impaired people soon.© 2023. The Author(s).DOI: 10.1038/s41598-023-29871-8PMCID: PMC9932078",pubmed,36792797,10.1038/s41598-023-29871-8
a recurrent variant in polr1b c3007ct parg1003cys associated with atresia of the external canal and microtia in treacher collins syndrome type 4,"761. Mol Syndromol. 2021 Apr;12(2):127-132. doi: 10.1159/000513224. Epub 2021 Mar 2.A Recurrent Variant in POLR1B, c.3007C>T; p.Arg1003Cys, Associated with Atresia of the External Canal and Microtia in Treacher Collins Syndrome Type 4.Enomoto Y(1), Tsurusaki Y(1), Tominaga M(2), Kobayashi S(3), Inoue M(4), Fujita K(5), Kumaki T(2), Murakami H(2), Kurosawa K(1)(2).Author information:(1)Clinical Research Institute, Kanagawa Children's Medical Center, Yokohama, Japan.(2)Division of Medical Genetics, Kanagawa Children's Medical Center, Yokohama, Japan.(3)Department of Plastic and Reconstructive Surgery, Kanagawa Children's Medical Center, Yokohama, Japan.(4)Department of Otolaryngology, Kanagawa Children's Medical Center, Yokohama, Japan.(5)Department of Radiology, Kanagawa Children's Medical Center, Yokohama, Japan.Treacher Collins syndrome (TCS) is a heterogenous malformation syndrome characterized by a distinct facial appearance including downslanting palpebral fissures, malar hypoplasia, conductive hearing loss, and mandibular hypoplasia. Recently, a new causative gene, POLR1B, encoding DNA-directed RNA polymerase I subunit RPA2, was identified as a fourth type of TCS (TCS4). We describe another patient with TCS4 caused by a recurrent POLR1B variant, c.3007C>T; p.Arg1003Cys. Including our patient, all 4 patients with p.(Arg1003Cys) had atresia of the external auditory canal and microtia. All of the reported pathogenic variants in POLR1B were clustered at only 2 residues. Our patient highlights the genotype-phenotype correlation in TCS4 associated with POLR1B.Copyright © 2021 by S. Karger GmbH, Freiburg.DOI: 10.1159/000513224PMCID: PMC8114036",pubmed,34012383,10.1159/000513224
increased crossmodal functional connectivity in cochlear implant users,"806. Sci Rep. 2017 Aug 30;7(1):10043. doi: 10.1038/s41598-017-10792-2.Increased cross-modal functional connectivity in cochlear implant users.Chen LC(1)(2), Puschmann S(3)(4), Debener S(5)(3)(6).Author information:(1)Neuropsychology Lab, Department of Psychology, European Medical School, University of Oldenburg, Oldenburg, Germany. ling-chia.chen@uni-oldenburg.de.(2)Cluster of Excellence Hearing4all, Oldenburg, Germany. ling-chia.chen@uni-oldenburg.de.(3)Cluster of Excellence Hearing4all, Oldenburg, Germany.(4)Biological Psychology Lab, Department of Psychology, European medical school, University of Oldenburg, Oldenburg, Germany.(5)Neuropsychology Lab, Department of Psychology, European Medical School, University of Oldenburg, Oldenburg, Germany.(6)Research Center Neurosensory Science, University of Oldenburg, Oldenburg, Germany.Previous studies have reported increased cross-modal auditory and visual cortical activation in cochlear implant (CI) users, suggesting cross-modal reorganization of both visual and auditory cortices in CI users as a consequence of sensory deprivation and restoration. How these processes affect the functional connectivity of the auditory and visual system in CI users is however unknown. We here investigated task-induced intra-modal functional connectivity between hemispheres for both visual and auditory cortices and cross-modal functional connectivity between visual and auditory cortices using functional near infrared spectroscopy in post-lingually deaf CI users and age-matched normal hearing controls. Compared to controls, CI users exhibited decreased intra-modal functional connectivity between hemispheres and increased cross-modal functional connectivity between visual and left auditory cortices for both visual and auditory stimulus processing. Importantly, the difference between cross-modal functional connectivity for visual and for auditory stimuli correlated with speech recognition outcome in CI users. Higher cross-modal connectivity for auditory than for visual stimuli was associated with better speech recognition abilities, pointing to a new pattern of functional reorganization that is related to successful hearing restoration with a CI.DOI: 10.1038/s41598-017-10792-2PMCID: PMC5577186",pubmed,28855675,10.1038/s41598-017-10792-2
a case of charge association hearing loss and language development,"CHARGE association is a recently described cluster of congenital defects including coloboma, heart disease, atresia choanae, retarded growth and/or retarded development and/or CNS anomalies, genital hypoplasia, ear anomalies and/or deafness. The authors reported a 4-year-old girl with CHARGE association and discussed especially on her hearing loss and language development. ABR and COR revealed a 40-50 dB hearing loss. CT scan showed middle ear anomalies which explained her hearing loss. We suspected developmental retardation of her language was associated with mental retardation.",scopus,2-s2.0-0028328518,
ribeye is required for presynaptic cav13a channel localization and afferent innervation of sensory hair cells,"Ribbon synapses of the ear, eye and pineal gland contain a unique protein component: Ribeye. Ribeye consists of a novel aggregation domain spliced to the transcription factor CtBP2 and is one of the most abundant proteins in synaptic ribbon bodies. Although the importance of Ribeye for the function and physical integrity of ribbon synapses has been shown, a specific role in synaptogenesis has not been described. Here, we have modulated Ribeye expression in zebrafish hair cells and have examined the role of Ribeye in synapse development. Knockdown of ribeye resulted in fewer stimulus-evoked action potentials from afferent neurons and loss of presynaptic CaV1.3a calcium channel clusters in hair cells. Additionally, afferent innervation of hair cells was reduced in ribeye morphants, and the reduction was correlated with depletion of Ribeye punctae. By contrast, transgenic overexpression of Ribeye resulted in CaV1.3a channels colocalized with ectopic aggregates of Ribeye protein. Overexpression of Ribeye, however, was not sufficient to create ectopic synapses. These findings reveal two distinct functions of Ribeye in ribbon synapse formation - clustering CaV1.3a channels at the presynapse and stabilizing contacts with afferent neurons - and suggest that Ribeye plays an organizing role in synaptogenesis. © 2011. Published by The Company of Biologists Ltd.",scopus,2-s2.0-79955158904,10.1242/dev.059451
crossmodal functional reorganization of visual and auditory cortex in adult cochlear implant users identified with fnirs,"264. Neural Plast. 2016;2016:4382656. doi: 10.1155/2016/4382656. Epub 2015 Dec 27.Cross-Modal Functional Reorganization of Visual and Auditory Cortex in Adult Cochlear Implant Users Identified with fNIRS.Chen LC(1), Sandmann P(2), Thorne JD(1), Bleichner MG(3), Debener S(4).Author information:(1)Neuropsychology Lab, Department of Psychology, European Medical School, Carl-von-Ossietzky University Oldenburg, 26129 Oldenburg, Germany.(2)Department of Neurology, Hannover Medical School, 30625 Hannover, Germany; Cluster of Excellence Hearing4all, 26129 Oldenburg, Germany.(3)Neuropsychology Lab, Department of Psychology, European Medical School, Carl-von-Ossietzky University Oldenburg, 26129 Oldenburg, Germany; Cluster of Excellence Hearing4all, 26129 Oldenburg, Germany.(4)Neuropsychology Lab, Department of Psychology, European Medical School, Carl-von-Ossietzky University Oldenburg, 26129 Oldenburg, Germany; Cluster of Excellence Hearing4all, 26129 Oldenburg, Germany; Research Center Neurosensory Science, University of Oldenburg, 26129 Oldenburg, Germany.Cochlear implant (CI) users show higher auditory-evoked activations in visual cortex and higher visual-evoked activation in auditory cortex compared to normal hearing (NH) controls, reflecting functional reorganization of both visual and auditory modalities. Visual-evoked activation in auditory cortex is a maladaptive functional reorganization whereas auditory-evoked activation in visual cortex is beneficial for speech recognition in CI users. We investigated their joint influence on CI users' speech recognition, by testing 20 postlingually deafened CI users and 20 NH controls with functional near-infrared spectroscopy (fNIRS). Optodes were placed over occipital and temporal areas to measure visual and auditory responses when presenting visual checkerboard and auditory word stimuli. Higher cross-modal activations were confirmed in both auditory and visual cortex for CI users compared to NH controls, demonstrating that functional reorganization of both auditory and visual cortex can be identified with fNIRS. Additionally, the combined reorganization of auditory and visual cortex was found to be associated with speech recognition performance. Speech performance was good as long as the beneficial auditory-evoked activation in visual cortex was higher than the visual-evoked activation in the auditory cortex. These results indicate the importance of considering cross-modal activations in both visual and auditory cortex for potential clinical outcome estimation.DOI: 10.1155/2016/4382656PMCID: PMC4706950",pubmed,26819766,10.1155/2016/4382656
neural response properties predict perceived contents and locations elicited by intracranial electrical stimulation of human auditory cortex,"789. Cereb Cortex. 2024 Jan 31;34(2):bhad517. doi: 10.1093/cercor/bhad517.Neural response properties predict perceived contents and locations elicited by intracranial electrical stimulation of human auditory cortex.Wang Q(1)(2)(3), Luo L(4), Xu N(5), Wang J(6), Yang R(1)(2)(7), Chen G(1)(2)(7), Ren J(8)(9), Luan G(8)(10), Fang F(1)(2)(7).Author information:(1)School of Psychological and Cognitive Sciences and Beijing Key Laboratory of Behavior and Mental Health, Peking University, Beijing 100871, China.(2)IDG/McGovern Institute for Brain Research, Peking University, Beijing 100871, China.(3)National Key Laboratory of General Artificial Intelligence, Peking University, Beijing 100871, China.(4)School of Psychology, Beijing Sport University, Beijing 100084, China.(5)Division of Brain Sciences, Changping Laboratory, Beijing 102206, China.(6)Department of Neurology, Sanbo Brain Hospital, Capital Medical University, Beijing 100093, China.(7)Peking-Tsinghua Center for Life Sciences, Peking University, Beijing 100871, China.(8)Department of Functional Neurosurgery, Beijing Key Laboratory of Epilepsy, Sanbo Brain Hospital, Capital Medical University, Beijing 100093, China.(9)Epilepsy Center, Kunming Sanbo Brain Hospital, Kunming 650100 China.(10)Beijing Institute for Brain Disorders, Beijing 100069, China.Intracranial electrical stimulation (iES) of auditory cortex can elicit sound experiences with a variety of perceived contents (hallucination or illusion) and locations (contralateral or bilateral side), independent of actual acoustic inputs. However, the neural mechanisms underlying this elicitation heterogeneity remain undiscovered. Here, we collected subjective reports following iES at 3062 intracranial sites in 28 patients (both sexes) and identified 113 auditory cortical sites with iES-elicited sound experiences. We then decomposed the sound-induced intracranial electroencephalogram (iEEG) signals recorded from all 113 sites into time-frequency features. We found that the iES-elicited perceived contents can be predicted by the early high-γ features extracted from sound-induced iEEG. In contrast, the perceived locations elicited by stimulating hallucination sites and illusion sites are determined by the late high-γ and long-lasting α features, respectively. Our study unveils the crucial neural signatures of iES-elicited sound experiences in human and presents a new strategy to hearing restoration for individuals suffering from deafness.© The Author(s) 2024. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.DOI: 10.1093/cercor/bhad517",pubmed,38185991,10.1093/cercor/bhad517
bai1 localizes ampa receptors at the cochlear afferent postsynaptic density and is essential for hearing,"777. Cell Rep. 2024 Apr 1;43(4):114025. doi: 10.1016/j.celrep.2024.114025. Online ahead of print.BAI1 localizes AMPA receptors at the cochlear afferent post-synaptic density and is essential for hearing.Carlton AJ(1), Jeng JY(1), Grandi FC(2), De Faveri F(1), Amariutei AE(1), De Tomasi L(1), O'Connor A(1), Johnson SL(3), Furness DN(4), Brown SDM(5), Ceriani F(1), Bowl MR(5), Mustapha M(3), Marcotti W(6).Author information:(1)School of Biosciences, University of Sheffield, Sheffield S10 2TN, UK.(2)Sorbonne Université, INSERM, Institute de Myologie, Centre de Recherche en Myologie, 75013 Paris, France.(3)School of Biosciences, University of Sheffield, Sheffield S10 2TN, UK; Neuroscience Institute, University of Sheffield, Sheffield S10 2TN, UK.(4)School of Life Sciences, Keele University, Keele ST5 5BG, UK.(5)Mammalian Genetics Unit, MRC Harwell Institute, Harwell Campus, Oxfordshire OX11 0RD, UK.(6)School of Biosciences, University of Sheffield, Sheffield S10 2TN, UK; Neuroscience Institute, University of Sheffield, Sheffield S10 2TN, UK. Electronic address: w.marcotti@sheffield.ac.uk.Type I spiral ganglion neurons (SGNs) convey sound information to the central auditory pathway by forming synapses with inner hair cells (IHCs) in the mammalian cochlea. The molecular mechanisms regulating the formation of the post-synaptic density (PSD) in the SGN afferent terminals are still unclear. Here, we demonstrate that brain-specific angiogenesis inhibitor 1 (BAI1) is required for the clustering of AMPA receptors GluR2-4 (glutamate receptors 2-4) at the PSD. Adult Bai1-deficient mice have functional IHCs but fail to transmit information to the SGNs, leading to highly raised hearing thresholds. Despite the almost complete absence of AMPA receptor subunits, the SGN fibers innervating the IHCs do not degenerate. Furthermore, we show that AMPA receptors are still expressed in the cochlea of Bai1-deficient mice, highlighting a role for BAI1 in trafficking or anchoring GluR2-4 to the PSDs. These findings identify molecular and functional mechanisms required for sound encoding at cochlear ribbon synapses.Copyright © 2024 The Author(s). Published by Elsevier Inc. All rights reserved.DOI: 10.1016/j.celrep.2024.114025",pubmed,38564333,10.1016/j.celrep.2024.114025
robotbased assistance in middle ear surgery and cochlear implantation first clinical report,"656. Eur Arch Otorhinolaryngol. 2021 Jan;278(1):77-85. doi: 10.1007/s00405-020-06070-z. Epub 2020 May 26.Robot-based assistance in middle ear surgery and cochlear implantation: first clinical report.Vittoria S(1)(2)(3), Lahlou G(1)(2), Torres R(4), Daoudi H(1)(2), Mosnier I(1)(2), Mazalaigue S(2)(5), Ferrary E(1)(2), Nguyen Y(6)(7), Sterkers O(1)(2).Author information:(1)AP-HP, GHU Pitié-Salpêtrière, DMU ChIR, Service ORL, GRC Robotique et Innovation Chirurgicale, Sorbonne Université, Paris, France.(2)Inserm UMR 1120 ""Innovative Technologies and Translational Therapeutics for Deafness"", Hearing Institute Paris, Paris, France.(3)Otorhinolaryngology Unit, Department of Health Sciences, ASST Santi Paolo E Carlo Hospital Università Degli Studi, Milan, Italy.(4)Departamento de Ciencias Fisiológicas, Facultad de Medicina, Universidad Nacional de San Agustín de Arequipa, Av. Alcides Carrión 101, 04000, Arequipa, Peru.(5)Collin Orl, Bagneux, France.(6)AP-HP, GHU Pitié-Salpêtrière, DMU ChIR, Service ORL, GRC Robotique et Innovation Chirurgicale, Sorbonne Université, Paris, France. yann.nguyen@inserm.fr.(7)Inserm UMR 1120 ""Innovative Technologies and Translational Therapeutics for Deafness"", Hearing Institute Paris, Paris, France. yann.nguyen@inserm.fr.PURPOSE: Middle ear surgery may benefit from robot-based assistance to hold micro-instruments or an endoscope. However, the surgical gesture performed by one hand may perturb surgeons accustomed to two-handed surgery. A robot-based holder may combine the benefits from endoscopic exposure and a two-handed technique. Furthermore, tremor suppression and accurate tool control might help the surgeon during critical surgical steps. The goal of this work was to study the safety of an otological robot-based assistant under clinical conditions in a limited series of patients.METHODS: The RobOtol system has been used as an endoscope or a micro instrument holder for this series. Eleven cases were operated on with the robot as an endoscope holder for chronic otitis. Twenty-one cases were operated on with the robot as a micro-instrument holder for otosclerosis (9 cases), transtympanic tube placement (2 cases), or cochlear implantation (10 cases).RESULTS: No complications related to the robot manipulation occurred during surgery nor in postoperative. In the chronic otitis group, all perforations were sealed and 3-month postoperative pure-tone average air-bone gap (PTA ABG) was 15 ± 2.6 dB. In the otosclerosis group, 1-month post-op PTA ABG was 10 ± 1 dB. For cochlear implantation cases, a scala tympani insertion, a vestibular scala translocation occurred and a full scala vestibuli insertion was observed in 7, 2 and 1 case, respectively.CONCLUSION: The RobOtol system has reached the clinical stage. It could be used safely and with accurate control as an endoscope holder or a micro instrument holder in 32 cases.DOI: 10.1007/s00405-020-06070-z",pubmed,32458123,10.1007/s00405-020-06070-z
eventrelated neuronal responses to acoustic novelty in singlesided deaf cochlear implant users initial findings,"140. Clin Neurophysiol. 2018 Jan;129(1):133-142. doi: 10.1016/j.clinph.2017.10.025. Epub 2017 Nov 7.Event-related neuronal responses to acoustic novelty in single-sided deaf cochlear implant users: Initial findings.Bönitz H(1), Kopp B(2), Büchner A(3), Lunner T(4), Lyxell B(5), Finke M(6).Author information:(1)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany.(2)Department of Neurology, Hannover Medical School, Hannover, Germany.(3)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany; Cluster of Excellence ""Hearing4all"", Hannover, Germany.(4)Department of Behavioural Sciences and Learning, Linnaeus Centre HEAD, Linköping University, Linköping, Sweden; Swedish Institute for Disability Research, Linköping University, Linköping, Sweden; Eriksholm Research Centre, Oticon A/S, Snekkersten, Denmark.(5)Department of Behavioural Sciences and Learning, Linnaeus Centre HEAD, Linköping University, Linköping, Sweden; Swedish Institute for Disability Research, Linköping University, Linköping, Sweden; Department of Behavioral Sciences and Learning, Linköping University, Linköping, Sweden.(6)Department of Otorhinolaryngology, Hannover Medical School, Hannover, Germany; Cluster of Excellence ""Hearing4all"", Hannover, Germany. Electronic address: Finke.Mareike@mh-hannover.de.OBJECTIVE: A cochlear implant (CI) is an auditory prosthesis restoring profound hearing loss. However, CI-transmitted sounds are degraded compared to normal acoustic hearing. We investigated cortical responses related to CI-degraded against acoustic listening.METHODS: Event-related potentials (ERPs) were recorded from eight single-sided deaf CI users who performed a three-stimulus oddball task, separately with their normal hearing ear and CI ear. The oddball tones were occasionally intermitted by novel sounds. ERP responses were compared between electric and acoustic listening for the auditory (N1) and auditory-cognitive (Novelty P3, Target-P3) ERP components.RESULTS: CI-degraded listening was associated with attenuated sensory processing (N1) and with attenuated early cortical responses to acoustic novelty whereas the late cortical responses to acoustic novelty and the target-P3 did not differ between NH and CI ears.CONCLUSION: The present study replicates the CI-attenuation of Novelty-P3 amplitudes in a within-subject comparison. Further, we show that the CI-attenuation of Novelty-P3 amplitudes extends to early cortical responses to acoustic novelty, but not to late novelty responses.SIGNIFICANCE: The dissociation into CI-attenuated P3 early Novelty-P3 amplitudes and CI-unaffected late Novelty-P3 amplitudes represents a cortical fingerprint of CI-degraded listening. It further contributes to general claims of distinct auditory Novelty-P3 sub-components.Copyright © 2017 International Federation of Clinical Neurophysiology. Published by Elsevier B.V. All rights reserved.DOI: 10.1016/j.clinph.2017.10.025",pubmed,29182915,10.1016/j.clinph.2017.10.025
multiple myosin isozymes and haircell function,"The above enumeration does not conclusively summarize identities and functions of all hair-cell myosin isozymes. The RT-PCR screen described by Sole et al. (1994) demonstrated that other myosin isozymes, including myosin Iα and myosin X, are also found in hair-cell-containing epithelia; in addition, primer sets used may have missed other classes; for example, homologous of myosins III and Vb. Additional cataloging of myosins mRNAs from frog sacculus and other auditory and vestibular tissues will no doubt identify additional myosin isozymes expressed in hair cells. Myosin molecules besides myosin VI and VIIa are likely to be essential for hair cells. Genes for several myosin isozymes, including those for myosins Iβ, Iγ, and VIIb, are predicted to map closely to the chromosomal locations of at least one human deafness gene (Hasson et al. 1996). Mutations in their hair-cell myosin isozymes may not manifest as human deafnesses if the isozyme in question is employed for other essential roles. Although distribution of myosin molecules within the auditory and vestibular systems is complicated, we believe that a specific, principal role can be ascribed to each isozyme. Myosin V is not found in hair cells and likely serves the same role in afferent nerve processes that it plays elsewhere in the nervous system. Myosin Ī probably mediates adaptation and may contribute to transport of proteins throughout stereocilia. Myosin VI appears to cross-link actin filaments within the cuticular plate and perhaps between the cuticular plate and stereociliary rootlets; mutations in myosin VI may disrupt these structures and prevent bundle formation. Myosin VIIa may actually hold stereocilia together, mutations in myosin VIIa may therefore cause stereocilia to splay apart, preventing proper assembly into a hair bundle. Myosin Iβ, VI, and VIIa are each at unusually high concentrations in bundles of the most newly formed hair cells, usually at the sensory epithelium's periphery (Gillespie et al. 1993; T. Hasson et al., in prep.); these three isozymes may therefore play additional, more specialized roles during hair-bundle development (Tilney et al. 1992). In the future, we expect that exciting results will come from experiments designed to test these predictions. A key conclusion from hair- cell myosin localization is that these myosin molecules are not necessarily concentrated in regions of high actin density. Despite the similarity of their catalytic domains, each of the myosin isozymes found in hair cells has a distinctive subcellular localization. Furthermore, each hair-cell myosin isozyme appears in multiple specific locations. The ability of myosin molecules to bind to and translocate along actin filaments must therefore be heavily regulated; for dictating location, the affinity of myosin molecules for other proteins may be just as important as their affinity for actin. The nonhomologous tail domains of each class must contribute substantially to this differential localization. A critical goal for the field is therefore identification of hair-cell proteins that bind to myosin molecules, because their localization and abundance relative to each myosin isozyme may establish the final destination of a given myosin molecule. Finally, we wonder why myosin molecules would be employed in capacities that appear, at first consideration, to be entirely structural. In particular, our hypothesized roles for myosins VI and VIIa-maintaining cuticular-plate and bundle structural integrity-do not require force-producing molecules. Proteins that simply cross-link actin or connect actin to specific membrane receptors would seem to function adequately. Because the cell expends considerable energy to transcribe and translate relatively large myosin motor domains instead of smaller actin-binding structures, it seems likely that actin-activated ATPase plays a fundamental role in the behavior of these myosin isozymes. Because myosins VI and VIIa are not simply targeted to distal ends of actin filaments, but are instead found in association with specific actin-filament domains, the role of the motor activity must be relatively subtle. It seems likely that by understanding this paradox, we will understand the precise role of each myosin isozyme in the hair cell.",scopus,2-s2.0-0030428855,10.1101/sqb.1996.061.01.034
preventing autosomaldominant hearing loss in bth mice with crisprcasrxbased rna editing,"3. Signal Transduct Target Ther. 2022 Mar 14;7(1):79. doi: 10.1038/s41392-022-00893-4.Preventing autosomal-dominant hearing loss in Bth mice with CRISPR/CasRx-based RNA editing.Zheng Z(#)(1)(2)(3), Li G(#)(4)(5), Cui C(#)(1)(2)(3), Wang F(1)(2)(3), Wang X(6), Xu Z(1)(2)(3), Guo H(1)(2)(3), Chen Y(1)(2)(3), Tang H(1)(2)(3), Wang D(1)(2)(3), Huang M(7)(8), Chen ZY(7)(8), Huang X(9)(10), Li H(1)(2)(3)(11), Li GL(12)(13)(14), Hu X(15)(16), Shu Y(17)(18)(19).Author information:(1)ENT Institute and Department of Otorhinolaryngology, Eye & ENT Hospital, State Key Laboratory of Medical Neurobiology and MOE Frontiers Center for Brain Science, Fudan University, Shanghai, 200031, China.(2)Institutes of Biomedical Sciences, Fudan University, Shanghai, 200032, China.(3)NHC Key Laboratory of Hearing Medicine (Fudan University), Shanghai, 200032, China.(4)State Key Laboratory of Agrobiotechnology, China Agricultural University, Beijing, 100193, China.(5)College of Biological Sciences, China Agricultural University, Beijing, 100193, China.(6)School of Life Science and Technology, Southeast University, Nanjing, 210096, China.(7)Department of Otolaryngology-Head and Neck Surgery, Graduate Program in Speech and Hearing Bioscience and Technology and Program in Neuroscience, Harvard Medical School, Boston, MA, 02115, USA.(8)Eaton-Peabody Laboratory, Massachusetts Eye and Ear Infirmary, Boston, MA, 02114, USA.(9)School of Life Science and Technology, ShanghaiTech University, Shanghai, 200031, China.(10)CAS Center for Excellence in Molecular Cell Science, Shanghai Institute of Biochemistry and Cell Biology, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Shanghai, 200031, China.(11)Institutes of Brain Science and the Collaborative Innovation Center for Brain Science, Fudan University, Shanghai, 200031, China.(12)ENT Institute and Department of Otorhinolaryngology, Eye & ENT Hospital, State Key Laboratory of Medical Neurobiology and MOE Frontiers Center for Brain Science, Fudan University, Shanghai, 200031, China. genglin.li@fdeent.org.(13)Institutes of Biomedical Sciences, Fudan University, Shanghai, 200032, China. genglin.li@fdeent.org.(14)NHC Key Laboratory of Hearing Medicine (Fudan University), Shanghai, 200032, China. genglin.li@fdeent.org.(15)State Key Laboratory of Agrobiotechnology, China Agricultural University, Beijing, 100193, China. huxx@cau.edu.cn.(16)College of Biological Sciences, China Agricultural University, Beijing, 100193, China. huxx@cau.edu.cn.(17)ENT Institute and Department of Otorhinolaryngology, Eye & ENT Hospital, State Key Laboratory of Medical Neurobiology and MOE Frontiers Center for Brain Science, Fudan University, Shanghai, 200031, China. yilai_shu@fudan.edu.cn.(18)Institutes of Biomedical Sciences, Fudan University, Shanghai, 200032, China. yilai_shu@fudan.edu.cn.(19)NHC Key Laboratory of Hearing Medicine (Fudan University), Shanghai, 200032, China. yilai_shu@fudan.edu.cn.(#)Contributed equallyCRISPR/RfxCas13d (CasRx) editing system can specifically and precisely cleave single-strand RNAs, which is a promising treatment for various disorders by downregulation of related gene expression. Here, we tested this RNA-editing approach on Beethoven (Bth) mice, an animal model for human DFNA36 due to a point mutation in Tmc1. We first screened 30 sgRNAs in cell cultures and found that CasRx with sgRNA3 reduced the Tmc1Bth transcript by 90.8%, and the Tmc1 wild type transcript (Tmc1+) by 44.3%. We then injected a newly developed AAV vector (AAV-PHP.eB) based CasRx into the inner ears of neonatal Bth mice, and we found that Tmc1Bth was reduced by 70.2% in 2 weeks with few off-target effects in the whole transcriptome. Consistently, we found improved hair cell survival, rescued hair bundle degeneration, and reduced mechanoelectrical transduction current. Importantly, the hearing performance, measured in both ABR and DPOAE thresholds, was improved significantly in all ages over 8 weeks. We, therefore, have validated the CRISPR/CasRx-based RNA editing strategy in treating autosomal-dominant hearing loss, paving way for its further application in many other hereditary diseases in hearing and beyond.© 2022. The Author(s).DOI: 10.1038/s41392-022-00893-4PMCID: PMC8918553",pubmed,35283480,10.1038/s41392-022-00893-4
cnr2 is important for ribbon synapse maturation and function in hair cells and photoreceptors,"The role of the cannabinoid receptor 2 (CNR2) is still poorly described in sensory epithelia. We found strong cnr2 expression in hair cells (HCs) of the inner ear and the lateral line (LL), a superficial sensory structure in fish. Next, we demonstrated that sensory synapses in HCs were severely perturbed in larvae lacking cnr2. Appearance and distribution of presynaptic ribbons and calcium channels (Cav1.3) were profoundly altered in mutant animals. Clustering of membrane-associated guanylate kinase (MAGUK) in post-synaptic densities (PSDs) was also heavily affected, suggesting a role for cnr2 for maintaining the sensory synapse. Furthermore, vesicular trafficking in HCs was strongly perturbed suggesting a retrograde action of the endocannabinoid system (ECs) via cnr2 that was modulating HC mechanotransduction. We found similar perturbations in retinal ribbon synapses. Finally, we showed that larval swimming behaviors after sound and light stimulations were significantly different in mutant animals. Thus, we propose that cnr2 is critical for the processing of sensory information in the developing larva. © Copyright © 2021 Colón-Cruz, Rodriguez-Morales, Santana-Cruz, Cantres-Velez, Torrado-Tapias, Lin, Yudowski, Kensler, Marie, Burgess, Renaud, Varshney and Behra.",scopus,2-s2.0-85105357639,10.3389/fnmol.2021.624265
categorization of common sounds by cochlear implanted and normal hearing adults,"410. Hear Res. 2016 May;335:207-219. doi: 10.1016/j.heares.2016.03.007. Epub 2016 Apr 2.Categorization of common sounds by cochlear implanted and normal hearing adults.Collett E(1), Marx M(2), Gaillard P(3), Roby B(2), Fraysse B(4), Deguine O(2), Barone P(5).Author information:(1)Université de Toulouse, CerCo UMR 5549 CNRS, Université Paul Sabatier, Toulouse, France; Université de Toulouse, CerCo UMR 5549 CNRS, Faculté de Médecine de Purpan, Toulouse, France; Advanced Bionics SARL, France.(2)Université de Toulouse, CerCo UMR 5549 CNRS, Université Paul Sabatier, Toulouse, France; Université de Toulouse, CerCo UMR 5549 CNRS, Faculté de Médecine de Purpan, Toulouse, France; Service d'Oto-Rhino-Laryngologie et Oto-Neurologie, Hopital Purpan, Toulouse, France.(3)Université de Toulouse, CLLE UMR 5263, CNRS, UT2J, Université de Toulouse Jean-Jaurès, Toulouse, France.(4)Service d'Oto-Rhino-Laryngologie et Oto-Neurologie, Hopital Purpan, Toulouse, France.(5)Université de Toulouse, CerCo UMR 5549 CNRS, Université Paul Sabatier, Toulouse, France; Université de Toulouse, CerCo UMR 5549 CNRS, Faculté de Médecine de Purpan, Toulouse, France. Electronic address: Pascal.barone@cerco.ups-tlse.fr.Auditory categorization involves grouping of acoustic events along one or more shared perceptual dimensions which can relate to both semantic and physical attributes. This process involves both high level cognitive processes (categorization) and low-level perceptual encoding of the acoustic signal, both of which are affected by the use of a cochlear implant (CI) device. The goal of this study was twofold: I) compare the categorization strategies of CI users and normal hearing listeners (NHL) II) investigate if any characteristics of the raw acoustic signal could explain the results. 16 experienced CI users and 20 NHL were tested using a Free-Sorting Task of 16 common sounds divided into 3 predefined categories of environmental, musical and vocal sounds. Multiple Correspondence Analysis (MCA) and Hierarchical Clustering based on Principal Components (HCPC) show that CI users followed a similar categorization strategy to that of NHL and were able to discriminate between the three different types of sounds. However results for CI users were more varied and showed less inter-participant agreement. Acoustic analysis also highlighted the average pitch salience and average autocorrelation peak as being important for the perception and categorization of the sounds. The results therefore show that on a broad level of categorization CI users may not have as many difficulties as previously thought in discriminating certain kinds of sound; however the perception of individual sounds remains challenging.Copyright © 2016 Elsevier B.V. All rights reserved.DOI: 10.1016/j.heares.2016.03.007",pubmed,27050944,10.1016/j.heares.2016.03.007
simultaneous zygotic inactivation of multiple genes in mouse through crisprcas9mediated base editing,"In vivo genetic mutation has become a powerful tool for dissecting gene function; however, multi-gene interaction and the compensatory mechanisms involved can make findings from single mutations, at best difficult to interpret, and, at worst, misleading. Hence, it is necessary to establish an efficient way to disrupt multiple genes simultaneously. CRISPR/Cas9-mediated base editing disrupts gene function by converting a protein-coding sequence into a stop codon; this is referred to as CRISPR-stop. Its application in generating zygotic mutations has not been well explored yet. Here, we first performed a proof-of-principle test by disrupting Atoh1, a gene crucial for auditory hair cell generation. Next, we individually mutated vGlut3 (Slc17a8), otoferlin (Otof) and prestin (Slc26a5), three genes needed for normal hearing function. Finally, we successfully disrupted vGlut3, Otof and prestin simultaneously. Our results show that CRISPR-stop can efficiently generate single or triple homozygous F0 mouse mutants, bypassing laborious mouse breeding. We believe that CRISPR-stop is a powerful method that will pave the way for high-throughput screening of mouse developmental and functional genes, matching the efficiency of methods available for model organisms such as Drosophila. © 2018. Published by The Company of Biologists Ltd.",scopus,2-s2.0-85055075855,10.1242/dev.168906
quantitative changes in calretinin immunostaining in the cochlear nuclei after unilateral cochlear removal in young ferrets,"629. J Comp Neurol. 2005 Mar 21;483(4):458-75. doi: 10.1002/cne.20437.Quantitative changes in calretinin immunostaining in the cochlear nuclei after unilateral cochlear removal in young ferrets.Fuentes-Santamaria V(1), Alvarado JC, Taylor AR, Brunso-Bechtold JK, Henkel CK.Author information:(1)Department of Neurobiology and Anatomy, Wake Forest University School of Medicine, Winston-Salem, North Carolina 27157-1010, USA. vfuentes@fubmc.eduNeurons of the cochlear nuclei receive axosomatic endings from primary afferent fibers from the cochlea and have projections that diverge to form parallel ascending auditory pathways. These cells are characterized by neurochemical phenotypes such as levels of calretinin. To test whether or not early deafferentation results in changes in calretinin immunostaining in the cochlear nucleus, unilateral cochlear ablations were performed in ferrets soon after hearing onset (postnatal day [P]30-P40). Two months later, changes in calretinin immunostaining as well as cell size, volume, and synaptophysin immunostaining were assessed in the anteroventral (AVCN), posteroventral (PVCN), and dorsal cochlear nucleus (DCN). A decrease in calretinin immunostaining was evident ipsilaterally within the AVCN and PVCN but not in the DCN. Further analysis revealed a decrease both in the calretinin-immunostained neuropil and in the calretinin-immunostained area within AVCN and PVCN neurons. These declines were accompanied by significant ipsilateral decreases in volume as well as neuron area in the AVCN and PVCN compared with the contralateral cochlear nucleus and unoperated animals, but not compared with the DCN. In addition, there was a significant contralateral increase in calretinin-immunostained area within AVCN and PVCN neurons compared with control animals. Finally, a decrease in area of synaptophysin immunostaining in both the ipsilateral AVCN and PVCN without changes in the number of boutons was found. The present data demonstrate that unilateral cochlear ablation leads to 1) decreased immunostaining of the neuropil in the AVCN and PVCN ipsilaterally, 2) decreased calretinin immunostaining within AVCN and PVCN neurons ipsilaterally, 3) synaptogenesis in the AVCN and PVCN ipsilaterally, and 4) increased calretinin immunostaining within AVCN and PVCN neurons contralaterally.Copyright 2005 Wiley-Liss, Inc.DOI: 10.1002/cne.20437PMCID: PMC1913210",pubmed,15700274,10.1002/cne.20437
cochlear implantation in auditory neuropathy spectrum disorders role of transtympanic electrically evoked auditory brainstem responses and serial neural response telemetry,"Objective: To evaluate the utility of pre-operative transtympanic electrically evoked auditory brainstem responses and post-operative neural response telemetry in auditory neuropathy spectrum disorder patients. Methods: Four auditory neuropathy spectrum disorder patients who had undergone cochlear implantation and used it for more than one year were studied. All four patients underwent pre-operative transtympanic electrically evoked auditory brainstem response testing, intra-operative and post-operative (at 3, 6 and 12 months after switch-on) neural response telemetry, and out-patient cochlear implant electrically evoked auditory brainstem response testing (at 12 months). Results: Patients with better waveforms on transtympanic electrically evoked auditory brainstem response testing showed superior performance after one year of implant use. Neural response telemetry and electrically evoked auditory brainstem response measures improved in all patients. Conclusion: Inferences related to cochlear implantation outcomes can be based on the waveform of transtympanic electrically evoked auditory brainstem responses. Robust transtympanic electrically evoked auditory brainstem responses suggest better performance. Improvements in electrically evoked auditory brainstem responses and neural response telemetry over time indicate that electrical stimulation is favourable in auditory neuropathy spectrum disorder patients. These measures provide an objective way to monitor changes and progress in auditory pathways following cochlear implantation.",cinahl,222151,10.1017/S0022215121001328
harnessing the power of artificial intelligence in otolaryngology and the communication sciences,"506. J Assoc Res Otolaryngol. 2022 Jun;23(3):319-349. doi: 10.1007/s10162-022-00846-2. Epub 2022 Apr 20.Harnessing the Power of Artificial Intelligence in Otolaryngology and the Communication Sciences.Wilson BS(1)(2)(3)(4)(5), Tucci DL(6)(7), Moses DA(8)(9), Chang EF(8)(9), Young NM(10)(11)(12), Zeng FG(13)(14)(15)(16)(17), Lesica NA(18), Bur AM(19), Kavookjian H(19), Mussatto C(19), Penn J(19), Goodwin S(19), Kraft S(19), Wang G(20), Cohen JM(6)(21), Ginsburg GS(22)(23)(24)(25)(26)(27), Dawson G(28)(29)(30), Francis HW(6).Author information:(1)Department of Head and Neck Surgery & Communication Sciences, Duke University School of Medicine, Durham, NC, 27710, USA. blake.wilson@duke.edu.(2)Duke Hearing Center, Duke University School of Medicine, Durham, NC, 27710, USA. blake.wilson@duke.edu.(3)Department of Electrical & Computer Engineering, Duke University, Durham, NC, 27708, USA. blake.wilson@duke.edu.(4)Department of Biomedical Engineering, Duke University, Durham, NC, 27708, USA. blake.wilson@duke.edu.(5)Department of Otolaryngology - Head & Neck Surgery, University of North Carolina, Chapel Hill, Chapel Hill, NC, 27599, USA. blake.wilson@duke.edu.(6)Department of Head and Neck Surgery & Communication Sciences, Duke University School of Medicine, Durham, NC, 27710, USA.(7)National Institute On Deafness and Other Communication Disorders, National Institutes of Health, Bethesda, MD, 20892, USA.(8)Department of Neurological Surgery, University of California, San Francisco, San Francisco, CA, 94143, USA.(9)UCSF Weill Institute for Neurosciences, University of California, San Francisco, San Francisco, CA, 94117, USA.(10)Division of Otolaryngology, Ann and Robert H. Lurie Childrens Hospital of Chicago, Chicago, IL, 60611, USA.(11)Department of Otolaryngology - Head and Neck Surgery, Northwestern University Feinberg School of Medicine, Chicago, IL, 60611, USA.(12)Department of Communication, Knowles Hearing Center, Northwestern University, Evanston, IL, 60208, USA.(13)Center for Hearing Research, University of California, Irvine, Irvine, CA, 92697, USA.(14)Department of Anatomy and Neurobiology, University of California, Irvine, Irvine, CA, 92697, USA.(15)Department of Biomedical Engineering, University of California, Irvine, Irvine, CA, 92697, USA.(16)Department of Cognitive Sciences, University of California, Irvine, Irvine, CA, 92697, USA.(17)Department of Otolaryngology - Head and Neck Surgery, University of California, Irvine, CA, 92697, USA.(18)UCL Ear Institute, University College London, London, WC1X 8EE, UK.(19)Department of Otolaryngology - Head and Neck Surgery, Medical Center, University of Kansas, Kansas City, KS, 66160, USA.(20)Department of Computer Science, Ryerson University, Toronto, ON, M5B 2K3, Canada.(21)ENT Department, Kaplan Medical Center, 7661041, Rehovot, Israel.(22)Department of Biomedical Engineering, Duke University, Durham, NC, 27708, USA.(23)MEDx (Medicine & Engineering at Duke), Duke University, Durham, NC, 27708, USA.(24)Center for Applied Genomics & Precision Medicine, Duke University School of Medicine, Durham, NC, 27710, USA.(25)Department of Medicine, Duke University School of Medicine, Durham, NC, 27710, USA.(26)Department of Pathology, Duke University School of Medicine, Durham, NC, 27710, USA.(27)Department of Biostatistics and Bioinformatics, Duke University School of Medicine, Durham, NC, 27710, USA.(28)Duke Institute for Brain Sciences, Duke University, Durham, NC, 27710, USA.(29)Duke Center for Autism and Brain Development, Duke University School of Medicine and the Duke Institute for Brain Sciences, NIH Autism Center of Excellence, Durham, NC, 27705, USA.(30)Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Durham, NC, 27701, USA.Use of artificial intelligence (AI) is a burgeoning field in otolaryngology and the communication sciences. A virtual symposium on the topic was convened from Duke University on October 26, 2020, and was attended by more than 170 participants worldwide. This review presents summaries of all but one of the talks presented during the symposium; recordings of all the talks, along with the discussions for the talks, are available at https://www.youtube.com/watch?v=ktfewrXvEFg and https://www.youtube.com/watch?v=-gQ5qX2v3rg . Each of the summaries is about 2500 words in length and each summary includes two figures. This level of detail far exceeds the brief summaries presented in traditional reviews and thus provides a more-informed glimpse into the power and diversity of current AI applications in otolaryngology and the communication sciences and how to harness that power for future applications.© 2022. The Author(s) under exclusive licence to Association for Research in Otolaryngology.DOI: 10.1007/s10162-022-00846-2PMCID: PMC9086071",pubmed,35441936,10.1007/s10162-022-00846-2
mannitol protects hair cells against tumor necrosis factor induced loss,"768. Otol Neurotol. 2012 Dec;33(9):1656-63. doi: 10.1097/MAO.0b013e31826bedd9.Mannitol protects hair cells against tumor necrosis factor α-induced loss.Infante EB(1), Channer GA, Telischi FF, Gupta C, Dinh JT, Vu L, Eshraghi AA, Van De Water TR.Author information:(1)Cochlear Implant Research Program, University of Miami Ear Institute, Miami, Florida 33136-1015, USA.HYPOTHESIS: Mannitol has otoprotective effects against tumor necrosis factor (TNF) α-induced auditory hair cell (HC) loss.BACKGROUND: Mannitol has been demonstrated to possess cytoprotective effects in several organ systems. Its protective effect on postischemic hearing loss has also been shown. Mannitol's otoprotective mechanism and site of action are at present unknown.MATERIALS AND METHODS: Organ of Corti (OC) explants were dissected from 3 day-old rat pups. The safety (nonototoxicity) of mannitol was assessed at 4 different concentrations (1-100 mM). Three experimental arms were designed including: a control group, TNFα group, and TNFα + mannitol group. Cell viability was determined by counts of fluorescein isothiocyanate (FITC) phalloidin stained HC. Immunofluorescence assay of phospho-c-Jun and the proapoptotic mediators, cleaved caspase-3, apoptosis inducing factor (AIF), and endonuclease G (Endo G) were performed.RESULTS: Analysis of HC density confirmed the safety of mannitol at concentration ranges of 1 to 100 mM. The ototoxic effect of TNFα was demonstrated (p < 0.05). The otoprotective effect of 100 mM mannitol in TNFα-challenged OC explants was also demonstrated (p < 0.001). Mannitol treatment reduced the high levels of phospho-c-Jun observed in the TNFα-challenged group. AIF cluster formation and EndoG translocation into the nuclei of HCs were also reduced by mannitol treatment.CONCLUSION: Mannitol significantly reduces the ototoxic effects of TNFα against auditory HC's potentially by inhibiting c-Jun N terminal kinase (JNK) activation pathway and AIF, EndoG nuclear translocation. This local otoprotective effect may have therapeutic implications in inner ear surgery, for example, cochlear implants, protection of residual hearing, as well as implications for postischemic inner ear insults.DOI: 10.1097/MAO.0b013e31826bedd9",pubmed,22996158,10.1097/MAO.0b013e31826bedd9
a mouse model of mir96 mir182 and mir183 misexpression implicates mirnas in cochlear cell fate and homeostasis,"308. Sci Rep. 2018 Feb 23;8(1):3569. doi: 10.1038/s41598-018-21811-1.A mouse model of miR-96, miR-182 and miR-183 misexpression implicates miRNAs in cochlear cell fate and homeostasis.Weston MD(1), Tarang S(2), Pierce ML(3), Pyakurel U(2), Rocha-Sanchez SM(2), McGee J(4), Walsh EJ(4), Soukup GA(5).Author information:(1)Department of Oral Biology, School of Dentistry, Creighton University, 780729 California Plaza, Omaha, NE 68178-0729, USA. michaelweston@creighton.edu.(2)Department of Oral Biology, School of Dentistry, Creighton University, 780729 California Plaza, Omaha, NE 68178-0729, USA.(3)Department of Pharmacology, School of Medicine, Creighton University, 2500 California Plaza, Omaha, NE 68178, USA.(4)Developmental Auditory Physiology Laboratory, Boys Town National Research Hospital, 555 North 30th Street, Omaha, NE 68131, USA.(5)Department of Biomedical Sciences, School of Medicine, Creighton University, 2500 California Plaza, Omaha, NE 68178, USA.Germline mutations in Mir96, one of three co-expressed polycistronic miRNA genes (Mir96, Mir182, Mir183), cause hereditary hearing loss in humans and mice. Transgenic FVB/NCrl- Tg(GFAP-Mir183,Mir96,Mir182)MDW1 mice (Tg1MDW), which overexpress this neurosensory-specific miRNA cluster in the inner ear, were developed as a model system to identify, in the aggregate, target genes and biologic processes regulated by the miR-183 cluster. Histological assessments demonstrate Tg1MDW/1MDW homozygotes have a modest increase in cochlear inner hair cells (IHCs). Affymetrix mRNA microarray data analysis revealed that downregulated genes in P5 Tg1MDW/1MDW cochlea are statistically enriched for evolutionarily conserved predicted miR-96, miR-182 or miR-183 target sites. ABR and DPOAE tests from 18 days to 3 months of age revealed that Tg1MDW/1MDW homozygotes develop progressive neurosensory hearing loss that correlates with histologic assessments showing massive losses of both IHCs and outer hair cells (OHCs). This mammalian miRNA misexpression model demonstrates a potency and specificity of cochlear homeostasis for one of the dozens of endogenously co-expressed, evolutionally conserved, small non-protein coding miRNA families. It should be a valuable tool to predict and elucidate miRNA-regulated genes and integrated functional gene expression networks that significantly influence neurosensory cell differentiation, maturation and homeostasis.DOI: 10.1038/s41598-018-21811-1PMCID: PMC5824881",pubmed,29476110,10.1038/s41598-018-21811-1
channelrhodopsin fluorescent tag replacement for clinical translation of optogenetic hearing restoration,"816. Mol Ther Methods Clin Dev. 2023 Mar 21;29:202-212. doi: 10.1016/j.omtm.2023.03.009. eCollection 2023 Jun 8.Channelrhodopsin fluorescent tag replacement for clinical translation of optogenetic hearing restoration.Zerche M(1)(2)(3)(4), Wrobel C(2)(5), Kusch K(1)(6), Moser T(1)(7)(8)(4), Mager T(1)(3)(4).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37075 Göttingen, Germany.(2)Department of Otolaryngology, University Medical Center Göttingen, 37075 Göttingen, Germany.(3)Advanced Optogenes Group, Institute for Auditory Neuroscience, University Medical Center Göttingen, 37075 Göttingen, Germany.(4)Cluster of Excellence ""Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells"" (MBExC), University of Göttingen, 37075 Göttingen, Germany.(5)Translational Inner Ear Research Group, InnerEarLab, University Medical Center Göttingen, 37075 Göttingen, Germany.(6)Functional Auditory Genomics, Institute for Auditory Neuroscience, University Medical Center Göttingen, 37075 Göttingen, Germany.(7)Auditory Neuroscience and Optogenetics laboratory, German Primate Center, 37077 Göttingen, Germany.(8)Auditory Neuroscience and Synaptic Nanophysiology Group, Max-Planck-Institute for Multidisciplinary Sciences, 37075 Göttingen, Germany.Sensory restoration by optogenetic neurostimulation provides a promising future alternative to current electrical stimulation approaches. So far, channelrhodopsins (ChRs) typically contain a C-terminal fluorescent protein (FP) tag for visualization that potentially poses an additional risk for clinical translation. Previous work indicated a reduction of optogenetic stimulation efficacy upon FP removal. Here, we further optimized the fast-gating, red-light-activated ChR f-Chrimson to achieve efficient optogenetic stimulation in the absence of the C-terminal FP. Upon FP removal, we observed a massive amplitude reduction of photocurrents in transfected cells in vitro and of optogenetically evoked activity of the adeno-associated virus (AAV) vector-transduced auditory nerve in mice in vivo. Increasing the AAV vector dose restored optogenetically evoked auditory nerve activity but was confounded by neural loss. Of various C-terminal modifications, we found the replacement of the FP by the Kir2.1 trafficking sequence (TSKir2.1) to best restore both photocurrents and optogenetically evoked auditory nerve activity with only mild neural loss few months after dosing. In conclusion, we consider f-Chrimson-TSKir2.1 to be a promising candidate for clinical translation of optogenetic neurostimulation such as by future optical cochlear implants.© 2023 The Authors.DOI: 10.1016/j.omtm.2023.03.009PMCID: PMC10111946",pubmed,37081855,10.1016/j.omtm.2023.03.009
extracellular vesicles from human multipotent stromal cells protect against hearing loss after noise trauma in vivo,"807. Clin Transl Med. 2020 Dec;10(8):e262. doi: 10.1002/ctm2.262.Extracellular vesicles from human multipotent stromal cells protect against hearing loss after noise trauma in vivo.Warnecke A(1), Harre J(1), Staecker H(2), Prenzler N(1), Strunk D(3), Couillard-Despres S(4)(5), Romanelli P(4), Hollerweger J(6), Lassacher T(6), Auer D(6), Pachler K(7), Wietzorrek G(8), Köhl U(9), Lenarz T(1), Schallmoser K(6)(10), Laner-Plamberger S(10), Falk CS(11), Rohde E(6)(10), Gimona M(6)(7).Author information:(1)Department of Otorhinolaryngology, Head and Neck Surgery, Hannover Medical School, Hannover, Germany.(2)Department of Otolaryngology, Head and Neck Surgery, University of Kansas School of Medicine, Kansas City, Kansas.(3)Institute of Experimental and Clinical Cell Therapy, Spinal Cord Injury and Tissue Regeneration Centre Salzburg (SCI-TReCS), Paracelsus Medical University, Salzburg, Austria.(4)Institute of Experimental Neuroregeneration, Spinal Cord Injury and Tissue Regeneration Centre Salzburg (SCI-TReCS), Paracelsus Medical University, Salzburg, Austria.(5)Austrian Cluster for Tissue Regeneration, Vienna, Austria.(6)GMP Unit, Spinal Cord Injury and Tissue Regeneration Centre Salzburg (SCI-TReCS), Paracelsus Medical University (PMU), Salzburg, Austria.(7)Research Program ""Nanovesicular Therapies,"", Paracelsus Medical University (PMU), Salzburg, Austria.(8)Institute of Molecular and Cellular Pharmacology, Medical University of Innsbruck, Innsbruck, Austria.(9)Institute of Cellular Therapeutics, Hannover Medical School and Clinical Immunology, University Leipzig, Fraunhofer Institute for Cell Therapy and Immunology, Leipzig, Germany.(10)Department of Transfusion Medicine, University Hospital, Salzburger Landeskliniken GesmbH (SALK) and Paracelsus Medical University (PMU), Salzburg, Austria.(11)Institute of Transplant Immunology, Hannover Medical School, Hannover, Germany.The lack of approved anti-inflammatory and neuroprotective therapies in otology has been acknowledged in the last decades and recent approaches are heralding a new era in the field. Extracellular vesicles (EVs) derived from human multipotent (mesenchymal) stromal cells (MSC) can be enriched in vesicular secretome fractions, which have been shown to exert effects (eg, neuroprotection and immunomodulation) of their parental cells. Hence, MSC-derived EVs may serve as novel drug candidates for several inner ear diseases. Here, we provide first evidence of a strong neuroprotective potential of human stromal cell-derived EVs on inner ear physiology. In vitro, MSC-EV preparations exerted immunomodulatory activity on T cells and microglial cells. Moreover, local application of MSC-EVs to the inner ear significantly attenuated hearing loss and protected auditory hair cells from noise-induced trauma in vivo. Thus, EVs derived from the vesicular secretome of human MSC may represent a next-generation biological drug that can exert protective therapeutic effects in a complex and nonregenerating organ like the inner ear.© 2020 The Authors. Clinical and Translational Medicine published by John Wiley & Sons Australia, Ltd on behalf of Shanghai Institute of Clinical Bioinformatics.DOI: 10.1002/ctm2.262PMCID: PMC7752163",pubmed,33377658,10.1002/ctm2.262
cochleaspecific deletion of cav13 calcium channels arrests inner hair cell differentiation and unravels pitfalls of conditional mouse models,"Inner hair cell (IHC) Cav1.3 Ca2+ channels are multifunctional channels mediating Ca2+ influx for exocytosis at ribbon synapses, the generation of Ca2+ action potentials in pre-hearing IHCs and gene expression. IHCs of deaf systemic Cav1.3-deficient (Cav1.3-/-) mice stay immature because they fail to up-regulate voltage- and Ca2+-activated K+ (BK) channels but persistently express small conductance Ca2+-activated K+ (SK2) channels. In pre-hearing wildtype mice, cholinergic neurons from the superior olivary complex (SOC) exert efferent inhibition onto spontaneously active immature IHCs by activating their SK2 channels. Because Cav1.3 plays an important role for survival, health and function of SOC neurons, SK2 channel persistence and lack of BK channels in systemic Cav1.3-/- IHCs may result from malfunctioning neurons of the SOC. Here we analyze cochlea-specific Cav1.3 knockout mice with green fluorescent protein (GFP) switch reporter function, Pax2::cre;Cacna1d-eGFPflex/flex and Pax2::cre;Cacna1d-eGFPflex/-. Profound hearing loss, lack of BK channels and persistence of SK2 channels in Pax2::cre;Cacna1d-eGFPflex/- mice recapitulated the phenotype of systemic Cav1.3-/- mice, indicating that in wildtype mice, regulation of SK2 and BK channel expression is independent of Cav1.3 expression in SOC neurons. In addition, we noticed dose-dependent GFP toxicity leading to death of basal coil IHCs of Pax2::cre;Cacna1d-eGFPflex/flex mice, likely because of high GFP concentration and small repair capacity. This and the slower time course of Pax2-driven Cre recombinase in switching two rather than one Cacna1d-eGFPflex allele lead us to study Pax2::cre;Cacna1d-eGFPflex/- mice. Notably, control Cacna1d-eGFPflex/- IHCs showed a significant reduction in Cav1.3 channel cluster sizes and currents, suggesting that the intronic construct interfered with gene translation or splicing. These pitfalls are likely to be a frequent problem of many genetically modified mice with complex or multiple gene-targeting constructs or fluorescent proteins. Great caution and appropriate controls are therefore required. © 2019 Eckrich, Hecker, Sorg, Blum, Fischer, Münkner, Wenzel, Schick and Engel.",scopus,2-s2.0-85068484668,10.3389/fncel.2019.00225
enhanced visual adaptation in cochlear implant users revealed by concurrent eegfnirs,"639. Neuroimage. 2017 Feb 1;146:600-608. doi: 10.1016/j.neuroimage.2016.09.033. Epub 2016 Sep 15.Enhanced visual adaptation in cochlear implant users revealed by concurrent EEG-fNIRS.Chen LC(1), Stropahl M(2), Schönwiesner M(3), Debener S(4).Author information:(1)Neuropsychology Lab, Department of Psychology, European Medical School, University of Oldenburg, Oldenburg, Germany. Electronic address: ling-chia.chen@uni-oldenburg.de.(2)Neuropsychology Lab, Department of Psychology, European Medical School, University of Oldenburg, Oldenburg, Germany.(3)International Laboratory for Brain, Music and Sound Research (BRAMS), Université de Montréal, Montréal, Québec, Canada.(4)Neuropsychology Lab, Department of Psychology, European Medical School, University of Oldenburg, Oldenburg, Germany; Cluster of Excellence Hearing4all, Germany; Research Center Neurosensory Science, University of Oldenburg, Germany.Previous studies have observed lower visual cortex activation for visual processing in cochlear implant (CI) users compared to normal hearing controls, while others reported enhanced visual speechreading abilities in CI users. The present work investigated whether lower visual cortical activation for visual processing can be explained by a more efficient visual sensory encoding in CI users. Specifically, we investigated whether CI users show enhanced stimulus-specific adaptation for visual stimuli compared to controls. Auditory sensory adaptation was also investigated to explore the sensory specificity of the predicted effect. Twenty post-lingually deafened adult CI users and twenty age-matched controls were presented with repeated visual and auditory stimuli during simultaneous acquisition of electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS). By integrating EEG and fNIRS signals we found significantly enhanced visual adaptation and lower visual cortex activation in CI users compared to controls. That is, responses to repeated visual stimuli decreased more prominently in CI users than in controls. The results suggest that CI users process visual stimuli more efficiently than controls.Copyright © 2016 Elsevier Inc. All rights reserved.DOI: 10.1016/j.neuroimage.2016.09.033",pubmed,27640748,10.1016/j.neuroimage.2016.09.033
rimbinding proteins are required for normal soundencoding at afferent inner hair cell synapses,"798. Front Mol Neurosci. 2021 Mar 23;14:651935. doi: 10.3389/fnmol.2021.651935. eCollection 2021.RIM-Binding Proteins Are Required for Normal Sound-Encoding at Afferent Inner Hair Cell Synapses.Krinner S(1)(2)(3), Predoehl F(1), Burfeind D(4), Vogl C(3)(4), Moser T(1)(2)(3)(5).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(2)Collaborative Research Center 1286, University of Göttingen, Göttingen, Germany.(3)Auditory Neuroscience Group, Max Planck Institute of Experimental Medicine, Göttingen, Germany.(4)Presynaptogenesis and Intracellular Transport in Hair Cells Group, Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, Göttingen, Germany.(5)Multiscale Bioimaging Cluster of Excellence, University of Göttingen, Göttingen, Germany.The afferent synapses between inner hair cells (IHC) and spiral ganglion neurons are specialized to faithfully encode sound with sub-millisecond precision over prolonged periods of time. Here, we studied the role of Rab3 interacting molecule-binding proteins (RIM-BP) 1 and 2 - multidomain proteins of the active zone known to directly interact with RIMs, Bassoon and Ca V 1.3 - in IHC presynaptic function and hearing. Recordings of auditory brainstem responses and otoacoustic emissions revealed that genetic disruption of RIM-BPs 1 and 2 in mice (RIM-BP1/2-/- ) causes a synaptopathic hearing impairment exceeding that found in mice lacking RIM-BP2 (RIM-BP2-/- ). Patch-clamp recordings from RIM-BP1/2-/- IHCs indicated a subtle impairment of exocytosis from the readily releasable pool of synaptic vesicles that had not been observed in RIM-BP2-/- IHCs. In contrast, the reduction of Ca2+-influx and sustained exocytosis was similar to that in RIMBP2-/- IHCs. We conclude that both RIM-BPs are required for normal sound encoding at the IHC synapse, whereby RIM-BP2 seems to take the leading role.Copyright © 2021 Krinner, Predoehl, Burfeind, Vogl and Moser.DOI: 10.3389/fnmol.2021.651935PMCID: PMC8044855",pubmed,33867935,10.3389/fnmol.2021.651935
clarin1 acts as a modulator of mechanotransduction activity and presynaptic ribbon assembly,"699. J Cell Biol. 2014 Nov 10;207(3):375-91. doi: 10.1083/jcb.201404016. Epub 2014 Nov 3.Clarin-1 acts as a modulator of mechanotransduction activity and presynaptic ribbon assembly.Ogun O(1), Zallocchi M(2).Author information:(1)Sensory Neuroscience Department, Boys Town National Research Hospital, Omaha, NE 68131.(2)Sensory Neuroscience Department, Boys Town National Research Hospital, Omaha, NE 68131 marisa.zallocchi@boystown.org.Clarin-1 is a four-transmembrane protein expressed by hair cells and photoreceptors. Mutations in its corresponding gene are associated with Usher syndrome type 3, characterized by late-onset and progressive hearing and vision loss in humans. Mice carrying mutations in the clarin-1 gene have hair bundle dysmorphology and a delay in synapse maturation. In this paper, we examined the expression and function of clarin-1 in zebrafish hair cells. We observed protein expression as early as 1 d postfertilization. Knockdown of clarin-1 resulted in inhibition of FM1-43 incorporation, shortening of the kinocilia, and mislocalization of ribeye b clusters. These phenotypes were fully prevented by co-injection with clarin-1 transcript, requiring its C-terminal tail. We also observed an in vivo interaction between clarin-1 and Pcdh15a. Altogether, our results suggest that clarin-1 is functionally important for mechanotransduction channel activity and for proper localization of synaptic components, establishing a critical role for clarin-1 at the apical and basal poles of hair cells.© 2014 Ogun and Zallocchi.DOI: 10.1083/jcb.201404016PMCID: PMC4226736",pubmed,25365995,10.1083/jcb.201404016
a crosssectional study of caregiver perceptions of congenital cytomegalovirus infection knowledge and attitudes about screening,"400. J Pediatr. 2020 Mar;218:151-156.e2. doi: 10.1016/j.jpeds.2019.12.005. Epub 2020 Jan 14.A Cross-Sectional Study of Caregiver Perceptions of Congenital Cytomegalovirus Infection: Knowledge and Attitudes about Screening.Diener ML(1), Shi K(2), Park AH(2).Author information:(1)Department of Family and Consumer Studies, University of Utah, Salt Lake City, UT. Electronic address: marissa.diener@fcs.utah.edu.(2)Department of Surgery, Division of Otolaryngology-Head and Neck Surgery, University of Utah, Salt Lake City, UT.OBJECTIVES: To understand caregiver knowledge of and attitudes toward congenital cytomegalovirus (cCMV) testing in Utah.STUDY DESIGN: We surveyed 365 caregivers whose children were being seen in an otolaryngology clinic at a tertiary pediatric hospital about their knowledge of and attitudes toward cCMV and cCMV screening. Descriptive statistics and cluster analysis were used to examine their responses.RESULTS: The majority of caregivers were unsure how cCMV was spread, the symptoms of cCMV, and why cCMV screening of infants was important. Most caregivers did not know that cCMV screening was required by law in Utah if an infant is referred after newborn hearing screening. A majority wanted to know if their child had cCMV even if asymptomatic and were willing to pay $20 for cCMV screening. Caregivers of children who had been tested for cCMV were significantly more likely to be strongly in favor of cCMV screening than expected by chance. Caregivers in the highly knowledgeable cluster were more likely to be strongly in favor of cCMV screening.CONCLUSIONS: Caregivers frequently were unaware of cCMV and its implications. Attitudes toward cCMV screening generally were positive. Education on epidemiology and impact of cCMV may benefit both prevention of infection and attitudes toward screening.Copyright © 2019 Elsevier Inc. All rights reserved.DOI: 10.1016/j.jpeds.2019.12.005",pubmed,31952844,10.1016/j.jpeds.2019.12.005
a nonneural mirna cluster mediates hearing via repression of two neural targets,"833. Genes Dev. 2023 Dec 26;37(21-24):1041-1051. doi: 10.1101/gad.351052.123.A nonneural miRNA cluster mediates hearing via repression of two neural targets.Zhang B(1), Duan H(1), Kavaler J(2), Wei L(1), Eberl DF(3), Lai EC(4).Author information:(1)Developmental Biology Program, Sloan Kettering Institute, New York, New York 10065, USA.(2)Department of Biology, Colby College, Waterville, Maine 04901, USA.(3)Department of Biology, University of Iowa, Iowa City, Iowa 52242, USA.(4)Developmental Biology Program, Sloan Kettering Institute, New York, New York 10065, USA; laie@mskcc.org.We show here that mir-279/996 are absolutely essential for development and function of Johnston's organ (JO), the primary proprioceptive and auditory organ in Drosophila Their deletion results in highly aberrant cell fate determination, including loss of scolopale cells and ectopic neurons, and mutants are electrophysiologically deaf. In vivo activity sensors and mosaic analyses indicate that these seed-related miRNAs function autonomously to suppress neural fate in nonneuronal cells. Finally, genetic interactions pinpoint two neural targets (elav and insensible) that underlie miRNA mutant JO phenotypes. This work uncovers how critical post-transcriptional regulation of specific miRNA targets governs cell specification and function of the auditory system.© 2023 Zhang et al.; Published by Cold Spring Harbor Laboratory Press.DOI: 10.1101/gad.351052.123PMCID: PMC10760640",pubmed,38110249,10.1101/gad.351052.123
ankrd24 organizes triobp to reinforce stereocilia insertion points,"The stereocilia rootlet is a key structure in vertebrate hair cells, anchoring stereocilia firmly into the cell’s cuticular plate and protecting them from overstimulation. Using superresolution microscopy, we show that the ankyrin-repeat protein ANKRD24 concentrates at the stereocilia insertion point, forming a ring at the junction between the lower and upper rootlets. Annular ANKRD24 continues into the lower rootlet, where it surrounds and binds TRIOBP-5, which itself bundles rootlet F-actin. TRIOBP-5 is mislocalized in Ankrd24KO/KO hair cells, and ANKRD24 no longer localizes with rootlets in mice lacking TRIOBP-5; exogenous DsRed–TRIOBP-5 restores endogenous ANKRD24 to rootlets in these mice. Ankrd24KO/KO mice show progressive hearing loss and diminished recovery of auditory function after noise damage, as well as increased susceptibility to overstimulation of the hair bundle. We propose that ANKRD24 bridges the apical plasma membrane with the lower rootlet, maintaining a normal distribution of TRIOBP-5. Together with TRIOBP-5, ANKRD24 organizes rootlets to enable hearing with long-term resilience. © 2022 Krey et al. This article is available under a Creative Commons License (Attribution 4.0 International, as described at https://creativecommons.org/licenses/by/4.0/).",scopus,2-s2.0-85124779761,10.1083/jcb.202109134
defective joint development and maintenance in gdf6related multiple synostoses syndrome,"Multiple synostoses syndromes (SYNS) are a group of rare genetic bone disorders characterized by multiple joint fusions. We previously reported an SYNS4-causing GDF6 c.1330 T > A (p.Tyr444Asn) mutation, which reduced Noggin-induced GDF6 inhibition and enhanced SMAD1/5/8 signaling. However, the mechanisms by which GDF6 gain-of-function mutation alters joint formation and the comprehensive molecular portraits of SYNS4 remain unclear. Herein, we introduce the p.Tyr443Asn (orthologous to the human GDF6 p.Tyr444Asn) mutation into the mouse Gdf6 locus and report the results of extensive phenotype analysis, joint development investigation, and transcriptome profiling of Gdf6 p.Tyr443Asn limb buds. Gdf6 p.Tyr443Asn knock-in mice recapitulated the morphological features of human SYNS4, showing joint fusion in the wrists, ankles, phalanges, and auditory ossicles. Analysis of mouse embryonic forelimbs demonstrated joint interzone formation defects and excess chondrogenesis in Gdf6 p.Tyr443Asn knock-in mice. Further, RNA sequencing of forelimb buds revealed enhanced bone formation and upregulated bone morphogenetic protein (BMP) signaling in mice carrying the Gdf6 p.Tyr443Asn mutation. Because tightly regulated BMP signaling is critical for skeletal development and joint morphogenesis, our study shows that enhancing GDF6 activity has a significant impact on both prenatal joint development and postnatal joint maintenance. © 2023 American Society for Bone and Mineral Research (ASBMR). © 2023 American Society for Bone and Mineral Research (ASBMR).",scopus,2-s2.0-85148510839,10.1002/jbmr.4785
categorization of everyday sounds by cochlear implanted children,"147. Sci Rep. 2019 Mar 5;9(1):3532. doi: 10.1038/s41598-019-39991-9.Categorization of everyday sounds by cochlear implanted children.Berland A(1)(2)(3), Collett E(1)(2), Gaillard P(3), Guidetti M(3), Strelnikov K(1)(2)(4), Cochard N(4), Barone P(5)(6), Deguine O(1)(2)(4).Author information:(1)UMR 5549, Faculté de Médecine Purpan, Centre National de la Recherche Scientifique, Toulouse, France.(2)Centre de Recherche Cerveau et Cognition, Université de Toulouse, Université Paul Sabatier, Toulouse, France.(3)Unité de Recherche Interdisciplinaire Octogone, EA4156, Laboratoire Cognition, Communication et Développement, Université de Toulouse Jean-Jaurès, Toulouse, France.(4)Faculté de Médecine de Purpan, Toulouse, France; Service d'Oto-Rhino-Laryngologie et Oto-Neurologie, Hopital Purpan, Toulouse, France.(5)UMR 5549, Faculté de Médecine Purpan, Centre National de la Recherche Scientifique, Toulouse, France. Pascal.barone@cnrs.fr.(6)Centre de Recherche Cerveau et Cognition, Université de Toulouse, Université Paul Sabatier, Toulouse, France. Pascal.barone@cnrs.fr.Auditory categorization is an important process in the perception and understanding of everyday sounds. The use of cochlear implants (CIs) may affect auditory categorization and result in poor abilities. The current study was designed to compare how children with normal hearing (NH) and children with CIs categorize a set of everyday sounds. We tested 24 NH children and 24 children with CI on a free-sorting task of 18 everyday sounds corresponding to four a priori categories: nonlinguistic human vocalizations, environmental sounds, musical sounds, and animal vocalizations. Multiple correspondence analysis revealed considerable variation within both groups of child listeners, although the human vocalizations and musical sounds were similarly categorized. In contrast to NH children, children with CIs categorized some sounds according to their acoustic content rather than their associated semantic information. These results show that despite identification deficits, children with CIs are able to categorize environmental and vocal sounds in a similar way to NH children, and are able to use categorization as an adaptive process when dealing with everyday sounds.DOI: 10.1038/s41598-019-39991-9PMCID: PMC6401047",pubmed,30837546,10.1038/s41598-019-39991-9
3d printing of a baha protective cap,"566. Ear Nose Throat J. 2021 Jun;100(3_suppl):204S-206S. doi: 10.1177/0145561320987642. Epub 2021 Jan 18.3D Printing of a BAHA Protective Cap.Siu AKY(1)(2), Lee LPY(3), Leung SML(4).Author information:(1)Paediatric Otorhinolaryngology, Kowloon East Cluster, Hospital Authority, Hong Kong.(2)Department of Otorhinolaryngology, Head and Neck Surgery, the Chinese University of Hong Kong, Hong Kong.(3)Department of Paediatrics and Adolescent Medicine, 36621United Christian Hospital, Kowloon East Cluster, Hospital Authority, Hong Kong.(4)Occupational Therapy, 36621United Christian Hospital, Kowloon East Cluster, Hospital Authority, Hong Kong.Mechanical feedback is one of the most common difficulties encountered when fitting hearing aids for toddlers and young children. We described the use of 3D printing to tailor a protective cap for a toddler with bilateral microtia/canal atresia to facilitate bone-anchoring hearing aid use.DOI: 10.1177/0145561320987642",pubmed,33459563,10.1177/0145561320987642
editorial digital hearing healthcare,"765. Front Digit Health. 2022 Jul 13;4:959761. doi: 10.3389/fdgth.2022.959761. eCollection 2022.Editorial: Digital hearing healthcare.Meng Q(1)(2), Chen J(3)(4), Zhang C(5), Wasmann JA(6), Barbour DL(7), Zeng FG(8).Author information:(1)Acoustics Laboratory, School of Physics and Optoelectronics, South China University of Technology, Guangzhou, China.(2)n3 Hearing Laboratory, Guangzhou, China.(3)Key Laboratory of Machine Perception (Ministry of Education), School of Artificial Intelligence, Speech and Hearing Research Center, Peking University, Beijing, China.(4)National Biomedical Imaging Center, College of Future Technology, Peking University, Beijing, China.(5)Faculty of Education, East China Normal University, Shanghai, China.(6)Department of Otorhinolaryngology, Donders Institute for Brain, Cognition and Behaviour, Radboud University Medical Center Nijmegen, Nijmegen, Netherlands.(7)Laboratory of Sensory Neuroscience and Neuroengineering, Department of Biomedical Engineering, Washington University in St. Louis, St. Louis, MO, United States.(8)Department of Otolaryngology - Head and Neck Surgery, Center for Hearing Research, University of California, Irvine, Irvine, CA, United States.Comment on    Editorial on the Research Topic Digital Hearing Healthcare.DOI: 10.3389/fdgth.2022.959761PMCID: PMC9326398",pubmed,35911617,10.3389/fdgth.2022.959761
maturation arrest in early postnatal sensory receptors by deletion of the mir18396182 cluster in mouse,"821. Proc Natl Acad Sci U S A. 2017 May 23;114(21):E4271-E4280. doi: 10.1073/pnas.1619442114. Epub 2017 May 8.Maturation arrest in early postnatal sensory receptors by deletion of the miR-183/96/182 cluster in mouse.Fan J(1), Jia L(2), Li Y(3), Ebrahim S(4), May-Simera H(5), Wood A(6), Morell RJ(7), Liu P(3), Lei J(3), Kachar B(4), Belluscio L(6), Qian H(8), Li T(5), Li W(9), Wistow G(10), Dong L(11).Author information:(1)Molecular Structure and Functional Genomics Section, National Eye Institute, NIH, Bethesda, MD 20892.(2)Retinal Neurophysiology Section, National Eye Institute, NIH, Bethesda, MD 20892.(3)Genetic Engineering Core, National Eye Institute, NIH, Bethesda, MD 20892.(4)Section on Structural Cell Biology, National Institute on Deafness and Other Communication Disorders, NIH, Bethesda, MD 20892.(5)Retinal Cell Biology and Degeneration, National Eye Institute, NIH, Bethesda, MD 20892.(6)Developmental Neural Plasticity Section, National Institute of Neurological Disorders and Stroke, NIH, Bethesda, MD 20892.(7)Laboratory of Molecular Genetics, National Institute on Deafness and Other Communication Disorders, NIH, Bethesda, MD 20892.(8)Visual Function Core, National Eye Institute, NIH, Bethesda, MD 20892.(9)Retinal Neurophysiology Section, National Eye Institute, NIH, Bethesda, MD 20892; graeme@helix.nih.gov liwei2@nei.nih.gov dongl@nei.nih.gov.(10)Molecular Structure and Functional Genomics Section, National Eye Institute, NIH, Bethesda, MD 20892; graeme@helix.nih.gov liwei2@nei.nih.gov dongl@nei.nih.gov.(11)Genetic Engineering Core, National Eye Institute, NIH, Bethesda, MD 20892; graeme@helix.nih.gov liwei2@nei.nih.gov dongl@nei.nih.gov.The polycistronic miR-183/96/182 cluster is preferentially and abundantly expressed in terminally differentiating sensory epithelia. To clarify its roles in the terminal differentiation of sensory receptors in vivo, we deleted the entire gene cluster in mouse germline through homologous recombination. The miR-183/96/182 null mice display impairment of the visual, auditory, vestibular, and olfactory systems, attributable to profound defects in sensory receptor terminal differentiation. Maturation of sensory receptor precursors is delayed, and they never attain a fully differentiated state. In the retina, delay in up-regulation of key photoreceptor genes underlies delayed outer segment elongation and possibly mispositioning of cone nuclei in the retina. Incomplete maturation of photoreceptors is followed shortly afterward by early-onset degeneration. Cell biologic and transcriptome analyses implicate dysregulation of ciliogenesis, nuclear translocation, and an epigenetic mechanism that may control timing of terminal differentiation in developing photoreceptors. In both the organ of Corti and the vestibular organ, impaired terminal differentiation manifests as immature stereocilia and kinocilia on the apical surface of hair cells. Our study thus establishes a dedicated role of the miR-183/96/182 cluster in driving the terminal differentiation of multiple sensory receptor cells.DOI: 10.1073/pnas.1619442114PMCID: PMC5448201",pubmed,28484004,10.1073/pnas.1619442114
otoferlin gene editing in sheep via crisprassisted ssodnmediated homology directed repair,"853. Sci Rep. 2020 Apr 7;10(1):5995. doi: 10.1038/s41598-020-62879-y.Otoferlin gene editing in sheep via CRISPR-assisted ssODN-mediated Homology Directed Repair.Menchaca A(1), Dos Santos-Neto PC(2), Souza-Neves M(2), Cuadro F(2), Mulet AP(3), Tesson L(4)(5), Chenouard V(4)(5), Guiffès A(4)(5), Heslan JM(4)(6), Gantier M(4)(6), Anegón I(7)(8)(9), Crispo M(10).Author information:(1)Instituto de Reproducción Animal Uruguay, Fundación IRAUy, Montevideo, Uruguay. menchaca.alejo@gmail.com.(2)Instituto de Reproducción Animal Uruguay, Fundación IRAUy, Montevideo, Uruguay.(3)Unidad de Animales Transgénicos y de Experimentación (UATE), Institut Pasteur de Montevideo, Montevideo, Uruguay.(4)Inserm, Centre de Recherche en Transplantation et Immunologie, UMR 1064, F-44000, Nantes, France.(5)Transgenesis Rat ImmunoPhenomic facility (TRIP), F-44000, Nantes, France.(6)GenoCellEdit facility, F-44000, Nantes, France.(7)Inserm, Centre de Recherche en Transplantation et Immunologie, UMR 1064, F-44000, Nantes, France. ianegon@nantes.inserm.fr.(8)Transgenesis Rat ImmunoPhenomic facility (TRIP), F-44000, Nantes, France. ianegon@nantes.inserm.fr.(9)GenoCellEdit facility, F-44000, Nantes, France. ianegon@nantes.inserm.fr.(10)Unidad de Animales Transgénicos y de Experimentación (UATE), Institut Pasteur de Montevideo, Montevideo, Uruguay. crispo@pasteur.edu.uy.Different mutations of the OTOF gene, encoding for otoferlin protein expressed in the cochlear inner hair cells, induces a form of deafness that is the major cause of nonsyndromic recessive auditory neuropathy spectrum disorder in humans. We report the generation of the first large animal model of OTOF mutations using the CRISPR system associated with different Cas9 components (mRNA or protein) assisted by single strand oligodeoxynucleotides (ssODN) to induce homology-directed repair (HDR). Zygote microinjection was performed with two sgRNA targeting exon 5 and 6 associated to Cas9 mRNA or protein (RNP) at different concentrations in a mix with an ssODN template targeting HDR in exon 5 containing two STOP sequences. A total of 73 lambs were born, 13 showing indel mutations (17.8%), 8 of which (61.5%) had knock-in mutations by HDR. Higher concentrations of Cas9-RNP induced targeted mutations more effectively, but negatively affected embryo survival and pregnancy rate. This study reports by the first time the generation of OTOF disrupted sheep, which may allow better understanding and development of new therapies for human deafness related to genetic disorders. These results support the use of CRISPR/Cas system assisted by ssODN as an effective tool for gene editing in livestock.DOI: 10.1038/s41598-020-62879-yPMCID: PMC7138848",pubmed,32265471,10.1038/s41598-020-62879-y
classification of complex signal patterns with artificial neural networks in hearing and voice diagnosis,496. Laryngorhinootologie. 2001 Jan;80(1):61-2. doi: 10.1055/s-2001-11035.[Classification of complex signal patterns with artificial neural networks in hearing and voice diagnosis].[Article in German]Schönweiler R(1).Author information:(1)Klinik und Poliklinik für Phoniatrie und Pädaudiologie Medizinische Hochschule Hannover Carl-Neuberg-Strasse 1 30625 Hannover.DOI: 10.1055/s-2001-11035,pubmed,11272251,10.1055/s-2001-11035
rab3interacting molecules 2 and 2 promote the abundance of voltagegated cav13 ca2 channels at hair cell active zones,"572. Proc Natl Acad Sci U S A. 2015 Jun 16;112(24):E3141-9. doi: 10.1073/pnas.1417207112. Epub 2015 Jun 1.Rab3-interacting molecules 2α and 2β promote the abundance of voltage-gated CaV1.3 Ca2+ channels at hair cell active zones.Jung S(1), Oshima-Takago T(2), Chakrabarti R(3), Wong AB(4), Jing Z(5), Yamanbaeva G(5), Picher MM(6), Wojcik SM(7), Göttfert F(8), Predoehl F(9), Michel K(10), Hell SW(11), Schoch S(10), Strenzke N(12), Wichmann C(13), Moser T(14).Author information:(1)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany; Center for Nanoscale Microscopy and Molecular Physiology of the Brain, University of Göttingen, 37099 Göttingen, Germany;(2)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany; Sensory and Motor Neuroscience Program, Göttingen Graduate School for Neurosciences, Biophysics and Molecular Biosciences, University of Göttingen, 37099 Göttingen, Germany;(3)Sensory and Motor Neuroscience Program, Göttingen Graduate School for Neurosciences, Biophysics and Molecular Biosciences, University of Göttingen, 37099 Göttingen, Germany; Molecular Architecture of Synapses Group, Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany; Collaborative Sensory Research Center 889, University of Göttingen, 37099 Göttingen, Germany;(4)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany; Sensory and Motor Neuroscience Program, Göttingen Graduate School for Neurosciences, Biophysics and Molecular Biosciences, University of Göttingen, 37099 Göttingen, Germany; Collaborative Sensory Research Center 889, University of Göttingen, 37099 Göttingen, Germany;(5)Sensory and Motor Neuroscience Program, Göttingen Graduate School for Neurosciences, Biophysics and Molecular Biosciences, University of Göttingen, 37099 Göttingen, Germany; Collaborative Sensory Research Center 889, University of Göttingen, 37099 Göttingen, Germany; Auditory Systems Physiology Group, InnerEarLab, Department of Otolaryngology, University of Göttingen Medical Center, 37099 Göttingen, Germany;(6)Sensory and Motor Neuroscience Program, Göttingen Graduate School for Neurosciences, Biophysics and Molecular Biosciences, University of Göttingen, 37099 Göttingen, Germany; Collaborative Sensory Research Center 889, University of Göttingen, 37099 Göttingen, Germany; Bernstein Center for Computational Neuroscience, University of Göttingen, 37073 Göttingen, Germany;(7)Department of Molecular Neurobiology, Max Planck Institute for Experimental Medicine, 37075 Göttingen, Germany;(8)Sensory and Motor Neuroscience Program, Göttingen Graduate School for Neurosciences, Biophysics and Molecular Biosciences, University of Göttingen, 37099 Göttingen, Germany; Department of Nanobiophotonics, Max Planck Institute for Biophysical Chemistry, 37077 Göttingen, Germany;(9)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany;(10)Institute of Neuropathology, University of Bonn, 53111 Bonn, Germany; Department of Epileptology, University of Bonn, 53111 Bonn, Germany.(11)Center for Nanoscale Microscopy and Molecular Physiology of the Brain, University of Göttingen, 37099 Göttingen, Germany; Department of Nanobiophotonics, Max Planck Institute for Biophysical Chemistry, 37077 Göttingen, Germany;(12)Collaborative Sensory Research Center 889, University of Göttingen, 37099 Göttingen, Germany; Auditory Systems Physiology Group, InnerEarLab, Department of Otolaryngology, University of Göttingen Medical Center, 37099 Göttingen, Germany; tmoser@gwdg.de NStrenzke@med.uni-goettingen.de cwichma@gwdg.de.(13)Molecular Architecture of Synapses Group, Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany; Collaborative Sensory Research Center 889, University of Göttingen, 37099 Göttingen, Germany; tmoser@gwdg.de NStrenzke@med.uni-goettingen.de cwichma@gwdg.de.(14)Institute for Auditory Neuroscience and InnerEarLab, University Medical Center Göttingen, 37099 Göttingen, Germany; Center for Nanoscale Microscopy and Molecular Physiology of the Brain, University of Göttingen, 37099 Göttingen, Germany; Collaborative Sensory Research Center 889, University of Göttingen, 37099 Göttingen, Germany; Bernstein Center for Computational Neuroscience, University of Göttingen, 37073 Göttingen, Germany; tmoser@gwdg.de NStrenzke@med.uni-goettingen.de cwichma@gwdg.de.Ca(2+) influx triggers the fusion of synaptic vesicles at the presynaptic active zone (AZ). Here we demonstrate a role of Ras-related in brain 3 (Rab3)-interacting molecules 2α and β (RIM2α and RIM2β) in clustering voltage-gated CaV1.3 Ca(2+) channels at the AZs of sensory inner hair cells (IHCs). We show that IHCs of hearing mice express mainly RIM2α, but also RIM2β and RIM3γ, which all localize to the AZs, as shown by immunofluorescence microscopy. Immunohistochemistry, patch-clamp, fluctuation analysis, and confocal Ca(2+) imaging demonstrate that AZs of RIM2α-deficient IHCs cluster fewer synaptic CaV1.3 Ca(2+) channels, resulting in reduced synaptic Ca(2+) influx. Using superresolution microscopy, we found that Ca(2+) channels remained clustered in stripes underneath anchored ribbons. Electron tomography of high-pressure frozen synapses revealed a reduced fraction of membrane-tethered vesicles, whereas the total number of membrane-proximal vesicles was unaltered. Membrane capacitance measurements revealed a reduction of exocytosis largely in proportion with the Ca(2+) current, whereas the apparent Ca(2+) dependence of exocytosis was unchanged. Hair cell-specific deletion of all RIM2 isoforms caused a stronger reduction of Ca(2+) influx and exocytosis and significantly impaired the encoding of sound onset in the postsynaptic spiral ganglion neurons. Auditory brainstem responses indicated a mild hearing impairment on hair cell-specific deletion of all RIM2 isoforms or global inactivation of RIM2α. We conclude that RIM2α and RIM2β promote a large complement of synaptic Ca(2+) channels at IHC AZs and are required for normal hearing.DOI: 10.1073/pnas.1417207112PMCID: PMC4475996",pubmed,26034270,10.1073/pnas.1417207112
first results on patient experiments with cinstim the southampton cochlear implantneural network stimulation framework,"500. Ann Otol Rhinol Laryngol Suppl. 1995 Sep;166:370-2.First results on patient experiments with CINSTIM: the Southampton Cochlear Implant-Neural Network stimulation framework.Leisenberg M(1), Southgate J.Author information:(1)University of Southampton, Institute for Sound and Vibration Research, England.",pubmed,7668711,
tactile captions augmenting visual captions,"We explore the efficacy of tactile captions as a supplement to online captioned video. Closed captions are not fully accessible, because many auditory signals are not easily represented by words, e.g., the sound of the ball being hit by a bat, or to describe a ring tone. The goal is to explore whether audiovisual information can be effectively represented through an equivalent tactile-visual interface. We compare viewers preferences between viewing video with captions alone, and captions plus tactile captions. Our study showed that viewers significantly preferred tactile captions to captions. © 2014 Springer International Publishing.",scopus,2-s2.0-84904152842,10.1007/978-3-319-08596-8_5
new type of corona virus induced acute otitis media in adult,"494. Am J Otolaryngol. 2020 May-Jun;41(3):102487. doi: 10.1016/j.amjoto.2020.102487. Epub 2020 Apr 16.New type of corona virus induced acute otitis media in adult.Fidan V(1).Author information:(1)Otorhinolaryngology Dept, Eskisehir City Hospital, Eskisehir, Turkey. Electronic address: vuralfidan@gmail.com.Since late December 2019, a new type of coronavirus (CIVID-19) causing a cluster of respiratory infections was first identified in Wuhan-China. And it disseminated to all countries. Generally, COVID-19 cases have fever, cough, respiratory distress findings (dyspnoea, intercostal retraction, cyanosis etc.). In this paper, we have presented an adult otitis media case whom infected with COVID-19, but she have not any classical COVID-19 symptoms.Copyright © 2020 Elsevier Inc. All rights reserved.DOI: 10.1016/j.amjoto.2020.102487PMCID: PMC7161479",pubmed,32336572,10.1016/j.amjoto.2020.102487
hearing aid technology modelbased concepts and assessment,"13. Int J Audiol. 2018 Jun;57(sup3):S1-S2. doi: 10.1080/14992027.2018.1426893. Epub 2018 Jan 17.Hearing aid technology: model-based concepts and assessment.Kollmeier B(1).Author information:(1)a Medizinische Physik and Cluster of Excellence ""Hearing4All"" , Universität Oldenburg , Oldenburg , Germany.DOI: 10.1080/14992027.2018.1426893",pubmed,29338464,10.1080/14992027.2018.1426893
rebuttal to neuroanatomical changes associated with agerelated hearing loss and listening effort,"275. Brain Struct Funct. 2021 Jun;226(5):1387-1388. doi: 10.1007/s00429-021-02263-2. Epub 2021 Apr 12.Rebuttal to: Neuroanatomical changes associated with age-related hearing loss and listening effort.Rosemann S(1)(2), Thiel C(3)(4).Author information:(1)Biological Psychology, Department of Psychology, Department for Medicine and Health Sciences, Carl-von-Ossietzky Universität Oldenburg, Ammerländer Heerstraße 114-118, 26111, Oldenburg, Germany.(2)Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, Ammerländer Heerstraße 114-118, 26111, Oldenburg, Germany.(3)Biological Psychology, Department of Psychology, Department for Medicine and Health Sciences, Carl-von-Ossietzky Universität Oldenburg, Ammerländer Heerstraße 114-118, 26111, Oldenburg, Germany. christiane.thiel@uni-oldenburg.de.(4)Cluster of Excellence ""Hearing4all"", Carl von Ossietzky Universität Oldenburg, Ammerländer Heerstraße 114-118, 26111, Oldenburg, Germany. christiane.thiel@uni-oldenburg.de.Comment on    Brain Struct Funct. 2020 Dec;225(9):2689-2700.DOI: 10.1007/s00429-021-02263-2PMCID: PMC8096735",pubmed,33844051,10.1007/s00429-021-02263-2
fdxr mutations cause sensorial neuropathies and expand the spectrum of mitochondrial fessynthesis diseases,"633. Am J Hum Genet. 2017 Oct 5;101(4):630-637. doi: 10.1016/j.ajhg.2017.09.007. Epub 2017 Sep 28.FDXR Mutations Cause Sensorial Neuropathies and Expand the Spectrum of Mitochondrial Fe-S-Synthesis Diseases.Paul A(1), Drecourt A(1), Petit F(1), Deguine DD(2), Vasnier C(3), Oufadem M(1), Masson C(1), Bonnet C(4), Masmoudi S(5), Mosnier I(6), Mahieu L(7), Bouccara D(6), Kaplan J(1), Challe G(8), Domange C(9), Mochel F(10), Sterkers O(6), Gerber S(1), Nitschke P(1), Bole-Feysot C(1), Jonard L(11), Gherbi S(12), Mercati O(12), Ben Aissa I(12), Lyonnet S(13), Rötig A(1), Delahodde A(3), Marlin S(14).Author information:(1)UMR 1163, Université Paris Descartes, Sorbonne Paris Cité, Institut IMAGINE, 24 Boulevard du Montparnasse, 75015 Paris, France.(2)Service de Génétique Médicale, Hôpital Purpan, 40031 Toulouse, France.(3)Institute for Integrative Biology of the Cell, Commissariat à l'Energie Atomique, Centre National de la Recherche Scientifique, Université Paris-Sud, Université Paris-Saclay, 91198, Gif-sur-Yvette Cedex, France.(4)UMRS 1120, Institut de la Vision, 75012 Paris, France.(5)Laboratory of Molecular and Cellular Screening Processes, Center of Biotechnology of Sfax, 1177 Sfax, Tunisia.(6)Service d'Oto-Rhino-Laryngologie, Hôpital Pitié-Salpêtrière, Assistance Publique-Hôpitaux de Paris, 75013 Paris, France.(7)Service d'Ophtalmologie, Hôpital Rangueil, 40031 Toulouse, France.(8)Service d'Ophtalmologie, Hôpital Pitié-Salpêtrière, Assistance Publique-Hôpitaux de Paris, 75013 Paris, France.(9)Service d' d'Oto-Rhino-Laryngologie, Hôpital Lariboisière, 75475 Paris, France.(10)Département de Génétique, Hôpital Pitié-Salpêtrière, Assistance Publique-Hôpitaux de Paris, 75013 Paris, France.(11)Service de Génétique, Laboratoire de Génétique Moléculaire, Hôpital Necker-Enfants Malades, Assistance Publique-Hôpitaux de Paris, 75015 Paris, France.(12)Centre de Référence des Surdités Génétiques, Service de Génétique, Hôpital Necker-Enfants Malades, Assistance Publique-Hôpitaux de Paris, 75015 Paris, France.(13)UMR 1163, Université Paris Descartes, Sorbonne Paris Cité, Institut IMAGINE, 24 Boulevard du Montparnasse, 75015 Paris, France; Service de Génétique, Hôpital Necker-Enfants Malades, Assistance Publique-Hôpitaux de Paris, 75015 Paris, France.(14)UMR 1163, Université Paris Descartes, Sorbonne Paris Cité, Institut IMAGINE, 24 Boulevard du Montparnasse, 75015 Paris, France; Centre de Référence des Surdités Génétiques, Service de Génétique, Hôpital Necker-Enfants Malades, Assistance Publique-Hôpitaux de Paris, 75015 Paris, France; Service de Génétique, Hôpital Necker-Enfants Malades, Assistance Publique-Hôpitaux de Paris, 75015 Paris, France. Electronic address: sandrine.marlin@aphp.fr.Hearing loss and visual impairment in childhood have mostly genetic origins, some of them being related to sensorial neuronal defects. Here, we report on eight subjects from four independent families affected by auditory neuropathy and optic atrophy. Whole-exome sequencing revealed biallelic mutations in FDXR in affected subjects of each family. FDXR encodes the mitochondrial ferredoxin reductase, the sole human ferredoxin reductase implicated in the biosynthesis of iron-sulfur clusters (ISCs) and in heme formation. ISC proteins are involved in enzymatic catalysis, gene expression, and DNA replication and repair. We observed deregulated iron homeostasis in FDXR mutant fibroblasts and indirect evidence of mitochondrial iron overload. Functional complementation in a yeast strain in which ARH1, the human FDXR ortholog, was deleted established the pathogenicity of these mutations. These data highlight the wide clinical heterogeneity of mitochondrial disorders related to ISC synthesis.Copyright © 2017 American Society of Human Genetics. Published by Elsevier Inc. All rights reserved.DOI: 10.1016/j.ajhg.2017.09.007PMCID: PMC5630197",pubmed,28965846,10.1016/j.ajhg.2017.09.007
preschool television programmes analysis using smartsound iq data logging,"556. Cochlear Implants Int. 2015 Jan;16 Suppl 1:S26-9. doi: 10.1179/1467010014Z.000000000229.Preschool television programmes: analysis using SmartSound IQ data logging.Hanvey K, DeBold L.DOI: 10.1179/1467010014Z.000000000229",pubmed,25614262,10.1179/1467010014Z.000000000229
editorial auditory perception and phantom perception in brains minds and machines,"697. Front Neurosci. 2023 Oct 6;17:1293552. doi: 10.3389/fnins.2023.1293552. eCollection 2023.Editorial: Auditory perception and phantom perception in brains, minds and machines.Schilling A(1)(2), Schaette R(3), Sedley W(4), Gerum RC(5), Maier A(6), Krauss P(1)(2).Author information:(1)Neuroscience Lab, University Hospital Erlangen, Erlangen, Germany.(2)Cognitive Computational Neuroscience Group, University Erlangen-Nürnberg, Erlangen, Germany.(3)UCL Ear Institute, University College London, London, United Kingdom.(4)Faculty of Medical Sciences, Newcastle University, Newcastle upon Tyne, United Kingdom.(5)Department of Physics and Astronomy, York University, Toronto, ON, Canada.(6)Pattern Recognition Lab, University Erlangen-Nürnberg, Erlangen, Germany.Comment on    Editorial on the Research Topic Auditory perception and phantom perception in brains, minds and machines.DOI: 10.3389/fnins.2023.1293552PMCID: PMC10588478",pubmed,37869508,10.3389/fnins.2023.1293552
a new concept for cochlear implant speech processing for prelingually deaf children,"498. Adv Otorhinolaryngol. 1995;50:96-101. doi: 10.1159/000424442.A new concept for cochlear implant speech processing for prelingually deaf children.Leisenberg M(1).Author information:(1)Southampton University, Institute for Sound and Vibration Research, Wessex Regional Audiology Centre, Hants, UK.DOI: 10.1159/000424442",pubmed,7610977,10.1159/000424442
AudiometricAI,,github,,None
Audiometer Calibration Checker,,github,,None
React + Vite,,github,,None
Tympanum,,github,,None
audiometric-services,,github,,None
audiometric-calculator,,github,,None
Audiometric-Testing-Interface,,github,,None
test-audiometrics,,github,,None
AudioMetrics,,github,,None
audiogram-erlang,,github,,None
Dispositivo de Alarme de Problemas no Setor de Tecnologia Atravs de Controle Audiomtrico Baseado em Arduno,,github,,None
SELF AUDIOMETRIC TEST,,github,,None
AudiogramDatabase,,github,,None
plot_audiogram,,github,,None
ThreADD,,github,,None
JAMA-OTO-Smoking,,github,,None
audiometry,,github,,None
audiometry,,github,,None
audiometry_mri,,github,,None
audiometry,,github,,None
audiometry-project,,github,,None
audiometry,,github,,None
Dhwani Sarathi,,github,,None
AudiometryTest,,github,,None
audiometry,,github,,None
AudiometryBinaryClassification,,github,,None
AudiometryMulticlassClassification,,github,,None
Tonal-Audiometry,,github,,None
AudiometryDataGeneration,,github,,None
Audiometry,,github,,None
Audiometry,,github,,None
VRA,,github,,None
audiometry,,github,,None
Audiometry,,github,,None
dfna9,,github,,None
audiometry,,github,,None
audiometry,,github,,None
Audiometry,,github,,None
audiometry,,github,,None
Audiometry,,github,,None
audiometry,,github,,None
Audiometry,,github,,None
AuditoryRuminator,,github,,None
audiometry,,github,,None
audiometry,,github,,None
Web Speech Audiometry Description,,github,,None
audiometryproject,,github,,None
audiometrygraph,,github,,None
dfna9-systematic-review,,github,,None
Serwis-Audiometryczny,,github,,None
Audiometry_GUI,,github,,None
audiometer,,github,,None
Shor_Audiometry,,github,,None
audiometry-slim,,github,,None
audiometry-tester,,github,,None
ToneAudiometry,,github,,None
Opensource-Audiometry,,github,,None
DPOAE-Audiometry,,github,,None
soundlift_audiometry,,github,,None
pi-audiometry,,github,,None
Audiometry-Plugin-Testing-Enviroment,,github,,None
Audiometry-Generator,,github,,None
Audiometry_challenge_Data_Analyst,,github,,None
pure-tone-audiometry,,github,,None
AppAudiometry,,github,,None
son-audiometry-tools,,github,,None
AUDIOMETRY-Object-Detection,,github,,None
MobileAudiometryApp,,github,,None
Low_Cost_Boothless_Audiometry,,github,,None
AppAudiometry_test,,github,,None
MaskingDemonstration,,github,,None
audiovisualdubbedMST,,github,,None
AcousticCalcs,,github,,None
Multiplex_amplitude_modulated_tone_audiometry,,github,,None
Audiometria,,github,,None
LexicalResources,,github,,None
Audiometria,,github,,None
Ain-t-no-sound-loud-enough-audiometry-in-MRI-settings,,github,,None
pure_tone,,github,,None
detection-theory-101,,github,,None
SoundSaved,,github,,None
audiogram-filtbank,,github,,None
VA-Audiology-App,,github,,None
VA-Audiology-Website,,github,,None
"
Audiology",,github,,None
avantageAudiology,,github,,None
ssss,,github,,None
dublinaudiology,,github,,None
2022-DTU-Deep-Learning-Speech-Synthesis,,github,,None
AudiologyTest,,github,,None
AudiologyNLP,,github,,None
audiology,,github,,None
AudiologyHardwareInventory,,github,,None
audiology,,github,,None
AudiologyBooth,,github,,None
Audiology,,github,,None
Audiology,,github,,None
audiology,,github,,None
Audiology-test,,github,,None
Audiology-Plus-Website,,github,,None
audiology,,github,,None
Audiology,,github,,None
Audiology,,github,,None
AudiologyHardwareInventory_Dev,,github,,None
audiology,,github,,None
Audiology,,github,,None
Manuscripts,,github,,None
AudiologyNLP,,github,,None
Audiology,,github,,None
audiology_ml,,github,,None
NCHCS_Audiology,,github,,None
audiology,,github,,None
audiology,,github,,None
Sravana-Audiology,,github,,None
bharti-audiology,,github,,None
ml_audiology,,github,,None
VamAudiology,,github,,None
audiology-remake,,github,,None
Ascending-audiology,,github,,None
audiologydesign.com,,github,,None
scout-audiology,,github,,None
TestAudiology,,github,,None
oo-audiology,,github,,None
AutomationAudiology,,github,,None
audiology.js,,github,,None
audiology-ecommerece,,github,,None
kmy-audiology,,github,,None
Audiology-Data-Mining,,github,,None
sequoia_audiology_tool,,github,,None
Audiology-Version-2,,github,,None
Audiology_assignment_Siemens,,github,,None
aStellarAudiology,,github,,None
Audiology-Taster,,github,,None
StanfordAudiology,,github,,None
Audiology-Plus-Coming-Soon,,github,,None
audiology-theme-7_0,,github,,None
Predicting the necessity for enteral nutrition in acute stroke patients,,github,,None
Learn_Audiology,,github,,None
Gaudio,,github,,None
VA_Audiology_CDs,,github,,None
https-github.com-Mrpankaj666-Audiology,,github,,None
ARDC_Analysis,,github,,None
Global-Audiology-Devices-Market-Trends-Size-Forecast---2019-2025-,,github,,None
Test Bank for Introduction to Audiology Today James W. Hall digital download immediately after payment is complete.,,github,,None
321-MCA.team.project,,github,,None
Open Rem,,github,,None
Audio-Vizzion,,github,,None
Hear-Me-Out-app,,github,,None
PHP-GUI-for-a-firm-that-has-a-large-retail-chain-that-offers-optical-and-audiology-services,,github,,None
SLAC,,github,,None
Golden Hearing Experimental Hearing Test,,github,,None
VCCA workshop 2020,,github,,None
AJA-Labor,,github,,None
HearingTest,,github,,None
https-www.prothomalo.com-,,github,,None