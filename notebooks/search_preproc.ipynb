{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Results Pre-processing\n",
    "This notebook aims to clean up the data from teh search term results across all databases used for the systematic review on audiometric data and UML.\n",
    "\n",
    "You can find the databases, search terms and number of returns under `~/README.md`\n",
    "\n",
    "## Necessary packages\n",
    "Outside of the usual python packages (numpy, pandas, etc.), you will need `xlrd`, `crossrefapi` and `bibtexparser`. Install using:\n",
    "\n",
    "`pip install xlrd`\n",
    "\n",
    "`pip install crossrefapi`\n",
    "\n",
    "`pip install bibtexparser`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import bibtexparser\n",
    "from crossref.restful import Works\n",
    "\n",
    "from IPython.display import display, Markdown, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bib(path_to_bib):\n",
    "    with open(path_to_bib) as bibtex_file:\n",
    "        bib_database = bibtexparser.load(bibtex_file)\n",
    "\n",
    "    # Convert the bibtex entries to a list of dictionaries\n",
    "    bib_list = bib_database.entries\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    bib_df = pd.DataFrame(bib_list)\n",
    "    \n",
    "    return bib_df\n",
    "\n",
    "def load_xls(path_to_xls, n_skip_rows):\n",
    "    xls_df = pd.read_excel(path_to_xls, engine='xlrd', skiprows=n_skip_rows)\n",
    "    return xls_df\n",
    "\n",
    "def standardised_df(df, title, abstract, database_handle, database_id, doi):\n",
    "    # check if database_id is None\n",
    "    if database_id == None:\n",
    "        database_id_series = np.full(df.shape[0], None)\n",
    "    else:\n",
    "        database_id_series = df[database_id]\n",
    "    \n",
    "    # check if abstract is None\n",
    "    if abstract == None:\n",
    "        abstract_series = np.full(df.shape[0], None)\n",
    "    else:\n",
    "        abstract_series = df[abstract]\n",
    "\n",
    "    # check if doi is None\n",
    "    if doi == None:\n",
    "        doi_series = np.full(df.shape[0], None)\n",
    "    else:\n",
    "        doi_series = df[doi]\n",
    "    standardised_df = pd.DataFrame({\n",
    "        'title': df[title],\n",
    "        'abstract': abstract_series,\n",
    "        'database': np.full(df.shape[0], database_handle),\n",
    "        'database_id': database_id_series,\n",
    "        'doi': doi_series\n",
    "    })\n",
    "\n",
    "    return standardised_df\n",
    "\n",
    "# Function to split abstracts\n",
    "# PubMed\n",
    "def split_pubmed_actracts(file_path, separator):\n",
    "    \"\"\"\n",
    "    Splits a text file into strings based on a specific separator.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "        separator (str): Separator to split the file content.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing split strings with indices as keys.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        split_strings = re.split(separator, content)\n",
    "        for idx, s in enumerate(split_strings):\n",
    "            if idx == 0:\n",
    "                clean_s = s\n",
    "            else:\n",
    "                # Remove irrelevant header\n",
    "                clean_s = ''.join(s.split('\\n')[1:])\n",
    "                try:\n",
    "                    # Find the index of the first digit (start of chunk)\n",
    "                    start_index = next(i for i, char in enumerate(clean_s) if char.isdigit())\n",
    "                    clean_s = clean_s[start_index:].strip()\n",
    "                except StopIteration:\n",
    "                    # If no digit found, keep the entire chunk\n",
    "                    pass\n",
    "            \n",
    "            result[idx] = clean_s.strip()\n",
    "    return result\n",
    "\n",
    "# Function to find rough duplicates based on cosine similarity of TF-IDF vectors\n",
    "def find_rough_duplicates(data, column='title', threshold=0.8):\n",
    "    # Vectorize the titles using TF-IDF\n",
    "    vectorizer = TfidfVectorizer().fit_transform(data[column])\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    cos_sim = cosine_similarity(vectorizer)\n",
    "    \n",
    "    # Find indices where similarity is above the threshold, ignoring self-similarity (diagonal)\n",
    "    duplicates = np.triu(np.where((cos_sim > threshold) & (cos_sim < 1), 1, 0), 1)\n",
    "    \n",
    "    # Extract indices of duplicates (only need one index per pair, so we choose the first)\n",
    "    duplicate_indices = np.unique(np.nonzero(duplicates)[0])\n",
    "    \n",
    "    # Drop the identified duplicates\n",
    "    data_deduped = data.drop(index=duplicate_indices)\n",
    "    \n",
    "    return data_deduped, len(duplicate_indices)\n",
    "\n",
    "def identify_rough_duplicate_pairs(data, column='title', threshold=0.8):\n",
    "    # Vectorize the titles using TF-IDF\n",
    "    vectorizer = TfidfVectorizer().fit_transform(data[column])\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    cos_sim = cosine_similarity(vectorizer)\n",
    "    \n",
    "    # Find indices where similarity is above the threshold, ignoring self-similarity (diagonal)\n",
    "    duplicates_matrix = np.triu(np.where((cos_sim > threshold) & (cos_sim < 1), 1, 0), 1)\n",
    "    \n",
    "    # Extract pairs of indices that are considered duplicates\n",
    "    duplicate_indices_pairs = np.column_stack(np.where(duplicates_matrix > 0))\n",
    "    \n",
    "    # Extract the titles of these duplicate pairs along with their similarity score\n",
    "    duplicate_pairs = [(data.iloc[pair[0]][column], data.iloc[pair[1]][column], cos_sim[pair[0], pair[1]]) for pair in duplicate_indices_pairs]\n",
    "    \n",
    "    return duplicate_pairs\n",
    "\n",
    "def jats_to_markdown(text):\n",
    "    if pd.isnull(text):  # Skip conversion if text is NaN\n",
    "        return text\n",
    "    # Convert section tags by removing them, assuming sections are already implied or handled by other means\n",
    "    text = re.sub(r'<\\/?jats:sec>', '', text)  # Remove opening and closing section tags without replacement\n",
    "    # Handle titles within sections (assuming bold in Markdown for titles)\n",
    "    text = re.sub(r'<jats:title>(.*?)</jats:title>', r'\\n**\\1**\\n', text)\n",
    "    # Convert paragraphs\n",
    "    text = re.sub(r'<jats:p>(.*?)</jats:p>', r'\\n\\1\\n', text)\n",
    "    # Convert bold and italic text\n",
    "    text = re.sub(r'<jats:bold>(.*?)</jats:bold>', r'**\\1**', text)\n",
    "    text = re.sub(r'<jats:italic>(.*?)</jats:italic>', r'*\\1*', text)\n",
    "    # Convert links - assuming 'href' is the attribute for URL\n",
    "    text = re.sub(r'<jats:ext-link href=\"(.*?)\">(.*?)</jats:ext-link>', r'[\\2](\\1)', text)\n",
    "    # Handle lists\n",
    "    text = re.sub(r'<jats:list list-type=\"bullet\">(.*?)</jats:list>', r'\\n\\1\\n', text)\n",
    "    text = re.sub(r'<jats:list-item>(.*?)</jats:list-item>', r'- \\1', text)\n",
    "    # General cleanup for any remaining JATS tags (opening and closing)\n",
    "    text = re.sub(r'<\\/?jats\\:.*?>', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove all non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    # Apply stemming\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    text = ' '.join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and joining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data and standardise\n",
    "#Â PUBMED\n",
    "pubmed_df = pd.read_csv('../data/search_returns/medline.csv')\n",
    "pubmed_standard_df = standardised_df(pubmed_df, 'Title', None, 'pubmed', 'PMID', 'DOI')\n",
    "\n",
    "# Scopus\n",
    "scopus_df = pd.read_csv('../data/search_returns/scopus.csv')\n",
    "scopus_standard_df = standardised_df(scopus_df, 'Title', 'Abstract', 'scopus', 'EID', 'DOI')\n",
    "\n",
    "# PsycINFO\n",
    "psychinfo_df = pd.read_csv('../data/search_returns/psychinfo.csv')\n",
    "psychinfo_standard_df = standardised_df(psychinfo_df, 'TI', 'AB', 'psychinfo', None, 'URL')\n",
    "\n",
    "# EMBASE\n",
    "embase_df = pd.read_csv('../data/search_returns/embase.csv')\n",
    "embase_standard_df = standardised_df(embase_df, 'TI', 'AB', 'embase', 'UI', 'FTURL')\n",
    "\n",
    "# CINAHL\n",
    "cinahl_df = pd.read_csv('../data/search_returns/cinahl.csv')\n",
    "cinahl_standard_df = standardised_df(cinahl_df, 'artinfo/tig/atl', 'artinfo/ab/0', 'cinahl', 'jinfo/issn', 'artinfo/ui/0/__text')\n",
    "\n",
    "# IEEE\n",
    "ieee_df = pd.read_csv('../data/search_returns/ieee.csv')\n",
    "ieee_standard_df = standardised_df(ieee_df, 'Document Title', 'Abstract', 'ieee', 'ISSN', 'DOI')\n",
    "\n",
    "# BASE\n",
    "base_df = pd.read_csv('../data/search_returns/base.csv')\n",
    "base_standard_df = standardised_df(base_df, 'title', None, 'base', 'ID', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pubmed\n",
    "file_path = \"../data/search_returns/pubmed_abstracts.txt\"\n",
    "separator = \"PMID\"\n",
    "split_strings_dict = split_pubmed_actracts(file_path, separator)\n",
    "\n",
    "last_id = max(split_strings_dict.keys())\n",
    "del split_strings_dict[last_id]\n",
    "\n",
    "abstract_series = pd.Series(split_strings_dict)\n",
    "abstract_series.reset_index(inplace=True, drop=True)\n",
    "\n",
    "pubmed_standard_df.abstract = abstract_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_standard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join standardised dataframes\n",
    "standard_db_df = pd.concat([\n",
    "    pubmed_standard_df, psychinfo_standard_df, scopus_standard_df, cinahl_standard_df, ieee_standard_df, base_standard_df\n",
    "], axis=0)\n",
    "standard_db_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters from title\n",
    "titles_cleaned = standard_db_df.title.str.replace('[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "standard_db_df.title = titles_cleaned\n",
    "\n",
    "standard_db_df['title'] = standard_db_df['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_db_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of missing abstracts\n",
    "sum(standard_db_df.abstract.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export this data for future use\n",
    "standard_db_df.to_csv('../data/search_returns/all_db.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1972, 5), (1402, 5))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deduplicate on exact matches\n",
    "standard_db_deduped_exact = standard_db_df.drop_duplicates(subset=['title'], keep='first')\n",
    "standard_db_deduped_exact.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "# drop row if title is NaN\n",
    "standard_db_deduped_exact = standard_db_deduped_exact.dropna(subset=['title'])\n",
    "\n",
    "# Display the shape of the original and deduplicated dataframes to see how many rows were removed\n",
    "original_shape = standard_db_df.shape\n",
    "deduped_exact_shape = standard_db_deduped_exact.shape\n",
    "\n",
    "\n",
    "original_shape, deduped_exact_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_db_deduped_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, (1343, 5))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deduplicate on fuzzy matches\n",
    "# Apply function to find and drop rough duplicates\n",
    "data_deduped_rough, num_rough_duplicates = find_rough_duplicates(standard_db_deduped_exact, 'title', 0.814)\n",
    "\n",
    "# Display the number of rough duplicates removed and the new shape of the dataframe\n",
    "num_rough_duplicates, data_deduped_rough.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rough duplicate pairs in the DataFrame before removing them\n",
    "rough_duplicate_pairs = identify_rough_duplicate_pairs(standard_db_deduped_exact, 'title', 0.814)\n",
    "\n",
    "# Convert the list of rough duplicate pairs into a pandas DataFrame for better readability and presentation\n",
    "rough_duplicate_pairs_df = pd.DataFrame(rough_duplicate_pairs, columns=['Title 1', 'Title 2', 'Similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_duplicate_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deduped_rough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add in missing abstract info where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating abstracts: 100%|ââââââââââ| 1343/1343 [00:03<00:00, 391.55it/s] \n"
     ]
    }
   ],
   "source": [
    "works = Works()\n",
    "\n",
    "doi_column = 'doi'\n",
    "abstract_column = 'abstract'\n",
    "rows = list(data_deduped_rough.iterrows())\n",
    "\n",
    "# Use tqdm to wrap around the iterrows loop for a progress bar\n",
    "for index, row in tqdm(rows, desc=\"Updating abstracts\"):\n",
    "    # Check if there's no abstract but a DOI is available\n",
    "    if pd.isnull(row[abstract_column]) and pd.notnull(row[doi_column]):\n",
    "        try:\n",
    "            # Attempt to retrieve the work from Crossref using the DOI\n",
    "            work = works.doi(row[doi_column])\n",
    "            # If an abstract is found, update the DataFrame\n",
    "            if work and 'abstract' in work:\n",
    "                data_deduped_rough.at[index, abstract_column] = work['abstract']\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving abstract for DOI {row[doi_column]}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy up new abstracts to markdown\n",
    "data_deduped_rough['abstract'] = data_deduped_rough['abstract'].apply(jats_to_markdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deduped_rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of missing abstracts\n",
    "sum(data_deduped_rough.abstract.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank the data by similarity to exemplar papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "exemplar_df = pd.read_csv('../data/search_returns/exemplar_papers.csv')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Combine titles and abstracts, with handling for missing abstracts\n",
    "combined_texts = data_deduped_rough['title'] + \" \" + data_deduped_rough['abstract'].fillna('no abstract')\n",
    "exemplar_combined_texts = exemplar_df['title'] + \" \" + exemplar_df['abstract'].fillna('no abstract')\n",
    "\n",
    "# Apply preprocessing\n",
    "combined_texts = combined_texts.apply(preprocess_text)\n",
    "exemplar_combined_texts = exemplar_combined_texts.apply(preprocess_text)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "all_texts = pd.concat([combined_texts, exemplar_combined_texts])\n",
    "vectorized_texts = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "num_dataset_docs = len(data_deduped_rough)\n",
    "num_exemplar_docs = len(exemplar_df)\n",
    "cosine_similarities = cosine_similarity(vectorized_texts[:num_dataset_docs], vectorized_texts[num_dataset_docs:])\n",
    "\n",
    "average_similarities = np.mean(cosine_similarities, axis=1)\n",
    "ranked_indices = np.argsort(-average_similarities)  # Negate for descending order\n",
    "ranked_similarities = average_similarities[ranked_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deduped_rough['similarity_score'] = average_similarities\n",
    "\n",
    "# Step 2: Sort the DataFrame by similarity scores in descending order\n",
    "data_deduped_rough_sorted = data_deduped_rough.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "# Optional: Add a ranking column based on the sorted order\n",
    "data_deduped_rough_sorted['rank'] = range(1, len(data_deduped_rough_sorted) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deduped_rough_sorted.iloc[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deduped_rough_sorted.iloc[-10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deduped_rough_sorted.reset_index(inplace=True, drop=True)\n",
    "data_deduped_rough_sorted.to_csv('../data/tiab/all_results_deduplicated_ordered.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
